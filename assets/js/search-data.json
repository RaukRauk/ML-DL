{
  
    
        "post0": {
            "title": "[BOAZ] Deeplab V3 Predictions",
            "content": "from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . !cp -r /content/drive/MyDrive/archive /content/archive . !cp -r /content/drive/MyDrive/archive2 /content/archive2 . !cp -r /content/drive/MyDrive/weights.pt /content . import torch import numpy as np import cv2 model = torch.load(&quot;/content/weights.pt&quot;) model.eval() . DeepLabV3( (backbone): IntermediateLayerGetter( (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (layer1): Sequential( (0): Bottleneck( (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer2): Sequential( (0): Bottleneck( (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer3): Sequential( (0): Bottleneck( (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (3): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (4): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (5): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (6): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (7): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (8): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (9): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (10): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (11): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (12): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (13): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (14): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (15): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (16): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (17): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (18): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (19): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (20): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (21): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (22): Bottleneck( (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) (layer4): Sequential( (0): Bottleneck( (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (downsample): Sequential( (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) (2): Bottleneck( (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False) (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) ) ) ) (classifier): DeepLabHead( (0): ASPP( (convs): ModuleList( (0): Sequential( (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (1): ASPPConv( (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (2): ASPPConv( (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (3): ASPPConv( (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ) (4): ASPPPooling( (0): AdaptiveAvgPool2d(output_size=1) (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU() ) ) (project): Sequential( (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Dropout(p=0.5, inplace=False) ) ) (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU() (4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1)) ) (aux_classifier): FCNHead( (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Dropout(p=0.1, inplace=False) (4): Conv2d(256, 21, kernel_size=(1, 1), stride=(1, 1)) ) ) . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/archive/images/pexels-極逵勻筠剋-_棘_棘克龜戟-2324837.jpg&#39; img1 = cv2.imread(filename) img2 = cv2.imread(&#39;/content/archive/masks/pexels-極逵勻筠剋-_棘_棘克龜戟-2324837.png&#39;) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) plt.figure(figsize=(15, 15)) ax1 = plt.subplot(1, 3, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) ax2 = plt.subplot(1, 3, 2) ax2.imshow(img2_rgb) plt.xlabel(&quot;LABEL&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 256 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 3, 3) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1920x1280 at 0x7FA5523422D0&gt; Using GPU! [[ 5.45466 5.45466 5.45466 ... -1.9461594 -1.9461594 -1.9461594] [ 5.45466 5.45466 5.45466 ... -1.9461594 -1.9461594 -1.9461594] [ 5.45466 5.45466 5.45466 ... -1.9461594 -1.9461594 -1.9461594] ... [ 2.2296925 2.2296925 2.2296925 ... 1.4804513 1.4804513 1.4804513] [ 2.2296925 2.2296925 2.2296925 ... 1.4804513 1.4804513 1.4804513] [ 2.2296925 2.2296925 2.2296925 ... 1.4804513 1.4804513 1.4804513]] 42617024.0 . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/archive/images/pexels-charlotte-may-5965620.jpg&#39; img1 = cv2.imread(filename) img2 = cv2.imread(&#39;/content/archive/masks/pexels-charlotte-may-5965620.png&#39;) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) plt.figure(figsize=(15, 15)) ax1 = plt.subplot(1, 3, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) ax2 = plt.subplot(1, 3, 2) ax2.imshow(img2_rgb) plt.xlabel(&quot;LABEL&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 3, 3) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1920x1280 at 0x7FA5503DC110&gt; Using GPU! [[3.091479 3.091479 3.091479 ... 0.7202423 0.7202423 0.7202423] [3.091479 3.091479 3.091479 ... 0.7202423 0.7202423 0.7202423] [3.091479 3.091479 3.091479 ... 0.7202423 0.7202423 0.7202423] ... [3.7924721 3.7924721 3.7924721 ... 1.4470047 1.4470047 1.4470047] [3.7924721 3.7924721 3.7924721 ... 1.4470047 1.4470047 1.4470047] [3.7924721 3.7924721 3.7924721 ... 1.4470047 1.4470047 1.4470047]] 9521460.0 . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/archive/images/pexels-jessica-ticozzelli-5670180.jpg&#39; img1 = cv2.imread(filename) img2 = cv2.imread(&#39;/content/archive/masks/pexels-jessica-ticozzelli-5670180.png&#39;) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) plt.figure(figsize=(15, 15)) ax1 = plt.subplot(1, 3, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) ax2 = plt.subplot(1, 3, 2) ax2.imshow(img2_rgb) plt.xlabel(&quot;LABEL&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 3, 3) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1920x1280 at 0x7FA53F649190&gt; Using GPU! [[4.573134 4.573134 4.573134 ... 1.3538351 1.3538351 1.3538351] [4.573134 4.573134 4.573134 ... 1.3538351 1.3538351 1.3538351] [4.573134 4.573134 4.573134 ... 1.3538351 1.3538351 1.3538351] ... [2.974099 2.974099 2.974099 ... 3.053765 3.053765 3.053765 ] [2.974099 2.974099 2.974099 ... 3.053765 3.053765 3.053765 ] [2.974099 2.974099 2.974099 ... 3.053765 3.053765 3.053765 ]] 16178913.0 . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/ferry-g58ee77489_1920.jpg&#39; img1 = cv2.imread(filename) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) plt.figure(figsize=(15, 15)) ax1 = plt.subplot(1, 2, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 2, 2) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1920x1191 at 0x7FA5502A8ED0&gt; Using GPU! [[4.7705436 4.7705436 4.7705436 ... 2.14626 2.14626 2.14626 ] [4.7705436 4.7705436 4.7705436 ... 2.14626 2.14626 2.14626 ] [4.7705436 4.7705436 4.7705436 ... 2.14626 2.14626 2.14626 ] ... [3.1820395 3.1820395 3.1820395 ... 3.0601 3.0601 3.0601 ] [3.1820395 3.1820395 3.1820395 ... 3.0601 3.0601 3.0601 ] [3.1820395 3.1820395 3.1820395 ... 3.0601 3.0601 3.0601 ]] 2829507.2 . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/people-wearing-face-mask-for-protection-3957986-scaled.jpg&#39; img1 = cv2.imread(filename) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) plt.figure(figsize=(15, 15)) ax1 = plt.subplot(1, 2, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 2, 2) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2560x1714 at 0x7FA550209790&gt; Using GPU! [[ 5.050884 5.050884 5.050884 ... 1.0496678 1.0496678 1.0496678 ] [ 5.050884 5.050884 5.050884 ... 1.0496678 1.0496678 1.0496678 ] [ 5.050884 5.050884 5.050884 ... 1.0496678 1.0496678 1.0496678 ] ... [ 2.374113 2.374113 2.374113 ... -0.24998383 -0.24998383 -0.24998383] [ 2.374113 2.374113 2.374113 ... -0.24998383 -0.24998383 -0.24998383] [ 2.374113 2.374113 2.374113 ... -0.24998383 -0.24998383 -0.24998383]] 113737416.0 . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/KakaoTalk_20211223_201004782.jpg&#39; img1 = cv2.imread(filename) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) plt.figure(figsize=(20, 20)) ax1 = plt.subplot(1, 2, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 2, 2) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=960x1280 at 0x7FA53F550210&gt; Using GPU! [[5.0268693 5.0268693 5.0268693 ... 2.353081 2.353081 2.353081 ] [5.0268693 5.0268693 5.0268693 ... 2.353081 2.353081 2.353081 ] [5.0268693 5.0268693 5.0268693 ... 2.353081 2.353081 2.353081 ] ... [1.0000278 1.0000278 1.0000278 ... 2.217442 2.217442 2.217442 ] [1.0000278 1.0000278 1.0000278 ... 2.217442 2.217442 2.217442 ] [1.0000278 1.0000278 1.0000278 ... 2.217442 2.217442 2.217442 ]] 1311989.6 . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/캡처22.JPG&#39; img1 = cv2.imread(filename) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) plt.figure(figsize=(25, 25)) ax1 = plt.subplot(1, 2, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 2, 2) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1366x768 at 0x7F9704831310&gt; Using GPU! [[6.118342 6.118342 6.118342 ... 1.5639745 1.5639745 1.5639745] [6.118342 6.118342 6.118342 ... 1.5639745 1.5639745 1.5639745] [6.118342 6.118342 6.118342 ... 1.5639745 1.5639745 1.5639745] ... [3.5671527 3.5671527 3.5671527 ... 3.0905356 3.0905356 3.0905356] [3.5671527 3.5671527 3.5671527 ... 3.0905356 3.0905356 3.0905356] [3.5671527 3.5671527 3.5671527 ... 3.0905356 3.0905356 3.0905356]] 4007741.8 . import cv2 import matplotlib.pyplot as plt filename = &#39;/content/캡처33.JPG&#39; img1 = cv2.imread(filename) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) plt.figure(figsize=(25, 25)) ax1 = plt.subplot(1, 2, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) # Deeplab V3 Prediction # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 2, 2) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1366x768 at 0x7F9704A372D0&gt; Using GPU! [[3.808771 3.808771 3.808771 ... 1.001113 1.001113 1.001113 ] [3.808771 3.808771 3.808771 ... 1.001113 1.001113 1.001113 ] [3.808771 3.808771 3.808771 ... 1.001113 1.001113 1.001113 ] ... [1.78548 1.78548 1.78548 ... 1.3692136 1.3692136 1.3692136] [1.78548 1.78548 1.78548 ... 1.3692136 1.3692136 1.3692136] [1.78548 1.78548 1.78548 ... 1.3692136 1.3692136 1.3692136]] 176825.69 .",
            "url": "https://raukrauk.github.io/ML-DL/boaz/deeplearning/2021/12/24/Deeplab_V3_prediction.html",
            "relUrl": "/boaz/deeplearning/2021/12/24/Deeplab_V3_prediction.html",
            "date": " • Dec 24, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "[BOAZ] Deeplab V3 Learning Process",
            "content": "&#45436;&#47928; &#45936;&#51060;&#53552; &#44396;&#54788; (VOC 2012 Dataset) . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . !git clone https://github.com/chenxi116/DeepLabv3.pytorch.git . Cloning into &#39;DeepLabv3.pytorch&#39;... remote: Enumerating objects: 31, done. remote: Total 31 (delta 0), reused 0 (delta 0), pack-reused 31 Unpacking objects: 100% (31/31), done. . %cd DeepLabv3.pytorch/ . /content/DeepLabv3.pytorch . Get_Pretrained_model . !wget https://cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth -P data/ . --2021-12-11 09:38:02-- https://cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth Resolving cs.jhu.edu (cs.jhu.edu)... 128.220.13.64 Connecting to cs.jhu.edu (cs.jhu.edu)|128.220.13.64|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://www.cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth [following] --2021-12-11 09:38:02-- https://www.cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth Resolving www.cs.jhu.edu (www.cs.jhu.edu)... 128.220.13.64 Connecting to www.cs.jhu.edu (www.cs.jhu.edu)|128.220.13.64|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 464941444 (443M) Saving to: ‘data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth’ deeplab_resnet101_p 100%[===================&gt;] 443.40M 34.0MB/s in 14s 2021-12-11 09:38:17 (32.5 MB/s) - ‘data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth’ saved [464941444/464941444] . %cd data !wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar !tar -xf VOCtrainval_11-May-2012.tar %cd VOCdevkit/VOC2012/ !wget http://cs.jhu.edu/~cxliu/data/SegmentationClassAug.zip !wget http://cs.jhu.edu/~cxliu/data/SegmentationClassAug_Visualization.zip !wget http://cs.jhu.edu/~cxliu/data/list.zip !unzip SegmentationClassAug.zip !unzip SegmentationClassAug_Visualization.zip !unzip list.zip . 스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다. inflating: SegmentationClassAug_Visualization/2008_003323.png inflating: SegmentationClassAug_Visualization/2009_005229.png inflating: SegmentationClassAug_Visualization/2008_004123.png inflating: SegmentationClassAug_Visualization/2010_003333.png inflating: SegmentationClassAug_Visualization/2011_001989.png inflating: SegmentationClassAug_Visualization/2010_001320.png inflating: SegmentationClassAug_Visualization/2010_001343.png inflating: SegmentationClassAug_Visualization/2008_004564.png inflating: SegmentationClassAug_Visualization/2008_002124.png inflating: SegmentationClassAug_Visualization/2009_001323.png inflating: SegmentationClassAug_Visualization/2009_002567.png inflating: SegmentationClassAug_Visualization/2010_005345.png inflating: SegmentationClassAug_Visualization/2008_006052.png inflating: SegmentationClassAug_Visualization/2008_002791.png inflating: SegmentationClassAug_Visualization/2008_007873.png inflating: SegmentationClassAug_Visualization/2008_001159.png inflating: SegmentationClassAug_Visualization/2010_000141.png inflating: SegmentationClassAug_Visualization/2010_000204.png inflating: SegmentationClassAug_Visualization/2009_002171.png inflating: SegmentationClassAug_Visualization/2010_002865.png inflating: SegmentationClassAug_Visualization/2011_002697.png inflating: SegmentationClassAug_Visualization/2011_001726.png inflating: SegmentationClassAug_Visualization/2008_003733.png inflating: SegmentationClassAug_Visualization/2010_000111.png inflating: SegmentationClassAug_Visualization/2010_005166.png inflating: SegmentationClassAug_Visualization/2010_004412.png inflating: SegmentationClassAug_Visualization/2010_004028.png inflating: SegmentationClassAug_Visualization/2007_001027.png inflating: SegmentationClassAug_Visualization/2010_003435.png inflating: SegmentationClassAug_Visualization/2008_007701.png inflating: SegmentationClassAug_Visualization/2008_000223.png inflating: SegmentationClassAug_Visualization/2009_004890.png inflating: SegmentationClassAug_Visualization/2009_004399.png inflating: SegmentationClassAug_Visualization/2007_002281.png inflating: SegmentationClassAug_Visualization/2008_001039.png inflating: SegmentationClassAug_Visualization/2011_000898.png inflating: SegmentationClassAug_Visualization/2010_002251.png inflating: SegmentationClassAug_Visualization/2009_002320.png inflating: SegmentationClassAug_Visualization/2008_003318.png inflating: SegmentationClassAug_Visualization/2008_003796.png inflating: SegmentationClassAug_Visualization/2011_002649.png inflating: SegmentationClassAug_Visualization/2008_005838.png inflating: SegmentationClassAug_Visualization/2009_000887.png inflating: SegmentationClassAug_Visualization/2010_002232.png inflating: SegmentationClassAug_Visualization/2009_004880.png inflating: SegmentationClassAug_Visualization/2010_005657.png inflating: SegmentationClassAug_Visualization/2008_003381.png inflating: SegmentationClassAug_Visualization/2008_008330.png inflating: SegmentationClassAug_Visualization/2008_003329.png inflating: SegmentationClassAug_Visualization/2007_006171.png inflating: SegmentationClassAug_Visualization/2010_004838.png inflating: SegmentationClassAug_Visualization/2009_002425.png inflating: SegmentationClassAug_Visualization/2009_003063.png inflating: SegmentationClassAug_Visualization/2010_004138.png inflating: SegmentationClassAug_Visualization/2011_001618.png inflating: SegmentationClassAug_Visualization/2009_005089.png inflating: SegmentationClassAug_Visualization/2011_002992.png inflating: SegmentationClassAug_Visualization/2008_001832.png inflating: SegmentationClassAug_Visualization/2009_005247.png inflating: SegmentationClassAug_Visualization/2008_004921.png inflating: SegmentationClassAug_Visualization/2008_004720.png inflating: SegmentationClassAug_Visualization/2008_000471.png inflating: SegmentationClassAug_Visualization/2010_000133.png inflating: SegmentationClassAug_Visualization/2007_001340.png inflating: SegmentationClassAug_Visualization/2008_006961.png inflating: SegmentationClassAug_Visualization/2010_004797.png inflating: SegmentationClassAug_Visualization/2009_001196.png inflating: SegmentationClassAug_Visualization/2010_003630.png inflating: SegmentationClassAug_Visualization/2011_000396.png inflating: SegmentationClassAug_Visualization/2008_001338.png inflating: SegmentationClassAug_Visualization/2008_007382.png inflating: SegmentationClassAug_Visualization/2009_005288.png inflating: SegmentationClassAug_Visualization/2011_001652.png inflating: SegmentationClassAug_Visualization/2011_000025.png inflating: SegmentationClassAug_Visualization/2010_002824.png inflating: SegmentationClassAug_Visualization/2009_002676.png inflating: SegmentationClassAug_Visualization/2011_002852.png inflating: SegmentationClassAug_Visualization/2009_000692.png inflating: SegmentationClassAug_Visualization/2008_003432.png inflating: SegmentationClassAug_Visualization/2008_002859.png inflating: SegmentationClassAug_Visualization/2010_001515.png inflating: SegmentationClassAug_Visualization/2011_002971.png inflating: SegmentationClassAug_Visualization/2010_000537.png inflating: SegmentationClassAug_Visualization/2011_000592.png inflating: SegmentationClassAug_Visualization/2009_001676.png inflating: SegmentationClassAug_Visualization/2008_003819.png inflating: SegmentationClassAug_Visualization/2010_002714.png inflating: SegmentationClassAug_Visualization/2010_004231.png inflating: SegmentationClassAug_Visualization/2010_004499.png inflating: SegmentationClassAug_Visualization/2010_001544.png inflating: SegmentationClassAug_Visualization/2010_001715.png inflating: SegmentationClassAug_Visualization/2008_002880.png inflating: SegmentationClassAug_Visualization/2008_006400.png inflating: SegmentationClassAug_Visualization/2010_002903.png inflating: SegmentationClassAug_Visualization/2010_001448.png inflating: SegmentationClassAug_Visualization/2008_003969.png inflating: SegmentationClassAug_Visualization/2010_005391.png inflating: SegmentationClassAug_Visualization/2011_001215.png inflating: SegmentationClassAug_Visualization/2008_003277.png inflating: SegmentationClassAug_Visualization/2011_003177.png inflating: SegmentationClassAug_Visualization/2008_003613.png inflating: SegmentationClassAug_Visualization/2009_004901.png inflating: SegmentationClassAug_Visualization/2010_003156.png inflating: SegmentationClassAug_Visualization/2009_005103.png inflating: SegmentationClassAug_Visualization/2010_001576.png inflating: SegmentationClassAug_Visualization/2008_001816.png inflating: SegmentationClassAug_Visualization/2008_001272.png inflating: SegmentationClassAug_Visualization/2010_001856.png inflating: SegmentationClassAug_Visualization/2008_005798.png inflating: SegmentationClassAug_Visualization/2008_003762.png inflating: SegmentationClassAug_Visualization/2010_005927.png inflating: SegmentationClassAug_Visualization/2011_000214.png inflating: SegmentationClassAug_Visualization/2008_000275.png inflating: SegmentationClassAug_Visualization/2009_004934.png inflating: SegmentationClassAug_Visualization/2008_008229.png inflating: SegmentationClassAug_Visualization/2009_001684.png inflating: SegmentationClassAug_Visualization/2008_003653.png inflating: SegmentationClassAug_Visualization/2009_004171.png inflating: SegmentationClassAug_Visualization/2008_007245.png inflating: SegmentationClassAug_Visualization/2008_008652.png inflating: SegmentationClassAug_Visualization/2009_000927.png inflating: SegmentationClassAug_Visualization/2009_002734.png inflating: SegmentationClassAug_Visualization/2008_001808.png inflating: SegmentationClassAug_Visualization/2011_001402.png inflating: SegmentationClassAug_Visualization/2011_001138.png inflating: SegmentationClassAug_Visualization/2009_002689.png inflating: SegmentationClassAug_Visualization/2007_001175.png inflating: SegmentationClassAug_Visualization/2011_003152.png inflating: SegmentationClassAug_Visualization/2009_002851.png inflating: SegmentationClassAug_Visualization/2008_000690.png inflating: SegmentationClassAug_Visualization/2010_001685.png inflating: SegmentationClassAug_Visualization/2010_000088.png inflating: SegmentationClassAug_Visualization/2010_004327.png inflating: SegmentationClassAug_Visualization/2011_001240.png inflating: SegmentationClassAug_Visualization/2008_005090.png inflating: SegmentationClassAug_Visualization/2010_004560.png inflating: SegmentationClassAug_Visualization/2008_008637.png inflating: SegmentationClassAug_Visualization/2009_002672.png inflating: SegmentationClassAug_Visualization/2010_001105.png inflating: SegmentationClassAug_Visualization/2009_000016.png inflating: SegmentationClassAug_Visualization/2010_002400.png inflating: SegmentationClassAug_Visualization/2009_001192.png inflating: SegmentationClassAug_Visualization/2008_001971.png inflating: SegmentationClassAug_Visualization/2011_001611.png inflating: SegmentationClassAug_Visualization/2008_004148.png inflating: SegmentationClassAug_Visualization/2010_003792.png inflating: SegmentationClassAug_Visualization/2010_002536.png inflating: SegmentationClassAug_Visualization/2007_002198.png inflating: SegmentationClassAug_Visualization/2008_002013.png inflating: SegmentationClassAug_Visualization/2011_002414.png inflating: SegmentationClassAug_Visualization/2008_003794.png inflating: SegmentationClassAug_Visualization/2008_001314.png inflating: SegmentationClassAug_Visualization/2010_002794.png inflating: SegmentationClassAug_Visualization/2007_004483.png inflating: SegmentationClassAug_Visualization/2008_005294.png inflating: SegmentationClassAug_Visualization/2008_004854.png inflating: SegmentationClassAug_Visualization/2008_008218.png inflating: SegmentationClassAug_Visualization/2009_001348.png inflating: SegmentationClassAug_Visualization/2008_008364.png inflating: SegmentationClassAug_Visualization/2010_005369.png inflating: SegmentationClassAug_Visualization/2008_001764.png inflating: SegmentationClassAug_Visualization/2008_002766.png inflating: SegmentationClassAug_Visualization/2009_003664.png inflating: SegmentationClassAug_Visualization/2008_006511.png inflating: SegmentationClassAug_Visualization/2008_001566.png inflating: SegmentationClassAug_Visualization/2009_000013.png inflating: SegmentationClassAug_Visualization/2009_000854.png inflating: SegmentationClassAug_Visualization/2010_004030.png inflating: SegmentationClassAug_Visualization/2008_005517.png inflating: SegmentationClassAug_Visualization/2010_000897.png inflating: SegmentationClassAug_Visualization/2008_002209.png inflating: SegmentationClassAug_Visualization/2010_002480.png inflating: SegmentationClassAug_Visualization/2011_002709.png inflating: SegmentationClassAug_Visualization/2007_004033.png inflating: SegmentationClassAug_Visualization/2008_007281.png inflating: SegmentationClassAug_Visualization/2010_006021.png inflating: SegmentationClassAug_Visualization/2007_003190.png inflating: SegmentationClassAug_Visualization/2010_002915.png inflating: SegmentationClassAug_Visualization/2010_000803.png inflating: SegmentationClassAug_Visualization/2008_005245.png inflating: SegmentationClassAug_Visualization/2008_001965.png inflating: SegmentationClassAug_Visualization/2008_001374.png inflating: SegmentationClassAug_Visualization/2010_003003.png inflating: SegmentationClassAug_Visualization/2009_002180.png inflating: SegmentationClassAug_Visualization/2010_000675.png inflating: SegmentationClassAug_Visualization/2011_000043.png inflating: SegmentationClassAug_Visualization/2008_006841.png inflating: SegmentationClassAug_Visualization/2010_002279.png inflating: SegmentationClassAug_Visualization/2010_005588.png inflating: SegmentationClassAug_Visualization/2008_002366.png inflating: SegmentationClassAug_Visualization/2008_006170.png inflating: SegmentationClassAug_Visualization/2009_004561.png inflating: SegmentationClassAug_Visualization/2010_004193.png inflating: SegmentationClassAug_Visualization/2010_004033.png inflating: SegmentationClassAug_Visualization/2007_004712.png inflating: SegmentationClassAug_Visualization/2007_009392.png inflating: SegmentationClassAug_Visualization/2009_001413.png inflating: SegmentationClassAug_Visualization/2009_003333.png inflating: SegmentationClassAug_Visualization/2010_001795.png inflating: SegmentationClassAug_Visualization/2008_007688.png inflating: SegmentationClassAug_Visualization/2009_004129.png inflating: SegmentationClassAug_Visualization/2009_005310.png inflating: SegmentationClassAug_Visualization/2011_001600.png inflating: SegmentationClassAug_Visualization/2010_002047.png inflating: SegmentationClassAug_Visualization/2010_002379.png inflating: SegmentationClassAug_Visualization/2009_004141.png inflating: SegmentationClassAug_Visualization/2011_003091.png inflating: SegmentationClassAug_Visualization/2008_007250.png inflating: SegmentationClassAug_Visualization/2009_003013.png inflating: SegmentationClassAug_Visualization/2009_004806.png inflating: SegmentationClassAug_Visualization/2010_001777.png inflating: SegmentationClassAug_Visualization/2009_005202.png inflating: SegmentationClassAug_Visualization/2009_003411.png inflating: SegmentationClassAug_Visualization/2009_001472.png inflating: SegmentationClassAug_Visualization/2008_005761.png inflating: SegmentationClassAug_Visualization/2010_003878.png inflating: SegmentationClassAug_Visualization/2009_004754.png inflating: SegmentationClassAug_Visualization/2008_006912.png inflating: SegmentationClassAug_Visualization/2008_007969.png inflating: SegmentationClassAug_Visualization/2007_001299.png inflating: SegmentationClassAug_Visualization/2008_002738.png inflating: SegmentationClassAug_Visualization/2010_005346.png inflating: SegmentationClassAug_Visualization/2010_005299.png inflating: SegmentationClassAug_Visualization/2008_006684.png inflating: SegmentationClassAug_Visualization/2010_001025.png inflating: SegmentationClassAug_Visualization/2009_004263.png inflating: SegmentationClassAug_Visualization/2010_000027.png inflating: SegmentationClassAug_Visualization/2008_004635.png inflating: SegmentationClassAug_Visualization/2008_003904.png inflating: SegmentationClassAug_Visualization/2008_002567.png inflating: SegmentationClassAug_Visualization/2008_000187.png inflating: SegmentationClassAug_Visualization/2010_004432.png inflating: SegmentationClassAug_Visualization/2009_000662.png inflating: SegmentationClassAug_Visualization/2008_004696.png inflating: SegmentationClassAug_Visualization/2009_003198.png inflating: SegmentationClassAug_Visualization/2008_002809.png inflating: SegmentationClassAug_Visualization/2009_003466.png inflating: SegmentationClassAug_Visualization/2009_004867.png inflating: SegmentationClassAug_Visualization/2011_001875.png inflating: SegmentationClassAug_Visualization/2010_005740.png inflating: SegmentationClassAug_Visualization/2008_001874.png inflating: SegmentationClassAug_Visualization/2009_003531.png inflating: SegmentationClassAug_Visualization/2007_003587.png inflating: SegmentationClassAug_Visualization/2008_007583.png inflating: SegmentationClassAug_Visualization/2010_001118.png inflating: SegmentationClassAug_Visualization/2008_000340.png inflating: SegmentationClassAug_Visualization/2011_002042.png inflating: SegmentationClassAug_Visualization/2008_008084.png inflating: SegmentationClassAug_Visualization/2008_006102.png inflating: SegmentationClassAug_Visualization/2008_001467.png inflating: SegmentationClassAug_Visualization/2008_008739.png inflating: SegmentationClassAug_Visualization/2009_003640.png inflating: SegmentationClassAug_Visualization/2009_002098.png inflating: SegmentationClassAug_Visualization/2011_002640.png inflating: SegmentationClassAug_Visualization/2011_002281.png inflating: SegmentationClassAug_Visualization/2011_001029.png inflating: SegmentationClassAug_Visualization/2008_008725.png inflating: SegmentationClassAug_Visualization/2009_002522.png inflating: SegmentationClassAug_Visualization/2011_002476.png inflating: SegmentationClassAug_Visualization/2010_001172.png inflating: SegmentationClassAug_Visualization/2011_000600.png inflating: SegmentationClassAug_Visualization/2010_001966.png inflating: SegmentationClassAug_Visualization/2009_000939.png inflating: SegmentationClassAug_Visualization/2011_002772.png inflating: SegmentationClassAug_Visualization/2008_008296.png inflating: SegmentationClassAug_Visualization/2010_002808.png inflating: SegmentationClassAug_Visualization/2009_001021.png inflating: SegmentationClassAug_Visualization/2010_004014.png inflating: SegmentationClassAug_Visualization/2009_000009.png inflating: SegmentationClassAug_Visualization/2009_001941.png inflating: SegmentationClassAug_Visualization/2008_008600.png inflating: SegmentationClassAug_Visualization/2011_003216.png inflating: SegmentationClassAug_Visualization/2009_003446.png inflating: SegmentationClassAug_Visualization/2010_000067.png inflating: SegmentationClassAug_Visualization/2011_002944.png inflating: SegmentationClassAug_Visualization/2009_003565.png inflating: SegmentationClassAug_Visualization/2010_005000.png inflating: SegmentationClassAug_Visualization/2009_005181.png inflating: SegmentationClassAug_Visualization/2011_002074.png inflating: SegmentationClassAug_Visualization/2007_002119.png inflating: SegmentationClassAug_Visualization/2010_005467.png inflating: SegmentationClassAug_Visualization/2011_001158.png inflating: SegmentationClassAug_Visualization/2011_000338.png inflating: SegmentationClassAug_Visualization/2011_001270.png inflating: SegmentationClassAug_Visualization/2010_004461.png inflating: SegmentationClassAug_Visualization/2008_001706.png inflating: SegmentationClassAug_Visualization/2008_004374.png inflating: SegmentationClassAug_Visualization/2010_000641.png inflating: SegmentationClassAug_Visualization/2011_003260.png inflating: SegmentationClassAug_Visualization/2009_002087.png inflating: SegmentationClassAug_Visualization/2008_000972.png inflating: SegmentationClassAug_Visualization/2011_000479.png inflating: SegmentationClassAug_Visualization/2008_006417.png inflating: SegmentationClassAug_Visualization/2009_004455.png inflating: SegmentationClassAug_Visualization/2011_001505.png inflating: SegmentationClassAug_Visualization/2009_004258.png inflating: SegmentationClassAug_Visualization/2009_003115.png inflating: SegmentationClassAug_Visualization/2008_008701.png inflating: SegmentationClassAug_Visualization/2008_004568.png inflating: SegmentationClassAug_Visualization/2009_003973.png inflating: SegmentationClassAug_Visualization/2010_004466.png inflating: SegmentationClassAug_Visualization/2008_005679.png inflating: SegmentationClassAug_Visualization/2008_003772.png inflating: SegmentationClassAug_Visualization/2008_001539.png inflating: SegmentationClassAug_Visualization/2008_001553.png inflating: SegmentationClassAug_Visualization/2009_002820.png inflating: SegmentationClassAug_Visualization/2010_004335.png inflating: SegmentationClassAug_Visualization/2008_003493.png inflating: SegmentationClassAug_Visualization/2010_004333.png inflating: SegmentationClassAug_Visualization/2010_003562.png inflating: SegmentationClassAug_Visualization/2008_007850.png inflating: SegmentationClassAug_Visualization/2010_004826.png inflating: SegmentationClassAug_Visualization/2008_005097.png inflating: SegmentationClassAug_Visualization/2008_004574.png inflating: SegmentationClassAug_Visualization/2008_008466.png inflating: SegmentationClassAug_Visualization/2010_001993.png inflating: SegmentationClassAug_Visualization/2008_005871.png inflating: SegmentationClassAug_Visualization/2007_004722.png inflating: SegmentationClassAug_Visualization/2009_004828.png inflating: SegmentationClassAug_Visualization/2009_003301.png inflating: SegmentationClassAug_Visualization/2007_004948.png inflating: SegmentationClassAug_Visualization/2009_004403.png inflating: SegmentationClassAug_Visualization/2009_000367.png inflating: SegmentationClassAug_Visualization/2007_003565.png inflating: SegmentationClassAug_Visualization/2010_001468.png inflating: SegmentationClassAug_Visualization/2008_005975.png inflating: SegmentationClassAug_Visualization/2008_000266.png inflating: SegmentationClassAug_Visualization/2010_003027.png inflating: SegmentationClassAug_Visualization/2008_001894.png inflating: SegmentationClassAug_Visualization/2008_007665.png inflating: SegmentationClassAug_Visualization/2010_003597.png inflating: SegmentationClassAug_Visualization/2010_001212.png inflating: SegmentationClassAug_Visualization/2010_005457.png inflating: SegmentationClassAug_Visualization/2010_001806.png inflating: SegmentationClassAug_Visualization/2010_001080.png inflating: SegmentationClassAug_Visualization/2008_001388.png inflating: SegmentationClassAug_Visualization/2008_001060.png inflating: SegmentationClassAug_Visualization/2011_001011.png inflating: SegmentationClassAug_Visualization/2007_009750.png inflating: SegmentationClassAug_Visualization/2009_003173.png inflating: SegmentationClassAug_Visualization/2011_002658.png inflating: SegmentationClassAug_Visualization/2010_003114.png inflating: SegmentationClassAug_Visualization/2007_005845.png inflating: SegmentationClassAug_Visualization/2007_008222.png inflating: SegmentationClassAug_Visualization/2008_008512.png inflating: SegmentationClassAug_Visualization/2011_000418.png inflating: SegmentationClassAug_Visualization/2008_005727.png inflating: SegmentationClassAug_Visualization/2010_003567.png inflating: SegmentationClassAug_Visualization/2008_006194.png inflating: SegmentationClassAug_Visualization/2010_002516.png inflating: SegmentationClassAug_Visualization/2011_001105.png inflating: SegmentationClassAug_Visualization/2011_000435.png inflating: SegmentationClassAug_Visualization/2008_001294.png inflating: SegmentationClassAug_Visualization/2011_000575.png inflating: SegmentationClassAug_Visualization/2009_002684.png inflating: SegmentationClassAug_Visualization/2011_003027.png inflating: SegmentationClassAug_Visualization/2007_000783.png inflating: SegmentationClassAug_Visualization/2008_002061.png inflating: SegmentationClassAug_Visualization/2008_001797.png inflating: SegmentationClassAug_Visualization/2010_000899.png inflating: SegmentationClassAug_Visualization/2011_002406.png inflating: SegmentationClassAug_Visualization/2008_003841.png inflating: SegmentationClassAug_Visualization/2007_000042.png inflating: SegmentationClassAug_Visualization/2008_003338.png inflating: SegmentationClassAug_Visualization/2009_004822.png inflating: SegmentationClassAug_Visualization/2009_000464.png inflating: SegmentationClassAug_Visualization/2009_004781.png inflating: SegmentationClassAug_Visualization/2010_001199.png inflating: SegmentationClassAug_Visualization/2009_004347.png inflating: SegmentationClassAug_Visualization/2010_003321.png inflating: SegmentationClassAug_Visualization/2008_004319.png inflating: SegmentationClassAug_Visualization/2010_002947.png inflating: SegmentationClassAug_Visualization/2010_002191.png inflating: SegmentationClassAug_Visualization/2008_004955.png inflating: SegmentationClassAug_Visualization/2010_005949.png inflating: SegmentationClassAug_Visualization/2007_005354.png inflating: SegmentationClassAug_Visualization/2010_005928.png inflating: SegmentationClassAug_Visualization/2009_000797.png inflating: SegmentationClassAug_Visualization/2011_003023.png inflating: SegmentationClassAug_Visualization/2010_000898.png inflating: SegmentationClassAug_Visualization/2010_005415.png inflating: SegmentationClassAug_Visualization/2009_003955.png inflating: SegmentationClassAug_Visualization/2008_008629.png inflating: SegmentationClassAug_Visualization/2010_001486.png inflating: SegmentationClassAug_Visualization/2008_003213.png inflating: SegmentationClassAug_Visualization/2010_004573.png inflating: SegmentationClassAug_Visualization/2010_005652.png inflating: SegmentationClassAug_Visualization/2009_003031.png inflating: SegmentationClassAug_Visualization/2010_006058.png inflating: SegmentationClassAug_Visualization/2011_000983.png inflating: SegmentationClassAug_Visualization/2011_003244.png inflating: SegmentationClassAug_Visualization/2011_003066.png inflating: SegmentationClassAug_Visualization/2010_000954.png inflating: SegmentationClassAug_Visualization/2010_004154.png inflating: SegmentationClassAug_Visualization/2010_004680.png inflating: SegmentationClassAug_Visualization/2008_003068.png inflating: SegmentationClassAug_Visualization/2008_000764.png inflating: SegmentationClassAug_Visualization/2008_004066.png inflating: SegmentationClassAug_Visualization/2008_002215.png inflating: SegmentationClassAug_Visualization/2009_003633.png inflating: SegmentationClassAug_Visualization/2008_000270.png inflating: SegmentationClassAug_Visualization/2007_000876.png inflating: SegmentationClassAug_Visualization/2009_004756.png inflating: SegmentationClassAug_Visualization/2009_001285.png inflating: SegmentationClassAug_Visualization/2008_004140.png inflating: SegmentationClassAug_Visualization/2009_001437.png inflating: SegmentationClassAug_Visualization/2009_003052.png inflating: SegmentationClassAug_Visualization/2009_002265.png inflating: SegmentationClassAug_Visualization/2010_005338.png inflating: SegmentationClassAug_Visualization/2008_007026.png inflating: SegmentationClassAug_Visualization/2010_004066.png inflating: SegmentationClassAug_Visualization/2010_000195.png inflating: SegmentationClassAug_Visualization/2008_002292.png inflating: SegmentationClassAug_Visualization/2009_001827.png inflating: SegmentationClassAug_Visualization/2009_003629.png inflating: SegmentationClassAug_Visualization/2008_001169.png inflating: SegmentationClassAug_Visualization/2010_005506.png inflating: SegmentationClassAug_Visualization/2009_004942.png inflating: SegmentationClassAug_Visualization/2008_004239.png inflating: SegmentationClassAug_Visualization/2011_000474.png inflating: SegmentationClassAug_Visualization/2010_004249.png inflating: SegmentationClassAug_Visualization/2008_008310.png inflating: SegmentationClassAug_Visualization/2008_005742.png inflating: SegmentationClassAug_Visualization/2008_006909.png inflating: SegmentationClassAug_Visualization/2009_004872.png inflating: SegmentationClassAug_Visualization/2009_002695.png inflating: SegmentationClassAug_Visualization/2008_007514.png inflating: SegmentationClassAug_Visualization/2009_005068.png inflating: SegmentationClassAug_Visualization/2009_003165.png inflating: SegmentationClassAug_Visualization/2010_003160.png inflating: SegmentationClassAug_Visualization/2010_004063.png inflating: SegmentationClassAug_Visualization/2008_006186.png inflating: SegmentationClassAug_Visualization/2009_001664.png inflating: SegmentationClassAug_Visualization/2008_001699.png inflating: SegmentationClassAug_Visualization/2009_000242.png inflating: SegmentationClassAug_Visualization/2010_004598.png inflating: SegmentationClassAug_Visualization/2010_000547.png inflating: SegmentationClassAug_Visualization/2010_005308.png inflating: SegmentationClassAug_Visualization/2008_002555.png inflating: SegmentationClassAug_Visualization/2009_002169.png inflating: SegmentationClassAug_Visualization/2010_000541.png inflating: SegmentationClassAug_Visualization/2010_000573.png inflating: SegmentationClassAug_Visualization/2008_006857.png inflating: SegmentationClassAug_Visualization/2008_003034.png inflating: SegmentationClassAug_Visualization/2007_001430.png inflating: SegmentationClassAug_Visualization/2008_003748.png inflating: SegmentationClassAug_Visualization/2008_004101.png inflating: SegmentationClassAug_Visualization/2010_005896.png inflating: SegmentationClassAug_Visualization/2008_004147.png inflating: SegmentationClassAug_Visualization/2008_005980.png inflating: SegmentationClassAug_Visualization/2010_005997.png inflating: SegmentationClassAug_Visualization/2008_003940.png inflating: SegmentationClassAug_Visualization/2009_000895.png inflating: SegmentationClassAug_Visualization/2011_001346.png inflating: SegmentationClassAug_Visualization/2009_000865.png inflating: SegmentationClassAug_Visualization/2010_005612.png inflating: SegmentationClassAug_Visualization/2010_005845.png inflating: SegmentationClassAug_Visualization/2008_001482.png inflating: SegmentationClassAug_Visualization/2008_006808.png inflating: SegmentationClassAug_Visualization/2008_002948.png inflating: SegmentationClassAug_Visualization/2010_002044.png inflating: SegmentationClassAug_Visualization/2008_002794.png inflating: SegmentationClassAug_Visualization/2009_001036.png inflating: SegmentationClassAug_Visualization/2007_004902.png inflating: SegmentationClassAug_Visualization/2008_007853.png inflating: SegmentationClassAug_Visualization/2009_001327.png inflating: SegmentationClassAug_Visualization/2010_005115.png inflating: SegmentationClassAug_Visualization/2009_003386.png inflating: SegmentationClassAug_Visualization/2008_006253.png inflating: SegmentationClassAug_Visualization/2010_003845.png inflating: SegmentationClassAug_Visualization/2009_002977.png inflating: SegmentationClassAug_Visualization/2011_000239.png inflating: SegmentationClassAug_Visualization/2009_002470.png inflating: SegmentationClassAug_Visualization/2010_000085.png inflating: SegmentationClassAug_Visualization/2008_007825.png inflating: SegmentationClassAug_Visualization/2008_002913.png inflating: SegmentationClassAug_Visualization/2008_007735.png inflating: SegmentationClassAug_Visualization/2009_004122.png inflating: SegmentationClassAug_Visualization/2008_007350.png inflating: SegmentationClassAug_Visualization/2008_003914.png inflating: SegmentationClassAug_Visualization/2008_000857.png inflating: SegmentationClassAug_Visualization/2010_002221.png inflating: SegmentationClassAug_Visualization/2009_000100.png inflating: SegmentationClassAug_Visualization/2009_004524.png inflating: SegmentationClassAug_Visualization/2008_007933.png inflating: SegmentationClassAug_Visualization/2011_000594.png inflating: SegmentationClassAug_Visualization/2007_004468.png inflating: SegmentationClassAug_Visualization/2008_006063.png inflating: SegmentationClassAug_Visualization/2010_003667.png inflating: SegmentationClassAug_Visualization/2011_001056.png inflating: SegmentationClassAug_Visualization/2008_007190.png inflating: SegmentationClassAug_Visualization/2009_002824.png inflating: SegmentationClassAug_Visualization/2008_001593.png inflating: SegmentationClassAug_Visualization/2009_001333.png inflating: SegmentationClassAug_Visualization/2009_001369.png inflating: SegmentationClassAug_Visualization/2010_005663.png inflating: SegmentationClassAug_Visualization/2009_003874.png inflating: SegmentationClassAug_Visualization/2008_001021.png inflating: SegmentationClassAug_Visualization/2010_002130.png inflating: SegmentationClassAug_Visualization/2010_001650.png inflating: SegmentationClassAug_Visualization/2011_001489.png inflating: SegmentationClassAug_Visualization/2008_007123.png inflating: SegmentationClassAug_Visualization/2009_000037.png inflating: SegmentationClassAug_Visualization/2008_001717.png inflating: SegmentationClassAug_Visualization/2008_003826.png inflating: SegmentationClassAug_Visualization/2008_005378.png inflating: SegmentationClassAug_Visualization/2010_000548.png inflating: SegmentationClassAug_Visualization/2007_004003.png inflating: SegmentationClassAug_Visualization/2010_005980.png inflating: SegmentationClassAug_Visualization/2008_006733.png inflating: SegmentationClassAug_Visualization/2008_008268.png inflating: SegmentationClassAug_Visualization/2008_006890.png inflating: SegmentationClassAug_Visualization/2009_004737.png inflating: SegmentationClassAug_Visualization/2010_003574.png inflating: SegmentationClassAug_Visualization/2010_000661.png inflating: SegmentationClassAug_Visualization/2010_004778.png inflating: SegmentationClassAug_Visualization/2008_004519.png inflating: SegmentationClassAug_Visualization/2008_004453.png inflating: SegmentationClassAug_Visualization/2008_000923.png inflating: SegmentationClassAug_Visualization/2010_001386.png inflating: SegmentationClassAug_Visualization/2010_004067.png inflating: SegmentationClassAug_Visualization/2008_006933.png inflating: SegmentationClassAug_Visualization/2008_000432.png inflating: SegmentationClassAug_Visualization/2009_001019.png inflating: SegmentationClassAug_Visualization/2008_003430.png inflating: SegmentationClassAug_Visualization/2011_002863.png inflating: SegmentationClassAug_Visualization/2009_002732.png inflating: SegmentationClassAug_Visualization/2008_004279.png inflating: SegmentationClassAug_Visualization/2010_004815.png inflating: SegmentationClassAug_Visualization/2008_002436.png inflating: SegmentationClassAug_Visualization/2008_002153.png inflating: SegmentationClassAug_Visualization/2008_003645.png inflating: SegmentationClassAug_Visualization/2010_001499.png inflating: SegmentationClassAug_Visualization/2010_002102.png inflating: SegmentationClassAug_Visualization/2011_002049.png inflating: SegmentationClassAug_Visualization/2009_001449.png inflating: SegmentationClassAug_Visualization/2008_008342.png inflating: SegmentationClassAug_Visualization/2009_002743.png inflating: SegmentationClassAug_Visualization/2009_000469.png inflating: SegmentationClassAug_Visualization/2010_002269.png inflating: SegmentationClassAug_Visualization/2008_008641.png inflating: SegmentationClassAug_Visualization/2009_004241.png inflating: SegmentationClassAug_Visualization/2009_003958.png inflating: SegmentationClassAug_Visualization/2008_003334.png inflating: SegmentationClassAug_Visualization/2010_005746.png inflating: SegmentationClassAug_Visualization/2010_000118.png inflating: SegmentationClassAug_Visualization/2011_000637.png inflating: SegmentationClassAug_Visualization/2008_008607.png inflating: SegmentationClassAug_Visualization/2010_004280.png inflating: SegmentationClassAug_Visualization/2010_003588.png inflating: SegmentationClassAug_Visualization/2008_005957.png inflating: SegmentationClassAug_Visualization/2008_001302.png inflating: SegmentationClassAug_Visualization/2007_005460.png inflating: SegmentationClassAug_Visualization/2009_003187.png inflating: SegmentationClassAug_Visualization/2009_005152.png inflating: SegmentationClassAug_Visualization/2010_002621.png inflating: SegmentationClassAug_Visualization/2010_001534.png inflating: SegmentationClassAug_Visualization/2009_002433.png inflating: SegmentationClassAug_Visualization/2011_002366.png inflating: SegmentationClassAug_Visualization/2009_004432.png inflating: SegmentationClassAug_Visualization/2009_003837.png inflating: SegmentationClassAug_Visualization/2009_000523.png inflating: SegmentationClassAug_Visualization/2009_000035.png inflating: SegmentationClassAug_Visualization/2010_003919.png inflating: SegmentationClassAug_Visualization/2008_001670.png inflating: SegmentationClassAug_Visualization/2010_000117.png inflating: SegmentationClassAug_Visualization/2009_000901.png inflating: SegmentationClassAug_Visualization/2007_003525.png inflating: SegmentationClassAug_Visualization/2010_005230.png inflating: SegmentationClassAug_Visualization/2008_000080.png inflating: SegmentationClassAug_Visualization/2008_008080.png inflating: SegmentationClassAug_Visualization/2009_000726.png inflating: SegmentationClassAug_Visualization/2010_002705.png inflating: SegmentationClassAug_Visualization/2008_003681.png inflating: SegmentationClassAug_Visualization/2007_007447.png inflating: SegmentationClassAug_Visualization/2009_001446.png inflating: SegmentationClassAug_Visualization/2008_008336.png inflating: SegmentationClassAug_Visualization/2008_007805.png inflating: SegmentationClassAug_Visualization/2008_008106.png inflating: SegmentationClassAug_Visualization/2008_007461.png inflating: SegmentationClassAug_Visualization/2009_005172.png inflating: SegmentationClassAug_Visualization/2010_001441.png inflating: SegmentationClassAug_Visualization/2008_007786.png inflating: SegmentationClassAug_Visualization/2011_000017.png inflating: SegmentationClassAug_Visualization/2010_000018.png inflating: SegmentationClassAug_Visualization/2009_002235.png inflating: SegmentationClassAug_Visualization/2009_001800.png inflating: SegmentationClassAug_Visualization/2009_004434.png inflating: SegmentationClassAug_Visualization/2009_003914.png inflating: SegmentationClassAug_Visualization/2008_004408.png inflating: SegmentationClassAug_Visualization/2008_004805.png inflating: SegmentationClassAug_Visualization/2010_000831.png inflating: SegmentationClassAug_Visualization/2010_005755.png inflating: SegmentationClassAug_Visualization/2008_006282.png inflating: SegmentationClassAug_Visualization/2010_000552.png inflating: SegmentationClassAug_Visualization/2010_004567.png inflating: SegmentationClassAug_Visualization/2010_003050.png inflating: SegmentationClassAug_Visualization/2011_002921.png inflating: SegmentationClassAug_Visualization/2010_000490.png inflating: SegmentationClassAug_Visualization/2011_002350.png inflating: SegmentationClassAug_Visualization/2011_000511.png inflating: SegmentationClassAug_Visualization/2010_002991.png inflating: SegmentationClassAug_Visualization/2010_004311.png inflating: SegmentationClassAug_Visualization/2010_004026.png inflating: SegmentationClassAug_Visualization/2010_002624.png inflating: SegmentationClassAug_Visualization/2010_004368.png inflating: SegmentationClassAug_Visualization/2008_002549.png inflating: SegmentationClassAug_Visualization/2009_001354.png inflating: SegmentationClassAug_Visualization/2011_000253.png inflating: SegmentationClassAug_Visualization/2008_004172.png inflating: SegmentationClassAug_Visualization/2008_006294.png inflating: SegmentationClassAug_Visualization/2009_004323.png inflating: SegmentationClassAug_Visualization/2010_002956.png inflating: SegmentationClassAug_Visualization/2011_000051.png inflating: SegmentationClassAug_Visualization/2010_001357.png inflating: SegmentationClassAug_Visualization/2010_005561.png inflating: SegmentationClassAug_Visualization/2008_004142.png inflating: SegmentationClassAug_Visualization/2009_001949.png inflating: SegmentationClassAug_Visualization/2009_002105.png inflating: SegmentationClassAug_Visualization/2007_009947.png inflating: SegmentationClassAug_Visualization/2009_004188.png inflating: SegmentationClassAug_Visualization/2008_008453.png inflating: SegmentationClassAug_Visualization/2007_006445.png inflating: SegmentationClassAug_Visualization/2007_001884.png inflating: SegmentationClassAug_Visualization/2010_000791.png inflating: SegmentationClassAug_Visualization/2009_004105.png inflating: SegmentationClassAug_Visualization/2009_004858.png inflating: SegmentationClassAug_Visualization/2008_005893.png inflating: SegmentationClassAug_Visualization/2008_002200.png inflating: SegmentationClassAug_Visualization/2010_005174.png inflating: SegmentationClassAug_Visualization/2008_001356.png inflating: SegmentationClassAug_Visualization/2008_003921.png inflating: SegmentationClassAug_Visualization/2010_004483.png inflating: SegmentationClassAug_Visualization/2009_004424.png inflating: SegmentationClassAug_Visualization/2008_007466.png inflating: SegmentationClassAug_Visualization/2008_003521.png inflating: SegmentationClassAug_Visualization/2008_007649.png inflating: SegmentationClassAug_Visualization/2009_002594.png inflating: SegmentationClassAug_Visualization/2008_002866.png inflating: SegmentationClassAug_Visualization/2011_003148.png inflating: SegmentationClassAug_Visualization/2010_000177.png inflating: SegmentationClassAug_Visualization/2008_004729.png inflating: SegmentationClassAug_Visualization/2009_004713.png inflating: SegmentationClassAug_Visualization/2008_007171.png inflating: SegmentationClassAug_Visualization/2008_007182.png inflating: SegmentationClassAug_Visualization/2010_002603.png inflating: SegmentationClassAug_Visualization/2011_002062.png inflating: SegmentationClassAug_Visualization/2008_002017.png inflating: SegmentationClassAug_Visualization/2008_002144.png inflating: SegmentationClassAug_Visualization/2008_001063.png inflating: SegmentationClassAug_Visualization/2010_003527.png inflating: SegmentationClassAug_Visualization/2009_001706.png inflating: SegmentationClassAug_Visualization/2011_001217.png inflating: SegmentationClassAug_Visualization/2011_001261.png inflating: SegmentationClassAug_Visualization/2009_000028.png inflating: SegmentationClassAug_Visualization/2010_004588.png inflating: SegmentationClassAug_Visualization/2008_005657.png inflating: SegmentationClassAug_Visualization/2009_002146.png inflating: SegmentationClassAug_Visualization/2008_004614.png inflating: SegmentationClassAug_Visualization/2008_003406.png inflating: SegmentationClassAug_Visualization/2008_004917.png inflating: SegmentationClassAug_Visualization/2008_002096.png inflating: SegmentationClassAug_Visualization/2008_004007.png inflating: SegmentationClassAug_Visualization/2008_004284.png inflating: SegmentationClassAug_Visualization/2009_004532.png inflating: SegmentationClassAug_Visualization/2009_001286.png inflating: SegmentationClassAug_Visualization/2010_002195.png inflating: SegmentationClassAug_Visualization/2010_004708.png inflating: SegmentationClassAug_Visualization/2008_008439.png inflating: SegmentationClassAug_Visualization/2009_002487.png inflating: SegmentationClassAug_Visualization/2010_000952.png inflating: SegmentationClassAug_Visualization/2009_004319.png inflating: SegmentationClassAug_Visualization/2010_001143.png inflating: SegmentationClassAug_Visualization/2008_002943.png inflating: SegmentationClassAug_Visualization/2008_003132.png inflating: SegmentationClassAug_Visualization/2009_000343.png inflating: SegmentationClassAug_Visualization/2009_000201.png inflating: SegmentationClassAug_Visualization/2011_001412.png inflating: SegmentationClassAug_Visualization/2011_001326.png inflating: SegmentationClassAug_Visualization/2008_000878.png inflating: SegmentationClassAug_Visualization/2009_004454.png inflating: SegmentationClassAug_Visualization/2008_004048.png inflating: SegmentationClassAug_Visualization/2007_009687.png inflating: SegmentationClassAug_Visualization/2010_001689.png inflating: SegmentationClassAug_Visualization/2008_005519.png inflating: SegmentationClassAug_Visualization/2008_007985.png inflating: SegmentationClassAug_Visualization/2008_006234.png inflating: SegmentationClassAug_Visualization/2010_000036.png inflating: SegmentationClassAug_Visualization/2010_003493.png inflating: SegmentationClassAug_Visualization/2011_001691.png inflating: SegmentationClassAug_Visualization/2008_007211.png inflating: SegmentationClassAug_Visualization/2009_000217.png inflating: SegmentationClassAug_Visualization/2010_004104.png inflating: SegmentationClassAug_Visualization/2010_003314.png inflating: SegmentationClassAug_Visualization/2008_001544.png inflating: SegmentationClassAug_Visualization/2010_001066.png inflating: SegmentationClassAug_Visualization/2009_004341.png inflating: SegmentationClassAug_Visualization/2008_008437.png inflating: SegmentationClassAug_Visualization/2011_000307.png inflating: SegmentationClassAug_Visualization/2009_003105.png inflating: SegmentationClassAug_Visualization/2008_005347.png inflating: SegmentationClassAug_Visualization/2010_003047.png inflating: SegmentationClassAug_Visualization/2009_004823.png inflating: SegmentationClassAug_Visualization/2008_002404.png inflating: SegmentationClassAug_Visualization/2010_006054.png inflating: SegmentationClassAug_Visualization/2008_001773.png inflating: SegmentationClassAug_Visualization/2011_002272.png inflating: SegmentationClassAug_Visualization/2010_004322.png inflating: SegmentationClassAug_Visualization/2009_004255.png inflating: SegmentationClassAug_Visualization/2008_004130.png inflating: SegmentationClassAug_Visualization/2008_001838.png inflating: SegmentationClassAug_Visualization/2011_001020.png inflating: SegmentationClassAug_Visualization/2009_004055.png inflating: SegmentationClassAug_Visualization/2008_001199.png inflating: SegmentationClassAug_Visualization/2008_007610.png inflating: SegmentationClassAug_Visualization/2008_000472.png inflating: SegmentationClassAug_Visualization/2010_005665.png inflating: SegmentationClassAug_Visualization/2009_004771.png inflating: SegmentationClassAug_Visualization/2009_002449.png inflating: SegmentationClassAug_Visualization/2010_005849.png inflating: SegmentationClassAug_Visualization/2009_004813.png inflating: SegmentationClassAug_Visualization/2007_003020.png inflating: SegmentationClassAug_Visualization/2008_004212.png inflating: SegmentationClassAug_Visualization/2010_000043.png inflating: SegmentationClassAug_Visualization/2010_003757.png inflating: SegmentationClassAug_Visualization/2008_006751.png inflating: SegmentationClassAug_Visualization/2008_002119.png inflating: SegmentationClassAug_Visualization/2007_009594.png inflating: SegmentationClassAug_Visualization/2008_007237.png inflating: SegmentationClassAug_Visualization/2009_002809.png inflating: SegmentationClassAug_Visualization/2009_003974.png inflating: SegmentationClassAug_Visualization/2010_004118.png inflating: SegmentationClassAug_Visualization/2007_006615.png inflating: SegmentationClassAug_Visualization/2010_005318.png inflating: SegmentationClassAug_Visualization/2010_001801.png inflating: SegmentationClassAug_Visualization/2010_005647.png inflating: SegmentationClassAug_Visualization/2010_002138.png inflating: SegmentationClassAug_Visualization/2008_007630.png inflating: SegmentationClassAug_Visualization/2010_003227.png inflating: SegmentationClassAug_Visualization/2008_007364.png inflating: SegmentationClassAug_Visualization/2010_002767.png inflating: SegmentationClassAug_Visualization/2011_002818.png inflating: SegmentationClassAug_Visualization/2010_000245.png inflating: SegmentationClassAug_Visualization/2011_001071.png inflating: SegmentationClassAug_Visualization/2011_001284.png inflating: SegmentationClassAug_Visualization/2008_003475.png inflating: SegmentationClassAug_Visualization/2008_000527.png inflating: SegmentationClassAug_Visualization/2009_001759.png inflating: SegmentationClassAug_Visualization/2008_003079.png inflating: SegmentationClassAug_Visualization/2009_001059.png inflating: SegmentationClassAug_Visualization/2010_004350.png inflating: SegmentationClassAug_Visualization/2011_002831.png inflating: SegmentationClassAug_Visualization/2010_003367.png inflating: SegmentationClassAug_Visualization/2008_008300.png inflating: SegmentationClassAug_Visualization/2011_002038.png inflating: SegmentationClassAug_Visualization/2011_001271.png inflating: SegmentationClassAug_Visualization/2009_001591.png inflating: SegmentationClassAug_Visualization/2008_002830.png inflating: SegmentationClassAug_Visualization/2007_002648.png inflating: SegmentationClassAug_Visualization/2011_001288.png inflating: SegmentationClassAug_Visualization/2011_000847.png inflating: SegmentationClassAug_Visualization/2008_007223.png inflating: SegmentationClassAug_Visualization/2010_000241.png inflating: SegmentationClassAug_Visualization/2007_000529.png inflating: SegmentationClassAug_Visualization/2009_001207.png inflating: SegmentationClassAug_Visualization/2009_003300.png inflating: SegmentationClassAug_Visualization/2008_007691.png inflating: SegmentationClassAug_Visualization/2009_001224.png inflating: SegmentationClassAug_Visualization/2011_001589.png inflating: SegmentationClassAug_Visualization/2011_002114.png inflating: SegmentationClassAug_Visualization/2008_001757.png inflating: SegmentationClassAug_Visualization/2010_003531.png inflating: SegmentationClassAug_Visualization/2008_003691.png inflating: SegmentationClassAug_Visualization/2010_005606.png inflating: SegmentationClassAug_Visualization/2008_006037.png inflating: SegmentationClassAug_Visualization/2010_002303.png inflating: SegmentationClassAug_Visualization/2008_003297.png inflating: SegmentationClassAug_Visualization/2008_007398.png inflating: SegmentationClassAug_Visualization/2008_003095.png inflating: SegmentationClassAug_Visualization/2008_008103.png inflating: SegmentationClassAug_Visualization/2010_001561.png inflating: SegmentationClassAug_Visualization/2009_000989.png inflating: SegmentationClassAug_Visualization/2010_004666.png inflating: SegmentationClassAug_Visualization/2010_004584.png inflating: SegmentationClassAug_Visualization/2009_001967.png inflating: SegmentationClassAug_Visualization/2008_006244.png inflating: SegmentationClassAug_Visualization/2008_006147.png inflating: SegmentationClassAug_Visualization/2008_005088.png inflating: SegmentationClassAug_Visualization/2008_000273.png inflating: SegmentationClassAug_Visualization/2009_001398.png inflating: SegmentationClassAug_Visualization/2009_002289.png inflating: SegmentationClassAug_Visualization/2011_002492.png inflating: SegmentationClassAug_Visualization/2008_008683.png inflating: SegmentationClassAug_Visualization/2008_002789.png inflating: SegmentationClassAug_Visualization/2007_006490.png inflating: SegmentationClassAug_Visualization/2008_001681.png inflating: SegmentationClassAug_Visualization/2009_001724.png inflating: SegmentationClassAug_Visualization/2008_007579.png inflating: SegmentationClassAug_Visualization/2008_001710.png inflating: SegmentationClassAug_Visualization/2008_003449.png inflating: SegmentationClassAug_Visualization/2010_005107.png inflating: SegmentationClassAug_Visualization/2008_006355.png inflating: SegmentationClassAug_Visualization/2010_004529.png inflating: SegmentationClassAug_Visualization/2009_002118.png inflating: SegmentationClassAug_Visualization/2009_000435.png inflating: SegmentationClassAug_Visualization/2010_003135.png inflating: SegmentationClassAug_Visualization/2011_001407.png inflating: SegmentationClassAug_Visualization/2011_001414.png inflating: SegmentationClassAug_Visualization/2008_005614.png inflating: SegmentationClassAug_Visualization/2010_005080.png inflating: SegmentationClassAug_Visualization/2008_000262.png inflating: SegmentationClassAug_Visualization/2009_000737.png inflating: SegmentationClassAug_Visualization/2010_005805.png inflating: SegmentationClassAug_Visualization/2009_004202.png inflating: SegmentationClassAug_Visualization/2011_003076.png inflating: SegmentationClassAug_Visualization/2009_000136.png inflating: SegmentationClassAug_Visualization/2007_006364.png inflating: SegmentationClassAug_Visualization/2009_003315.png inflating: SegmentationClassAug_Visualization/2009_002634.png inflating: SegmentationClassAug_Visualization/2009_003783.png inflating: SegmentationClassAug_Visualization/2010_000063.png inflating: SegmentationClassAug_Visualization/2009_005185.png inflating: SegmentationClassAug_Visualization/2010_001768.png inflating: SegmentationClassAug_Visualization/2009_001713.png inflating: SegmentationClassAug_Visualization/2008_000423.png inflating: SegmentationClassAug_Visualization/2007_007524.png inflating: SegmentationClassAug_Visualization/2011_002532.png inflating: SegmentationClassAug_Visualization/2008_007138.png inflating: SegmentationClassAug_Visualization/2008_001076.png inflating: SegmentationClassAug_Visualization/2008_003193.png inflating: SegmentationClassAug_Visualization/2009_005162.png inflating: SegmentationClassAug_Visualization/2008_007058.png inflating: SegmentationClassAug_Visualization/2008_007444.png inflating: SegmentationClassAug_Visualization/2008_000904.png inflating: SegmentationClassAug_Visualization/2008_001946.png inflating: SegmentationClassAug_Visualization/2009_000093.png inflating: SegmentationClassAug_Visualization/2010_001747.png inflating: SegmentationClassAug_Visualization/2010_002948.png inflating: SegmentationClassAug_Visualization/2008_007811.png inflating: SegmentationClassAug_Visualization/2008_004296.png inflating: SegmentationClassAug_Visualization/2008_005681.png inflating: SegmentationClassAug_Visualization/2008_007321.png inflating: SegmentationClassAug_Visualization/2008_005610.png inflating: SegmentationClassAug_Visualization/2009_000377.png inflating: SegmentationClassAug_Visualization/2008_007806.png inflating: SegmentationClassAug_Visualization/2010_001148.png inflating: SegmentationClassAug_Visualization/2008_000305.png inflating: SegmentationClassAug_Visualization/2010_004822.png inflating: SegmentationClassAug_Visualization/2009_000412.png inflating: SegmentationClassAug_Visualization/2009_004768.png inflating: SegmentationClassAug_Visualization/2011_002657.png inflating: SegmentationClassAug_Visualization/2008_004269.png inflating: SegmentationClassAug_Visualization/2011_001785.png inflating: SegmentationClassAug_Visualization/2011_002421.png inflating: SegmentationClassAug_Visualization/2009_000058.png inflating: SegmentationClassAug_Visualization/2010_002960.png inflating: SegmentationClassAug_Visualization/2008_002835.png inflating: SegmentationClassAug_Visualization/2008_005111.png inflating: SegmentationClassAug_Visualization/2010_003737.png inflating: SegmentationClassAug_Visualization/2008_002749.png inflating: SegmentationClassAug_Visualization/2010_001410.png inflating: SegmentationClassAug_Visualization/2008_007854.png inflating: SegmentationClassAug_Visualization/2008_001329.png inflating: SegmentationClassAug_Visualization/2009_002910.png inflating: SegmentationClassAug_Visualization/2010_005629.png inflating: SegmentationClassAug_Visualization/2010_002532.png inflating: SegmentationClassAug_Visualization/2011_001586.png inflating: SegmentationClassAug_Visualization/2010_000564.png inflating: SegmentationClassAug_Visualization/2008_003114.png inflating: SegmentationClassAug_Visualization/2009_003230.png inflating: SegmentationClassAug_Visualization/2009_002133.png inflating: SegmentationClassAug_Visualization/2007_008932.png inflating: SegmentationClassAug_Visualization/2011_001226.png inflating: SegmentationClassAug_Visualization/2010_001970.png inflating: SegmentationClassAug_Visualization/2010_000735.png inflating: SegmentationClassAug_Visualization/2009_002595.png inflating: SegmentationClassAug_Visualization/2011_002623.png inflating: SegmentationClassAug_Visualization/2010_003016.png inflating: SegmentationClassAug_Visualization/2009_001625.png inflating: SegmentationClassAug_Visualization/2010_005062.png inflating: SegmentationClassAug_Visualization/2010_005500.png inflating: SegmentationClassAug_Visualization/2008_004804.png inflating: SegmentationClassAug_Visualization/2010_002816.png inflating: SegmentationClassAug_Visualization/2010_003695.png inflating: SegmentationClassAug_Visualization/2008_001640.png inflating: SegmentationClassAug_Visualization/2008_006215.png inflating: SegmentationClassAug_Visualization/2008_006068.png inflating: SegmentationClassAug_Visualization/2011_001625.png inflating: SegmentationClassAug_Visualization/2007_008084.png inflating: SegmentationClassAug_Visualization/2009_004841.png inflating: SegmentationClassAug_Visualization/2008_000383.png inflating: SegmentationClassAug_Visualization/2008_003429.png inflating: SegmentationClassAug_Visualization/2008_007494.png inflating: SegmentationClassAug_Visualization/2008_005160.png inflating: SegmentationClassAug_Visualization/2009_001907.png inflating: SegmentationClassAug_Visualization/2008_006715.png inflating: SegmentationClassAug_Visualization/2008_001523.png inflating: SegmentationClassAug_Visualization/2007_006483.png inflating: SegmentationClassAug_Visualization/2008_007981.png inflating: SegmentationClassAug_Visualization/2010_002315.png inflating: SegmentationClassAug_Visualization/2010_004387.png inflating: SegmentationClassAug_Visualization/2009_000085.png inflating: SegmentationClassAug_Visualization/2010_002492.png inflating: SegmentationClassAug_Visualization/2009_001082.png inflating: SegmentationClassAug_Visualization/2009_005144.png inflating: SegmentationClassAug_Visualization/2010_000189.png inflating: SegmentationClassAug_Visualization/2008_006991.png inflating: SegmentationClassAug_Visualization/2008_007161.png inflating: SegmentationClassAug_Visualization/2008_007225.png inflating: SegmentationClassAug_Visualization/2007_003134.png inflating: SegmentationClassAug_Visualization/2008_008292.png inflating: SegmentationClassAug_Visualization/2010_004554.png inflating: SegmentationClassAug_Visualization/2009_002420.png inflating: SegmentationClassAug_Visualization/2009_001190.png inflating: SegmentationClassAug_Visualization/2009_000867.png inflating: SegmentationClassAug_Visualization/2009_001687.png inflating: SegmentationClassAug_Visualization/2011_000016.png inflating: SegmentationClassAug_Visualization/2011_003211.png inflating: SegmentationClassAug_Visualization/2008_006347.png inflating: SegmentationClassAug_Visualization/2011_001779.png inflating: SegmentationClassAug_Visualization/2010_003689.png inflating: SegmentationClassAug_Visualization/2008_005891.png inflating: SegmentationClassAug_Visualization/2010_003994.png inflating: SegmentationClassAug_Visualization/2010_002586.png inflating: SegmentationClassAug_Visualization/2010_005148.png inflating: SegmentationClassAug_Visualization/2010_003635.png inflating: SegmentationClassAug_Visualization/2008_008501.png inflating: SegmentationClassAug_Visualization/2010_001205.png inflating: SegmentationClassAug_Visualization/2009_002764.png inflating: SegmentationClassAug_Visualization/2010_001246.png inflating: SegmentationClassAug_Visualization/2010_002647.png inflating: SegmentationClassAug_Visualization/2009_000449.png inflating: SegmentationClassAug_Visualization/2010_004570.png inflating: SegmentationClassAug_Visualization/2011_001873.png inflating: SegmentationClassAug_Visualization/2007_007003.png inflating: SegmentationClassAug_Visualization/2008_003492.png inflating: SegmentationClassAug_Visualization/2009_004390.png inflating: SegmentationClassAug_Visualization/2009_003375.png inflating: SegmentationClassAug_Visualization/2009_002803.png inflating: SegmentationClassAug_Visualization/2010_000435.png inflating: SegmentationClassAug_Visualization/2008_006898.png inflating: SegmentationClassAug_Visualization/2011_000780.png inflating: SegmentationClassAug_Visualization/2010_002763.png inflating: SegmentationClassAug_Visualization/2009_000283.png inflating: SegmentationClassAug_Visualization/2008_000203.png inflating: SegmentationClassAug_Visualization/2008_006280.png inflating: SegmentationClassAug_Visualization/2011_001032.png inflating: SegmentationClassAug_Visualization/2008_000811.png inflating: SegmentationClassAug_Visualization/2009_002127.png inflating: SegmentationClassAug_Visualization/2009_004686.png inflating: SegmentationClassAug_Visualization/2008_000512.png inflating: SegmentationClassAug_Visualization/2009_000067.png inflating: SegmentationClassAug_Visualization/2007_006317.png inflating: SegmentationClassAug_Visualization/2008_000176.png inflating: SegmentationClassAug_Visualization/2008_008020.png inflating: SegmentationClassAug_Visualization/2007_004052.png inflating: SegmentationClassAug_Visualization/2011_003167.png inflating: SegmentationClassAug_Visualization/2008_001741.png inflating: SegmentationClassAug_Visualization/2009_004874.png inflating: SegmentationClassAug_Visualization/2010_005398.png inflating: SegmentationClassAug_Visualization/2010_005028.png inflating: SegmentationClassAug_Visualization/2007_002260.png inflating: SegmentationClassAug_Visualization/2010_000822.png inflating: SegmentationClassAug_Visualization/2008_003578.png inflating: SegmentationClassAug_Visualization/2008_006021.png inflating: SegmentationClassAug_Visualization/2008_000215.png inflating: SegmentationClassAug_Visualization/2010_003939.png inflating: SegmentationClassAug_Visualization/2007_007523.png inflating: SegmentationClassAug_Visualization/2008_001932.png inflating: SegmentationClassAug_Visualization/2009_003537.png inflating: SegmentationClassAug_Visualization/2008_007394.png inflating: SegmentationClassAug_Visualization/2011_000252.png inflating: SegmentationClassAug_Visualization/2009_002099.png inflating: SegmentationClassAug_Visualization/2009_001040.png inflating: SegmentationClassAug_Visualization/2008_004161.png inflating: SegmentationClassAug_Visualization/2009_003396.png inflating: SegmentationClassAug_Visualization/2008_000373.png inflating: SegmentationClassAug_Visualization/2010_002408.png inflating: SegmentationClassAug_Visualization/2008_003075.png inflating: SegmentationClassAug_Visualization/2011_002917.png inflating: SegmentationClassAug_Visualization/2008_006989.png inflating: SegmentationClassAug_Visualization/2008_004966.png inflating: SegmentationClassAug_Visualization/2008_008694.png inflating: SegmentationClassAug_Visualization/2007_005469.png inflating: SegmentationClassAug_Visualization/2011_001868.png inflating: SegmentationClassAug_Visualization/2008_005627.png inflating: SegmentationClassAug_Visualization/2010_003828.png inflating: SegmentationClassAug_Visualization/2009_004987.png inflating: SegmentationClassAug_Visualization/2008_002236.png inflating: SegmentationClassAug_Visualization/2010_000132.png inflating: SegmentationClassAug_Visualization/2010_003670.png inflating: SegmentationClassAug_Visualization/2008_004636.png inflating: SegmentationClassAug_Visualization/2009_001135.png inflating: SegmentationClassAug_Visualization/2010_005053.png inflating: SegmentationClassAug_Visualization/2008_002823.png inflating: SegmentationClassAug_Visualization/2008_006656.png inflating: SegmentationClassAug_Visualization/2009_002897.png inflating: SegmentationClassAug_Visualization/2010_000874.png inflating: SegmentationClassAug_Visualization/2010_001633.png inflating: SegmentationClassAug_Visualization/2010_003303.png inflating: SegmentationClassAug_Visualization/2007_006803.png inflating: SegmentationClassAug_Visualization/2008_005194.png inflating: SegmentationClassAug_Visualization/2010_005340.png inflating: SegmentationClassAug_Visualization/2008_001937.png inflating: SegmentationClassAug_Visualization/2009_002431.png inflating: SegmentationClassAug_Visualization/2009_004139.png inflating: SegmentationClassAug_Visualization/2008_008113.png inflating: SegmentationClassAug_Visualization/2010_005306.png inflating: SegmentationClassAug_Visualization/2007_003189.png inflating: SegmentationClassAug_Visualization/2010_005992.png inflating: SegmentationClassAug_Visualization/2011_002100.png inflating: SegmentationClassAug_Visualization/2009_004845.png inflating: SegmentationClassAug_Visualization/2011_000512.png inflating: SegmentationClassAug_Visualization/2007_004627.png inflating: SegmentationClassAug_Visualization/2010_000747.png inflating: SegmentationClassAug_Visualization/2011_000070.png inflating: SegmentationClassAug_Visualization/2010_000285.png inflating: SegmentationClassAug_Visualization/2011_001769.png inflating: SegmentationClassAug_Visualization/2010_003538.png inflating: SegmentationClassAug_Visualization/2008_002031.png inflating: SegmentationClassAug_Visualization/2011_001272.png inflating: SegmentationClassAug_Visualization/2008_003951.png inflating: SegmentationClassAug_Visualization/2010_001586.png inflating: SegmentationClassAug_Visualization/2008_006605.png inflating: SegmentationClassAug_Visualization/2009_004317.png inflating: SegmentationClassAug_Visualization/2009_003897.png inflating: SegmentationClassAug_Visualization/2007_008897.png inflating: SegmentationClassAug_Visualization/2008_006365.png inflating: SegmentationClassAug_Visualization/2008_008116.png inflating: SegmentationClassAug_Visualization/2008_006877.png inflating: SegmentationClassAug_Visualization/2009_004340.png inflating: SegmentationClassAug_Visualization/2010_001473.png inflating: SegmentationClassAug_Visualization/2007_001901.png inflating: SegmentationClassAug_Visualization/2009_000932.png inflating: SegmentationClassAug_Visualization/2011_000975.png inflating: SegmentationClassAug_Visualization/2008_000435.png inflating: SegmentationClassAug_Visualization/2010_005815.png inflating: SegmentationClassAug_Visualization/2008_003761.png inflating: SegmentationClassAug_Visualization/2010_004756.png inflating: SegmentationClassAug_Visualization/2009_003232.png inflating: SegmentationClassAug_Visualization/2008_005218.png inflating: SegmentationClassAug_Visualization/2010_005855.png inflating: SegmentationClassAug_Visualization/2008_002272.png inflating: SegmentationClassAug_Visualization/2011_000197.png inflating: SegmentationClassAug_Visualization/2010_002002.png inflating: SegmentationClassAug_Visualization/2008_002920.png inflating: SegmentationClassAug_Visualization/2008_005382.png inflating: SegmentationClassAug_Visualization/2008_004630.png inflating: SegmentationClassAug_Visualization/2011_001855.png inflating: SegmentationClassAug_Visualization/2010_003191.png inflating: SegmentationClassAug_Visualization/2008_007970.png inflating: SegmentationClassAug_Visualization/2008_002267.png inflating: SegmentationClassAug_Visualization/2011_002215.png inflating: SegmentationClassAug_Visualization/2007_009794.png inflating: SegmentationClassAug_Visualization/2008_002150.png inflating: SegmentationClassAug_Visualization/2009_003212.png inflating: SegmentationClassAug_Visualization/2008_006649.png inflating: SegmentationClassAug_Visualization/2011_002568.png inflating: SegmentationClassAug_Visualization/2010_003093.png inflating: SegmentationClassAug_Visualization/2009_001933.png inflating: SegmentationClassAug_Visualization/2009_000981.png inflating: SegmentationClassAug_Visualization/2010_005079.png inflating: SegmentationClassAug_Visualization/2010_002310.png inflating: SegmentationClassAug_Visualization/2008_006250.png inflating: SegmentationClassAug_Visualization/2009_004745.png inflating: SegmentationClassAug_Visualization/2009_002935.png inflating: SegmentationClassAug_Visualization/2008_001813.png inflating: SegmentationClassAug_Visualization/2007_001586.png inflating: SegmentationClassAug_Visualization/2010_002973.png inflating: SegmentationClassAug_Visualization/2008_003726.png inflating: SegmentationClassAug_Visualization/2008_003472.png inflating: SegmentationClassAug_Visualization/2010_003133.png inflating: SegmentationClassAug_Visualization/2008_002716.png inflating: SegmentationClassAug_Visualization/2010_003653.png inflating: SegmentationClassAug_Visualization/2010_003910.png inflating: SegmentationClassAug_Visualization/2010_003157.png inflating: SegmentationClassAug_Visualization/2010_005120.png inflating: SegmentationClassAug_Visualization/2008_003542.png inflating: SegmentationClassAug_Visualization/2008_005982.png inflating: SegmentationClassAug_Visualization/2009_003185.png inflating: SegmentationClassAug_Visualization/2011_000641.png inflating: SegmentationClassAug_Visualization/2010_005379.png inflating: SegmentationClassAug_Visualization/2010_004591.png inflating: SegmentationClassAug_Visualization/2008_002452.png inflating: SegmentationClassAug_Visualization/2008_001613.png inflating: SegmentationClassAug_Visualization/2010_000904.png inflating: SegmentationClassAug_Visualization/2008_004900.png inflating: SegmentationClassAug_Visualization/2009_005183.png inflating: SegmentationClassAug_Visualization/2010_002124.png inflating: SegmentationClassAug_Visualization/2010_000806.png inflating: SegmentationClassAug_Visualization/2011_001290.png inflating: SegmentationClassAug_Visualization/2008_005791.png inflating: SegmentationClassAug_Visualization/2007_002845.png inflating: SegmentationClassAug_Visualization/2009_002536.png inflating: SegmentationClassAug_Visualization/2010_000979.png inflating: SegmentationClassAug_Visualization/2009_004606.png inflating: SegmentationClassAug_Visualization/2008_005055.png inflating: SegmentationClassAug_Visualization/2010_002307.png inflating: SegmentationClassAug_Visualization/2008_003926.png inflating: SegmentationClassAug_Visualization/2009_001091.png inflating: SegmentationClassAug_Visualization/2008_002562.png inflating: SegmentationClassAug_Visualization/2008_008193.png inflating: SegmentationClassAug_Visualization/2008_007273.png inflating: SegmentationClassAug_Visualization/2010_000461.png inflating: SegmentationClassAug_Visualization/2008_001629.png inflating: SegmentationClassAug_Visualization/2008_008338.png inflating: SegmentationClassAug_Visualization/2008_002429.png inflating: SegmentationClassAug_Visualization/2010_003522.png inflating: SegmentationClassAug_Visualization/2009_000229.png inflating: SegmentationClassAug_Visualization/2008_002951.png inflating: SegmentationClassAug_Visualization/2011_000579.png inflating: SegmentationClassAug_Visualization/2009_002746.png inflating: SegmentationClassAug_Visualization/2011_002571.png inflating: SegmentationClassAug_Visualization/2007_005797.png inflating: SegmentationClassAug_Visualization/2010_005732.png inflating: SegmentationClassAug_Visualization/2009_000040.png inflating: SegmentationClassAug_Visualization/2009_001057.png inflating: SegmentationClassAug_Visualization/2011_001568.png inflating: SegmentationClassAug_Visualization/2010_006040.png inflating: SegmentationClassAug_Visualization/2008_007932.png inflating: SegmentationClassAug_Visualization/2009_003059.png inflating: SegmentationClassAug_Visualization/2008_002772.png inflating: SegmentationClassAug_Visualization/2009_002705.png inflating: SegmentationClassAug_Visualization/2010_005566.png inflating: SegmentationClassAug_Visualization/2008_001290.png inflating: SegmentationClassAug_Visualization/2011_003141.png inflating: SegmentationClassAug_Visualization/2009_004486.png inflating: SegmentationClassAug_Visualization/2008_004292.png inflating: SegmentationClassAug_Visualization/2009_003758.png inflating: SegmentationClassAug_Visualization/2008_003127.png inflating: SegmentationClassAug_Visualization/2011_001841.png inflating: SegmentationClassAug_Visualization/2008_004976.png inflating: SegmentationClassAug_Visualization/2010_002045.png inflating: SegmentationClassAug_Visualization/2008_003943.png inflating: SegmentationClassAug_Visualization/2009_003634.png inflating: SegmentationClassAug_Visualization/2007_000925.png inflating: SegmentationClassAug_Visualization/2010_000945.png inflating: SegmentationClassAug_Visualization/2010_003535.png inflating: SegmentationClassAug_Visualization/2008_003883.png inflating: SegmentationClassAug_Visualization/2010_004808.png inflating: SegmentationClassAug_Visualization/2008_004766.png inflating: SegmentationClassAug_Visualization/2008_002395.png inflating: SegmentationClassAug_Visualization/2011_000947.png inflating: SegmentationClassAug_Visualization/2009_003947.png inflating: SegmentationClassAug_Visualization/2011_003085.png inflating: SegmentationClassAug_Visualization/2007_007849.png inflating: SegmentationClassAug_Visualization/2008_004293.png inflating: SegmentationClassAug_Visualization/2011_000895.png inflating: SegmentationClassAug_Visualization/2009_003309.png inflating: SegmentationClassAug_Visualization/2008_000376.png inflating: SegmentationClassAug_Visualization/2010_005676.png inflating: SegmentationClassAug_Visualization/2011_000436.png inflating: SegmentationClassAug_Visualization/2008_008708.png inflating: SegmentationClassAug_Visualization/2010_005208.png inflating: SegmentationClassAug_Visualization/2008_003970.png inflating: SegmentationClassAug_Visualization/2008_002885.png inflating: SegmentationClassAug_Visualization/2010_001794.png inflating: SegmentationClassAug_Visualization/2011_001508.png inflating: SegmentationClassAug_Visualization/2009_000647.png inflating: SegmentationClassAug_Visualization/2010_004297.png inflating: SegmentationClassAug_Visualization/2010_000712.png inflating: SegmentationClassAug_Visualization/2009_001164.png inflating: SegmentationClassAug_Visualization/2008_006219.png inflating: SegmentationClassAug_Visualization/2011_003236.png inflating: SegmentationClassAug_Visualization/2009_003241.png inflating: SegmentationClassAug_Visualization/2011_000428.png inflating: SegmentationClassAug_Visualization/2010_004596.png inflating: SegmentationClassAug_Visualization/2011_002947.png inflating: SegmentationClassAug_Visualization/2007_009938.png inflating: SegmentationClassAug_Visualization/2008_003154.png inflating: SegmentationClassAug_Visualization/2008_006668.png inflating: SegmentationClassAug_Visualization/2010_002320.png inflating: SegmentationClassAug_Visualization/2008_007320.png inflating: SegmentationClassAug_Visualization/2008_002523.png inflating: SegmentationClassAug_Visualization/2010_002965.png inflating: SegmentationClassAug_Visualization/2008_008464.png inflating: SegmentationClassAug_Visualization/2011_002050.png inflating: SegmentationClassAug_Visualization/2010_005505.png inflating: SegmentationClassAug_Visualization/2009_002078.png inflating: SegmentationClassAug_Visualization/2008_007323.png inflating: SegmentationClassAug_Visualization/2008_007714.png inflating: SegmentationClassAug_Visualization/2010_001543.png inflating: SegmentationClassAug_Visualization/2010_005250.png inflating: SegmentationClassAug_Visualization/2008_004977.png inflating: SegmentationClassAug_Visualization/2010_000427.png inflating: SegmentationClassAug_Visualization/2009_003497.png inflating: SegmentationClassAug_Visualization/2008_006796.png inflating: SegmentationClassAug_Visualization/2010_002661.png inflating: SegmentationClassAug_Visualization/2008_006907.png inflating: SegmentationClassAug_Visualization/2007_000804.png inflating: SegmentationClassAug_Visualization/2009_001747.png inflating: SegmentationClassAug_Visualization/2008_006256.png inflating: SegmentationClassAug_Visualization/2010_005216.png inflating: SegmentationClassAug_Visualization/2008_003485.png inflating: SegmentationClassAug_Visualization/2008_008323.png inflating: SegmentationClassAug_Visualization/2010_001257.png inflating: SegmentationClassAug_Visualization/2010_001924.png inflating: SegmentationClassAug_Visualization/2008_001722.png inflating: SegmentationClassAug_Visualization/2010_000515.png inflating: SegmentationClassAug_Visualization/2010_001305.png inflating: SegmentationClassAug_Visualization/2008_001729.png inflating: SegmentationClassAug_Visualization/2008_002906.png inflating: SegmentationClassAug_Visualization/2007_007688.png inflating: SegmentationClassAug_Visualization/2011_002418.png inflating: SegmentationClassAug_Visualization/2010_003687.png inflating: SegmentationClassAug_Visualization/2008_002908.png inflating: SegmentationClassAug_Visualization/2010_005672.png inflating: SegmentationClassAug_Visualization/2008_007242.png inflating: SegmentationClassAug_Visualization/2011_001596.png inflating: SegmentationClassAug_Visualization/2010_004369.png inflating: SegmentationClassAug_Visualization/2007_002539.png inflating: SegmentationClassAug_Visualization/2009_001973.png inflating: SegmentationClassAug_Visualization/2009_000183.png inflating: SegmentationClassAug_Visualization/2010_006010.png inflating: SegmentationClassAug_Visualization/2008_004195.png inflating: SegmentationClassAug_Visualization/2011_002150.png inflating: SegmentationClassAug_Visualization/2008_005843.png inflating: SegmentationClassAug_Visualization/2008_003350.png inflating: SegmentationClassAug_Visualization/2011_000022.png inflating: SegmentationClassAug_Visualization/2008_006216.png inflating: SegmentationClassAug_Visualization/2008_006130.png inflating: SegmentationClassAug_Visualization/2009_004899.png inflating: SegmentationClassAug_Visualization/2010_003379.png inflating: SegmentationClassAug_Visualization/2010_004557.png inflating: SegmentationClassAug_Visualization/2011_000420.png inflating: SegmentationClassAug_Visualization/2009_004033.png inflating: SegmentationClassAug_Visualization/2009_001152.png inflating: SegmentationClassAug_Visualization/2009_003857.png inflating: SegmentationClassAug_Visualization/2010_001908.png inflating: SegmentationClassAug_Visualization/2007_001774.png inflating: SegmentationClassAug_Visualization/2008_000405.png inflating: SegmentationClassAug_Visualization/2010_002348.png inflating: SegmentationClassAug_Visualization/2008_008572.png inflating: SegmentationClassAug_Visualization/2011_000713.png inflating: SegmentationClassAug_Visualization/2010_002245.png inflating: SegmentationClassAug_Visualization/2010_001759.png inflating: SegmentationClassAug_Visualization/2010_003981.png inflating: SegmentationClassAug_Visualization/2007_009346.png inflating: SegmentationClassAug_Visualization/2010_004910.png inflating: SegmentationClassAug_Visualization/2007_005951.png inflating: SegmentationClassAug_Visualization/2007_006553.png inflating: SegmentationClassAug_Visualization/2010_005071.png inflating: SegmentationClassAug_Visualization/2009_004940.png inflating: SegmentationClassAug_Visualization/2008_003330.png inflating: SegmentationClassAug_Visualization/2008_002438.png inflating: SegmentationClassAug_Visualization/2010_006057.png inflating: SegmentationClassAug_Visualization/2008_007646.png inflating: SegmentationClassAug_Visualization/2008_008307.png inflating: SegmentationClassAug_Visualization/2010_004646.png inflating: SegmentationClassAug_Visualization/2008_007166.png inflating: SegmentationClassAug_Visualization/2009_002753.png inflating: SegmentationClassAug_Visualization/2008_004003.png inflating: SegmentationClassAug_Visualization/2008_002036.png inflating: SegmentationClassAug_Visualization/2010_004991.png inflating: SegmentationClassAug_Visualization/2009_000723.png inflating: SegmentationClassAug_Visualization/2009_003644.png inflating: SegmentationClassAug_Visualization/2008_000545.png inflating: SegmentationClassAug_Visualization/2008_006058.png inflating: SegmentationClassAug_Visualization/2009_001657.png inflating: SegmentationClassAug_Visualization/2009_003852.png inflating: SegmentationClassAug_Visualization/2011_001073.png inflating: SegmentationClassAug_Visualization/2008_000496.png inflating: SegmentationClassAug_Visualization/2009_001541.png inflating: SegmentationClassAug_Visualization/2009_005309.png inflating: SegmentationClassAug_Visualization/2008_005788.png inflating: SegmentationClassAug_Visualization/2008_000657.png inflating: SegmentationClassAug_Visualization/2010_001529.png inflating: SegmentationClassAug_Visualization/2008_008154.png inflating: SegmentationClassAug_Visualization/2009_001539.png inflating: SegmentationClassAug_Visualization/2008_007106.png inflating: SegmentationClassAug_Visualization/2008_000470.png inflating: SegmentationClassAug_Visualization/2008_001987.png inflating: SegmentationClassAug_Visualization/2011_000098.png inflating: SegmentationClassAug_Visualization/2010_002623.png inflating: SegmentationClassAug_Visualization/2011_002966.png inflating: SegmentationClassAug_Visualization/2008_006356.png inflating: SegmentationClassAug_Visualization/2008_005777.png inflating: SegmentationClassAug_Visualization/2010_003768.png inflating: SegmentationClassAug_Visualization/2008_004462.png inflating: SegmentationClassAug_Visualization/2008_008319.png inflating: SegmentationClassAug_Visualization/2007_003178.png inflating: SegmentationClassAug_Visualization/2010_002662.png inflating: SegmentationClassAug_Visualization/2008_000381.png inflating: SegmentationClassAug_Visualization/2008_004208.png inflating: SegmentationClassAug_Visualization/2008_007142.png inflating: SegmentationClassAug_Visualization/2008_008544.png inflating: SegmentationClassAug_Visualization/2008_006758.png inflating: SegmentationClassAug_Visualization/2010_004404.png inflating: SegmentationClassAug_Visualization/2007_002643.png inflating: SegmentationClassAug_Visualization/2011_003255.png inflating: SegmentationClassAug_Visualization/2010_000519.png inflating: SegmentationClassAug_Visualization/2008_002399.png inflating: SegmentationClassAug_Visualization/2009_002717.png inflating: SegmentationClassAug_Visualization/2010_003395.png inflating: SegmentationClassAug_Visualization/2008_003039.png inflating: SegmentationClassAug_Visualization/2010_002583.png inflating: SegmentationClassAug_Visualization/2009_003310.png inflating: SegmentationClassAug_Visualization/2011_002003.png inflating: SegmentationClassAug_Visualization/2008_005456.png inflating: SegmentationClassAug_Visualization/2008_007478.png inflating: SegmentationClassAug_Visualization/2009_003247.png inflating: SegmentationClassAug_Visualization/2008_000034.png inflating: SegmentationClassAug_Visualization/2010_004551.png inflating: SegmentationClassAug_Visualization/2008_007425.png inflating: SegmentationClassAug_Visualization/2010_004441.png inflating: SegmentationClassAug_Visualization/2010_005471.png inflating: SegmentationClassAug_Visualization/2010_004631.png inflating: SegmentationClassAug_Visualization/2008_005350.png inflating: SegmentationClassAug_Visualization/2011_000498.png inflating: SegmentationClassAug_Visualization/2009_001884.png inflating: SegmentationClassAug_Visualization/2010_001263.png inflating: SegmentationClassAug_Visualization/2010_003439.png inflating: SegmentationClassAug_Visualization/2009_002525.png inflating: SegmentationClassAug_Visualization/2010_005901.png inflating: SegmentationClassAug_Visualization/2008_006626.png inflating: SegmentationClassAug_Visualization/2008_001437.png inflating: SegmentationClassAug_Visualization/2010_002543.png inflating: SegmentationClassAug_Visualization/2008_003886.png inflating: SegmentationClassAug_Visualization/2009_001902.png inflating: SegmentationClassAug_Visualization/2007_008994.png inflating: SegmentationClassAug_Visualization/2010_000090.png inflating: SegmentationClassAug_Visualization/2010_004337.png inflating: SegmentationClassAug_Visualization/2009_004903.png inflating: SegmentationClassAug_Visualization/2008_000516.png inflating: SegmentationClassAug_Visualization/2008_003202.png inflating: SegmentationClassAug_Visualization/2009_002202.png inflating: SegmentationClassAug_Visualization/2008_006591.png inflating: SegmentationClassAug_Visualization/2009_002535.png inflating: SegmentationClassAug_Visualization/2008_006178.png inflating: SegmentationClassAug_Visualization/2009_001873.png inflating: SegmentationClassAug_Visualization/2010_004201.png inflating: SegmentationClassAug_Visualization/2011_000090.png inflating: SegmentationClassAug_Visualization/2011_000703.png inflating: SegmentationClassAug_Visualization/2008_008266.png inflating: SegmentationClassAug_Visualization/2008_007752.png inflating: SegmentationClassAug_Visualization/2007_000713.png inflating: SegmentationClassAug_Visualization/2008_003394.png inflating: SegmentationClassAug_Visualization/2009_005076.png inflating: SegmentationClassAug_Visualization/2009_000874.png inflating: SegmentationClassAug_Visualization/2008_008671.png inflating: SegmentationClassAug_Visualization/2009_000899.png inflating: SegmentationClassAug_Visualization/2010_001726.png inflating: SegmentationClassAug_Visualization/2008_002448.png inflating: SegmentationClassAug_Visualization/2008_007472.png inflating: SegmentationClassAug_Visualization/2008_006599.png inflating: SegmentationClassAug_Visualization/2009_001007.png inflating: SegmentationClassAug_Visualization/2011_002494.png inflating: SegmentationClassAug_Visualization/2009_000935.png inflating: SegmentationClassAug_Visualization/2008_002441.png inflating: SegmentationClassAug_Visualization/2009_000366.png inflating: SegmentationClassAug_Visualization/2008_006986.png inflating: SegmentationClassAug_Visualization/2011_000248.png inflating: SegmentationClassAug_Visualization/2008_003187.png inflating: SegmentationClassAug_Visualization/2008_005221.png inflating: SegmentationClassAug_Visualization/2011_001641.png inflating: SegmentationClassAug_Visualization/2008_004590.png inflating: SegmentationClassAug_Visualization/2008_002679.png inflating: SegmentationClassAug_Visualization/2009_005279.png inflating: SegmentationClassAug_Visualization/2011_000413.png inflating: SegmentationClassAug_Visualization/2010_002143.png inflating: SegmentationClassAug_Visualization/2009_004956.png inflating: SegmentationClassAug_Visualization/2010_004635.png inflating: SegmentationClassAug_Visualization/2008_007491.png inflating: SegmentationClassAug_Visualization/2010_000876.png inflating: SegmentationClassAug_Visualization/2009_000182.png inflating: SegmentationClassAug_Visualization/2008_004648.png inflating: SegmentationClassAug_Visualization/2008_003636.png inflating: SegmentationClassAug_Visualization/2008_003746.png inflating: SegmentationClassAug_Visualization/2009_001677.png inflating: SegmentationClassAug_Visualization/2009_004037.png inflating: SegmentationClassAug_Visualization/2011_001967.png inflating: SegmentationClassAug_Visualization/2008_006718.png inflating: SegmentationClassAug_Visualization/2009_003201.png inflating: SegmentationClassAug_Visualization/2008_005380.png inflating: SegmentationClassAug_Visualization/2009_000611.png inflating: SegmentationClassAug_Visualization/2009_004100.png inflating: SegmentationClassAug_Visualization/2009_004451.png inflating: SegmentationClassAug_Visualization/2010_001431.png inflating: SegmentationClassAug_Visualization/2009_000825.png inflating: SegmentationClassAug_Visualization/2008_005255.png inflating: SegmentationClassAug_Visualization/2007_003889.png inflating: SegmentationClassAug_Visualization/2007_003431.png inflating: SegmentationClassAug_Visualization/2008_003393.png inflating: SegmentationClassAug_Visualization/2010_003437.png inflating: SegmentationClassAug_Visualization/2008_002495.png inflating: SegmentationClassAug_Visualization/2011_001666.png inflating: SegmentationClassAug_Visualization/2008_007363.png inflating: SegmentationClassAug_Visualization/2010_003955.png inflating: SegmentationClassAug_Visualization/2008_003063.png inflating: SegmentationClassAug_Visualization/2008_006797.png inflating: SegmentationClassAug_Visualization/2010_005023.png inflating: SegmentationClassAug_Visualization/2008_004893.png inflating: SegmentationClassAug_Visualization/2008_004585.png inflating: SegmentationClassAug_Visualization/2010_003701.png inflating: SegmentationClassAug_Visualization/2009_003683.png inflating: SegmentationClassAug_Visualization/2009_000804.png inflating: SegmentationClassAug_Visualization/2010_001630.png inflating: SegmentationClassAug_Visualization/2009_003829.png inflating: SegmentationClassAug_Visualization/2010_002479.png inflating: SegmentationClassAug_Visualization/2010_002924.png inflating: SegmentationClassAug_Visualization/2010_004957.png inflating: SegmentationClassAug_Visualization/2009_002299.png inflating: SegmentationClassAug_Visualization/2011_001014.png inflating: SegmentationClassAug_Visualization/2009_000575.png inflating: SegmentationClassAug_Visualization/2009_000290.png inflating: SegmentationClassAug_Visualization/2010_005921.png inflating: SegmentationClassAug_Visualization/2008_007156.png inflating: SegmentationClassAug_Visualization/2010_002294.png inflating: SegmentationClassAug_Visualization/2007_002387.png inflating: SegmentationClassAug_Visualization/2008_005750.png inflating: SegmentationClassAug_Visualization/2008_003181.png inflating: SegmentationClassAug_Visualization/2010_002499.png inflating: SegmentationClassAug_Visualization/2011_001451.png inflating: SegmentationClassAug_Visualization/2008_003065.png inflating: SegmentationClassAug_Visualization/2008_002234.png inflating: SegmentationClassAug_Visualization/2008_008379.png inflating: SegmentationClassAug_Visualization/2009_001475.png inflating: SegmentationClassAug_Visualization/2010_004533.png inflating: SegmentationClassAug_Visualization/2008_005812.png inflating: SegmentationClassAug_Visualization/2009_002348.png inflating: SegmentationClassAug_Visualization/2008_001493.png inflating: SegmentationClassAug_Visualization/2009_005127.png inflating: SegmentationClassAug_Visualization/2010_005198.png inflating: SegmentationClassAug_Visualization/2009_000466.png inflating: SegmentationClassAug_Visualization/2010_003598.png inflating: SegmentationClassAug_Visualization/2009_000655.png inflating: SegmentationClassAug_Visualization/2011_002559.png inflating: SegmentationClassAug_Visualization/2007_005428.png inflating: SegmentationClassAug_Visualization/2009_000742.png inflating: SegmentationClassAug_Visualization/2011_001266.png inflating: SegmentationClassAug_Visualization/2008_006397.png inflating: SegmentationClassAug_Visualization/2008_002741.png inflating: SegmentationClassAug_Visualization/2009_004181.png inflating: SegmentationClassAug_Visualization/2008_000174.png inflating: SegmentationClassAug_Visualization/2008_000981.png inflating: SegmentationClassAug_Visualization/2009_004545.png inflating: SegmentationClassAug_Visualization/2010_001572.png inflating: SegmentationClassAug_Visualization/2010_000488.png inflating: SegmentationClassAug_Visualization/2010_003717.png inflating: SegmentationClassAug_Visualization/2011_000221.png inflating: SegmentationClassAug_Visualization/2010_002830.png inflating: SegmentationClassAug_Visualization/2008_003264.png inflating: SegmentationClassAug_Visualization/2008_001185.png inflating: SegmentationClassAug_Visualization/2008_005790.png inflating: SegmentationClassAug_Visualization/2008_003924.png inflating: SegmentationClassAug_Visualization/2010_004227.png inflating: SegmentationClassAug_Visualization/2008_003157.png inflating: SegmentationClassAug_Visualization/2009_003986.png inflating: SegmentationClassAug_Visualization/2008_006188.png inflating: SegmentationClassAug_Visualization/2008_001046.png inflating: SegmentationClassAug_Visualization/2009_000966.png inflating: SegmentationClassAug_Visualization/2008_007098.png inflating: SegmentationClassAug_Visualization/2010_003604.png inflating: SegmentationClassAug_Visualization/2010_001980.png inflating: SegmentationClassAug_Visualization/2008_002621.png inflating: SegmentationClassAug_Visualization/2009_004651.png inflating: SegmentationClassAug_Visualization/2008_006121.png inflating: SegmentationClassAug_Visualization/2009_003563.png inflating: SegmentationClassAug_Visualization/2009_000452.png inflating: SegmentationClassAug_Visualization/2010_001954.png inflating: SegmentationClassAug_Visualization/2010_004698.png inflating: SegmentationClassAug_Visualization/2011_000434.png inflating: SegmentationClassAug_Visualization/2011_002443.png inflating: SegmentationClassAug_Visualization/2009_004212.png inflating: SegmentationClassAug_Visualization/2009_000849.png inflating: SegmentationClassAug_Visualization/2008_001430.png inflating: SegmentationClassAug_Visualization/2008_006200.png inflating: SegmentationClassAug_Visualization/2008_001350.png inflating: SegmentationClassAug_Visualization/2008_001070.png inflating: SegmentationClassAug_Visualization/2010_003608.png inflating: SegmentationClassAug_Visualization/2007_001568.png inflating: SegmentationClassAug_Visualization/2007_006581.png inflating: SegmentationClassAug_Visualization/2009_003666.png inflating: SegmentationClassAug_Visualization/2009_000142.png inflating: SegmentationClassAug_Visualization/2009_004201.png inflating: SegmentationClassAug_Visualization/2009_003504.png inflating: SegmentationClassAug_Visualization/2008_007241.png inflating: SegmentationClassAug_Visualization/2011_001679.png inflating: SegmentationClassAug_Visualization/2009_004882.png inflating: SegmentationClassAug_Visualization/2008_001154.png inflating: SegmentationClassAug_Visualization/2011_001449.png inflating: SegmentationClassAug_Visualization/2011_000312.png inflating: SegmentationClassAug_Visualization/2011_001447.png inflating: SegmentationClassAug_Visualization/2009_002710.png inflating: SegmentationClassAug_Visualization/2009_000096.png inflating: SegmentationClassAug_Visualization/2010_003938.png inflating: SegmentationClassAug_Visualization/2008_001257.png inflating: SegmentationClassAug_Visualization/2010_004738.png inflating: SegmentationClassAug_Visualization/2008_000817.png inflating: SegmentationClassAug_Visualization/2008_004705.png inflating: SegmentationClassAug_Visualization/2009_003679.png inflating: SegmentationClassAug_Visualization/2009_003360.png inflating: SegmentationClassAug_Visualization/2011_001845.png inflating: SegmentationClassAug_Visualization/2010_001456.png inflating: SegmentationClassAug_Visualization/2011_001537.png inflating: SegmentationClassAug_Visualization/2011_002163.png inflating: SegmentationClassAug_Visualization/2009_003383.png inflating: SegmentationClassAug_Visualization/2011_001842.png inflating: SegmentationClassAug_Visualization/2008_001652.png inflating: SegmentationClassAug_Visualization/2009_004631.png inflating: SegmentationClassAug_Visualization/2010_003936.png inflating: SegmentationClassAug_Visualization/2009_003200.png inflating: SegmentationClassAug_Visualization/2011_001424.png inflating: SegmentationClassAug_Visualization/2010_000404.png inflating: SegmentationClassAug_Visualization/2008_002350.png inflating: SegmentationClassAug_Visualization/2010_003374.png inflating: SegmentationClassAug_Visualization/2008_000342.png inflating: SegmentationClassAug_Visualization/2009_003353.png inflating: SegmentationClassAug_Visualization/2009_001890.png inflating: SegmentationClassAug_Visualization/2011_002273.png inflating: SegmentationClassAug_Visualization/2008_004579.png inflating: SegmentationClassAug_Visualization/2008_000033.png inflating: SegmentationClassAug_Visualization/2011_001173.png inflating: SegmentationClassAug_Visualization/2010_005929.png inflating: SegmentationClassAug_Visualization/2008_006159.png inflating: SegmentationClassAug_Visualization/2009_003351.png inflating: SegmentationClassAug_Visualization/2008_004881.png inflating: SegmentationClassAug_Visualization/2009_003294.png inflating: SegmentationClassAug_Visualization/2008_003067.png inflating: SegmentationClassAug_Visualization/2009_002001.png inflating: SegmentationClassAug_Visualization/2008_007336.png inflating: SegmentationClassAug_Visualization/2008_005977.png inflating: SegmentationClassAug_Visualization/2009_004648.png inflating: SegmentationClassAug_Visualization/2010_003168.png inflating: SegmentationClassAug_Visualization/2008_001955.png inflating: SegmentationClassAug_Visualization/2008_002820.png inflating: SegmentationClassAug_Visualization/2009_004588.png inflating: SegmentationClassAug_Visualization/2008_002668.png inflating: SegmentationClassAug_Visualization/2009_000042.png inflating: SegmentationClassAug_Visualization/2010_005164.png inflating: SegmentationClassAug_Visualization/2008_008690.png inflating: SegmentationClassAug_Visualization/2011_002516.png inflating: SegmentationClassAug_Visualization/2008_007966.png inflating: SegmentationClassAug_Visualization/2010_005016.png inflating: SegmentationClassAug_Visualization/2008_002461.png inflating: SegmentationClassAug_Visualization/2008_002501.png inflating: SegmentationClassAug_Visualization/2007_000364.png inflating: SegmentationClassAug_Visualization/2008_006425.png inflating: SegmentationClassAug_Visualization/2008_007289.png inflating: SegmentationClassAug_Visualization/2008_005369.png inflating: SegmentationClassAug_Visualization/2008_002383.png inflating: SegmentationClassAug_Visualization/2009_004464.png inflating: SegmentationClassAug_Visualization/2009_001971.png inflating: SegmentationClassAug_Visualization/2010_000336.png inflating: SegmentationClassAug_Visualization/2008_001945.png inflating: SegmentationClassAug_Visualization/2010_001206.png inflating: SegmentationClassAug_Visualization/2008_001340.png inflating: SegmentationClassAug_Visualization/2009_004121.png inflating: SegmentationClassAug_Visualization/2008_001638.png inflating: SegmentationClassAug_Visualization/2009_001364.png inflating: SegmentationClassAug_Visualization/2008_003813.png inflating: SegmentationClassAug_Visualization/2008_007298.png inflating: SegmentationClassAug_Visualization/2010_004950.png inflating: SegmentationClassAug_Visualization/2010_002615.png inflating: SegmentationClassAug_Visualization/2008_004008.png inflating: SegmentationClassAug_Visualization/2008_006924.png inflating: SegmentationClassAug_Visualization/2009_002333.png inflating: SegmentationClassAug_Visualization/2008_003811.png inflating: SegmentationClassAug_Visualization/2009_003194.png inflating: SegmentationClassAug_Visualization/2010_003219.png inflating: SegmentationClassAug_Visualization/2010_001057.png inflating: SegmentationClassAug_Visualization/2010_002211.png inflating: SegmentationClassAug_Visualization/2008_004767.png inflating: SegmentationClassAug_Visualization/2007_000033.png inflating: SegmentationClassAug_Visualization/2010_000260.png inflating: SegmentationClassAug_Visualization/2010_006031.png inflating: SegmentationClassAug_Visualization/2010_000862.png inflating: SegmentationClassAug_Visualization/2009_004999.png inflating: SegmentationClassAug_Visualization/2009_000149.png inflating: SegmentationClassAug_Visualization/2008_000531.png inflating: SegmentationClassAug_Visualization/2010_001618.png inflating: SegmentationClassAug_Visualization/2010_005128.png inflating: SegmentationClassAug_Visualization/2011_000453.png inflating: SegmentationClassAug_Visualization/2010_001525.png inflating: SegmentationClassAug_Visualization/2008_004506.png inflating: SegmentationClassAug_Visualization/2007_002445.png inflating: SegmentationClassAug_Visualization/2009_001044.png inflating: SegmentationClassAug_Visualization/2008_007146.png inflating: SegmentationClassAug_Visualization/2010_003856.png inflating: SegmentationClassAug_Visualization/2011_001440.png inflating: SegmentationClassAug_Visualization/2009_003346.png inflating: SegmentationClassAug_Visualization/2008_005527.png inflating: SegmentationClassAug_Visualization/2009_003545.png inflating: SegmentationClassAug_Visualization/2011_000103.png inflating: SegmentationClassAug_Visualization/2010_004247.png inflating: SegmentationClassAug_Visualization/2010_005002.png inflating: SegmentationClassAug_Visualization/2009_000119.png inflating: SegmentationClassAug_Visualization/2008_000825.png inflating: SegmentationClassAug_Visualization/2009_002477.png inflating: SegmentationClassAug_Visualization/2010_004052.png inflating: SegmentationClassAug_Visualization/2011_000704.png inflating: SegmentationClassAug_Visualization/2010_003789.png inflating: SegmentationClassAug_Visualization/2010_004506.png inflating: SegmentationClassAug_Visualization/2008_006014.png inflating: SegmentationClassAug_Visualization/2008_007781.png inflating: SegmentationClassAug_Visualization/2010_005837.png inflating: SegmentationClassAug_Visualization/2008_006458.png inflating: SegmentationClassAug_Visualization/2008_001564.png inflating: SegmentationClassAug_Visualization/2011_001927.png inflating: SegmentationClassAug_Visualization/2008_001099.png inflating: SegmentationClassAug_Visualization/2010_005190.png inflating: SegmentationClassAug_Visualization/2007_004289.png inflating: SegmentationClassAug_Visualization/2008_003768.png inflating: SegmentationClassAug_Visualization/2010_004918.png inflating: SegmentationClassAug_Visualization/2009_001395.png inflating: SegmentationClassAug_Visualization/2009_001534.png inflating: SegmentationClassAug_Visualization/2008_005150.png inflating: SegmentationClassAug_Visualization/2008_002220.png inflating: SegmentationClassAug_Visualization/2008_008649.png inflating: SegmentationClassAug_Visualization/2010_003546.png inflating: SegmentationClassAug_Visualization/2008_006203.png inflating: SegmentationClassAug_Visualization/2008_004231.png inflating: SegmentationClassAug_Visualization/2007_008801.png inflating: SegmentationClassAug_Visualization/2010_005468.png inflating: SegmentationClassAug_Visualization/2009_004571.png inflating: SegmentationClassAug_Visualization/2008_007279.png inflating: SegmentationClassAug_Visualization/2010_005996.png inflating: SegmentationClassAug_Visualization/2008_003415.png inflating: SegmentationClassAug_Visualization/2009_002314.png inflating: SegmentationClassAug_Visualization/2009_003598.png inflating: SegmentationClassAug_Visualization/2011_000234.png inflating: SegmentationClassAug_Visualization/2010_005865.png inflating: SegmentationClassAug_Visualization/2009_001734.png inflating: SegmentationClassAug_Visualization/2008_003719.png inflating: SegmentationClassAug_Visualization/2007_000904.png inflating: SegmentationClassAug_Visualization/2011_000831.png inflating: SegmentationClassAug_Visualization/2010_003192.png inflating: SegmentationClassAug_Visualization/2011_002943.png inflating: SegmentationClassAug_Visualization/2007_000799.png inflating: SegmentationClassAug_Visualization/2008_002251.png inflating: SegmentationClassAug_Visualization/2009_001069.png inflating: SegmentationClassAug_Visualization/2009_001240.png inflating: SegmentationClassAug_Visualization/2009_003627.png inflating: SegmentationClassAug_Visualization/2008_007120.png inflating: SegmentationClassAug_Visualization/2010_003368.png inflating: SegmentationClassAug_Visualization/2008_000082.png inflating: SegmentationClassAug_Visualization/2007_002903.png inflating: SegmentationClassAug_Visualization/2008_001192.png inflating: SegmentationClassAug_Visualization/2008_000563.png inflating: SegmentationClassAug_Visualization/2010_005914.png inflating: SegmentationClassAug_Visualization/2010_001682.png inflating: SegmentationClassAug_Visualization/2008_003203.png inflating: SegmentationClassAug_Visualization/2007_009413.png inflating: SegmentationClassAug_Visualization/2008_004077.png inflating: SegmentationClassAug_Visualization/2009_003511.png inflating: SegmentationClassAug_Visualization/2008_008190.png inflating: SegmentationClassAug_Visualization/2010_001649.png inflating: SegmentationClassAug_Visualization/2010_000342.png inflating: SegmentationClassAug_Visualization/2008_001161.png inflating: SegmentationClassAug_Visualization/2008_001694.png inflating: SegmentationClassAug_Visualization/2008_001805.png inflating: SegmentationClassAug_Visualization/2009_002515.png inflating: SegmentationClassAug_Visualization/2009_003606.png inflating: SegmentationClassAug_Visualization/2008_006227.png inflating: SegmentationClassAug_Visualization/2008_003128.png inflating: SegmentationClassAug_Visualization/2009_004284.png inflating: SegmentationClassAug_Visualization/2008_005168.png inflating: SegmentationClassAug_Visualization/2010_004717.png inflating: SegmentationClassAug_Visualization/2008_003083.png inflating: SegmentationClassAug_Visualization/2010_005584.png inflating: SegmentationClassAug_Visualization/2010_000320.png inflating: SegmentationClassAug_Visualization/2008_004833.png inflating: SegmentationClassAug_Visualization/2008_007261.png inflating: SegmentationClassAug_Visualization/2008_005903.png inflating: SegmentationClassAug_Visualization/2008_000257.png inflating: SegmentationClassAug_Visualization/2007_007930.png inflating: SegmentationClassAug_Visualization/2009_001720.png inflating: SegmentationClassAug_Visualization/2011_000386.png inflating: SegmentationClassAug_Visualization/2010_005188.png inflating: SegmentationClassAug_Visualization/2010_002510.png inflating: SegmentationClassAug_Visualization/2007_003788.png inflating: SegmentationClassAug_Visualization/2011_000317.png inflating: SegmentationClassAug_Visualization/2010_001797.png inflating: SegmentationClassAug_Visualization/2010_005982.png inflating: SegmentationClassAug_Visualization/2010_000474.png inflating: SegmentationClassAug_Visualization/2008_002204.png inflating: SegmentationClassAug_Visualization/2008_008767.png inflating: SegmentationClassAug_Visualization/2009_000681.png inflating: SegmentationClassAug_Visualization/2008_000939.png inflating: SegmentationClassAug_Visualization/2008_006488.png inflating: SegmentationClassAug_Visualization/2010_000256.png inflating: SegmentationClassAug_Visualization/2010_002065.png inflating: SegmentationClassAug_Visualization/2008_007632.png inflating: SegmentationClassAug_Visualization/2008_000912.png inflating: SegmentationClassAug_Visualization/2009_003093.png inflating: SegmentationClassAug_Visualization/2010_004036.png inflating: SegmentationClassAug_Visualization/2011_002097.png inflating: SegmentationClassAug_Visualization/2010_001174.png inflating: SegmentationClassAug_Visualization/2009_000992.png inflating: SegmentationClassAug_Visualization/2011_000520.png inflating: SegmentationClassAug_Visualization/2008_003854.png inflating: SegmentationClassAug_Visualization/2009_002445.png inflating: SegmentationClassAug_Visualization/2008_000306.png inflating: SegmentationClassAug_Visualization/2011_002234.png inflating: SegmentationClassAug_Visualization/2008_001089.png inflating: SegmentationClassAug_Visualization/2008_008552.png inflating: SegmentationClassAug_Visualization/2010_001016.png inflating: SegmentationClassAug_Visualization/2010_003199.png inflating: SegmentationClassAug_Visualization/2011_001891.png inflating: SegmentationClassAug_Visualization/2011_000162.png inflating: SegmentationClassAug_Visualization/2007_009897.png inflating: SegmentationClassAug_Visualization/2008_002652.png inflating: SegmentationClassAug_Visualization/2008_000309.png inflating: SegmentationClassAug_Visualization/2010_000083.png inflating: SegmentationClassAug_Visualization/2008_005094.png inflating: SegmentationClassAug_Visualization/2008_002860.png inflating: SegmentationClassAug_Visualization/2009_004763.png inflating: SegmentationClassAug_Visualization/2008_004288.png inflating: SegmentationClassAug_Visualization/2010_002854.png inflating: SegmentationClassAug_Visualization/2010_000944.png inflating: SegmentationClassAug_Visualization/2011_003274.png inflating: SegmentationClassAug_Visualization/2008_006182.png inflating: SegmentationClassAug_Visualization/2008_000721.png inflating: SegmentationClassAug_Visualization/2011_000475.png inflating: SegmentationClassAug_Visualization/2010_000426.png inflating: SegmentationClassAug_Visualization/2009_001440.png inflating: SegmentationClassAug_Visualization/2010_005610.png inflating: SegmentationClassAug_Visualization/2010_001644.png inflating: SegmentationClassAug_Visualization/2008_002009.png inflating: SegmentationClassAug_Visualization/2010_005049.png inflating: SegmentationClassAug_Visualization/2010_002993.png inflating: SegmentationClassAug_Visualization/2008_004190.png inflating: SegmentationClassAug_Visualization/2011_001741.png inflating: SegmentationClassAug_Visualization/2009_002282.png inflating: SegmentationClassAug_Visualization/2009_004279.png inflating: SegmentationClassAug_Visualization/2008_005417.png inflating: SegmentationClassAug_Visualization/2010_004005.png inflating: SegmentationClassAug_Visualization/2009_000574.png inflating: SegmentationClassAug_Visualization/2008_007236.png inflating: SegmentationClassAug_Visualization/2009_000979.png inflating: SegmentationClassAug_Visualization/2010_004363.png inflating: SegmentationClassAug_Visualization/2011_000830.png inflating: SegmentationClassAug_Visualization/2009_000001.png inflating: SegmentationClassAug_Visualization/2010_002625.png inflating: SegmentationClassAug_Visualization/2007_005828.png inflating: SegmentationClassAug_Visualization/2010_001390.png inflating: SegmentationClassAug_Visualization/2008_007388.png inflating: SegmentationClassAug_Visualization/2011_000954.png inflating: SegmentationClassAug_Visualization/2009_004234.png inflating: SegmentationClassAug_Visualization/2010_003735.png inflating: SegmentationClassAug_Visualization/2007_004500.png inflating: SegmentationClassAug_Visualization/2009_000073.png inflating: SegmentationClassAug_Visualization/2011_001319.png inflating: SegmentationClassAug_Visualization/2011_002650.png inflating: SegmentationClassAug_Visualization/2008_003025.png inflating: SegmentationClassAug_Visualization/2009_001767.png inflating: SegmentationClassAug_Visualization/2008_002269.png inflating: SegmentationClassAug_Visualization/2009_002058.png inflating: SegmentationClassAug_Visualization/2009_004871.png inflating: SegmentationClassAug_Visualization/2008_003619.png inflating: SegmentationClassAug_Visualization/2011_002826.png inflating: SegmentationClassAug_Visualization/2011_000793.png inflating: SegmentationClassAug_Visualization/2010_003628.png inflating: SegmentationClassAug_Visualization/2009_001991.png inflating: SegmentationClassAug_Visualization/2008_001919.png inflating: SegmentationClassAug_Visualization/2009_000820.png inflating: SegmentationClassAug_Visualization/2008_001404.png inflating: SegmentationClassAug_Visualization/2011_002760.png inflating: SegmentationClassAug_Visualization/2009_001027.png inflating: SegmentationClassAug_Visualization/2010_004945.png inflating: SegmentationClassAug_Visualization/2010_002309.png inflating: SegmentationClassAug_Visualization/2009_001505.png inflating: SegmentationClassAug_Visualization/2010_003138.png inflating: SegmentationClassAug_Visualization/2008_001836.png inflating: SegmentationClassAug_Visualization/2011_001025.png inflating: SegmentationClassAug_Visualization/2010_004479.png inflating: SegmentationClassAug_Visualization/2008_000726.png inflating: SegmentationClassAug_Visualization/2008_003499.png inflating: SegmentationClassAug_Visualization/2009_002558.png inflating: SegmentationClassAug_Visualization/2007_005266.png inflating: SegmentationClassAug_Visualization/2009_001481.png inflating: SegmentationClassAug_Visualization/2010_002469.png inflating: SegmentationClassAug_Visualization/2009_000591.png inflating: SegmentationClassAug_Visualization/2008_000566.png inflating: SegmentationClassAug_Visualization/2008_008233.png inflating: SegmentationClassAug_Visualization/2008_002208.png inflating: SegmentationClassAug_Visualization/2009_004359.png inflating: SegmentationClassAug_Visualization/2008_006281.png inflating: SegmentationClassAug_Visualization/2007_004476.png inflating: SegmentationClassAug_Visualization/2009_005278.png inflating: SegmentationClassAug_Visualization/2011_002341.png inflating: SegmentationClassAug_Visualization/2007_008543.png inflating: SegmentationClassAug_Visualization/2007_000793.png inflating: SegmentationClassAug_Visualization/2009_002453.png inflating: SegmentationClassAug_Visualization/2010_004228.png inflating: SegmentationClassAug_Visualization/2010_002887.png inflating: SegmentationClassAug_Visualization/2010_005985.png inflating: SegmentationClassAug_Visualization/2008_000992.png inflating: SegmentationClassAug_Visualization/2010_002632.png inflating: SegmentationClassAug_Visualization/2010_000829.png inflating: SegmentationClassAug_Visualization/2009_001751.png inflating: SegmentationClassAug_Visualization/2008_006579.png inflating: SegmentationClassAug_Visualization/2008_003461.png inflating: SegmentationClassAug_Visualization/2009_000635.png inflating: SegmentationClassAug_Visualization/2009_000763.png inflating: SegmentationClassAug_Visualization/2011_001422.png inflating: SegmentationClassAug_Visualization/2007_007649.png inflating: SegmentationClassAug_Visualization/2010_002172.png inflating: SegmentationClassAug_Visualization/2008_000660.png inflating: SegmentationClassAug_Visualization/2010_004891.png inflating: SegmentationClassAug_Visualization/2009_004494.png inflating: SegmentationClassAug_Visualization/2010_003603.png inflating: SegmentationClassAug_Visualization/2010_002180.png inflating: SegmentationClassAug_Visualization/2008_001610.png inflating: SegmentationClassAug_Visualization/2008_003650.png inflating: SegmentationClassAug_Visualization/2010_002301.png inflating: SegmentationClassAug_Visualization/2011_001110.png inflating: SegmentationClassAug_Visualization/2008_005869.png inflating: SegmentationClassAug_Visualization/2010_000386.png inflating: SegmentationClassAug_Visualization/2011_002189.png inflating: SegmentationClassAug_Visualization/2009_003140.png inflating: SegmentationClassAug_Visualization/2008_001454.png inflating: SegmentationClassAug_Visualization/2008_000235.png inflating: SegmentationClassAug_Visualization/2009_002546.png inflating: SegmentationClassAug_Visualization/2010_000436.png inflating: SegmentationClassAug_Visualization/2009_002416.png inflating: SegmentationClassAug_Visualization/2010_005752.png inflating: SegmentationClassAug_Visualization/2009_000846.png inflating: SegmentationClassAug_Visualization/2010_003561.png inflating: SegmentationClassAug_Visualization/2010_003898.png inflating: SegmentationClassAug_Visualization/2009_000026.png inflating: SegmentationClassAug_Visualization/2011_002295.png inflating: SegmentationClassAug_Visualization/2009_003576.png inflating: SegmentationClassAug_Visualization/2009_003164.png inflating: SegmentationClassAug_Visualization/2008_007115.png inflating: SegmentationClassAug_Visualization/2010_002373.png inflating: SegmentationClassAug_Visualization/2008_001402.png inflating: SegmentationClassAug_Visualization/2010_003415.png inflating: SegmentationClassAug_Visualization/2009_003394.png inflating: SegmentationClassAug_Visualization/2010_001394.png inflating: SegmentationClassAug_Visualization/2007_008085.png inflating: SegmentationClassAug_Visualization/2009_001536.png inflating: SegmentationClassAug_Visualization/2010_002365.png inflating: SegmentationClassAug_Visualization/2008_008051.png inflating: SegmentationClassAug_Visualization/2008_007585.png inflating: SegmentationClassAug_Visualization/2008_005796.png inflating: SegmentationClassAug_Visualization/2010_005756.png inflating: SegmentationClassAug_Visualization/2008_007891.png inflating: SegmentationClassAug_Visualization/2010_001422.png inflating: SegmentationClassAug_Visualization/2010_000213.png inflating: SegmentationClassAug_Visualization/2008_006119.png inflating: SegmentationClassAug_Visualization/2010_001256.png inflating: SegmentationClassAug_Visualization/2009_000573.png inflating: SegmentationClassAug_Visualization/2010_005534.png inflating: SegmentationClassAug_Visualization/2010_001931.png inflating: SegmentationClassAug_Visualization/2009_001343.png inflating: SegmentationClassAug_Visualization/2010_005788.png inflating: SegmentationClassAug_Visualization/2011_002228.png inflating: SegmentationClassAug_Visualization/2008_001231.png inflating: SegmentationClassAug_Visualization/2008_003647.png inflating: SegmentationClassAug_Visualization/2008_008545.png inflating: SegmentationClassAug_Visualization/2009_002998.png inflating: SegmentationClassAug_Visualization/2011_001885.png inflating: SegmentationClassAug_Visualization/2007_004143.png inflating: SegmentationClassAug_Visualization/2009_003387.png inflating: SegmentationClassAug_Visualization/2008_000119.png inflating: SegmentationClassAug_Visualization/2007_005360.png inflating: SegmentationClassAug_Visualization/2010_000069.png inflating: SegmentationClassAug_Visualization/2007_009592.png inflating: SegmentationClassAug_Visualization/2009_001853.png inflating: SegmentationClassAug_Visualization/2008_002973.png inflating: SegmentationClassAug_Visualization/2008_002361.png inflating: SegmentationClassAug_Visualization/2010_005651.png inflating: SegmentationClassAug_Visualization/2008_008588.png inflating: SegmentationClassAug_Visualization/2009_000606.png inflating: SegmentationClassAug_Visualization/2011_001382.png inflating: SegmentationClassAug_Visualization/2008_006490.png inflating: SegmentationClassAug_Visualization/2007_003118.png inflating: SegmentationClassAug_Visualization/2008_003342.png inflating: SegmentationClassAug_Visualization/2009_001826.png inflating: SegmentationClassAug_Visualization/2007_003778.png inflating: SegmentationClassAug_Visualization/2008_000753.png inflating: SegmentationClassAug_Visualization/2010_001364.png inflating: SegmentationClassAug_Visualization/2008_003958.png inflating: SegmentationClassAug_Visualization/2008_000742.png inflating: SegmentationClassAug_Visualization/2010_001967.png inflating: SegmentationClassAug_Visualization/2008_002418.png inflating: SegmentationClassAug_Visualization/2008_003671.png inflating: SegmentationClassAug_Visualization/2008_008254.png inflating: SegmentationClassAug_Visualization/2010_002487.png inflating: SegmentationClassAug_Visualization/2008_004412.png inflating: SegmentationClassAug_Visualization/2009_000289.png inflating: SegmentationClassAug_Visualization/2008_005395.png inflating: SegmentationClassAug_Visualization/2008_007819.png inflating: SegmentationClassAug_Visualization/2008_006712.png inflating: SegmentationClassAug_Visualization/2008_003722.png inflating: SegmentationClassAug_Visualization/2009_004732.png inflating: SegmentationClassAug_Visualization/2008_006692.png inflating: SegmentationClassAug_Visualization/2008_008001.png inflating: SegmentationClassAug_Visualization/2010_001366.png inflating: SegmentationClassAug_Visualization/2009_001988.png inflating: SegmentationClassAug_Visualization/2011_002908.png inflating: SegmentationClassAug_Visualization/2009_004859.png inflating: SegmentationClassAug_Visualization/2010_002388.png inflating: SegmentationClassAug_Visualization/2009_003343.png inflating: SegmentationClassAug_Visualization/2008_006834.png inflating: SegmentationClassAug_Visualization/2011_000642.png inflating: SegmentationClassAug_Visualization/2010_005389.png inflating: SegmentationClassAug_Visualization/2008_001761.png inflating: SegmentationClassAug_Visualization/2009_002838.png inflating: SegmentationClassAug_Visualization/2008_008591.png inflating: SegmentationClassAug_Visualization/2010_001647.png inflating: SegmentationClassAug_Visualization/2009_001198.png inflating: SegmentationClassAug_Visualization/2010_000846.png inflating: SegmentationClassAug_Visualization/2008_000154.png inflating: SegmentationClassAug_Visualization/2010_005229.png inflating: SegmentationClassAug_Visualization/2008_008315.png inflating: SegmentationClassAug_Visualization/2009_001646.png inflating: SegmentationClassAug_Visualization/2008_002464.png inflating: SegmentationClassAug_Visualization/2010_003686.png inflating: SegmentationClassAug_Visualization/2007_006151.png inflating: SegmentationClassAug_Visualization/2008_001598.png inflating: SegmentationClassAug_Visualization/2007_008945.png inflating: SegmentationClassAug_Visualization/2009_000328.png inflating: SegmentationClassAug_Visualization/2011_002186.png inflating: SegmentationClassAug_Visualization/2010_005252.png inflating: SegmentationClassAug_Visualization/2009_000779.png inflating: SegmentationClassAug_Visualization/2009_000834.png inflating: SegmentationClassAug_Visualization/2009_005232.png inflating: SegmentationClassAug_Visualization/2010_001584.png inflating: SegmentationClassAug_Visualization/2008_000880.png inflating: SegmentationClassAug_Visualization/2011_000321.png inflating: SegmentationClassAug_Visualization/2008_003458.png inflating: SegmentationClassAug_Visualization/2008_007938.png inflating: SegmentationClassAug_Visualization/2009_000734.png inflating: SegmentationClassAug_Visualization/2010_004073.png inflating: SegmentationClassAug_Visualization/2010_005033.png inflating: SegmentationClassAug_Visualization/2007_006866.png inflating: SegmentationClassAug_Visualization/2008_002483.png inflating: SegmentationClassAug_Visualization/2008_006792.png inflating: SegmentationClassAug_Visualization/2010_002107.png inflating: SegmentationClassAug_Visualization/2008_000414.png inflating: SegmentationClassAug_Visualization/2010_000465.png inflating: SegmentationClassAug_Visualization/2009_003806.png inflating: SegmentationClassAug_Visualization/2008_004171.png inflating: SegmentationClassAug_Visualization/2010_005608.png inflating: SegmentationClassAug_Visualization/2009_005218.png inflating: SegmentationClassAug_Visualization/2008_003417.png inflating: SegmentationClassAug_Visualization/2009_003255.png inflating: SegmentationClassAug_Visualization/2008_001227.png inflating: SegmentationClassAug_Visualization/2010_001094.png inflating: SegmentationClassAug_Visualization/2009_004504.png inflating: SegmentationClassAug_Visualization/2010_000770.png inflating: SegmentationClassAug_Visualization/2008_001414.png inflating: SegmentationClassAug_Visualization/2008_000763.png inflating: SegmentationClassAug_Visualization/2008_001977.png inflating: SegmentationClassAug_Visualization/2009_002530.png inflating: SegmentationClassAug_Visualization/2009_004002.png inflating: SegmentationClassAug_Visualization/2009_003695.png inflating: SegmentationClassAug_Visualization/2008_003802.png inflating: SegmentationClassAug_Visualization/2009_002505.png inflating: SegmentationClassAug_Visualization/2011_002347.png inflating: SegmentationClassAug_Visualization/2010_003013.png inflating: SegmentationClassAug_Visualization/2010_002455.png inflating: SegmentationClassAug_Visualization/2008_003874.png inflating: SegmentationClassAug_Visualization/2008_003945.png inflating: SegmentationClassAug_Visualization/2011_002300.png inflating: SegmentationClassAug_Visualization/2007_001526.png inflating: SegmentationClassAug_Visualization/2010_004638.png inflating: SegmentationClassAug_Visualization/2007_002376.png inflating: SegmentationClassAug_Visualization/2008_004550.png inflating: SegmentationClassAug_Visualization/2011_000598.png inflating: SegmentationClassAug_Visualization/2010_001458.png inflating: SegmentationClassAug_Visualization/2011_000105.png inflating: SegmentationClassAug_Visualization/2009_002245.png inflating: SegmentationClassAug_Visualization/2011_000053.png inflating: SegmentationClassAug_Visualization/2008_002484.png inflating: SegmentationClassAug_Visualization/2010_001325.png inflating: SegmentationClassAug_Visualization/2009_005215.png inflating: SegmentationClassAug_Visualization/2009_000474.png inflating: SegmentationClassAug_Visualization/2010_002930.png inflating: SegmentationClassAug_Visualization/2010_004677.png inflating: SegmentationClassAug_Visualization/2011_000731.png inflating: SegmentationClassAug_Visualization/2008_003001.png inflating: SegmentationClassAug_Visualization/2010_003956.png inflating: SegmentationClassAug_Visualization/2009_000961.png inflating: SegmentationClassAug_Visualization/2010_000330.png inflating: SegmentationClassAug_Visualization/2011_001208.png inflating: SegmentationClassAug_Visualization/2009_001308.png inflating: SegmentationClassAug_Visualization/2010_004081.png inflating: SegmentationClassAug_Visualization/2007_009458.png inflating: SegmentationClassAug_Visualization/2009_002847.png inflating: SegmentationClassAug_Visualization/2011_000471.png inflating: SegmentationClassAug_Visualization/2010_005626.png inflating: SegmentationClassAug_Visualization/2010_001273.png inflating: SegmentationClassAug_Visualization/2008_002612.png inflating: SegmentationClassAug_Visualization/2011_001524.png inflating: SegmentationClassAug_Visualization/2010_002026.png inflating: SegmentationClassAug_Visualization/2008_002344.png inflating: SegmentationClassAug_Visualization/2008_008297.png inflating: SegmentationClassAug_Visualization/2010_005245.png inflating: SegmentationClassAug_Visualization/2010_001773.png inflating: SegmentationClassAug_Visualization/2011_002244.png inflating: SegmentationClassAug_Visualization/2007_009706.png inflating: SegmentationClassAug_Visualization/2008_007685.png inflating: SegmentationClassAug_Visualization/2008_008461.png inflating: SegmentationClassAug_Visualization/2009_002386.png inflating: SegmentationClassAug_Visualization/2010_002556.png inflating: SegmentationClassAug_Visualization/2010_002346.png inflating: SegmentationClassAug_Visualization/2009_003253.png inflating: SegmentationClassAug_Visualization/2008_004549.png inflating: SegmentationClassAug_Visualization/2009_000184.png inflating: SegmentationClassAug_Visualization/2010_000805.png inflating: SegmentationClassAug_Visualization/2008_002579.png inflating: SegmentationClassAug_Visualization/2008_001461.png inflating: SegmentationClassAug_Visualization/2007_009323.png inflating: SegmentationClassAug_Visualization/2009_004721.png inflating: SegmentationClassAug_Visualization/2011_000465.png inflating: SegmentationClassAug_Visualization/2009_004140.png inflating: SegmentationClassAug_Visualization/2008_002999.png inflating: SegmentationClassAug_Visualization/2010_001776.png inflating: SegmentationClassAug_Visualization/2009_001205.png inflating: SegmentationClassAug_Visualization/2010_002283.png inflating: SegmentationClassAug_Visualization/2008_002335.png inflating: SegmentationClassAug_Visualization/2010_003609.png inflating: SegmentationClassAug_Visualization/2008_007265.png inflating: SegmentationClassAug_Visualization/2008_006235.png inflating: SegmentationClassAug_Visualization/2010_000209.png inflating: SegmentationClassAug_Visualization/2011_003048.png inflating: SegmentationClassAug_Visualization/2007_002268.png inflating: SegmentationClassAug_Visualization/2009_001344.png inflating: SegmentationClassAug_Visualization/2010_004209.png inflating: SegmentationClassAug_Visualization/2008_004581.png inflating: SegmentationClassAug_Visualization/2008_005071.png inflating: SegmentationClassAug_Visualization/2011_002113.png inflating: SegmentationClassAug_Visualization/2009_003172.png inflating: SegmentationClassAug_Visualization/2008_003635.png inflating: SegmentationClassAug_Visualization/2008_004634.png inflating: SegmentationClassAug_Visualization/2008_001783.png inflating: SegmentationClassAug_Visualization/2009_000212.png inflating: SegmentationClassAug_Visualization/2008_006776.png inflating: SegmentationClassAug_Visualization/2011_001432.png inflating: SegmentationClassAug_Visualization/2008_001967.png inflating: SegmentationClassAug_Visualization/2008_000464.png inflating: SegmentationClassAug_Visualization/2009_002893.png inflating: SegmentationClassAug_Visualization/2010_004143.png inflating: SegmentationClassAug_Visualization/2011_002775.png inflating: SegmentationClassAug_Visualization/2011_001624.png inflating: SegmentationClassAug_Visualization/2008_004794.png inflating: SegmentationClassAug_Visualization/2009_003489.png inflating: SegmentationClassAug_Visualization/2008_006855.png inflating: SegmentationClassAug_Visualization/2008_008421.png inflating: SegmentationClassAug_Visualization/2010_000602.png inflating: SegmentationClassAug_Visualization/2011_001705.png inflating: SegmentationClassAug_Visualization/2009_002749.png inflating: SegmentationClassAug_Visualization/2007_007624.png inflating: SegmentationClassAug_Visualization/2008_003313.png inflating: SegmentationClassAug_Visualization/2007_008260.png inflating: SegmentationClassAug_Visualization/2008_004670.png inflating: SegmentationClassAug_Visualization/2008_007877.png inflating: SegmentationClassAug_Visualization/2010_003532.png inflating: SegmentationClassAug_Visualization/2011_001503.png inflating: SegmentationClassAug_Visualization/2008_003559.png inflating: SegmentationClassAug_Visualization/2008_004520.png inflating: SegmentationClassAug_Visualization/2009_004716.png inflating: SegmentationClassAug_Visualization/2010_003747.png inflating: SegmentationClassAug_Visualization/2009_004958.png inflating: SegmentationClassAug_Visualization/2009_003601.png inflating: SegmentationClassAug_Visualization/2010_003755.png inflating: SegmentationClassAug_Visualization/2008_006317.png inflating: SegmentationClassAug_Visualization/2010_002594.png inflating: SegmentationClassAug_Visualization/2011_000048.png inflating: SegmentationClassAug_Visualization/2010_002688.png inflating: SegmentationClassAug_Visualization/2010_001234.png inflating: SegmentationClassAug_Visualization/2009_004261.png inflating: SegmentationClassAug_Visualization/2008_005953.png inflating: SegmentationClassAug_Visualization/2010_002813.png inflating: SegmentationClassAug_Visualization/2010_002891.png inflating: SegmentationClassAug_Visualization/2009_001163.png inflating: SegmentationClassAug_Visualization/2007_000061.png inflating: SegmentationClassAug_Visualization/2009_005145.png inflating: SegmentationClassAug_Visualization/2010_001282.png inflating: SegmentationClassAug_Visualization/2010_003260.png inflating: SegmentationClassAug_Visualization/2009_003347.png inflating: SegmentationClassAug_Visualization/2011_001866.png inflating: SegmentationClassAug_Visualization/2009_000696.png inflating: SegmentationClassAug_Visualization/2011_002158.png inflating: SegmentationClassAug_Visualization/2011_000258.png inflating: SegmentationClassAug_Visualization/2008_002113.png inflating: SegmentationClassAug_Visualization/2009_000385.png inflating: SegmentationClassAug_Visualization/2009_001549.png inflating: SegmentationClassAug_Visualization/2010_003362.png inflating: SegmentationClassAug_Visualization/2010_004288.png inflating: SegmentationClassAug_Visualization/2008_006394.png inflating: SegmentationClassAug_Visualization/2010_001275.png inflating: SegmentationClassAug_Visualization/2010_005978.png inflating: SegmentationClassAug_Visualization/2010_001497.png inflating: SegmentationClassAug_Visualization/2008_002527.png inflating: SegmentationClassAug_Visualization/2010_002696.png inflating: SegmentationClassAug_Visualization/2009_000214.png inflating: SegmentationClassAug_Visualization/2008_003707.png inflating: SegmentationClassAug_Visualization/2008_001986.png inflating: SegmentationClassAug_Visualization/2009_003118.png inflating: SegmentationClassAug_Visualization/2010_003149.png inflating: SegmentationClassAug_Visualization/2008_005978.png inflating: SegmentationClassAug_Visualization/2008_008467.png inflating: SegmentationClassAug_Visualization/2008_005455.png inflating: SegmentationClassAug_Visualization/2010_003051.png inflating: SegmentationClassAug_Visualization/2011_000536.png inflating: SegmentationClassAug_Visualization/2011_002583.png inflating: SegmentationClassAug_Visualization/2007_004423.png inflating: SegmentationClassAug_Visualization/2008_007286.png inflating: SegmentationClassAug_Visualization/2008_001007.png inflating: SegmentationClassAug_Visualization/2009_004078.png inflating: SegmentationClassAug_Visualization/2010_003279.png inflating: SegmentationClassAug_Visualization/2008_008359.png inflating: SegmentationClassAug_Visualization/2008_003384.png inflating: SegmentationClassAug_Visualization/2009_001804.png inflating: SegmentationClassAug_Visualization/2009_002056.png inflating: SegmentationClassAug_Visualization/2008_000202.png inflating: SegmentationClassAug_Visualization/2008_002583.png inflating: SegmentationClassAug_Visualization/2009_004994.png inflating: SegmentationClassAug_Visualization/2008_007057.png inflating: SegmentationClassAug_Visualization/2010_002938.png inflating: SegmentationClassAug_Visualization/2010_004896.png inflating: SegmentationClassAug_Visualization/2010_000076.png inflating: SegmentationClassAug_Visualization/2011_002579.png inflating: SegmentationClassAug_Visualization/2008_005563.png inflating: SegmentationClassAug_Visualization/2009_001835.png inflating: SegmentationClassAug_Visualization/2008_006028.png inflating: SegmentationClassAug_Visualization/2011_000477.png inflating: SegmentationClassAug_Visualization/2011_000444.png inflating: SegmentationClassAug_Visualization/2008_004387.png inflating: SegmentationClassAug_Visualization/2010_001452.png inflating: SegmentationClassAug_Visualization/2010_002841.png inflating: SegmentationClassAug_Visualization/2011_001471.png inflating: SegmentationClassAug_Visualization/2011_002284.png inflating: SegmentationClassAug_Visualization/2009_002984.png inflating: SegmentationClassAug_Visualization/2010_005853.png inflating: SegmentationClassAug_Visualization/2008_005283.png inflating: SegmentationClassAug_Visualization/2008_005145.png inflating: SegmentationClassAug_Visualization/2008_000328.png inflating: SegmentationClassAug_Visualization/2011_002327.png inflating: SegmentationClassAug_Visualization/2009_001663.png inflating: SegmentationClassAug_Visualization/2007_007480.png inflating: SegmentationClassAug_Visualization/2009_003348.png inflating: SegmentationClassAug_Visualization/2008_007108.png inflating: SegmentationClassAug_Visualization/2008_005534.png inflating: SegmentationClassAug_Visualization/2009_003500.png inflating: SegmentationClassAug_Visualization/2011_001655.png inflating: SegmentationClassAug_Visualization/2008_004995.png inflating: SegmentationClassAug_Visualization/2009_003064.png inflating: SegmentationClassAug_Visualization/2011_002457.png inflating: SegmentationClassAug_Visualization/2007_007477.png inflating: SegmentationClassAug_Visualization/2010_001052.png inflating: SegmentationClassAug_Visualization/2007_002565.png inflating: SegmentationClassAug_Visualization/2009_000828.png inflating: SegmentationClassAug_Visualization/2009_001522.png inflating: SegmentationClassAug_Visualization/2009_001526.png inflating: SegmentationClassAug_Visualization/2011_000655.png inflating: SegmentationClassAug_Visualization/2010_006033.png inflating: SegmentationClassAug_Visualization/2008_003680.png inflating: SegmentationClassAug_Visualization/2009_004567.png inflating: SegmentationClassAug_Visualization/2008_006325.png inflating: SegmentationClassAug_Visualization/2009_004804.png inflating: SegmentationClassAug_Visualization/2009_004556.png inflating: SegmentationClassAug_Visualization/2010_005995.png inflating: SegmentationClassAug_Visualization/2009_003400.png inflating: SegmentationClassAug_Visualization/2010_004933.png inflating: SegmentationClassAug_Visualization/2011_001137.png inflating: SegmentationClassAug_Visualization/2010_000855.png inflating: SegmentationClassAug_Visualization/2009_002975.png inflating: SegmentationClassAug_Visualization/2009_003660.png inflating: SegmentationClassAug_Visualization/2008_004538.png inflating: SegmentationClassAug_Visualization/2009_002226.png inflating: SegmentationClassAug_Visualization/2009_002488.png inflating: SegmentationClassAug_Visualization/2010_005664.png inflating: SegmentationClassAug_Visualization/2009_002978.png inflating: SegmentationClassAug_Visualization/2008_002425.png inflating: SegmentationClassAug_Visualization/2010_002877.png inflating: SegmentationClassAug_Visualization/2008_005181.png inflating: SegmentationClassAug_Visualization/2008_008632.png inflating: SegmentationClassAug_Visualization/2009_000816.png inflating: SegmentationClassAug_Visualization/2008_007668.png inflating: SegmentationClassAug_Visualization/2011_002881.png inflating: SegmentationClassAug_Visualization/2008_006289.png inflating: SegmentationClassAug_Visualization/2008_000848.png inflating: SegmentationClassAug_Visualization/2008_008770.png inflating: SegmentationClassAug_Visualization/2009_001871.png inflating: SegmentationClassAug_Visualization/2010_004163.png inflating: SegmentationClassAug_Visualization/2009_002885.png inflating: SegmentationClassAug_Visualization/2008_001775.png inflating: SegmentationClassAug_Visualization/2007_003668.png inflating: SegmentationClassAug_Visualization/2010_003703.png inflating: SegmentationClassAug_Visualization/2009_002094.png inflating: SegmentationClassAug_Visualization/2011_002535.png inflating: SegmentationClassAug_Visualization/2008_000343.png inflating: SegmentationClassAug_Visualization/2009_001934.png inflating: SegmentationClassAug_Visualization/2008_005561.png inflating: SegmentationClassAug_Visualization/2008_000054.png inflating: SegmentationClassAug_Visualization/2009_002302.png inflating: SegmentationClassAug_Visualization/2008_006221.png inflating: SegmentationClassAug_Visualization/2008_002182.png inflating: SegmentationClassAug_Visualization/2010_006082.png inflating: SegmentationClassAug_Visualization/2008_005068.png inflating: SegmentationClassAug_Visualization/2010_005136.png inflating: SegmentationClassAug_Visualization/2008_004505.png inflating: SegmentationClassAug_Visualization/2008_003316.png inflating: SegmentationClassAug_Visualization/2010_000572.png inflating: SegmentationClassAug_Visualization/2010_002811.png inflating: SegmentationClassAug_Visualization/2011_001016.png inflating: SegmentationClassAug_Visualization/2009_000145.png inflating: SegmentationClassAug_Visualization/2008_001336.png inflating: SegmentationClassAug_Visualization/2011_001220.png inflating: SegmentationClassAug_Visualization/2010_002050.png inflating: SegmentationClassAug_Visualization/2008_001040.png inflating: SegmentationClassAug_Visualization/2008_007417.png inflating: SegmentationClassAug_Visualization/2008_006386.png inflating: SegmentationClassAug_Visualization/2010_005993.png inflating: SegmentationClassAug_Visualization/2009_000695.png inflating: SegmentationClassAug_Visualization/2008_007915.png inflating: SegmentationClassAug_Visualization/2008_007084.png inflating: SegmentationClassAug_Visualization/2010_006066.png inflating: SegmentationClassAug_Visualization/2008_001115.png inflating: SegmentationClassAug_Visualization/2010_001505.png inflating: SegmentationClassAug_Visualization/2009_004096.png inflating: SegmentationClassAug_Visualization/2010_002860.png inflating: SegmentationClassAug_Visualization/2008_008480.png inflating: SegmentationClassAug_Visualization/2010_002937.png inflating: SegmentationClassAug_Visualization/2010_002921.png inflating: SegmentationClassAug_Visualization/2009_002434.png inflating: SegmentationClassAug_Visualization/2010_003779.png inflating: SegmentationClassAug_Visualization/2011_000809.png inflating: SegmentationClassAug_Visualization/2008_006474.png inflating: SegmentationClassAug_Visualization/2011_001192.png inflating: SegmentationClassAug_Visualization/2008_004092.png inflating: SegmentationClassAug_Visualization/2011_001169.png inflating: SegmentationClassAug_Visualization/2011_001389.png inflating: SegmentationClassAug_Visualization/2009_002612.png inflating: SegmentationClassAug_Visualization/2008_008211.png inflating: SegmentationClassAug_Visualization/2008_007528.png inflating: SegmentationClassAug_Visualization/2011_000784.png inflating: SegmentationClassAug_Visualization/2010_001370.png inflating: SegmentationClassAug_Visualization/2010_001944.png inflating: SegmentationClassAug_Visualization/2010_000682.png inflating: SegmentationClassAug_Visualization/2009_001095.png inflating: SegmentationClassAug_Visualization/2009_002933.png inflating: SegmentationClassAug_Visualization/2011_002241.png inflating: SegmentationClassAug_Visualization/2011_000496.png inflating: SegmentationClassAug_Visualization/2009_000661.png inflating: SegmentationClassAug_Visualization/2009_004007.png inflating: SegmentationClassAug_Visualization/2011_002335.png inflating: SegmentationClassAug_Visualization/2010_002684.png inflating: SegmentationClassAug_Visualization/2008_008380.png inflating: SegmentationClassAug_Visualization/2010_002839.png inflating: SegmentationClassAug_Visualization/2008_006631.png inflating: SegmentationClassAug_Visualization/2009_000494.png inflating: SegmentationClassAug_Visualization/2007_004841.png inflating: SegmentationClassAug_Visualization/2008_006104.png inflating: SegmentationClassAug_Visualization/2010_000530.png inflating: SegmentationClassAug_Visualization/2008_004246.png inflating: SegmentationClassAug_Visualization/2008_002278.png inflating: SegmentationClassAug_Visualization/2009_003454.png inflating: SegmentationClassAug_Visualization/2010_002424.png inflating: SegmentationClassAug_Visualization/2009_003189.png inflating: SegmentationClassAug_Visualization/2008_000413.png inflating: SegmentationClassAug_Visualization/2011_002096.png inflating: SegmentationClassAug_Visualization/2010_005791.png inflating: SegmentationClassAug_Visualization/2008_004777.png inflating: SegmentationClassAug_Visualization/2009_000987.png inflating: SegmentationClassAug_Visualization/2009_004697.png inflating: SegmentationClassAug_Visualization/2011_001084.png inflating: SegmentationClassAug_Visualization/2008_000339.png inflating: SegmentationClassAug_Visualization/2010_005359.png inflating: SegmentationClassAug_Visualization/2009_001740.png inflating: SegmentationClassAug_Visualization/2009_001466.png inflating: SegmentationClassAug_Visualization/2009_004231.png inflating: SegmentationClassAug_Visualization/2009_004161.png inflating: SegmentationClassAug_Visualization/2011_002551.png inflating: SegmentationClassAug_Visualization/2010_005903.png inflating: SegmentationClassAug_Visualization/2010_000759.png inflating: SegmentationClassAug_Visualization/2010_003569.png inflating: SegmentationClassAug_Visualization/2010_005906.png inflating: SegmentationClassAug_Visualization/2008_004289.png inflating: SegmentationClassAug_Visualization/2010_005820.png inflating: SegmentationClassAug_Visualization/2008_005846.png inflating: SegmentationClassAug_Visualization/2010_005068.png inflating: SegmentationClassAug_Visualization/2008_001649.png inflating: SegmentationClassAug_Visualization/2008_006602.png inflating: SegmentationClassAug_Visualization/2008_007902.png inflating: SegmentationClassAug_Visualization/2007_009655.png inflating: SegmentationClassAug_Visualization/2009_005031.png inflating: SegmentationClassAug_Visualization/2010_004219.png inflating: SegmentationClassAug_Visualization/2010_004197.png inflating: SegmentationClassAug_Visualization/2007_003022.png inflating: SegmentationClassAug_Visualization/2009_001361.png inflating: SegmentationClassAug_Visualization/2007_000636.png inflating: SegmentationClassAug_Visualization/2011_001847.png inflating: SegmentationClassAug_Visualization/2010_005046.png inflating: SegmentationClassAug_Visualization/2008_005260.png inflating: SegmentationClassAug_Visualization/2011_003182.png inflating: SegmentationClassAug_Visualization/2007_009832.png inflating: SegmentationClassAug_Visualization/2008_006370.png inflating: SegmentationClassAug_Visualization/2009_003522.png inflating: SegmentationClassAug_Visualization/2009_001412.png inflating: SegmentationClassAug_Visualization/2008_004276.png inflating: SegmentationClassAug_Visualization/2007_009446.png inflating: SegmentationClassAug_Visualization/2009_003459.png inflating: SegmentationClassAug_Visualization/2009_004134.png inflating: SegmentationClassAug_Visualization/2008_000780.png inflating: SegmentationClassAug_Visualization/2009_003905.png inflating: SegmentationClassAug_Visualization/2008_007591.png inflating: SegmentationClassAug_Visualization/2009_004799.png inflating: SegmentationClassAug_Visualization/2011_001922.png inflating: SegmentationClassAug_Visualization/2008_006665.png inflating: SegmentationClassAug_Visualization/2008_003523.png inflating: SegmentationClassAug_Visualization/2011_003154.png inflating: SegmentationClassAug_Visualization/2010_001301.png inflating: SegmentationClassAug_Visualization/2011_002075.png inflating: SegmentationClassAug_Visualization/2008_000666.png inflating: SegmentationClassAug_Visualization/2008_007855.png inflating: SegmentationClassAug_Visualization/2008_006046.png inflating: SegmentationClassAug_Visualization/2010_002060.png inflating: SegmentationClassAug_Visualization/2008_006335.png inflating: SegmentationClassAug_Visualization/2010_004741.png inflating: SegmentationClassAug_Visualization/2011_001971.png inflating: SegmentationClassAug_Visualization/2010_003169.png inflating: SegmentationClassAug_Visualization/2007_006549.png inflating: SegmentationClassAug_Visualization/2008_001871.png inflating: SegmentationClassAug_Visualization/2008_003107.png inflating: SegmentationClassAug_Visualization/2010_001563.png inflating: SegmentationClassAug_Visualization/2010_000091.png inflating: SegmentationClassAug_Visualization/2010_002678.png inflating: SegmentationClassAug_Visualization/2010_003688.png inflating: SegmentationClassAug_Visualization/2008_005266.png inflating: SegmentationClassAug_Visualization/2009_004309.png inflating: SegmentationClassAug_Visualization/2008_003560.png inflating: SegmentationClassAug_Visualization/2009_004165.png inflating: SegmentationClassAug_Visualization/2009_004527.png inflating: SegmentationClassAug_Visualization/2010_005836.png inflating: SegmentationClassAug_Visualization/2011_000009.png inflating: SegmentationClassAug_Visualization/2011_002269.png inflating: SegmentationClassAug_Visualization/2008_004046.png inflating: SegmentationClassAug_Visualization/2008_001322.png inflating: SegmentationClassAug_Visualization/2009_002962.png inflating: SegmentationClassAug_Visualization/2009_002611.png inflating: SegmentationClassAug_Visualization/2009_001393.png inflating: SegmentationClassAug_Visualization/2007_001185.png inflating: SegmentationClassAug_Visualization/2010_005576.png inflating: SegmentationClassAug_Visualization/2010_001562.png inflating: SegmentationClassAug_Visualization/2008_002504.png inflating: SegmentationClassAug_Visualization/2009_002264.png inflating: SegmentationClassAug_Visualization/2010_003481.png inflating: SegmentationClassAug_Visualization/2009_003377.png inflating: SegmentationClassAug_Visualization/2007_007891.png inflating: SegmentationClassAug_Visualization/2008_007231.png inflating: SegmentationClassAug_Visualization/2008_002890.png inflating: SegmentationClassAug_Visualization/2010_002845.png inflating: SegmentationClassAug_Visualization/2010_002896.png inflating: SegmentationClassAug_Visualization/2008_001697.png inflating: SegmentationClassAug_Visualization/2011_001566.png inflating: SegmentationClassAug_Visualization/2011_002291.png inflating: SegmentationClassAug_Visualization/2009_001147.png inflating: SegmentationClassAug_Visualization/2008_001498.png inflating: SegmentationClassAug_Visualization/2010_003629.png inflating: SegmentationClassAug_Visualization/2011_001350.png inflating: SegmentationClassAug_Visualization/2008_004513.png inflating: SegmentationClassAug_Visualization/2009_003312.png inflating: SegmentationClassAug_Visualization/2010_003643.png inflating: SegmentationClassAug_Visualization/2011_002177.png inflating: SegmentationClassAug_Visualization/2008_007118.png inflating: SegmentationClassAug_Visualization/2009_002155.png inflating: SegmentationClassAug_Visualization/2010_001215.png inflating: SegmentationClassAug_Visualization/2010_002439.png inflating: SegmentationClassAug_Visualization/2008_002515.png inflating: SegmentationClassAug_Visualization/2008_004015.png inflating: SegmentationClassAug_Visualization/2009_004328.png inflating: SegmentationClassAug_Visualization/2011_000229.png inflating: SegmentationClassAug_Visualization/2009_002024.png inflating: SegmentationClassAug_Visualization/2008_006092.png inflating: SegmentationClassAug_Visualization/2009_003756.png inflating: SegmentationClassAug_Visualization/2010_001193.png inflating: SegmentationClassAug_Visualization/2008_005643.png inflating: SegmentationClassAug_Visualization/2007_004459.png inflating: SegmentationClassAug_Visualization/2010_005110.png inflating: SegmentationClassAug_Visualization/2008_008519.png inflating: SegmentationClassAug_Visualization/2009_004361.png inflating: SegmentationClassAug_Visualization/2008_000510.png inflating: SegmentationClassAug_Visualization/2008_007114.png inflating: SegmentationClassAug_Visualization/2010_005882.png inflating: SegmentationClassAug_Visualization/2008_001692.png inflating: SegmentationClassAug_Visualization/2010_001271.png inflating: SegmentationClassAug_Visualization/2008_003003.png inflating: SegmentationClassAug_Visualization/2009_001484.png inflating: SegmentationClassAug_Visualization/2008_006865.png inflating: SegmentationClassAug_Visualization/2009_003010.png inflating: SegmentationClassAug_Visualization/2010_002605.png inflating: SegmentationClassAug_Visualization/2011_001986.png inflating: SegmentationClassAug_Visualization/2010_004238.png inflating: SegmentationClassAug_Visualization/2008_000421.png inflating: SegmentationClassAug_Visualization/2010_000737.png inflating: SegmentationClassAug_Visualization/2010_000695.png inflating: SegmentationClassAug_Visualization/2009_003688.png inflating: SegmentationClassAug_Visualization/2010_003153.png inflating: SegmentationClassAug_Visualization/2008_002988.png inflating: SegmentationClassAug_Visualization/2009_001823.png inflating: SegmentationClassAug_Visualization/2011_002429.png inflating: SegmentationClassAug_Visualization/2010_003375.png inflating: SegmentationClassAug_Visualization/2010_002628.png inflating: SegmentationClassAug_Visualization/2008_006951.png inflating: SegmentationClassAug_Visualization/2011_001464.png inflating: SegmentationClassAug_Visualization/2008_002487.png inflating: SegmentationClassAug_Visualization/2009_003415.png inflating: SegmentationClassAug_Visualization/2010_004469.png inflating: SegmentationClassAug_Visualization/2008_001307.png inflating: SegmentationClassAug_Visualization/2008_000683.png inflating: SegmentationClassAug_Visualization/2011_001375.png inflating: SegmentationClassAug_Visualization/2009_003066.png inflating: SegmentationClassAug_Visualization/2008_004502.png inflating: SegmentationClassAug_Visualization/2007_002099.png inflating: SegmentationClassAug_Visualization/2009_003494.png inflating: SegmentationClassAug_Visualization/2009_002253.png inflating: SegmentationClassAug_Visualization/2010_004362.png inflating: SegmentationClassAug_Visualization/2010_005206.png inflating: SegmentationClassAug_Visualization/2011_000768.png inflating: SegmentationClassAug_Visualization/2007_006076.png inflating: SegmentationClassAug_Visualization/2008_003504.png inflating: SegmentationClassAug_Visualization/2010_003534.png inflating: SegmentationClassAug_Visualization/2008_002932.png inflating: SegmentationClassAug_Visualization/2009_004019.png inflating: SegmentationClassAug_Visualization/2011_000138.png inflating: SegmentationClassAug_Visualization/2009_003697.png inflating: SegmentationClassAug_Visualization/2007_009709.png inflating: SegmentationClassAug_Visualization/2008_005863.png inflating: SegmentationClassAug_Visualization/2008_001081.png inflating: SegmentationClassAug_Visualization/2007_004510.png inflating: SegmentationClassAug_Visualization/2010_000302.png inflating: SegmentationClassAug_Visualization/2008_001653.png inflating: SegmentationClassAug_Visualization/2008_001121.png inflating: SegmentationClassAug_Visualization/2008_007103.png inflating: SegmentationClassAug_Visualization/2009_000545.png inflating: SegmentationClassAug_Visualization/2010_004361.png inflating: SegmentationClassAug_Visualization/2009_001391.png inflating: SegmentationClassAug_Visualization/2009_004497.png inflating: SegmentationClassAug_Visualization/2010_001054.png inflating: SegmentationClassAug_Visualization/2011_000669.png inflating: SegmentationClassAug_Visualization/2008_006874.png inflating: SegmentationClassAug_Visualization/2007_000452.png inflating: SegmentationClassAug_Visualization/2010_004642.png inflating: SegmentationClassAug_Visualization/2009_000059.png inflating: SegmentationClassAug_Visualization/2009_000748.png inflating: SegmentationClassAug_Visualization/2008_002564.png inflating: SegmentationClassAug_Visualization/2008_005962.png inflating: SegmentationClassAug_Visualization/2011_002979.png inflating: SegmentationClassAug_Visualization/2010_001024.png inflating: SegmentationClassAug_Visualization/2009_003938.png inflating: SegmentationClassAug_Visualization/2008_003018.png inflating: SegmentationClassAug_Visualization/2009_003110.png inflating: SegmentationClassAug_Visualization/2008_003805.png inflating: SegmentationClassAug_Visualization/2009_001225.png inflating: SegmentationClassAug_Visualization/2009_002390.png inflating: SegmentationClassAug_Visualization/2009_000731.png inflating: SegmentationClassAug_Visualization/2008_008318.png inflating: SegmentationClassAug_Visualization/2008_001907.png inflating: SegmentationClassAug_Visualization/2009_003417.png inflating: SegmentationClassAug_Visualization/2011_002678.png inflating: SegmentationClassAug_Visualization/2010_001973.png inflating: SegmentationClassAug_Visualization/2008_007510.png inflating: SegmentationClassAug_Visualization/2009_001874.png inflating: SegmentationClassAug_Visualization/2008_008341.png inflating: SegmentationClassAug_Visualization/2011_000770.png inflating: SegmentationClassAug_Visualization/2009_003961.png inflating: SegmentationClassAug_Visualization/2007_009665.png inflating: SegmentationClassAug_Visualization/2008_001466.png inflating: SegmentationClassAug_Visualization/2011_001369.png inflating: SegmentationClassAug_Visualization/2009_001735.png inflating: SegmentationClassAug_Visualization/2007_006864.png inflating: SegmentationClassAug_Visualization/2009_001589.png inflating: SegmentationClassAug_Visualization/2011_002616.png inflating: SegmentationClassAug_Visualization/2008_000291.png inflating: SegmentationClassAug_Visualization/2008_006129.png inflating: SegmentationClassAug_Visualization/2008_002032.png inflating: SegmentationClassAug_Visualization/2008_004613.png inflating: SegmentationClassAug_Visualization/2008_004647.png inflating: SegmentationClassAug_Visualization/2009_000157.png inflating: SegmentationClassAug_Visualization/2008_003905.png inflating: SegmentationClassAug_Visualization/2008_000548.png inflating: SegmentationClassAug_Visualization/2010_002462.png inflating: SegmentationClassAug_Visualization/2009_004971.png inflating: SegmentationClassAug_Visualization/2009_004728.png inflating: SegmentationClassAug_Visualization/2010_000453.png inflating: SegmentationClassAug_Visualization/2009_001653.png inflating: SegmentationClassAug_Visualization/2008_004036.png inflating: SegmentationClassAug_Visualization/2008_005502.png inflating: SegmentationClassAug_Visualization/2011_000692.png inflating: SegmentationClassAug_Visualization/2009_000831.png inflating: SegmentationClassAug_Visualization/2008_006696.png inflating: SegmentationClassAug_Visualization/2008_005695.png inflating: SegmentationClassAug_Visualization/2008_001577.png inflating: SegmentationClassAug_Visualization/2010_000938.png inflating: SegmentationClassAug_Visualization/2008_006271.png inflating: SegmentationClassAug_Visualization/2008_001735.png inflating: SegmentationClassAug_Visualization/2008_003242.png inflating: SegmentationClassAug_Visualization/2008_005013.png inflating: SegmentationClassAug_Visualization/2008_002864.png inflating: SegmentationClassAug_Visualization/2009_001102.png inflating: SegmentationClassAug_Visualization/2007_005304.png inflating: SegmentationClassAug_Visualization/2009_002901.png inflating: SegmentationClassAug_Visualization/2008_003209.png inflating: SegmentationClassAug_Visualization/2011_000400.png inflating: SegmentationClassAug_Visualization/2008_006748.png inflating: SegmentationClassAug_Visualization/2010_001783.png inflating: SegmentationClassAug_Visualization/2008_003335.png inflating: SegmentationClassAug_Visualization/2008_007812.png inflating: SegmentationClassAug_Visualization/2008_008109.png inflating: SegmentationClassAug_Visualization/2008_006944.png inflating: SegmentationClassAug_Visualization/2010_000975.png inflating: SegmentationClassAug_Visualization/2009_002638.png inflating: SegmentationClassAug_Visualization/2009_001375.png inflating: SegmentationClassAug_Visualization/2011_000683.png inflating: SegmentationClassAug_Visualization/2009_005302.png inflating: SegmentationClassAug_Visualization/2009_000962.png inflating: SegmentationClassAug_Visualization/2008_007736.png inflating: SegmentationClassAug_Visualization/2009_001126.png inflating: SegmentationClassAug_Visualization/2008_001158.png inflating: SegmentationClassAug_Visualization/2008_001448.png inflating: SegmentationClassAug_Visualization/2010_003931.png inflating: SegmentationClassAug_Visualization/2007_000549.png inflating: SegmentationClassAug_Visualization/2010_000175.png inflating: SegmentationClassAug_Visualization/2010_003729.png inflating: SegmentationClassAug_Visualization/2008_002752.png inflating: SegmentationClassAug_Visualization/2009_003144.png inflating: SegmentationClassAug_Visualization/2009_002713.png inflating: SegmentationClassAug_Visualization/2009_002921.png inflating: SegmentationClassAug_Visualization/2008_008423.png inflating: SegmentationClassAug_Visualization/2010_001986.png inflating: SegmentationClassAug_Visualization/2008_005078.png inflating: SegmentationClassAug_Visualization/2008_004265.png inflating: SegmentationClassAug_Visualization/2009_003810.png inflating: SegmentationClassAug_Visualization/2009_004022.png inflating: SegmentationClassAug_Visualization/2009_004677.png inflating: SegmentationClassAug_Visualization/2008_001745.png inflating: SegmentationClassAug_Visualization/2008_003852.png inflating: SegmentationClassAug_Visualization/2008_001451.png inflating: SegmentationClassAug_Visualization/2010_002208.png inflating: SegmentationClassAug_Visualization/2008_004426.png inflating: SegmentationClassAug_Visualization/2007_001724.png inflating: SegmentationClassAug_Visualization/2008_002162.png inflating: SegmentationClassAug_Visualization/2009_004830.png inflating: SegmentationClassAug_Visualization/2010_000604.png inflating: SegmentationClassAug_Visualization/2008_006965.png inflating: SegmentationClassAug_Visualization/2010_004789.png inflating: SegmentationClassAug_Visualization/2010_000244.png inflating: SegmentationClassAug_Visualization/2009_000577.png inflating: SegmentationClassAug_Visualization/2010_003240.png inflating: SegmentationClassAug_Visualization/2009_003406.png inflating: SegmentationClassAug_Visualization/2011_000102.png inflating: SegmentationClassAug_Visualization/2008_008567.png inflating: SegmentationClassAug_Visualization/2008_003489.png inflating: SegmentationClassAug_Visualization/2010_002271.png inflating: SegmentationClassAug_Visualization/2008_008337.png inflating: SegmentationClassAug_Visualization/2011_001031.png inflating: SegmentationClassAug_Visualization/2009_002920.png inflating: SegmentationClassAug_Visualization/2010_005635.png inflating: SegmentationClassAug_Visualization/2008_002705.png inflating: SegmentationClassAug_Visualization/2009_000789.png inflating: SegmentationClassAug_Visualization/2010_002781.png inflating: SegmentationClassAug_Visualization/2009_005069.png inflating: SegmentationClassAug_Visualization/2009_004734.png inflating: SegmentationClassAug_Visualization/2010_005937.png inflating: SegmentationClassAug_Visualization/2009_004542.png inflating: SegmentationClassAug_Visualization/2010_000927.png inflating: SegmentationClassAug_Visualization/2007_003191.png inflating: SegmentationClassAug_Visualization/2010_004655.png inflating: SegmentationClassAug_Visualization/2008_005663.png inflating: SegmentationClassAug_Visualization/2009_004537.png inflating: SegmentationClassAug_Visualization/2011_000755.png inflating: SegmentationClassAug_Visualization/2010_005376.png inflating: SegmentationClassAug_Visualization/2010_000480.png inflating: SegmentationClassAug_Visualization/2011_001357.png inflating: SegmentationClassAug_Visualization/2011_003073.png inflating: SegmentationClassAug_Visualization/2009_000550.png inflating: SegmentationClassAug_Visualization/2008_001655.png inflating: SegmentationClassAug_Visualization/2008_000552.png inflating: SegmentationClassAug_Visualization/2010_005456.png inflating: SegmentationClassAug_Visualization/2010_005522.png inflating: SegmentationClassAug_Visualization/2008_007643.png inflating: SegmentationClassAug_Visualization/2009_001948.png inflating: SegmentationClassAug_Visualization/2007_002227.png inflating: SegmentationClassAug_Visualization/2011_001030.png inflating: SegmentationClassAug_Visualization/2009_004886.png inflating: SegmentationClassAug_Visualization/2010_004692.png inflating: SegmentationClassAug_Visualization/2009_003950.png inflating: SegmentationClassAug_Visualization/2009_002054.png inflating: SegmentationClassAug_Visualization/2008_005748.png inflating: SegmentationClassAug_Visualization/2008_003662.png inflating: SegmentationClassAug_Visualization/2010_003293.png inflating: SegmentationClassAug_Visualization/2010_000024.png inflating: SegmentationClassAug_Visualization/2009_000131.png inflating: SegmentationClassAug_Visualization/2008_004837.png inflating: SegmentationClassAug_Visualization/2008_005677.png inflating: SegmentationClassAug_Visualization/2008_005096.png inflating: SegmentationClassAug_Visualization/2010_000309.png inflating: SegmentationClassAug_Visualization/2008_002194.png inflating: SegmentationClassAug_Visualization/2008_004726.png inflating: SegmentationClassAug_Visualization/2011_001336.png inflating: SegmentationClassAug_Visualization/2009_003022.png inflating: SegmentationClassAug_Visualization/2010_003665.png inflating: SegmentationClassAug_Visualization/2008_002715.png inflating: SegmentationClassAug_Visualization/2010_002907.png inflating: SegmentationClassAug_Visualization/2008_003220.png inflating: SegmentationClassAug_Visualization/2008_000622.png inflating: SegmentationClassAug_Visualization/2007_003580.png inflating: SegmentationClassAug_Visualization/2009_002790.png inflating: SegmentationClassAug_Visualization/2010_005981.png inflating: SegmentationClassAug_Visualization/2011_000596.png inflating: SegmentationClassAug_Visualization/2011_001330.png inflating: SegmentationClassAug_Visualization/2011_003109.png inflating: SegmentationClassAug_Visualization/2008_003015.png inflating: SegmentationClassAug_Visualization/2010_005744.png inflating: SegmentationClassAug_Visualization/2009_001462.png inflating: SegmentationClassAug_Visualization/2010_001687.png inflating: SegmentationClassAug_Visualization/2008_008406.png inflating: SegmentationClassAug_Visualization/2011_002556.png inflating: SegmentationClassAug_Visualization/2010_001184.png inflating: SegmentationClassAug_Visualization/2009_003804.png inflating: SegmentationClassAug_Visualization/2008_007524.png inflating: SegmentationClassAug_Visualization/2009_001922.png inflating: SegmentationClassAug_Visualization/2010_004380.png inflating: SegmentationClassAug_Visualization/2008_000491.png inflating: SegmentationClassAug_Visualization/2010_004102.png inflating: SegmentationClassAug_Visualization/2010_003837.png inflating: SegmentationClassAug_Visualization/2008_000345.png inflating: SegmentationClassAug_Visualization/2011_000815.png inflating: SegmentationClassAug_Visualization/2009_000297.png inflating: SegmentationClassAug_Visualization/2009_003265.png inflating: SegmentationClassAug_Visualization/2008_004795.png inflating: SegmentationClassAug_Visualization/2007_007783.png inflating: SegmentationClassAug_Visualization/2008_002887.png inflating: SegmentationClassAug_Visualization/2009_002053.png inflating: SegmentationClassAug_Visualization/2008_007931.png inflating: SegmentationClassAug_Visualization/2008_006477.png inflating: SegmentationClassAug_Visualization/2008_004934.png inflating: SegmentationClassAug_Visualization/2008_005159.png inflating: SegmentationClassAug_Visualization/2008_006980.png inflating: SegmentationClassAug_Visualization/2010_002223.png inflating: SegmentationClassAug_Visualization/2010_004188.png inflating: SegmentationClassAug_Visualization/2008_007383.png inflating: SegmentationClassAug_Visualization/2008_004512.png inflating: SegmentationClassAug_Visualization/2008_002682.png inflating: SegmentationClassAug_Visualization/2008_005853.png inflating: SegmentationClassAug_Visualization/2009_003873.png inflating: SegmentationClassAug_Visualization/2009_005114.png inflating: SegmentationClassAug_Visualization/2009_001781.png inflating: SegmentationClassAug_Visualization/2010_004344.png inflating: SegmentationClassAug_Visualization/2008_005780.png inflating: SegmentationClassAug_Visualization/2011_002132.png inflating: SegmentationClassAug_Visualization/2011_000096.png inflating: SegmentationClassAug_Visualization/2009_002230.png inflating: SegmentationClassAug_Visualization/2008_000090.png inflating: SegmentationClassAug_Visualization/2008_008689.png inflating: SegmentationClassAug_Visualization/2008_007004.png inflating: SegmentationClassAug_Visualization/2010_003944.png inflating: SegmentationClassAug_Visualization/2008_008072.png inflating: SegmentationClassAug_Visualization/2009_003251.png inflating: SegmentationClassAug_Visualization/2008_005324.png inflating: SegmentationClassAug_Visualization/2009_004758.png inflating: SegmentationClassAug_Visualization/2010_004604.png inflating: SegmentationClassAug_Visualization/2008_006074.png inflating: SegmentationClassAug_Visualization/2008_000424.png inflating: SegmentationClassAug_Visualization/2008_001220.png inflating: SegmentationClassAug_Visualization/2008_006462.png inflating: SegmentationClassAug_Visualization/2007_007530.png inflating: SegmentationClassAug_Visualization/2008_005043.png inflating: SegmentationClassAug_Visualization/2008_003582.png inflating: SegmentationClassAug_Visualization/2008_005443.png inflating: SegmentationClassAug_Visualization/2009_001054.png inflating: SegmentationClassAug_Visualization/2007_005074.png inflating: SegmentationClassAug_Visualization/2011_000685.png inflating: SegmentationClassAug_Visualization/2009_003657.png inflating: SegmentationClassAug_Visualization/2010_003214.png inflating: SegmentationClassAug_Visualization/2011_001653.png inflating: SegmentationClassAug_Visualization/2009_001103.png inflating: SegmentationClassAug_Visualization/2009_003739.png inflating: SegmentationClassAug_Visualization/2009_004907.png inflating: SegmentationClassAug_Visualization/2008_001028.png inflating: SegmentationClassAug_Visualization/2009_001820.png inflating: SegmentationClassAug_Visualization/2010_003520.png inflating: SegmentationClassAug_Visualization/2008_006997.png inflating: SegmentationClassAug_Visualization/2009_001493.png inflating: SegmentationClassAug_Visualization/2008_001970.png inflating: SegmentationClassAug_Visualization/2008_000028.png inflating: SegmentationClassAug_Visualization/2009_003993.png inflating: SegmentationClassAug_Visualization/2008_007216.png inflating: SegmentationClassAug_Visualization/2008_006007.png inflating: SegmentationClassAug_Visualization/2009_002407.png inflating: SegmentationClassAug_Visualization/2010_005031.png inflating: SegmentationClassAug_Visualization/2009_003091.png inflating: SegmentationClassAug_Visualization/2011_000347.png inflating: SegmentationClassAug_Visualization/2008_002248.png inflating: SegmentationClassAug_Visualization/2009_002563.png inflating: SegmentationClassAug_Visualization/2009_001498.png inflating: SegmentationClassAug_Visualization/2010_001140.png inflating: SegmentationClassAug_Visualization/2008_000646.png inflating: SegmentationClassAug_Visualization/2008_008073.png inflating: SegmentationClassAug_Visualization/2008_001351.png inflating: SegmentationClassAug_Visualization/2011_002379.png inflating: SegmentationClassAug_Visualization/2009_000619.png inflating: SegmentationClassAug_Visualization/2007_008596.png inflating: SegmentationClassAug_Visualization/2008_007537.png inflating: SegmentationClassAug_Visualization/2010_003300.png inflating: SegmentationClassAug_Visualization/2008_007842.png inflating: SegmentationClassAug_Visualization/2009_003707.png inflating: SegmentationClassAug_Visualization/2009_003125.png inflating: SegmentationClassAug_Visualization/2008_006136.png inflating: SegmentationClassAug_Visualization/2010_002656.png inflating: SegmentationClassAug_Visualization/2011_002854.png inflating: SegmentationClassAug_Visualization/2008_007319.png inflating: SegmentationClassAug_Visualization/2010_003933.png inflating: SegmentationClassAug_Visualization/2009_003043.png inflating: SegmentationClassAug_Visualization/2009_002865.png inflating: SegmentationClassAug_Visualization/2009_004723.png inflating: SegmentationClassAug_Visualization/2009_000926.png inflating: SegmentationClassAug_Visualization/2011_001754.png inflating: SegmentationClassAug_Visualization/2008_005550.png inflating: SegmentationClassAug_Visualization/2010_003724.png inflating: SegmentationClassAug_Visualization/2010_002789.png inflating: SegmentationClassAug_Visualization/2009_004099.png inflating: SegmentationClassAug_Visualization/2011_000502.png inflating: SegmentationClassAug_Visualization/2009_001148.png inflating: SegmentationClassAug_Visualization/2009_003290.png inflating: SegmentationClassAug_Visualization/2010_005658.png inflating: SegmentationClassAug_Visualization/2008_007280.png inflating: SegmentationClassAug_Visualization/2008_008052.png inflating: SegmentationClassAug_Visualization/2008_004684.png inflating: SegmentationClassAug_Visualization/2008_001055.png inflating: SegmentationClassAug_Visualization/2008_004662.png inflating: SegmentationClassAug_Visualization/2009_000904.png inflating: SegmentationClassAug_Visualization/2008_007254.png inflating: SegmentationClassAug_Visualization/2011_000917.png inflating: SegmentationClassAug_Visualization/2010_004157.png inflating: SegmentationClassAug_Visualization/2008_007169.png inflating: SegmentationClassAug_Visualization/2009_004953.png inflating: SegmentationClassAug_Visualization/2007_000645.png inflating: SegmentationClassAug_Visualization/2010_002614.png inflating: SegmentationClassAug_Visualization/2011_003145.png inflating: SegmentationClassAug_Visualization/2008_006868.png inflating: SegmentationClassAug_Visualization/2009_001752.png inflating: SegmentationClassAug_Visualization/2010_003958.png inflating: SegmentationClassAug_Visualization/2009_001360.png inflating: SegmentationClassAug_Visualization/2008_008617.png inflating: SegmentationClassAug_Visualization/2009_002198.png inflating: SegmentationClassAug_Visualization/2007_000661.png inflating: SegmentationClassAug_Visualization/2008_001183.png inflating: SegmentationClassAug_Visualization/2008_000138.png inflating: SegmentationClassAug_Visualization/2008_002312.png inflating: SegmentationClassAug_Visualization/2009_002599.png inflating: SegmentationClassAug_Visualization/2009_001782.png inflating: SegmentationClassAug_Visualization/2010_000459.png inflating: SegmentationClassAug_Visualization/2008_005582.png inflating: SegmentationClassAug_Visualization/2008_000756.png inflating: SegmentationClassAug_Visualization/2010_004601.png inflating: SegmentationClassAug_Visualization/2009_001252.png inflating: SegmentationClassAug_Visualization/2011_001720.png inflating: SegmentationClassAug_Visualization/2009_000833.png inflating: SegmentationClassAug_Visualization/2008_001574.png inflating: SegmentationClassAug_Visualization/2011_002592.png inflating: SegmentationClassAug_Visualization/2009_002441.png inflating: SegmentationClassAug_Visualization/2010_002496.png inflating: SegmentationClassAug_Visualization/2008_000656.png inflating: SegmentationClassAug_Visualization/2008_006534.png inflating: SegmentationClassAug_Visualization/2010_001338.png inflating: SegmentationClassAug_Visualization/2011_000315.png inflating: SegmentationClassAug_Visualization/2011_002348.png inflating: SegmentationClassAug_Visualization/2010_003887.png inflating: SegmentationClassAug_Visualization/2011_000839.png inflating: SegmentationClassAug_Visualization/2010_004942.png inflating: SegmentationClassAug_Visualization/2007_009724.png inflating: SegmentationClassAug_Visualization/2008_003610.png inflating: SegmentationClassAug_Visualization/2007_006560.png inflating: SegmentationClassAug_Visualization/2008_000107.png inflating: SegmentationClassAug_Visualization/2008_007709.png inflating: SegmentationClassAug_Visualization/2008_000706.png inflating: SegmentationClassAug_Visualization/2009_003076.png inflating: SegmentationClassAug_Visualization/2009_002046.png inflating: SegmentationClassAug_Visualization/2008_006701.png inflating: SegmentationClassAug_Visualization/2010_002037.png inflating: SegmentationClassAug_Visualization/2009_004168.png inflating: SegmentationClassAug_Visualization/2008_003160.png inflating: SegmentationClassAug_Visualization/2008_007704.png inflating: SegmentationClassAug_Visualization/2009_001755.png inflating: SegmentationClassAug_Visualization/2008_005721.png inflating: SegmentationClassAug_Visualization/2010_004812.png inflating: SegmentationClassAug_Visualization/2010_005127.png inflating: SegmentationClassAug_Visualization/2008_000952.png inflating: SegmentationClassAug_Visualization/2011_001015.png inflating: SegmentationClassAug_Visualization/2009_003088.png inflating: SegmentationClassAug_Visualization/2008_003409.png inflating: SegmentationClassAug_Visualization/2008_007640.png inflating: SegmentationClassAug_Visualization/2010_000497.png inflating: SegmentationClassAug_Visualization/2010_005767.png inflating: SegmentationClassAug_Visualization/2009_004977.png inflating: SegmentationClassAug_Visualization/2009_001237.png inflating: SegmentationClassAug_Visualization/2011_000987.png inflating: SegmentationClassAug_Visualization/2008_000700.png inflating: SegmentationClassAug_Visualization/2010_001516.png inflating: SegmentationClassAug_Visualization/2009_004316.png inflating: SegmentationClassAug_Visualization/2009_001869.png inflating: SegmentationClassAug_Visualization/2009_000254.png inflating: SegmentationClassAug_Visualization/2009_003838.png inflating: SegmentationClassAug_Visualization/2011_001748.png inflating: SegmentationClassAug_Visualization/2011_001005.png inflating: SegmentationClassAug_Visualization/2009_003863.png inflating: SegmentationClassAug_Visualization/2010_002095.png inflating: SegmentationClassAug_Visualization/2010_002067.png inflating: SegmentationClassAug_Visualization/2010_004054.png inflating: SegmentationClassAug_Visualization/2008_005964.png inflating: SegmentationClassAug_Visualization/2008_007374.png inflating: SegmentationClassAug_Visualization/2010_005066.png inflating: SegmentationClassAug_Visualization/2007_000762.png inflating: SegmentationClassAug_Visualization/2010_005543.png inflating: SegmentationClassAug_Visualization/2009_003083.png inflating: SegmentationClassAug_Visualization/2008_000227.png inflating: SegmentationClassAug_Visualization/2008_004270.png inflating: SegmentationClassAug_Visualization/2008_000776.png inflating: SegmentationClassAug_Visualization/2011_002022.png inflating: SegmentationClassAug_Visualization/2011_000481.png inflating: SegmentationClassAug_Visualization/2008_007656.png inflating: SegmentationClassAug_Visualization/2009_001434.png inflating: SegmentationClassAug_Visualization/2010_002243.png inflating: SegmentationClassAug_Visualization/2008_006959.png inflating: SegmentationClassAug_Visualization/2008_004850.png inflating: SegmentationClassAug_Visualization/2007_007996.png inflating: SegmentationClassAug_Visualization/2007_009088.png inflating: SegmentationClassAug_Visualization/2009_002231.png inflating: SegmentationClassAug_Visualization/2008_001730.png inflating: SegmentationClassAug_Visualization/2011_001124.png inflating: SegmentationClassAug_Visualization/2010_003344.png inflating: SegmentationClassAug_Visualization/2009_002281.png inflating: SegmentationClassAug_Visualization/2008_007441.png inflating: SegmentationClassAug_Visualization/2011_001932.png inflating: SegmentationClassAug_Visualization/2010_003091.png inflating: SegmentationClassAug_Visualization/2008_006242.png inflating: SegmentationClassAug_Visualization/2008_008215.png inflating: SegmentationClassAug_Visualization/2010_000054.png inflating: SegmentationClassAug_Visualization/2009_004150.png inflating: SegmentationClassAug_Visualization/2010_005810.png inflating: SegmentationClassAug_Visualization/2008_004006.png inflating: SegmentationClassAug_Visualization/2008_008388.png inflating: SegmentationClassAug_Visualization/2011_001476.png inflating: SegmentationClassAug_Visualization/2010_001287.png inflating: SegmentationClassAug_Visualization/2008_003524.png inflating: SegmentationClassAug_Visualization/2010_003871.png inflating: SegmentationClassAug_Visualization/2010_004278.png inflating: SegmentationClassAug_Visualization/2011_001257.png inflating: SegmentationClassAug_Visualization/2008_006553.png inflating: SegmentationClassAug_Visualization/2008_008517.png inflating: SegmentationClassAug_Visualization/2007_002046.png inflating: SegmentationClassAug_Visualization/2011_001381.png inflating: SegmentationClassAug_Visualization/2008_001218.png inflating: SegmentationClassAug_Visualization/2009_002983.png inflating: SegmentationClassAug_Visualization/2008_005609.png inflating: SegmentationClassAug_Visualization/2008_002325.png inflating: SegmentationClassAug_Visualization/2011_002474.png inflating: SegmentationClassAug_Visualization/2008_002321.png inflating: SegmentationClassAug_Visualization/2008_005297.png inflating: SegmentationClassAug_Visualization/2009_003128.png inflating: SegmentationClassAug_Visualization/2009_003895.png inflating: SegmentationClassAug_Visualization/2009_003461.png inflating: SegmentationClassAug_Visualization/2009_001683.png inflating: SegmentationClassAug_Visualization/2008_004984.png inflating: SegmentationClassAug_Visualization/2010_000674.png inflating: SegmentationClassAug_Visualization/2008_003777.png inflating: SegmentationClassAug_Visualization/2009_005260.png inflating: SegmentationClassAug_Visualization/2011_002561.png inflating: SegmentationClassAug_Visualization/2008_002250.png inflating: SegmentationClassAug_Visualization/2009_000910.png inflating: SegmentationClassAug_Visualization/2010_001940.png inflating: SegmentationClassAug_Visualization/2010_005005.png inflating: SegmentationClassAug_Visualization/2007_005430.png inflating: SegmentationClassAug_Visualization/2010_001152.png inflating: SegmentationClassAug_Visualization/2009_002665.png inflating: SegmentationClassAug_Visualization/2010_005353.png inflating: SegmentationClassAug_Visualization/2008_002984.png inflating: SegmentationClassAug_Visualization/2010_002674.png inflating: SegmentationClassAug_Visualization/2011_002463.png inflating: SegmentationClassAug_Visualization/2008_004201.png inflating: SegmentationClassAug_Visualization/2008_005046.png inflating: SegmentationClassAug_Visualization/2008_008665.png inflating: SegmentationClassAug_Visualization/2009_005000.png inflating: SegmentationClassAug_Visualization/2010_000590.png inflating: SegmentationClassAug_Visualization/2009_004375.png inflating: SegmentationClassAug_Visualization/2009_001146.png inflating: SegmentationClassAug_Visualization/2008_000182.png inflating: SegmentationClassAug_Visualization/2010_000174.png inflating: SegmentationClassAug_Visualization/2009_001640.png inflating: SegmentationClassAug_Visualization/2008_007256.png inflating: SegmentationClassAug_Visualization/2009_002843.png inflating: SegmentationClassAug_Visualization/2009_003071.png inflating: SegmentationClassAug_Visualization/2010_000145.png inflating: SegmentationClassAug_Visualization/2009_000029.png inflating: SegmentationClassAug_Visualization/2010_002741.png inflating: SegmentationClassAug_Visualization/2009_001607.png inflating: SegmentationClassAug_Visualization/2010_004457.png inflating: SegmentationClassAug_Visualization/2010_003453.png inflating: SegmentationClassAug_Visualization/2008_007576.png inflating: SegmentationClassAug_Visualization/2008_004768.png inflating: SegmentationClassAug_Visualization/2008_004499.png inflating: SegmentationClassAug_Visualization/2010_001522.png inflating: SegmentationClassAug_Visualization/2007_006046.png inflating: SegmentationClassAug_Visualization/2008_001731.png inflating: SegmentationClassAug_Visualization/2008_005618.png inflating: SegmentationClassAug_Visualization/2010_003744.png inflating: SegmentationClassAug_Visualization/2008_006879.png inflating: SegmentationClassAug_Visualization/2008_007559.png inflating: SegmentationClassAug_Visualization/2008_006310.png inflating: SegmentationClassAug_Visualization/2008_007402.png inflating: SegmentationClassAug_Visualization/2009_001474.png inflating: SegmentationClassAug_Visualization/2010_000710.png inflating: SegmentationClassAug_Visualization/2010_001463.png inflating: SegmentationClassAug_Visualization/2010_001718.png inflating: SegmentationClassAug_Visualization/2009_004300.png inflating: SegmentationClassAug_Visualization/2008_003658.png inflating: SegmentationClassAug_Visualization/2010_005224.png inflating: SegmentationClassAug_Visualization/2008_001969.png inflating: SegmentationClassAug_Visualization/2008_004584.png inflating: SegmentationClassAug_Visualization/2010_005433.png inflating: SegmentationClassAug_Visualization/2009_003667.png inflating: SegmentationClassAug_Visualization/2008_007260.png inflating: SegmentationClassAug_Visualization/2008_008074.png inflating: SegmentationClassAug_Visualization/2010_000738.png inflating: SegmentationClassAug_Visualization/2009_001623.png inflating: SegmentationClassAug_Visualization/2008_000887.png inflating: SegmentationClassAug_Visualization/2010_002456.png inflating: SegmentationClassAug_Visualization/2010_000148.png inflating: SegmentationClassAug_Visualization/2009_004683.png inflating: SegmentationClassAug_Visualization/2008_001809.png inflating: SegmentationClassAug_Visualization/2008_008185.png inflating: SegmentationClassAug_Visualization/2008_007989.png inflating: SegmentationClassAug_Visualization/2010_000633.png inflating: SegmentationClassAug_Visualization/2009_003855.png inflating: SegmentationClassAug_Visualization/2008_008642.png inflating: SegmentationClassAug_Visualization/2009_004655.png inflating: SegmentationClassAug_Visualization/2008_007662.png inflating: SegmentationClassAug_Visualization/2009_001822.png inflating: SegmentationClassAug_Visualization/2011_000087.png inflating: SegmentationClassAug_Visualization/2010_002409.png inflating: SegmentationClassAug_Visualization/2009_002917.png inflating: SegmentationClassAug_Visualization/2009_002591.png inflating: SegmentationClassAug_Visualization/2010_004275.png inflating: SegmentationClassAug_Visualization/2010_005800.png inflating: SegmentationClassAug_Visualization/2010_003143.png inflating: SegmentationClassAug_Visualization/2008_007059.png inflating: SegmentationClassAug_Visualization/2009_000336.png inflating: SegmentationClassAug_Visualization/2008_006067.png inflating: SegmentationClassAug_Visualization/2009_003571.png inflating: SegmentationClassAug_Visualization/2008_005032.png inflating: SegmentationClassAug_Visualization/2011_001671.png inflating: SegmentationClassAug_Visualization/2011_003259.png inflating: SegmentationClassAug_Visualization/2011_002687.png inflating: SegmentationClassAug_Visualization/2008_003607.png inflating: SegmentationClassAug_Visualization/2010_004171.png inflating: SegmentationClassAug_Visualization/2009_001911.png inflating: SegmentationClassAug_Visualization/2008_008134.png inflating: SegmentationClassAug_Visualization/2011_000558.png inflating: SegmentationClassAug_Visualization/2008_004357.png inflating: SegmentationClassAug_Visualization/2010_003197.png inflating: SegmentationClassAug_Visualization/2010_000870.png inflating: SegmentationClassAug_Visualization/2010_005712.png inflating: SegmentationClassAug_Visualization/2008_005501.png inflating: SegmentationClassAug_Visualization/2010_002695.png inflating: SegmentationClassAug_Visualization/2008_004126.png inflating: SegmentationClassAug_Visualization/2008_001041.png inflating: SegmentationClassAug_Visualization/2007_007772.png inflating: SegmentationClassAug_Visualization/2010_002450.png inflating: SegmentationClassAug_Visualization/2009_004670.png inflating: SegmentationClassAug_Visualization/2008_003988.png inflating: SegmentationClassAug_Visualization/2010_005825.png inflating: SegmentationClassAug_Visualization/2008_006621.png inflating: SegmentationClassAug_Visualization/2007_005626.png inflating: SegmentationClassAug_Visualization/2009_000898.png inflating: SegmentationClassAug_Visualization/2008_002411.png inflating: SegmentationClassAug_Visualization/2011_002803.png inflating: SegmentationClassAug_Visualization/2010_005891.png inflating: SegmentationClassAug_Visualization/2011_002636.png inflating: SegmentationClassAug_Visualization/2008_000839.png inflating: SegmentationClassAug_Visualization/2008_006523.png inflating: SegmentationClassAug_Visualization/2009_001368.png inflating: SegmentationClassAug_Visualization/2010_002980.png inflating: SegmentationClassAug_Visualization/2008_006175.png inflating: SegmentationClassAug_Visualization/2010_000875.png inflating: SegmentationClassAug_Visualization/2011_001040.png inflating: SegmentationClassAug_Visualization/2009_002687.png inflating: SegmentationClassAug_Visualization/2008_000655.png inflating: SegmentationClassAug_Visualization/2008_002558.png inflating: SegmentationClassAug_Visualization/2010_000211.png inflating: SegmentationClassAug_Visualization/2009_002817.png inflating: SegmentationClassAug_Visualization/2010_005303.png inflating: SegmentationClassAug_Visualization/2008_005954.png inflating: SegmentationClassAug_Visualization/2008_007205.png inflating: SegmentationClassAug_Visualization/2009_003613.png inflating: SegmentationClassAug_Visualization/2011_002802.png inflating: SegmentationClassAug_Visualization/2009_002401.png inflating: SegmentationClassAug_Visualization/2008_007105.png inflating: SegmentationClassAug_Visualization/2009_001608.png inflating: SegmentationClassAug_Visualization/2009_004929.png inflating: SegmentationClassAug_Visualization/2008_000177.png inflating: SegmentationClassAug_Visualization/2010_002747.png inflating: SegmentationClassAug_Visualization/2011_001893.png inflating: SegmentationClassAug_Visualization/2009_003709.png inflating: SegmentationClassAug_Visualization/2010_002760.png inflating: SegmentationClassAug_Visualization/2009_004162.png inflating: SegmentationClassAug_Visualization/2008_007285.png inflating: SegmentationClassAug_Visualization/2009_002780.png inflating: SegmentationClassAug_Visualization/2009_002980.png inflating: SegmentationClassAug_Visualization/2008_002795.png inflating: SegmentationClassAug_Visualization/2008_002960.png inflating: SegmentationClassAug_Visualization/2010_001796.png inflating: SegmentationClassAug_Visualization/2010_003248.png inflating: SegmentationClassAug_Visualization/2008_001373.png inflating: SegmentationClassAug_Visualization/2008_003916.png inflating: SegmentationClassAug_Visualization/2008_003844.png inflating: SegmentationClassAug_Visualization/2007_003131.png inflating: SegmentationClassAug_Visualization/2009_000945.png inflating: SegmentationClassAug_Visualization/2010_002340.png inflating: SegmentationClassAug_Visualization/2011_000427.png inflating: SegmentationClassAug_Visualization/2010_004088.png inflating: SegmentationClassAug_Visualization/2009_003669.png inflating: SegmentationClassAug_Visualization/2010_000622.png inflating: SegmentationClassAug_Visualization/2008_004633.png inflating: SegmentationClassAug_Visualization/2007_004166.png inflating: SegmentationClassAug_Visualization/2011_001091.png inflating: SegmentationClassAug_Visualization/2008_007724.png inflating: SegmentationClassAug_Visualization/2009_000195.png inflating: SegmentationClassAug_Visualization/2008_007352.png inflating: SegmentationClassAug_Visualization/2009_002221.png inflating: SegmentationClassAug_Visualization/2008_002199.png inflating: SegmentationClassAug_Visualization/2008_000392.png inflating: SegmentationClassAug_Visualization/2008_008455.png inflating: SegmentationClassAug_Visualization/2008_000725.png inflating: SegmentationClassAug_Visualization/2007_001397.png inflating: SegmentationClassAug_Visualization/2009_004191.png inflating: SegmentationClassAug_Visualization/2008_003231.png inflating: SegmentationClassAug_Visualization/2010_005987.png inflating: SegmentationClassAug_Visualization/2009_003276.png inflating: SegmentationClassAug_Visualization/2008_004812.png inflating: SegmentationClassAug_Visualization/2008_002437.png inflating: SegmentationClassAug_Visualization/2011_000149.png inflating: SegmentationClassAug_Visualization/2008_004722.png inflating: SegmentationClassAug_Visualization/2008_008506.png inflating: SegmentationClassAug_Visualization/2010_001164.png inflating: SegmentationClassAug_Visualization/2008_007864.png inflating: SegmentationClassAug_Visualization/2010_002042.png inflating: SegmentationClassAug_Visualization/2010_005511.png inflating: SegmentationClassAug_Visualization/2009_003282.png inflating: SegmentationClassAug_Visualization/2008_007247.png inflating: SegmentationClassAug_Visualization/2008_004760.png inflating: SegmentationClassAug_Visualization/2011_002330.png inflating: SegmentationClassAug_Visualization/2010_001192.png inflating: SegmentationClassAug_Visualization/2008_004964.png inflating: SegmentationClassAug_Visualization/2011_001498.png inflating: SegmentationClassAug_Visualization/2010_001395.png inflating: SegmentationClassAug_Visualization/2008_005117.png inflating: SegmentationClassAug_Visualization/2008_001888.png inflating: SegmentationClassAug_Visualization/2011_000250.png inflating: SegmentationClassAug_Visualization/2010_003231.png inflating: SegmentationClassAug_Visualization/2008_006815.png inflating: SegmentationClassAug_Visualization/2010_006063.png inflating: SegmentationClassAug_Visualization/2008_001275.png inflating: SegmentationClassAug_Visualization/2009_003714.png inflating: SegmentationClassAug_Visualization/2010_004681.png inflating: SegmentationClassAug_Visualization/2011_001824.png inflating: SegmentationClassAug_Visualization/2011_001944.png inflating: SegmentationClassAug_Visualization/2007_008072.png inflating: SegmentationClassAug_Visualization/2008_004084.png inflating: SegmentationClassAug_Visualization/2009_004857.png inflating: SegmentationClassAug_Visualization/2010_005044.png inflating: SegmentationClassAug_Visualization/2007_000392.png inflating: SegmentationClassAug_Visualization/2011_002108.png inflating: SegmentationClassAug_Visualization/2007_007250.png inflating: SegmentationClassAug_Visualization/2010_002147.png inflating: SegmentationClassAug_Visualization/2011_000682.png inflating: SegmentationClassAug_Visualization/2009_001898.png inflating: SegmentationClassAug_Visualization/2011_001086.png inflating: SegmentationClassAug_Visualization/2009_001270.png inflating: SegmentationClassAug_Visualization/2009_000109.png inflating: SegmentationClassAug_Visualization/2008_006041.png inflating: SegmentationClassAug_Visualization/2008_002971.png inflating: SegmentationClassAug_Visualization/2009_005057.png inflating: SegmentationClassAug_Visualization/2008_001751.png inflating: SegmentationClassAug_Visualization/2008_006163.png inflating: SegmentationClassAug_Visualization/2008_006614.png inflating: SegmentationClassAug_Visualization/2010_004676.png inflating: SegmentationClassAug_Visualization/2008_008043.png inflating: SegmentationClassAug_Visualization/2010_002357.png inflating: SegmentationClassAug_Visualization/2009_001585.png inflating: SegmentationClassAug_Visualization/2010_003120.png inflating: SegmentationClassAug_Visualization/2009_002869.png inflating: SegmentationClassAug_Visualization/2010_003086.png inflating: SegmentationClassAug_Visualization/2009_003671.png inflating: SegmentationClassAug_Visualization/2009_003942.png inflating: SegmentationClassAug_Visualization/2008_007335.png inflating: SegmentationClassAug_Visualization/2008_006267.png inflating: SegmentationClassAug_Visualization/2010_005867.png inflating: SegmentationClassAug_Visualization/2010_004248.png inflating: SegmentationClassAug_Visualization/2008_002710.png inflating: SegmentationClassAug_Visualization/2009_004805.png inflating: SegmentationClassAug_Visualization/2011_002303.png inflating: SegmentationClassAug_Visualization/2007_001761.png inflating: SegmentationClassAug_Visualization/2010_005082.png inflating: SegmentationClassAug_Visualization/2008_006225.png inflating: SegmentationClassAug_Visualization/2011_002391.png inflating: SegmentationClassAug_Visualization/2009_000752.png inflating: SegmentationClassAug_Visualization/2008_004778.png inflating: SegmentationClassAug_Visualization/2008_004428.png inflating: SegmentationClassAug_Visualization/2010_003094.png inflating: SegmentationClassAug_Visualization/2009_005033.png inflating: SegmentationClassAug_Visualization/2009_002257.png inflating: SegmentationClassAug_Visualization/2008_002540.png inflating: SegmentationClassAug_Visualization/2011_000369.png inflating: SegmentationClassAug_Visualization/2010_001008.png inflating: SegmentationClassAug_Visualization/2009_005001.png inflating: SegmentationClassAug_Visualization/2010_004704.png inflating: SegmentationClassAug_Visualization/2009_004919.png inflating: SegmentationClassAug_Visualization/2008_006108.png inflating: SegmentationClassAug_Visualization/2008_008611.png inflating: SegmentationClassAug_Visualization/2009_003922.png inflating: SegmentationClassAug_Visualization/2008_002327.png inflating: SegmentationClassAug_Visualization/2008_002117.png inflating: SegmentationClassAug_Visualization/2010_004357.png inflating: SegmentationClassAug_Visualization/2009_002668.png inflating: SegmentationClassAug_Visualization/2008_000321.png inflating: SegmentationClassAug_Visualization/2008_008309.png inflating: SegmentationClassAug_Visualization/2009_001852.png inflating: SegmentationClassAug_Visualization/2011_001910.png inflating: SegmentationClassAug_Visualization/2009_005216.png inflating: SegmentationClassAug_Visualization/2008_005431.png inflating: SegmentationClassAug_Visualization/2009_003551.png inflating: SegmentationClassAug_Visualization/2010_004992.png inflating: SegmentationClassAug_Visualization/2008_007145.png inflating: SegmentationClassAug_Visualization/2011_002185.png inflating: SegmentationClassAug_Visualization/2008_000901.png inflating: SegmentationClassAug_Visualization/2010_001160.png inflating: SegmentationClassAug_Visualization/2011_001305.png inflating: SegmentationClassAug_Visualization/2009_001271.png inflating: SegmentationClassAug_Visualization/2010_005042.png inflating: SegmentationClassAug_Visualization/2008_006154.png inflating: SegmentationClassAug_Visualization/2010_000624.png inflating: SegmentationClassAug_Visualization/2010_003745.png inflating: SegmentationClassAug_Visualization/2011_002381.png inflating: SegmentationClassAug_Visualization/2008_001122.png inflating: SegmentationClassAug_Visualization/2008_001264.png inflating: SegmentationClassAug_Visualization/2010_005242.png inflating: SegmentationClassAug_Visualization/2009_003019.png inflating: SegmentationClassAug_Visualization/2009_002887.png inflating: SegmentationClassAug_Visualization/2010_000120.png inflating: SegmentationClassAug_Visualization/2008_000380.png inflating: SegmentationClassAug_Visualization/2008_004570.png inflating: SegmentationClassAug_Visualization/2009_001026.png inflating: SegmentationClassAug_Visualization/2010_001540.png inflating: SegmentationClassAug_Visualization/2008_007163.png inflating: SegmentationClassAug_Visualization/2009_002374.png inflating: SegmentationClassAug_Visualization/2008_007975.png inflating: SegmentationClassAug_Visualization/2009_000501.png inflating: SegmentationClassAug_Visualization/2010_001013.png inflating: SegmentationClassAug_Visualization/2011_001929.png inflating: SegmentationClassAug_Visualization/2010_005266.png inflating: SegmentationClassAug_Visualization/2008_006538.png inflating: SegmentationClassAug_Visualization/2009_002626.png inflating: SegmentationClassAug_Visualization/2008_002357.png inflating: SegmentationClassAug_Visualization/2009_002128.png inflating: SegmentationClassAug_Visualization/2008_005656.png inflating: SegmentationClassAug_Visualization/2008_004076.png inflating: SegmentationClassAug_Visualization/2008_003276.png inflating: SegmentationClassAug_Visualization/2007_001834.png inflating: SegmentationClassAug_Visualization/2011_000684.png inflating: SegmentationClassAug_Visualization/2009_005086.png inflating: SegmentationClassAug_Visualization/2008_005566.png inflating: SegmentationClassAug_Visualization/2009_004272.png inflating: SegmentationClassAug_Visualization/2010_002892.png inflating: SegmentationClassAug_Visualization/2010_003250.png inflating: SegmentationClassAug_Visualization/2008_000009.png inflating: SegmentationClassAug_Visualization/2008_000540.png inflating: SegmentationClassAug_Visualization/2008_007432.png inflating: SegmentationClassAug_Visualization/2008_000694.png inflating: SegmentationClassAug_Visualization/2011_000990.png inflating: SegmentationClassAug_Visualization/2008_004122.png inflating: SegmentationClassAug_Visualization/2010_000648.png inflating: SegmentationClassAug_Visualization/2009_000725.png inflating: SegmentationClassAug_Visualization/2008_002801.png inflating: SegmentationClassAug_Visualization/2009_005126.png inflating: SegmentationClassAug_Visualization/2010_000492.png inflating: SegmentationClassAug_Visualization/2008_004234.png inflating: SegmentationClassAug_Visualization/2008_005136.png inflating: SegmentationClassAug_Visualization/2008_003546.png inflating: SegmentationClassAug_Visualization/2010_001919.png inflating: SegmentationClassAug_Visualization/2008_006889.png inflating: SegmentationClassAug_Visualization/2011_000673.png inflating: SegmentationClassAug_Visualization/2008_007635.png inflating: SegmentationClassAug_Visualization/2010_001976.png inflating: SegmentationClassAug_Visualization/2008_000446.png inflating: SegmentationClassAug_Visualization/2010_001039.png inflating: SegmentationClassAug_Visualization/2009_004228.png inflating: SegmentationClassAug_Visualization/2007_007084.png inflating: SegmentationClassAug_Visualization/2010_003579.png inflating: SegmentationClassAug_Visualization/2009_003373.png inflating: SegmentationClassAug_Visualization/2010_002868.png inflating: SegmentationClassAug_Visualization/2011_000494.png inflating: SegmentationClassAug_Visualization/2009_001553.png inflating: SegmentationClassAug_Visualization/2009_001138.png inflating: SegmentationClassAug_Visualization/2009_004662.png inflating: SegmentationClassAug_Visualization/2009_000233.png inflating: SegmentationClassAug_Visualization/2009_003267.png inflating: SegmentationClassAug_Visualization/2010_002857.png inflating: SegmentationClassAug_Visualization/2009_004887.png inflating: SegmentationClassAug_Visualization/2008_004445.png inflating: SegmentationClassAug_Visualization/2010_001877.png inflating: SegmentationClassAug_Visualization/2011_003240.png inflating: SegmentationClassAug_Visualization/2011_001531.png inflating: SegmentationClassAug_Visualization/2008_003482.png inflating: SegmentationClassAug_Visualization/2009_000443.png inflating: SegmentationClassAug_Visualization/2010_005183.png inflating: SegmentationClassAug_Visualization/2009_005171.png inflating: SegmentationClassAug_Visualization/2008_007694.png inflating: SegmentationClassAug_Visualization/2008_001602.png inflating: SegmentationClassAug_Visualization/2011_000509.png inflating: SegmentationClassAug_Visualization/2011_001263.png inflating: SegmentationClassAug_Visualization/2011_001009.png inflating: SegmentationClassAug_Visualization/2007_000847.png inflating: SegmentationClassAug_Visualization/2008_003239.png inflating: SegmentationClassAug_Visualization/2008_005860.png inflating: SegmentationClassAug_Visualization/2011_000060.png inflating: SegmentationClassAug_Visualization/2008_006819.png inflating: SegmentationClassAug_Visualization/2010_002880.png inflating: SegmentationClassAug_Visualization/2008_007134.png inflating: SegmentationClassAug_Visualization/2009_000902.png inflating: SegmentationClassAug_Visualization/2008_003533.png inflating: SegmentationClassAug_Visualization/2009_001305.png inflating: SegmentationClassAug_Visualization/2008_005460.png inflating: SegmentationClassAug_Visualization/2009_001387.png inflating: SegmentationClassAug_Visualization/2011_000362.png inflating: SegmentationClassAug_Visualization/2008_008470.png inflating: SegmentationClassAug_Visualization/2009_003921.png inflating: SegmentationClassAug_Visualization/2008_008302.png inflating: SegmentationClassAug_Visualization/2010_005064.png inflating: SegmentationClassAug_Visualization/2011_002983.png inflating: SegmentationClassAug_Visualization/2010_001907.png inflating: SegmentationClassAug_Visualization/2008_001712.png inflating: SegmentationClassAug_Visualization/2009_003860.png inflating: SegmentationClassAug_Visualization/2007_000663.png inflating: SegmentationClassAug_Visualization/2008_006467.png inflating: SegmentationClassAug_Visualization/2011_001213.png inflating: SegmentationClassAug_Visualization/2008_004422.png inflating: SegmentationClassAug_Visualization/2008_001770.png inflating: SegmentationClassAug_Visualization/2010_003730.png inflating: SegmentationClassAug_Visualization/2010_005597.png inflating: SegmentationClassAug_Visualization/2008_002379.png inflating: SegmentationClassAug_Visualization/2008_005757.png inflating: SegmentationClassAug_Visualization/2008_007529.png inflating: SegmentationClassAug_Visualization/2008_007793.png inflating: SegmentationClassAug_Visualization/2010_005160.png inflating: SegmentationClassAug_Visualization/2008_005847.png inflating: SegmentationClassAug_Visualization/2007_001764.png inflating: SegmentationClassAug_Visualization/2009_001864.png inflating: SegmentationClassAug_Visualization/2011_000503.png inflating: SegmentationClassAug_Visualization/2009_002185.png inflating: SegmentationClassAug_Visualization/2008_001941.png inflating: SegmentationClassAug_Visualization/2008_000724.png inflating: SegmentationClassAug_Visualization/2009_001177.png inflating: SegmentationClassAug_Visualization/2009_000284.png inflating: SegmentationClassAug_Visualization/2008_002243.png inflating: SegmentationClassAug_Visualization/2008_005548.png inflating: SegmentationClassAug_Visualization/2008_004754.png inflating: SegmentationClassAug_Visualization/2010_005666.png inflating: SegmentationClassAug_Visualization/2009_001249.png inflating: SegmentationClassAug_Visualization/2011_002413.png inflating: SegmentationClassAug_Visualization/2011_002160.png inflating: SegmentationClassAug_Visualization/2009_003870.png inflating: SegmentationClassAug_Visualization/2009_002506.png inflating: SegmentationClassAug_Visualization/2010_002752.png inflating: SegmentationClassAug_Visualization/2008_003992.png inflating: SegmentationClassAug_Visualization/2009_000760.png inflating: SegmentationClassAug_Visualization/2011_000210.png inflating: SegmentationClassAug_Visualization/2009_005120.png inflating: SegmentationClassAug_Visualization/2009_001645.png inflating: SegmentationClassAug_Visualization/2010_001555.png inflating: SegmentationClassAug_Visualization/2008_004665.png inflating: SegmentationClassAug_Visualization/2010_000923.png inflating: SegmentationClassAug_Visualization/2010_006076.png inflating: SegmentationClassAug_Visualization/2009_005220.png inflating: SegmentationClassAug_Visualization/2008_002992.png inflating: SegmentationClassAug_Visualization/2009_001741.png inflating: SegmentationClassAug_Visualization/2008_002649.png inflating: SegmentationClassAug_Visualization/2009_000565.png inflating: SegmentationClassAug_Visualization/2009_005085.png inflating: SegmentationClassAug_Visualization/2011_000932.png inflating: SegmentationClassAug_Visualization/2009_002518.png inflating: SegmentationClassAug_Visualization/2009_001562.png inflating: SegmentationClassAug_Visualization/2010_001923.png inflating: SegmentationClassAug_Visualization/2011_001946.png inflating: SegmentationClassAug_Visualization/2009_000675.png inflating: SegmentationClassAug_Visualization/2008_005889.png inflating: SegmentationClassAug_Visualization/2008_006497.png inflating: SegmentationClassAug_Visualization/2009_003867.png inflating: SegmentationClassAug_Visualization/2008_000645.png inflating: SegmentationClassAug_Visualization/2011_000072.png inflating: SegmentationClassAug_Visualization/2009_002532.png inflating: SegmentationClassAug_Visualization/2009_000446.png inflating: SegmentationClassAug_Visualization/2008_002891.png inflating: SegmentationClassAug_Visualization/2011_002779.png inflating: SegmentationClassAug_Visualization/2008_005676.png inflating: SegmentationClassAug_Visualization/2008_005057.png inflating: SegmentationClassAug_Visualization/2008_004933.png inflating: SegmentationClassAug_Visualization/2010_004691.png inflating: SegmentationClassAug_Visualization/2010_003190.png inflating: SegmentationClassAug_Visualization/2010_004594.png inflating: SegmentationClassAug_Visualization/2010_001625.png inflating: SegmentationClassAug_Visualization/2009_004789.png inflating: SegmentationClassAug_Visualization/2008_000674.png inflating: SegmentationClassAug_Visualization/2008_001782.png inflating: SegmentationClassAug_Visualization/2010_000312.png inflating: SegmentationClassAug_Visualization/2008_001401.png inflating: SegmentationClassAug_Visualization/2010_001960.png inflating: SegmentationClassAug_Visualization/2008_002541.png inflating: SegmentationClassAug_Visualization/2008_001223.png inflating: SegmentationClassAug_Visualization/2010_000526.png inflating: SegmentationClassAug_Visualization/2009_000339.png inflating: SegmentationClassAug_Visualization/2008_007012.png inflating: SegmentationClassAug_Visualization/2008_000853.png inflating: SegmentationClassAug_Visualization/2008_001387.png inflating: SegmentationClassAug_Visualization/2008_003344.png inflating: SegmentationClassAug_Visualization/2011_000734.png inflating: SegmentationClassAug_Visualization/2008_001810.png inflating: SegmentationClassAug_Visualization/2010_005108.png inflating: SegmentationClassAug_Visualization/2009_004038.png inflating: SegmentationClassAug_Visualization/2008_007733.png inflating: SegmentationClassAug_Visualization/2008_000359.png inflating: SegmentationClassAug_Visualization/2008_005451.png inflating: SegmentationClassAug_Visualization/2010_000075.png inflating: SegmentationClassAug_Visualization/2008_008432.png inflating: SegmentationClassAug_Visualization/2007_009889.png inflating: SegmentationClassAug_Visualization/2008_004378.png inflating: SegmentationClassAug_Visualization/2008_003577.png inflating: SegmentationClassAug_Visualization/2008_000561.png inflating: SegmentationClassAug_Visualization/2010_004204.png inflating: SegmentationClassAug_Visualization/2010_004048.png inflating: SegmentationClassAug_Visualization/2008_003013.png inflating: SegmentationClassAug_Visualization/2008_007003.png inflating: SegmentationClassAug_Visualization/2010_000347.png inflating: SegmentationClassAug_Visualization/2008_001814.png inflating: SegmentationClassAug_Visualization/2010_004974.png inflating: SegmentationClassAug_Visualization/2011_001794.png inflating: SegmentationClassAug_Visualization/2010_005246.png inflating: SegmentationClassAug_Visualization/2010_005943.png inflating: SegmentationClassAug_Visualization/2008_008595.png inflating: SegmentationClassAug_Visualization/2010_005930.png inflating: SegmentationClassAug_Visualization/2008_007716.png inflating: SegmentationClassAug_Visualization/2008_006710.png inflating: SegmentationClassAug_Visualization/2009_001754.png inflating: SegmentationClassAug_Visualization/2010_003241.png inflating: SegmentationClassAug_Visualization/2009_000995.png inflating: SegmentationClassAug_Visualization/2008_001992.png inflating: SegmentationClassAug_Visualization/2008_004695.png inflating: SegmentationClassAug_Visualization/2008_002160.png inflating: SegmentationClassAug_Visualization/2009_000277.png inflating: SegmentationClassAug_Visualization/2010_003942.png inflating: SegmentationClassAug_Visualization/2008_002131.png inflating: SegmentationClassAug_Visualization/2009_003773.png inflating: SegmentationClassAug_Visualization/2011_002091.png inflating: SegmentationClassAug_Visualization/2011_002488.png inflating: SegmentationClassAug_Visualization/2009_002267.png inflating: SegmentationClassAug_Visualization/2009_004933.png inflating: SegmentationClassAug_Visualization/2008_001551.png inflating: SegmentationClassAug_Visualization/2011_001862.png inflating: SegmentationClassAug_Visualization/2010_004782.png inflating: SegmentationClassAug_Visualization/2009_000996.png inflating: SegmentationClassAug_Visualization/2009_005203.png inflating: SegmentationClassAug_Visualization/2008_000488.png inflating: SegmentationClassAug_Visualization/2009_002285.png inflating: SegmentationClassAug_Visualization/2008_004749.png inflating: SegmentationClassAug_Visualization/2011_003011.png inflating: SegmentationClassAug_Visualization/2008_007761.png inflating: SegmentationClassAug_Visualization/2008_002714.png inflating: SegmentationClassAug_Visualization/2008_004410.png inflating: SegmentationClassAug_Visualization/2010_001411.png inflating: SegmentationClassAug_Visualization/2010_002312.png inflating: SegmentationClassAug_Visualization/2011_000057.png inflating: SegmentationClassAug_Visualization/2008_001434.png inflating: SegmentationClassAug_Visualization/2011_000208.png inflating: SegmentationClassAug_Visualization/2008_006140.png inflating: SegmentationClassAug_Visualization/2010_004211.png inflating: SegmentationClassAug_Visualization/2008_002473.png inflating: SegmentationClassAug_Visualization/2008_002746.png inflating: SegmentationClassAug_Visualization/2008_000076.png inflating: SegmentationClassAug_Visualization/2010_002382.png inflating: SegmentationClassAug_Visualization/2010_001863.png inflating: SegmentationClassAug_Visualization/2008_000714.png inflating: SegmentationClassAug_Visualization/2010_002406.png inflating: SegmentationClassAug_Visualization/2010_003251.png inflating: SegmentationClassAug_Visualization/2008_007207.png inflating: SegmentationClassAug_Visualization/2010_006011.png inflating: SegmentationClassAug_Visualization/2007_003714.png inflating: SegmentationClassAug_Visualization/2008_001245.png inflating: SegmentationClassAug_Visualization/2010_002692.png inflating: SegmentationClassAug_Visualization/2008_006430.png inflating: SegmentationClassAug_Visualization/2008_000074.png inflating: SegmentationClassAug_Visualization/2008_000318.png inflating: SegmentationClassAug_Visualization/2009_002961.png inflating: SegmentationClassAug_Visualization/2009_003636.png inflating: SegmentationClassAug_Visualization/2010_004816.png inflating: SegmentationClassAug_Visualization/2007_002967.png inflating: SegmentationClassAug_Visualization/2010_001537.png inflating: SegmentationClassAug_Visualization/2010_005536.png inflating: SegmentationClassAug_Visualization/2008_001071.png inflating: SegmentationClassAug_Visualization/2008_004920.png inflating: SegmentationClassAug_Visualization/2010_002831.png inflating: SegmentationClassAug_Visualization/2010_000711.png inflating: SegmentationClassAug_Visualization/2010_003187.png inflating: SegmentationClassAug_Visualization/2011_001069.png inflating: SegmentationClassAug_Visualization/2008_008479.png inflating: SegmentationClassAug_Visualization/2009_004307.png inflating: SegmentationClassAug_Visualization/2009_000532.png inflating: SegmentationClassAug_Visualization/2011_000185.png inflating: SegmentationClassAug_Visualization/2008_004103.png inflating: SegmentationClassAug_Visualization/2010_005967.png inflating: SegmentationClassAug_Visualization/2008_001226.png inflating: SegmentationClassAug_Visualization/2011_003246.png inflating: SegmentationClassAug_Visualization/2010_004540.png inflating: SegmentationClassAug_Visualization/2011_001834.png inflating: SegmentationClassAug_Visualization/2008_002073.png inflating: SegmentationClassAug_Visualization/2008_001591.png inflating: SegmentationClassAug_Visualization/2010_005538.png inflating: SegmentationClassAug_Visualization/2011_001962.png inflating: SegmentationClassAug_Visualization/2010_005462.png inflating: SegmentationClassAug_Visualization/2007_001321.png inflating: SegmentationClassAug_Visualization/2008_001669.png inflating: SegmentationClassAug_Visualization/2008_007500.png inflating: SegmentationClassAug_Visualization/2007_006409.png inflating: SegmentationClassAug_Visualization/2008_003405.png inflating: SegmentationClassAug_Visualization/2010_000152.png inflating: SegmentationClassAug_Visualization/2007_000346.png inflating: SegmentationClassAug_Visualization/2008_004784.png inflating: SegmentationClassAug_Visualization/2011_000176.png inflating: SegmentationClassAug_Visualization/2010_004809.png inflating: SegmentationClassAug_Visualization/2009_000638.png inflating: SegmentationClassAug_Visualization/2008_004589.png inflating: SegmentationClassAug_Visualization/2010_000658.png inflating: SegmentationClassAug_Visualization/2008_008601.png inflating: SegmentationClassAug_Visualization/2010_000415.png inflating: SegmentationClassAug_Visualization/2008_005376.png inflating: SegmentationClassAug_Visualization/2009_001098.png inflating: SegmentationClassAug_Visualization/2008_006657.png inflating: SegmentationClassAug_Visualization/2007_001457.png inflating: SegmentationClassAug_Visualization/2007_005813.png inflating: SegmentationClassAug_Visualization/2008_002869.png inflating: SegmentationClassAug_Visualization/2011_001608.png inflating: SegmentationClassAug_Visualization/2007_003715.png inflating: SegmentationClassAug_Visualization/2009_004536.png inflating: SegmentationClassAug_Visualization/2008_001520.png inflating: SegmentationClassAug_Visualization/2009_002527.png inflating: SegmentationClassAug_Visualization/2007_009436.png inflating: SegmentationClassAug_Visualization/2010_005374.png inflating: SegmentationClassAug_Visualization/2008_007430.png inflating: SegmentationClassAug_Visualization/2010_003891.png inflating: SegmentationClassAug_Visualization/2009_004710.png inflating: SegmentationClassAug_Visualization/2010_004987.png inflating: SegmentationClassAug_Visualization/2008_003244.png inflating: SegmentationClassAug_Visualization/2008_003343.png inflating: SegmentationClassAug_Visualization/2008_004188.png inflating: SegmentationClassAug_Visualization/2008_003743.png inflating: SegmentationClassAug_Visualization/2010_005167.png inflating: SegmentationClassAug_Visualization/2011_000137.png inflating: SegmentationClassAug_Visualization/2009_000457.png inflating: SegmentationClassAug_Visualization/2011_002924.png inflating: SegmentationClassAug_Visualization/2008_000103.png inflating: SegmentationClassAug_Visualization/2009_002205.png inflating: SegmentationClassAug_Visualization/2008_007588.png inflating: SegmentationClassAug_Visualization/2008_004399.png inflating: SegmentationClassAug_Visualization/2008_000075.png inflating: SegmentationClassAug_Visualization/2011_002662.png inflating: SegmentationClassAug_Visualization/2010_002538.png inflating: SegmentationClassAug_Visualization/2009_002605.png inflating: SegmentationClassAug_Visualization/2008_004588.png inflating: SegmentationClassAug_Visualization/2010_003365.png inflating: SegmentationClassAug_Visualization/2011_001136.png inflating: SegmentationClassAug_Visualization/2011_000514.png inflating: SegmentationClassAug_Visualization/2009_004383.png inflating: SegmentationClassAug_Visualization/2009_004708.png inflating: SegmentationClassAug_Visualization/2007_005857.png inflating: SegmentationClassAug_Visualization/2008_000640.png inflating: SegmentationClassAug_Visualization/2010_002349.png inflating: SegmentationClassAug_Visualization/2008_005664.png inflating: SegmentationClassAug_Visualization/2009_000340.png inflating: SegmentationClassAug_Visualization/2008_000056.png inflating: SegmentationClassAug_Visualization/2011_002433.png inflating: SegmentationClassAug_Visualization/2007_001678.png inflating: SegmentationClassAug_Visualization/2011_002786.png inflating: SegmentationClassAug_Visualization/2010_000879.png inflating: SegmentationClassAug_Visualization/2008_001850.png inflating: SegmentationClassAug_Visualization/2009_002586.png inflating: SegmentationClassAug_Visualization/2009_001197.png inflating: SegmentationClassAug_Visualization/2007_009654.png inflating: SegmentationClassAug_Visualization/2010_002616.png inflating: SegmentationClassAug_Visualization/2008_003146.png inflating: SegmentationClassAug_Visualization/2010_001569.png inflating: SegmentationClassAug_Visualization/2008_001134.png inflating: SegmentationClassAug_Visualization/2008_006849.png inflating: SegmentationClassAug_Visualization/2010_002527.png inflating: SegmentationClassAug_Visualization/2010_002653.png inflating: SegmentationClassAug_Visualization/2008_001219.png inflating: SegmentationClassAug_Visualization/2009_003384.png inflating: SegmentationClassAug_Visualization/2008_007086.png inflating: SegmentationClassAug_Visualization/2008_005558.png inflating: SegmentationClassAug_Visualization/2010_003106.png inflating: SegmentationClassAug_Visualization/2009_000188.png inflating: SegmentationClassAug_Visualization/2010_000409.png inflating: SegmentationClassAug_Visualization/2007_000333.png inflating: SegmentationClassAug_Visualization/2009_000970.png inflating: SegmentationClassAug_Visualization/2009_002204.png inflating: SegmentationClassAug_Visualization/2009_004519.png inflating: SegmentationClassAug_Visualization/2008_002377.png inflating: SegmentationClassAug_Visualization/2009_000088.png inflating: SegmentationClassAug_Visualization/2010_004917.png inflating: SegmentationClassAug_Visualization/2010_004349.png inflating: SegmentationClassAug_Visualization/2011_000690.png inflating: SegmentationClassAug_Visualization/2008_007246.png inflating: SegmentationClassAug_Visualization/2009_000991.png inflating: SegmentationClassAug_Visualization/2008_002852.png inflating: SegmentationClassAug_Visualization/2009_000387.png inflating: SegmentationClassAug_Visualization/2011_000499.png inflating: SegmentationClassAug_Visualization/2008_006111.png inflating: SegmentationClassAug_Visualization/2010_001218.png inflating: SegmentationClassAug_Visualization/2008_004430.png inflating: SegmentationClassAug_Visualization/2008_002922.png inflating: SegmentationClassAug_Visualization/2008_000745.png inflating: SegmentationClassAug_Visualization/2008_000461.png inflating: SegmentationClassAug_Visualization/2008_008500.png inflating: SegmentationClassAug_Visualization/2010_001036.png inflating: SegmentationClassAug_Visualization/2008_008508.png inflating: SegmentationClassAug_Visualization/2009_001833.png inflating: SegmentationClassAug_Visualization/2007_004969.png inflating: SegmentationClassAug_Visualization/2010_005284.png inflating: SegmentationClassAug_Visualization/2009_004368.png inflating: SegmentationClassAug_Visualization/2009_001868.png inflating: SegmentationClassAug_Visualization/2008_007339.png inflating: SegmentationClassAug_Visualization/2008_005984.png inflating: SegmentationClassAug_Visualization/2008_003948.png inflating: SegmentationClassAug_Visualization/2008_000801.png inflating: SegmentationClassAug_Visualization/2010_002686.png inflating: SegmentationClassAug_Visualization/2010_004332.png inflating: SegmentationClassAug_Visualization/2009_000268.png inflating: SegmentationClassAug_Visualization/2011_000951.png inflating: SegmentationClassAug_Visualization/2010_004686.png inflating: SegmentationClassAug_Visualization/2008_008257.png inflating: SegmentationClassAug_Visualization/2009_002569.png inflating: SegmentationClassAug_Visualization/2010_001344.png inflating: SegmentationClassAug_Visualization/2008_002156.png inflating: SegmentationClassAug_Visualization/2011_000391.png inflating: SegmentationClassAug_Visualization/2008_002052.png inflating: SegmentationClassAug_Visualization/2008_006663.png inflating: SegmentationClassAug_Visualization/2007_009096.png inflating: SegmentationClassAug_Visualization/2009_005201.png inflating: SegmentationClassAug_Visualization/2008_004832.png inflating: SegmentationClassAug_Visualization/2008_001769.png inflating: SegmentationClassAug_Visualization/2008_005673.png inflating: SegmentationClassAug_Visualization/2009_001840.png inflating: SegmentationClassAug_Visualization/2007_007748.png inflating: SegmentationClassAug_Visualization/2011_002200.png inflating: SegmentationClassAug_Visualization/2011_002641.png inflating: SegmentationClassAug_Visualization/2008_004069.png inflating: SegmentationClassAug_Visualization/2011_002460.png inflating: SegmentationClassAug_Visualization/2008_008203.png inflating: SegmentationClassAug_Visualization/2008_003978.png inflating: SegmentationClassAug_Visualization/2010_004037.png inflating: SegmentationClassAug_Visualization/2010_001104.png inflating: SegmentationClassAug_Visualization/2010_004960.png inflating: SegmentationClassAug_Visualization/2008_008070.png inflating: SegmentationClassAug_Visualization/2009_004580.png inflating: SegmentationClassAug_Visualization/2008_008357.png inflating: SegmentationClassAug_Visualization/2009_002150.png inflating: SegmentationClassAug_Visualization/2011_001858.png inflating: SegmentationClassAug_Visualization/2007_000121.png inflating: SegmentationClassAug_Visualization/2009_002414.png inflating: SegmentationClassAug_Visualization/2010_004634.png inflating: SegmentationClassAug_Visualization/2008_005134.png inflating: SegmentationClassAug_Visualization/2011_003205.png inflating: SegmentationClassAug_Visualization/2008_004235.png inflating: SegmentationClassAug_Visualization/2009_003074.png inflating: SegmentationClassAug_Visualization/2008_000407.png inflating: SegmentationClassAug_Visualization/2010_000702.png inflating: SegmentationClassAug_Visualization/2010_004662.png inflating: SegmentationClassAug_Visualization/2008_005536.png inflating: SegmentationClassAug_Visualization/2011_000449.png inflating: SegmentationClassAug_Visualization/2008_002662.png inflating: SegmentationClassAug_Visualization/2010_000647.png inflating: SegmentationClassAug_Visualization/2008_003825.png inflating: SegmentationClassAug_Visualization/2010_002142.png inflating: SegmentationClassAug_Visualization/2009_004959.png inflating: SegmentationClassAug_Visualization/2010_004149.png inflating: SegmentationClassAug_Visualization/2011_002951.png inflating: SegmentationClassAug_Visualization/2008_004342.png inflating: SegmentationClassAug_Visualization/2008_008434.png inflating: SegmentationClassAug_Visualization/2008_006810.png inflating: SegmentationClassAug_Visualization/2011_000701.png inflating: SegmentationClassAug_Visualization/2008_001631.png inflating: SegmentationClassAug_Visualization/2010_002181.png inflating: SegmentationClassAug_Visualization/2008_003402.png inflating: SegmentationClassAug_Visualization/2008_003814.png inflating: SegmentationClassAug_Visualization/2010_003690.png inflating: SegmentationClassAug_Visualization/2008_000278.png inflating: SegmentationClassAug_Visualization/2010_001123.png inflating: SegmentationClassAug_Visualization/2008_005967.png inflating: SegmentationClassAug_Visualization/2011_002748.png inflating: SegmentationClassAug_Visualization/2008_006351.png inflating: SegmentationClassAug_Visualization/2010_004773.png inflating: SegmentationClassAug_Visualization/2011_001455.png inflating: SegmentationClassAug_Visualization/2009_001002.png inflating: SegmentationClassAug_Visualization/2010_003651.png inflating: SegmentationClassAug_Visualization/2008_003718.png inflating: SegmentationClassAug_Visualization/2008_003112.png inflating: SegmentationClassAug_Visualization/2010_005458.png inflating: SegmentationClassAug_Visualization/2008_002773.png inflating: SegmentationClassAug_Visualization/2008_006662.png inflating: SegmentationClassAug_Visualization/2008_006211.png inflating: SegmentationClassAug_Visualization/2008_008695.png inflating: SegmentationClassAug_Visualization/2010_005320.png inflating: SegmentationClassAug_Visualization/2009_002808.png inflating: SegmentationClassAug_Visualization/2009_000896.png inflating: SegmentationClassAug_Visualization/2010_004312.png inflating: SegmentationClassAug_Visualization/2008_003652.png inflating: SegmentationClassAug_Visualization/2010_001978.png inflating: SegmentationClassAug_Visualization/2009_002559.png inflating: SegmentationClassAug_Visualization/2008_003088.png inflating: SegmentationClassAug_Visualization/2010_001933.png inflating: SegmentationClassAug_Visualization/2009_001456.png inflating: SegmentationClassAug_Visualization/2008_002917.png inflating: SegmentationClassAug_Visualization/2008_006956.png inflating: SegmentationClassAug_Visualization/2008_006998.png inflating: SegmentationClassAug_Visualization/2011_002974.png inflating: SegmentationClassAug_Visualization/2010_002418.png inflating: SegmentationClassAug_Visualization/2010_003752.png inflating: SegmentationClassAug_Visualization/2009_002752.png inflating: SegmentationClassAug_Visualization/2010_004806.png inflating: SegmentationClassAug_Visualization/2008_001666.png inflating: SegmentationClassAug_Visualization/2011_002652.png inflating: SegmentationClassAug_Visualization/2010_002982.png inflating: SegmentationClassAug_Visualization/2007_003742.png inflating: SegmentationClassAug_Visualization/2008_002603.png inflating: SegmentationClassAug_Visualization/2009_002669.png inflating: SegmentationClassAug_Visualization/2008_004771.png inflating: SegmentationClassAug_Visualization/2008_005637.png inflating: SegmentationClassAug_Visualization/2010_003547.png inflating: SegmentationClassAug_Visualization/2009_002553.png inflating: SegmentationClassAug_Visualization/2009_001409.png inflating: SegmentationClassAug_Visualization/2008_007274.png inflating: SegmentationClassAug_Visualization/2009_003638.png inflating: SegmentationClassAug_Visualization/2010_003632.png inflating: SegmentationClassAug_Visualization/2010_001921.png inflating: SegmentationClassAug_Visualization/2009_002268.png inflating: SegmentationClassAug_Visualization/2008_000881.png inflating: SegmentationClassAug_Visualization/2010_003656.png inflating: SegmentationClassAug_Visualization/2011_000068.png inflating: SegmentationClassAug_Visualization/2007_004988.png inflating: SegmentationClassAug_Visualization/2010_003204.png inflating: SegmentationClassAug_Visualization/2010_006067.png inflating: SegmentationClassAug_Visualization/2009_003900.png inflating: SegmentationClassAug_Visualization/2008_005686.png inflating: SegmentationClassAug_Visualization/2009_003018.png inflating: SegmentationClassAug_Visualization/2008_005531.png inflating: SegmentationClassAug_Visualization/2008_006409.png inflating: SegmentationClassAug_Visualization/2008_002492.png inflating: SegmentationClassAug_Visualization/2008_000673.png inflating: SegmentationClassAug_Visualization/2011_000027.png inflating: SegmentationClassAug_Visualization/2008_002910.png inflating: SegmentationClassAug_Visualization/2009_002514.png inflating: SegmentationClassAug_Visualization/2011_000152.png inflating: SegmentationClassAug_Visualization/2010_005594.png inflating: SegmentationClassAug_Visualization/2009_003656.png inflating: SegmentationClassAug_Visualization/2008_002155.png inflating: SegmentationClassAug_Visualization/2008_007164.png inflating: SegmentationClassAug_Visualization/2010_005512.png inflating: SegmentationClassAug_Visualization/2008_003488.png inflating: SegmentationClassAug_Visualization/2008_004583.png inflating: SegmentationClassAug_Visualization/2011_000202.png inflating: SegmentationClassAug_Visualization/2008_000599.png inflating: SegmentationClassAug_Visualization/2010_000295.png inflating: SegmentationClassAug_Visualization/2008_003684.png inflating: SegmentationClassAug_Visualization/2008_003856.png inflating: SegmentationClassAug_Visualization/2009_005246.png inflating: SegmentationClassAug_Visualization/2011_002227.png inflating: SegmentationClassAug_Visualization/2008_005816.png inflating: SegmentationClassAug_Visualization/2007_007881.png inflating: SegmentationClassAug_Visualization/2010_000246.png inflating: SegmentationClassAug_Visualization/2009_002792.png inflating: SegmentationClassAug_Visualization/2008_006746.png inflating: SegmentationClassAug_Visualization/2009_003425.png inflating: SegmentationClassAug_Visualization/2009_004118.png inflating: SegmentationClassAug_Visualization/2008_007021.png inflating: SegmentationClassAug_Visualization/2010_003451.png inflating: SegmentationClassAug_Visualization/2010_002457.png inflating: SegmentationClassAug_Visualization/2008_003437.png inflating: SegmentationClassAug_Visualization/2009_004095.png inflating: SegmentationClassAug_Visualization/2010_004959.png inflating: SegmentationClassAug_Visualization/2008_007101.png inflating: SegmentationClassAug_Visualization/2008_007504.png inflating: SegmentationClassAug_Visualization/2010_002128.png inflating: SegmentationClassAug_Visualization/2011_000850.png inflating: SegmentationClassAug_Visualization/2009_001519.png inflating: SegmentationClassAug_Visualization/2008_002838.png inflating: SegmentationClassAug_Visualization/2008_002067.png inflating: SegmentationClassAug_Visualization/2010_002501.png inflating: SegmentationClassAug_Visualization/2010_001405.png inflating: SegmentationClassAug_Visualization/2008_000829.png inflating: SegmentationClassAug_Visualization/2009_003776.png inflating: SegmentationClassAug_Visualization/2008_001789.png inflating: SegmentationClassAug_Visualization/2010_000293.png inflating: SegmentationClassAug_Visualization/2008_008755.png inflating: SegmentationClassAug_Visualization/2010_002884.png inflating: SegmentationClassAug_Visualization/2009_003607.png inflating: SegmentationClassAug_Visualization/2010_001784.png inflating: SegmentationClassAug_Visualization/2010_004027.png inflating: SegmentationClassAug_Visualization/2011_001602.png inflating: SegmentationClassAug_Visualization/2008_006224.png inflating: SegmentationClassAug_Visualization/2008_001542.png inflating: SegmentationClassAug_Visualization/2008_003045.png inflating: SegmentationClassAug_Visualization/2008_001057.png inflating: SegmentationClassAug_Visualization/2010_004478.png inflating: SegmentationClassAug_Visualization/2011_002174.png inflating: SegmentationClassAug_Visualization/2009_004083.png inflating: SegmentationClassAug_Visualization/2009_000626.png inflating: SegmentationClassAug_Visualization/2010_001893.png inflating: SegmentationClassAug_Visualization/2008_006561.png inflating: SegmentationClassAug_Visualization/2010_001590.png inflating: SegmentationClassAug_Visualization/2008_003782.png inflating: SegmentationClassAug_Visualization/2011_002997.png inflating: SegmentationClassAug_Visualization/2010_005551.png inflating: SegmentationClassAug_Visualization/2008_007882.png inflating: SegmentationClassAug_Visualization/2008_000445.png inflating: SegmentationClassAug_Visualization/2011_000749.png inflating: SegmentationClassAug_Visualization/2009_004166.png inflating: SegmentationClassAug_Visualization/2010_000374.png inflating: SegmentationClassAug_Visualization/2008_006793.png inflating: SegmentationClassAug_Visualization/2008_005367.png inflating: SegmentationClassAug_Visualization/2010_004930.png inflating: SegmentationClassAug_Visualization/2009_001312.png inflating: SegmentationClassAug_Visualization/2010_000811.png inflating: SegmentationClassAug_Visualization/2010_002000.png inflating: SegmentationClassAug_Visualization/2008_003835.png inflating: SegmentationClassAug_Visualization/2009_003367.png inflating: SegmentationClassAug_Visualization/2010_001951.png inflating: SegmentationClassAug_Visualization/2008_002064.png inflating: SegmentationClassAug_Visualization/2009_000930.png inflating: SegmentationClassAug_Visualization/2008_006133.png inflating: SegmentationClassAug_Visualization/2010_000327.png inflating: SegmentationClassAug_Visualization/2008_008628.png inflating: SegmentationClassAug_Visualization/2009_003847.png inflating: SegmentationClassAug_Visualization/2010_004325.png inflating: SegmentationClassAug_Visualization/2008_001068.png inflating: SegmentationClassAug_Visualization/2010_001219.png inflating: SegmentationClassAug_Visualization/2011_000112.png inflating: SegmentationClassAug_Visualization/2008_007185.png inflating: SegmentationClassAug_Visualization/2010_000679.png inflating: SegmentationClassAug_Visualization/2008_003021.png inflating: SegmentationClassAug_Visualization/2011_002812.png inflating: SegmentationClassAug_Visualization/2010_004848.png inflating: SegmentationClassAug_Visualization/2008_005774.png inflating: SegmentationClassAug_Visualization/2010_000197.png inflating: SegmentationClassAug_Visualization/2011_002102.png inflating: SegmentationClassAug_Visualization/2010_002274.png inflating: SegmentationClassAug_Visualization/2008_001690.png inflating: SegmentationClassAug_Visualization/2008_007909.png inflating: SegmentationClassAug_Visualization/2010_000103.png inflating: SegmentationClassAug_Visualization/2010_006041.png inflating: SegmentationClassAug_Visualization/2009_003572.png inflating: SegmentationClassAug_Visualization/2008_000443.png inflating: SegmentationClassAug_Visualization/2010_002693.png inflating: SegmentationClassAug_Visualization/2009_000664.png inflating: SegmentationClassAug_Visualization/2010_003232.png inflating: SegmentationClassAug_Visualization/2010_001240.png inflating: SegmentationClassAug_Visualization/2008_003967.png inflating: SegmentationClassAug_Visualization/2010_002734.png inflating: SegmentationClassAug_Visualization/2008_002198.png inflating: SegmentationClassAug_Visualization/2008_003515.png inflating: SegmentationClassAug_Visualization/2011_002548.png inflating: SegmentationClassAug_Visualization/2008_007912.png inflating: SegmentationClassAug_Visualization/2008_004945.png inflating: SegmentationClassAug_Visualization/2008_003020.png inflating: SegmentationClassAug_Visualization/2010_000484.png inflating: SegmentationClassAug_Visualization/2008_006948.png inflating: SegmentationClassAug_Visualization/2009_001965.png inflating: SegmentationClassAug_Visualization/2010_002244.png inflating: SegmentationClassAug_Visualization/2010_000821.png inflating: SegmentationClassAug_Visualization/2010_001998.png inflating: SegmentationClassAug_Visualization/2009_001264.png inflating: SegmentationClassAug_Visualization/2009_000411.png inflating: SegmentationClassAug_Visualization/2010_002219.png inflating: SegmentationClassAug_Visualization/2008_007060.png inflating: SegmentationClassAug_Visualization/2011_001323.png inflating: SegmentationClassAug_Visualization/2008_000123.png inflating: SegmentationClassAug_Visualization/2011_000541.png inflating: SegmentationClassAug_Visualization/2008_000196.png inflating: SegmentationClassAug_Visualization/2007_009052.png inflating: SegmentationClassAug_Visualization/2008_000330.png inflating: SegmentationClassAug_Visualization/2008_000222.png inflating: SegmentationClassAug_Visualization/2008_003618.png inflating: SegmentationClassAug_Visualization/2010_005463.png inflating: SegmentationClassAug_Visualization/2008_000676.png inflating: SegmentationClassAug_Visualization/2010_004757.png inflating: SegmentationClassAug_Visualization/2011_002021.png inflating: SegmentationClassAug_Visualization/2008_000567.png inflating: SegmentationClassAug_Visualization/2007_006400.png inflating: SegmentationClassAug_Visualization/2007_009084.png inflating: SegmentationClassAug_Visualization/2008_005213.png inflating: SegmentationClassAug_Visualization/2011_002055.png inflating: SegmentationClassAug_Visualization/2011_000791.png inflating: SegmentationClassAug_Visualization/2008_002762.png inflating: SegmentationClassAug_Visualization/2010_001807.png inflating: SegmentationClassAug_Visualization/2008_002094.png inflating: SegmentationClassAug_Visualization/2009_003090.png inflating: SegmentationClassAug_Visualization/2010_000557.png inflating: SegmentationClassAug_Visualization/2008_001135.png inflating: SegmentationClassAug_Visualization/2010_003961.png inflating: SegmentationClassAug_Visualization/2010_000863.png inflating: SegmentationClassAug_Visualization/2008_004866.png inflating: SegmentationClassAug_Visualization/2009_004904.png inflating: SegmentationClassAug_Visualization/2008_007596.png inflating: SegmentationClassAug_Visualization/2008_000032.png inflating: SegmentationClassAug_Visualization/2008_002177.png inflating: SegmentationClassAug_Visualization/2008_004620.png inflating: SegmentationClassAug_Visualization/2009_000097.png inflating: SegmentationClassAug_Visualization/2011_001984.png inflating: SegmentationClassAug_Visualization/2008_006941.png inflating: SegmentationClassAug_Visualization/2007_005314.png inflating: SegmentationClassAug_Visualization/2008_005938.png inflating: SegmentationClassAug_Visualization/2008_007165.png inflating: SegmentationClassAug_Visualization/2009_003320.png inflating: SegmentationClassAug_Visualization/2011_001163.png inflating: SegmentationClassAug_Visualization/2009_004419.png inflating: SegmentationClassAug_Visualization/2008_006811.png inflating: SegmentationClassAug_Visualization/2007_001420.png inflating: SegmentationClassAug_Visualization/2008_007521.png inflating: SegmentationClassAug_Visualization/2007_009817.png inflating: SegmentationClassAug_Visualization/2010_000678.png inflating: SegmentationClassAug_Visualization/2010_004980.png inflating: SegmentationClassAug_Visualization/2008_008040.png inflating: SegmentationClassAug_Visualization/2009_004091.png inflating: SegmentationClassAug_Visualization/2009_004766.png inflating: SegmentationClassAug_Visualization/2009_002443.png inflating: SegmentationClassAug_Visualization/2009_002103.png inflating: SegmentationClassAug_Visualization/2008_004910.png inflating: SegmentationClassAug_Visualization/2009_000006.png inflating: SegmentationClassAug_Visualization/2010_005187.png inflating: SegmentationClassAug_Visualization/2008_001119.png inflating: SegmentationClassAug_Visualization/2008_006908.png inflating: SegmentationClassAug_Visualization/2010_002507.png inflating: SegmentationClassAug_Visualization/2010_003074.png inflating: SegmentationClassAug_Visualization/2008_003820.png inflating: SegmentationClassAug_Visualization/2011_000233.png inflating: SegmentationClassAug_Visualization/2010_004971.png inflating: SegmentationClassAug_Visualization/2009_001621.png inflating: SegmentationClassAug_Visualization/2008_002471.png inflating: SegmentationClassAug_Visualization/2008_001389.png inflating: SegmentationClassAug_Visualization/2008_002666.png inflating: SegmentationClassAug_Visualization/2007_002293.png inflating: SegmentationClassAug_Visualization/2008_000003.png inflating: SegmentationClassAug_Visualization/2010_000996.png inflating: SegmentationClassAug_Visualization/2010_004751.png inflating: SegmentationClassAug_Visualization/2007_004392.png inflating: SegmentationClassAug_Visualization/2009_004322.png inflating: SegmentationClassAug_Visualization/2008_005494.png inflating: SegmentationClassAug_Visualization/2008_004756.png inflating: SegmentationClassAug_Visualization/2010_005725.png inflating: SegmentationClassAug_Visualization/2010_004828.png inflating: SegmentationClassAug_Visualization/2011_001950.png inflating: SegmentationClassAug_Visualization/2011_001974.png inflating: SegmentationClassAug_Visualization/2008_002526.png inflating: SegmentationClassAug_Visualization/2009_000882.png inflating: SegmentationClassAug_Visualization/2009_003214.png inflating: SegmentationClassAug_Visualization/2008_008440.png inflating: SegmentationClassAug_Visualization/2011_000071.png inflating: SegmentationClassAug_Visualization/2008_000917.png inflating: SegmentationClassAug_Visualization/2009_004888.png inflating: SegmentationClassAug_Visualization/2008_005295.png inflating: SegmentationClassAug_Visualization/2008_004752.png inflating: SegmentationClassAug_Visualization/2008_006816.png inflating: SegmentationClassAug_Visualization/2008_000045.png inflating: SegmentationClassAug_Visualization/2009_002137.png inflating: SegmentationClassAug_Visualization/2008_004914.png inflating: SegmentationClassAug_Visualization/2010_005133.png inflating: SegmentationClassAug_Visualization/2008_006410.png inflating: SegmentationClassAug_Visualization/2008_004053.png inflating: SegmentationClassAug_Visualization/2009_000558.png inflating: SegmentationClassAug_Visualization/2011_001333.png inflating: SegmentationClassAug_Visualization/2008_003447.png inflating: SegmentationClassAug_Visualization/2009_003278.png inflating: SegmentationClassAug_Visualization/2008_003565.png inflating: SegmentationClassAug_Visualization/2009_003995.png inflating: SegmentationClassAug_Visualization/2009_004730.png inflating: SegmentationClassAug_Visualization/2010_005474.png inflating: SegmentationClassAug_Visualization/2008_004624.png inflating: SegmentationClassAug_Visualization/2010_005273.png inflating: SegmentationClassAug_Visualization/2008_000236.png inflating: SegmentationClassAug_Visualization/2008_004297.png inflating: SegmentationClassAug_Visualization/2008_003590.png inflating: SegmentationClassAug_Visualization/2008_000422.png inflating: SegmentationClassAug_Visualization/2010_004417.png inflating: SegmentationClassAug_Visualization/2007_006841.png inflating: SegmentationClassAug_Visualization/2008_000364.png inflating: SegmentationClassAug_Visualization/2007_008219.png inflating: SegmentationClassAug_Visualization/2008_003830.png inflating: SegmentationClassAug_Visualization/2009_004404.png inflating: SegmentationClassAug_Visualization/2010_002445.png inflating: SegmentationClassAug_Visualization/2008_003280.png inflating: SegmentationClassAug_Visualization/2008_005404.png inflating: SegmentationClassAug_Visualization/2009_005025.png inflating: SegmentationClassAug_Visualization/2007_005688.png inflating: SegmentationClassAug_Visualization/2009_002579.png inflating: SegmentationClassAug_Visualization/2008_007045.png inflating: SegmentationClassAug_Visualization/2007_008407.png inflating: SegmentationClassAug_Visualization/2010_005764.png inflating: SegmentationClassAug_Visualization/2009_001106.png inflating: SegmentationClassAug_Visualization/2008_007994.png inflating: SegmentationClassAug_Visualization/2010_004448.png inflating: SegmentationClassAug_Visualization/2010_002817.png inflating: SegmentationClassAug_Visualization/2009_001113.png inflating: SegmentationClassAug_Visualization/2011_002260.png inflating: SegmentationClassAug_Visualization/2008_008547.png inflating: SegmentationClassAug_Visualization/2008_007064.png inflating: SegmentationClassAug_Visualization/2011_001791.png inflating: SegmentationClassAug_Visualization/2008_003289.png inflating: SegmentationClassAug_Visualization/2008_007011.png inflating: SegmentationClassAug_Visualization/2011_002167.png inflating: SegmentationClassAug_Visualization/2008_008112.png inflating: SegmentationClassAug_Visualization/2010_005232.png inflating: SegmentationClassAug_Visualization/2009_003541.png inflating: SegmentationClassAug_Visualization/2011_000554.png inflating: SegmentationClassAug_Visualization/2008_005641.png inflating: SegmentationClassAug_Visualization/2011_002614.png inflating: SegmentationClassAug_Visualization/2008_006837.png inflating: SegmentationClassAug_Visualization/2008_001787.png inflating: SegmentationClassAug_Visualization/2009_002999.png inflating: SegmentationClassAug_Visualization/2009_000559.png inflating: SegmentationClassAug_Visualization/2008_005623.png inflating: SegmentationClassAug_Visualization/2009_004453.png inflating: SegmentationClassAug_Visualization/2010_001674.png inflating: SegmentationClassAug_Visualization/2008_002647.png inflating: SegmentationClassAug_Visualization/2008_000134.png inflating: SegmentationClassAug_Visualization/2008_008122.png inflating: SegmentationClassAug_Visualization/2009_003971.png inflating: SegmentationClassAug_Visualization/2008_007131.png inflating: SegmentationClassAug_Visualization/2008_003362.png inflating: SegmentationClassAug_Visualization/2007_009221.png inflating: SegmentationClassAug_Visualization/2009_004543.png inflating: SegmentationClassAug_Visualization/2008_002792.png inflating: SegmentationClassAug_Visualization/2009_003138.png inflating: SegmentationClassAug_Visualization/2009_002662.png inflating: SegmentationClassAug_Visualization/2008_003439.png inflating: SegmentationClassAug_Visualization/2008_007676.png inflating: SegmentationClassAug_Visualization/2010_003239.png inflating: SegmentationClassAug_Visualization/2009_002191.png inflating: SegmentationClassAug_Visualization/2008_000697.png inflating: SegmentationClassAug_Visualization/2008_004610.png inflating: SegmentationClassAug_Visualization/2009_001172.png inflating: SegmentationClassAug_Visualization/2010_005827.png inflating: SegmentationClassAug_Visualization/2008_000564.png inflating: SegmentationClassAug_Visualization/2009_001651.png inflating: SegmentationClassAug_Visualization/2009_001301.png inflating: SegmentationClassAug_Visualization/2008_001271.png inflating: SegmentationClassAug_Visualization/2007_008374.png inflating: SegmentationClassAug_Visualization/2008_004391.png inflating: SegmentationClassAug_Visualization/2011_000578.png inflating: SegmentationClassAug_Visualization/2009_001349.png inflating: SegmentationClassAug_Visualization/2007_007154.png inflating: SegmentationClassAug_Visualization/2008_002701.png inflating: SegmentationClassAug_Visualization/2008_002212.png inflating: SegmentationClassAug_Visualization/2009_002573.png inflating: SegmentationClassAug_Visualization/2010_005274.png inflating: SegmentationClassAug_Visualization/2008_008155.png inflating: SegmentationClassAug_Visualization/2008_008615.png inflating: SegmentationClassAug_Visualization/2009_001715.png inflating: SegmentationClassAug_Visualization/2008_005359.png inflating: SegmentationClassAug_Visualization/2008_002650.png inflating: SegmentationClassAug_Visualization/2010_004208.png inflating: SegmentationClassAug_Visualization/2010_003409.png inflating: SegmentationClassAug_Visualization/2008_005631.png inflating: SegmentationClassAug_Visualization/2010_003772.png inflating: SegmentationClassAug_Visualization/2011_002357.png inflating: SegmentationClassAug_Visualization/2008_001869.png inflating: SegmentationClassAug_Visualization/2008_005133.png inflating: SegmentationClassAug_Visualization/2007_000648.png inflating: SegmentationClassAug_Visualization/2010_001360.png inflating: SegmentationClassAug_Visualization/2010_004973.png inflating: SegmentationClassAug_Visualization/2009_003209.png inflating: SegmentationClassAug_Visualization/2007_003876.png inflating: SegmentationClassAug_Visualization/2008_004547.png inflating: SegmentationClassAug_Visualization/2009_003323.png inflating: SegmentationClassAug_Visualization/2007_007591.png inflating: SegmentationClassAug_Visualization/2008_003766.png inflating: SegmentationClassAug_Visualization/2007_004275.png inflating: SegmentationClassAug_Visualization/2008_008150.png inflating: SegmentationClassAug_Visualization/2010_001697.png inflating: SegmentationClassAug_Visualization/2010_000080.png inflating: SegmentationClassAug_Visualization/2008_000185.png inflating: SegmentationClassAug_Visualization/2010_001382.png inflating: SegmentationClassAug_Visualization/2010_004505.png inflating: SegmentationClassAug_Visualization/2008_006778.png inflating: SegmentationClassAug_Visualization/2010_001328.png inflating: SegmentationClassAug_Visualization/2008_004725.png inflating: SegmentationClassAug_Visualization/2008_004510.png inflating: SegmentationClassAug_Visualization/2008_006585.png inflating: SegmentationClassAug_Visualization/2010_000889.png inflating: SegmentationClassAug_Visualization/2009_001266.png inflating: SegmentationClassAug_Visualization/2008_004659.png inflating: SegmentationClassAug_Visualization/2011_002156.png inflating: SegmentationClassAug_Visualization/2008_004677.png inflating: SegmentationClassAug_Visualization/2010_003784.png inflating: SegmentationClassAug_Visualization/2009_004895.png inflating: SegmentationClassAug_Visualization/2010_005715.png inflating: SegmentationClassAug_Visualization/2009_004921.png inflating: SegmentationClassAug_Visualization/2008_006900.png inflating: SegmentationClassAug_Visualization/2008_002965.png inflating: SegmentationClassAug_Visualization/2009_001160.png inflating: SegmentationClassAug_Visualization/2010_002448.png inflating: SegmentationClassAug_Visualization/2009_004619.png inflating: SegmentationClassAug_Visualization/2011_001188.png inflating: SegmentationClassAug_Visualization/2010_000748.png inflating: SegmentationClassAug_Visualization/2008_008314.png inflating: SegmentationClassAug_Visualization/2010_005292.png inflating: SegmentationClassAug_Visualization/2008_007623.png inflating: SegmentationClassAug_Visualization/2008_007914.png inflating: SegmentationClassAug_Visualization/2008_006303.png inflating: SegmentationClassAug_Visualization/2008_007485.png inflating: SegmentationClassAug_Visualization/2009_002236.png inflating: SegmentationClassAug_Visualization/2008_007936.png inflating: SegmentationClassAug_Visualization/2009_000522.png inflating: SegmentationClassAug_Visualization/2010_002771.png inflating: SegmentationClassAug_Visualization/2009_000624.png inflating: SegmentationClassAug_Visualization/2008_006276.png inflating: SegmentationClassAug_Visualization/2007_002266.png inflating: SegmentationClassAug_Visualization/2009_001689.png inflating: SegmentationClassAug_Visualization/2010_006028.png inflating: SegmentationClassAug_Visualization/2010_002876.png inflating: SegmentationClassAug_Visualization/2010_003673.png inflating: SegmentationClassAug_Visualization/2008_001866.png inflating: SegmentationClassAug_Visualization/2011_000848.png inflating: SegmentationClassAug_Visualization/2008_004242.png inflating: SegmentationClassAug_Visualization/2008_002616.png inflating: SegmentationClassAug_Visualization/2009_000756.png inflating: SegmentationClassAug_Visualization/2008_005243.png inflating: SegmentationClassAug_Visualization/2008_006447.png inflating: SegmentationClassAug_Visualization/2011_003183.png inflating: SegmentationClassAug_Visualization/2008_003048.png inflating: SegmentationClassAug_Visualization/2010_005614.png inflating: SegmentationClassAug_Visualization/2008_005612.png inflating: SegmentationClassAug_Visualization/2008_005505.png inflating: SegmentationClassAug_Visualization/2010_004179.png inflating: SegmentationClassAug_Visualization/2008_001221.png inflating: SegmentationClassAug_Visualization/2009_002499.png inflating: SegmentationClassAug_Visualization/2009_002867.png inflating: SegmentationClassAug_Visualization/2008_007841.png inflating: SegmentationClassAug_Visualization/2009_002107.png inflating: SegmentationClassAug_Visualization/2011_001591.png inflating: SegmentationClassAug_Visualization/2011_001406.png inflating: SegmentationClassAug_Visualization/2009_000730.png inflating: SegmentationClassAug_Visualization/2010_005192.png inflating: SegmentationClassAug_Visualization/2008_001478.png inflating: SegmentationClassAug_Visualization/2011_001198.png inflating: SegmentationClassAug_Visualization/2008_003675.png inflating: SegmentationClassAug_Visualization/2010_005671.png inflating: SegmentationClassAug_Visualization/2009_000104.png inflating: SegmentationClassAug_Visualization/2011_000268.png inflating: SegmentationClassAug_Visualization/2008_005473.png inflating: SegmentationClassAug_Visualization/2008_008320.png inflating: SegmentationClassAug_Visualization/2008_007816.png inflating: SegmentationClassAug_Visualization/2010_002814.png inflating: SegmentationClassAug_Visualization/2008_002080.png inflating: SegmentationClassAug_Visualization/2008_005821.png inflating: SegmentationClassAug_Visualization/2010_001126.png inflating: SegmentationClassAug_Visualization/2008_001899.png inflating: SegmentationClassAug_Visualization/2010_005123.png inflating: SegmentationClassAug_Visualization/2008_004783.png inflating: SegmentationClassAug_Visualization/2011_000213.png inflating: SegmentationClassAug_Visualization/2008_002026.png inflating: SegmentationClassAug_Visualization/2009_001105.png inflating: SegmentationClassAug_Visualization/2011_002925.png inflating: SegmentationClassAug_Visualization/2011_001977.png inflating: SegmentationClassAug_Visualization/2009_002391.png inflating: SegmentationClassAug_Visualization/2010_001154.png inflating: SegmentationClassAug_Visualization/2011_001302.png inflating: SegmentationClassAug_Visualization/2009_000409.png inflating: SegmentationClassAug_Visualization/2008_006239.png inflating: SegmentationClassAug_Visualization/2009_000488.png inflating: SegmentationClassAug_Visualization/2008_005956.png inflating: SegmentationClassAug_Visualization/2008_004606.png inflating: SegmentationClassAug_Visualization/2010_005285.png inflating: SegmentationClassAug_Visualization/2008_001249.png inflating: SegmentationClassAug_Visualization/2008_002000.png inflating: SegmentationClassAug_Visualization/2010_005804.png inflating: SegmentationClassAug_Visualization/2007_003349.png inflating: SegmentationClassAug_Visualization/2010_004520.png inflating: SegmentationClassAug_Visualization/2009_004969.png inflating: SegmentationClassAug_Visualization/2008_005439.png inflating: SegmentationClassAug_Visualization/2009_003903.png inflating: SegmentationClassAug_Visualization/2010_002227.png inflating: SegmentationClassAug_Visualization/2010_003035.png inflating: SegmentationClassAug_Visualization/2011_000806.png inflating: SegmentationClassAug_Visualization/2007_007230.png inflating: SegmentationClassAug_Visualization/2010_005223.png inflating: SegmentationClassAug_Visualization/2011_000109.png inflating: SegmentationClassAug_Visualization/2009_000078.png inflating: SegmentationClassAug_Visualization/2008_005512.png inflating: SegmentationClassAug_Visualization/2010_002851.png inflating: SegmentationClassAug_Visualization/2009_001107.png inflating: SegmentationClassAug_Visualization/2008_005008.png inflating: SegmentationClassAug_Visualization/2008_004328.png inflating: SegmentationClassAug_Visualization/2008_002672.png inflating: SegmentationClassAug_Visualization/2011_002724.png inflating: SegmentationClassAug_Visualization/2009_000370.png inflating: SegmentationClassAug_Visualization/2008_003167.png inflating: SegmentationClassAug_Visualization/2009_005111.png inflating: SegmentationClassAug_Visualization/2008_001318.png inflating: SegmentationClassAug_Visualization/2007_001872.png inflating: SegmentationClassAug_Visualization/2009_004572.png inflating: SegmentationClassAug_Visualization/2008_004367.png inflating: SegmentationClassAug_Visualization/2008_003576.png inflating: SegmentationClassAug_Visualization/2010_002780.png inflating: SegmentationClassAug_Visualization/2008_003956.png inflating: SegmentationClassAug_Visualization/2009_002457.png inflating: SegmentationClassAug_Visualization/2008_004697.png inflating: SegmentationClassAug_Visualization/2010_004282.png inflating: SegmentationClassAug_Visualization/2008_001709.png inflating: SegmentationClassAug_Visualization/2008_007197.png inflating: SegmentationClassAug_Visualization/2008_000112.png inflating: SegmentationClassAug_Visualization/2008_001196.png inflating: SegmentationClassAug_Visualization/2007_009422.png inflating: SegmentationClassAug_Visualization/2008_005467.png inflating: SegmentationClassAug_Visualization/2008_002719.png inflating: SegmentationClassAug_Visualization/2009_002588.png inflating: SegmentationClassAug_Visualization/2011_003256.png inflating: SegmentationClassAug_Visualization/2008_003585.png inflating: SegmentationClassAug_Visualization/2010_006034.png inflating: SegmentationClassAug_Visualization/2008_008365.png inflating: SegmentationClassAug_Visualization/2010_002758.png inflating: SegmentationClassAug_Visualization/2010_003770.png inflating: SegmentationClassAug_Visualization/2010_003366.png inflating: SegmentationClassAug_Visualization/2010_001481.png inflating: SegmentationClassAug_Visualization/2008_002118.png inflating: SegmentationClassAug_Visualization/2009_002635.png inflating: SegmentationClassAug_Visualization/2010_004728.png inflating: SegmentationClassAug_Visualization/2008_006429.png inflating: SegmentationClassAug_Visualization/2010_003773.png inflating: SegmentationClassAug_Visualization/2010_001849.png inflating: SegmentationClassAug_Visualization/2009_003702.png inflating: SegmentationClassAug_Visualization/2008_000782.png inflating: SegmentationClassAug_Visualization/2008_005338.png inflating: SegmentationClassAug_Visualization/2010_005958.png inflating: SegmentationClassAug_Visualization/2010_001719.png inflating: SegmentationClassAug_Visualization/2011_003151.png inflating: SegmentationClassAug_Visualization/2008_004689.png inflating: SegmentationClassAug_Visualization/2009_002972.png inflating: SegmentationClassAug_Visualization/2011_001535.png inflating: SegmentationClassAug_Visualization/2008_004615.png inflating: SegmentationClassAug_Visualization/2008_001781.png inflating: SegmentationClassAug_Visualization/2009_000305.png inflating: SegmentationClassAug_Visualization/2011_002006.png inflating: SegmentationClassAug_Visualization/2009_001024.png inflating: SegmentationClassAug_Visualization/2007_006444.png inflating: SegmentationClassAug_Visualization/2008_008166.png inflating: SegmentationClassAug_Visualization/2011_000069.png inflating: SegmentationClassAug_Visualization/2010_005063.png inflating: SegmentationClassAug_Visualization/2011_000957.png inflating: SegmentationClassAug_Visualization/2008_005115.png inflating: SegmentationClassAug_Visualization/2009_004744.png inflating: SegmentationClassAug_Visualization/2009_002703.png inflating: SegmentationClassAug_Visualization/2010_003290.png inflating: SegmentationClassAug_Visualization/2008_007697.png inflating: SegmentationClassAug_Visualization/2010_003301.png inflating: SegmentationClassAug_Visualization/2008_007534.png inflating: SegmentationClassAug_Visualization/2010_001288.png inflating: SegmentationClassAug_Visualization/2009_002173.png inflating: SegmentationClassAug_Visualization/2008_008606.png inflating: SegmentationClassAug_Visualization/2011_001698.png inflating: SegmentationClassAug_Visualization/2010_006073.png inflating: SegmentationClassAug_Visualization/2008_006000.png inflating: SegmentationClassAug_Visualization/2009_002954.png inflating: SegmentationClassAug_Visualization/2010_000915.png inflating: SegmentationClassAug_Visualization/2008_006032.png inflating: SegmentationClassAug_Visualization/2009_004635.png inflating: SegmentationClassAug_Visualization/2010_005055.png inflating: SegmentationClassAug_Visualization/2010_002531.png inflating: SegmentationClassAug_Visualization/2009_001535.png inflating: SegmentationClassAug_Visualization/2008_000976.png inflating: SegmentationClassAug_Visualization/2011_001327.png inflating: SegmentationClassAug_Visualization/2009_000399.png inflating: SegmentationClassAug_Visualization/2008_006384.png inflating: SegmentationClassAug_Visualization/2009_005257.png inflating: SegmentationClassAug_Visualization/2010_004352.png inflating: SegmentationClassAug_Visualization/2010_003805.png inflating: SegmentationClassAug_Visualization/2008_008105.png inflating: SegmentationClassAug_Visualization/2010_004244.png inflating: SegmentationClassAug_Visualization/2007_009348.png inflating: SegmentationClassAug_Visualization/2008_002225.png inflating: SegmentationClassAug_Visualization/2010_005747.png inflating: SegmentationClassAug_Visualization/2010_000545.png inflating: SegmentationClassAug_Visualization/2010_005483.png inflating: SegmentationClassAug_Visualization/2008_003359.png inflating: SegmentationClassAug_Visualization/2007_003169.png inflating: SegmentationClassAug_Visualization/2007_005844.png inflating: SegmentationClassAug_Visualization/2009_003912.png inflating: SegmentationClassAug_Visualization/2010_003257.png inflating: SegmentationClassAug_Visualization/2010_004861.png inflating: SegmentationClassAug_Visualization/2010_005748.png inflating: SegmentationClassAug_Visualization/2010_004948.png inflating: SegmentationClassAug_Visualization/2009_005205.png inflating: SegmentationClassAug_Visualization/2009_002993.png inflating: SegmentationClassAug_Visualization/2008_004533.png inflating: SegmentationClassAug_Visualization/2011_002933.png inflating: SegmentationClassAug_Visualization/2009_004177.png inflating: SegmentationClassAug_Visualization/2009_000330.png inflating: SegmentationClassAug_Visualization/2009_003732.png inflating: SegmentationClassAug_Visualization/2010_005130.png inflating: SegmentationClassAug_Visualization/2008_007459.png inflating: SegmentationClassAug_Visualization/2008_004245.png inflating: SegmentationClassAug_Visualization/2008_001170.png inflating: SegmentationClassAug_Visualization/2010_001838.png inflating: SegmentationClassAug_Visualization/2008_008294.png inflating: SegmentationClassAug_Visualization/2009_002002.png inflating: SegmentationClassAug_Visualization/2008_001112.png inflating: SegmentationClassAug_Visualization/2010_002976.png inflating: SegmentationClassAug_Visualization/2008_006433.png inflating: SegmentationClassAug_Visualization/2008_001137.png inflating: SegmentationClassAug_Visualization/2008_007613.png inflating: SegmentationClassAug_Visualization/2010_002786.png inflating: SegmentationClassAug_Visualization/2008_005716.png inflating: SegmentationClassAug_Visualization/2010_003634.png inflating: SegmentationClassAug_Visualization/2009_001414.png inflating: SegmentationClassAug_Visualization/2010_003648.png inflating: SegmentationClassAug_Visualization/2008_005092.png inflating: SegmentationClassAug_Visualization/2009_003790.png inflating: SegmentationClassAug_Visualization/2008_002510.png inflating: SegmentationClassAug_Visualization/2011_001826.png inflating: SegmentationClassAug_Visualization/2008_004175.png inflating: SegmentationClassAug_Visualization/2010_001746.png inflating: SegmentationClassAug_Visualization/2011_002270.png inflating: SegmentationClassAug_Visualization/2008_007893.png inflating: SegmentationClassAug_Visualization/2010_000483.png inflating: SegmentationClassAug_Visualization/2010_000721.png inflating: SegmentationClassAug_Visualization/2009_001055.png inflating: SegmentationClassAug_Visualization/2010_001913.png inflating: SegmentationClassAug_Visualization/2007_009691.png inflating: SegmentationClassAug_Visualization/2009_004579.png inflating: SegmentationClassAug_Visualization/2008_001142.png inflating: SegmentationClassAug_Visualization/2011_001662.png inflating: SegmentationClassAug_Visualization/2009_000542.png inflating: SegmentationClassAug_Visualization/2009_001809.png inflating: SegmentationClassAug_Visualization/2009_004264.png inflating: SegmentationClassAug_Visualization/2008_006530.png inflating: SegmentationClassAug_Visualization/2011_001434.png inflating: SegmentationClassAug_Visualization/2011_002308.png inflating: SegmentationClassAug_Visualization/2008_006483.png inflating: SegmentationClassAug_Visualization/2009_004062.png inflating: SegmentationClassAug_Visualization/2010_004624.png inflating: SegmentationClassAug_Visualization/2011_000893.png inflating: SegmentationClassAug_Visualization/2009_000658.png inflating: SegmentationClassAug_Visualization/2009_001775.png inflating: SegmentationClassAug_Visualization/2011_002410.png inflating: SegmentationClassAug_Visualization/2008_008624.png inflating: SegmentationClassAug_Visualization/2008_005108.png inflating: SegmentationClassAug_Visualization/2008_007949.png inflating: SegmentationClassAug_Visualization/2010_005692.png inflating: SegmentationClassAug_Visualization/2010_003067.png inflating: SegmentationClassAug_Visualization/2010_005596.png inflating: SegmentationClassAug_Visualization/2011_002116.png inflating: SegmentationClassAug_Visualization/2008_002485.png inflating: SegmentationClassAug_Visualization/2010_003302.png inflating: SegmentationClassAug_Visualization/2008_008037.png inflating: SegmentationClassAug_Visualization/2008_006731.png inflating: SegmentationClassAug_Visualization/2010_001465.png inflating: SegmentationClassAug_Visualization/2010_001142.png inflating: SegmentationClassAug_Visualization/2008_005309.png inflating: SegmentationClassAug_Visualization/2010_001869.png inflating: SegmentationClassAug_Visualization/2009_001319.png inflating: SegmentationClassAug_Visualization/2010_000202.png inflating: SegmentationClassAug_Visualization/2008_001806.png inflating: SegmentationClassAug_Visualization/2009_003832.png inflating: SegmentationClassAug_Visualization/2008_006882.png inflating: SegmentationClassAug_Visualization/2010_004060.png inflating: SegmentationClassAug_Visualization/2010_000014.png inflating: SegmentationClassAug_Visualization/2010_002917.png inflating: SegmentationClassAug_Visualization/2008_000858.png inflating: SegmentationClassAug_Visualization/2008_006567.png inflating: SegmentationClassAug_Visualization/2009_003299.png inflating: SegmentationClassAug_Visualization/2010_003162.png inflating: SegmentationClassAug_Visualization/2009_000041.png inflating: SegmentationClassAug_Visualization/2011_001479.png inflating: SegmentationClassAug_Visualization/2009_003992.png inflating: SegmentationClassAug_Visualization/2010_004791.png inflating: SegmentationClassAug_Visualization/2009_002850.png inflating: SegmentationClassAug_Visualization/2010_005935.png inflating: SegmentationClassAug_Visualization/2008_005897.png inflating: SegmentationClassAug_Visualization/2011_003012.png inflating: SegmentationClassAug_Visualization/2008_000943.png inflating: SegmentationClassAug_Visualization/2007_005547.png inflating: SegmentationClassAug_Visualization/2010_003811.png inflating: SegmentationClassAug_Visualization/2010_004043.png inflating: SegmentationClassAug_Visualization/2009_001357.png inflating: SegmentationClassAug_Visualization/2010_002255.png inflating: SegmentationClassAug_Visualization/2011_002912.png inflating: SegmentationClassAug_Visualization/2011_002422.png inflating: SegmentationClassAug_Visualization/2008_005015.png inflating: SegmentationClassAug_Visualization/2010_004263.png inflating: SegmentationClassAug_Visualization/2009_001806.png inflating: SegmentationClassAug_Visualization/2009_002876.png inflating: SegmentationClassAug_Visualization/2010_003864.png inflating: SegmentationClassAug_Visualization/2010_003097.png inflating: SegmentationClassAug_Visualization/2009_004410.png inflating: SegmentationClassAug_Visualization/2008_000691.png inflating: SegmentationClassAug_Visualization/2007_003143.png inflating: SegmentationClassAug_Visualization/2008_000748.png inflating: SegmentationClassAug_Visualization/2010_005776.png inflating: SegmentationClassAug_Visualization/2007_003194.png inflating: SegmentationClassAug_Visualization/2009_000350.png inflating: SegmentationClassAug_Visualization/2008_003251.png inflating: SegmentationClassAug_Visualization/2009_003711.png inflating: SegmentationClassAug_Visualization/2009_000778.png inflating: SegmentationClassAug_Visualization/2010_001124.png inflating: SegmentationClassAug_Visualization/2008_003996.png inflating: SegmentationClassAug_Visualization/2010_002068.png inflating: SegmentationClassAug_Visualization/2010_003458.png inflating: SegmentationClassAug_Visualization/2008_008125.png inflating: SegmentationClassAug_Visualization/2010_005022.png inflating: SegmentationClassAug_Visualization/2010_002791.png inflating: SegmentationClassAug_Visualization/2008_005839.png inflating: SegmentationClassAug_Visualization/2009_001805.png inflating: SegmentationClassAug_Visualization/2008_005313.png inflating: SegmentationClassAug_Visualization/2009_001376.png inflating: SegmentationClassAug_Visualization/2009_004614.png inflating: SegmentationClassAug_Visualization/2008_002665.png inflating: SegmentationClassAug_Visualization/2010_001253.png inflating: SegmentationClassAug_Visualization/2009_004812.png inflating: SegmentationClassAug_Visualization/2010_001417.png inflating: SegmentationClassAug_Visualization/2009_002615.png inflating: SegmentationClassAug_Visualization/2009_001255.png inflating: SegmentationClassAug_Visualization/2008_003531.png inflating: SegmentationClassAug_Visualization/2010_002425.png inflating: SegmentationClassAug_Visualization/2011_000290.png inflating: SegmentationClassAug_Visualization/2008_002847.png inflating: SegmentationClassAug_Visualization/2008_002961.png inflating: SegmentationClassAug_Visualization/2009_003883.png inflating: SegmentationClassAug_Visualization/2009_002152.png inflating: SegmentationClassAug_Visualization/2008_000541.png inflating: SegmentationClassAug_Visualization/2010_000379.png inflating: SegmentationClassAug_Visualization/2007_006647.png inflating: SegmentationClassAug_Visualization/2009_001372.png inflating: SegmentationClassAug_Visualization/2008_002643.png inflating: SegmentationClassAug_Visualization/2008_002543.png inflating: SegmentationClassAug_Visualization/2008_005850.png inflating: SegmentationClassAug_Visualization/2008_007745.png inflating: SegmentationClassAug_Visualization/2008_002899.png inflating: SegmentationClassAug_Visualization/2008_006936.png inflating: SegmentationClassAug_Visualization/2008_007291.png inflating: SegmentationClassAug_Visualization/2010_002978.png inflating: SegmentationClassAug_Visualization/2011_002916.png inflating: SegmentationClassAug_Visualization/2011_003054.png inflating: SegmentationClassAug_Visualization/2008_002601.png inflating: SegmentationClassAug_Visualization/2011_000129.png inflating: SegmentationClassAug_Visualization/2008_001715.png inflating: SegmentationClassAug_Visualization/2009_003686.png inflating: SegmentationClassAug_Visualization/2008_001114.png inflating: SegmentationClassAug_Visualization/2009_001643.png inflating: SegmentationClassAug_Visualization/2008_001444.png inflating: SegmentationClassAug_Visualization/2008_007390.png inflating: SegmentationClassAug_Visualization/2009_002439.png inflating: SegmentationClassAug_Visualization/2009_004687.png inflating: SegmentationClassAug_Visualization/2009_000915.png inflating: SegmentationClassAug_Visualization/2009_000356.png inflating: SegmentationClassAug_Visualization/2008_001150.png inflating: SegmentationClassAug_Visualization/2008_008208.png inflating: SegmentationClassAug_Visualization/2008_000950.png inflating: SegmentationClassAug_Visualization/2011_003074.png inflating: SegmentationClassAug_Visualization/2008_001390.png inflating: SegmentationClassAug_Visualization/2009_003663.png inflating: SegmentationClassAug_Visualization/2008_004251.png inflating: SegmentationClassAug_Visualization/2010_002313.png inflating: SegmentationClassAug_Visualization/2008_005023.png inflating: SegmentationClassAug_Visualization/2008_005040.png inflating: SegmentationClassAug_Visualization/2008_002882.png inflating: SegmentationClassAug_Visualization/2011_002515.png inflating: SegmentationClassAug_Visualization/2009_004112.png inflating: SegmentationClassAug_Visualization/2011_000789.png inflating: SegmentationClassAug_Visualization/2011_001062.png inflating: SegmentationClassAug_Visualization/2008_006158.png inflating: SegmentationClassAug_Visualization/2009_001975.png inflating: SegmentationClassAug_Visualization/2010_000635.png inflating: SegmentationClassAug_Visualization/2010_000358.png inflating: SegmentationClassAug_Visualization/2008_003252.png inflating: SegmentationClassAug_Visualization/2008_004414.png inflating: SegmentationClassAug_Visualization/2008_001024.png inflating: SegmentationClassAug_Visualization/2008_007997.png inflating: SegmentationClassAug_Visualization/2010_002029.png inflating: SegmentationClassAug_Visualization/2010_002905.png inflating: SegmentationClassAug_Visualization/2009_005078.png inflating: SegmentationClassAug_Visualization/2008_000473.png inflating: SegmentationClassAug_Visualization/2008_003920.png inflating: SegmentationClassAug_Visualization/2009_004590.png inflating: SegmentationClassAug_Visualization/2008_000562.png inflating: SegmentationClassAug_Visualization/2009_004200.png inflating: SegmentationClassAug_Visualization/2009_004138.png inflating: SegmentationClassAug_Visualization/2010_002577.png inflating: SegmentationClassAug_Visualization/2009_004749.png inflating: SegmentationClassAug_Visualization/2008_003866.png inflating: SegmentationClassAug_Visualization/2009_000585.png inflating: SegmentationClassAug_Visualization/2011_001004.png inflating: SegmentationClassAug_Visualization/2008_000915.png inflating: SegmentationClassAug_Visualization/2011_000666.png inflating: SegmentationClassAug_Visualization/2007_003110.png inflating: SegmentationClassAug_Visualization/2010_000667.png inflating: SegmentationClassAug_Visualization/2011_000922.png inflating: SegmentationClassAug_Visualization/2011_002929.png inflating: SegmentationClassAug_Visualization/2008_001320.png inflating: SegmentationClassAug_Visualization/2008_004663.png inflating: SegmentationClassAug_Visualization/2008_002673.png inflating: SegmentationClassAug_Visualization/2009_004232.png inflating: SegmentationClassAug_Visualization/2008_004822.png inflating: SegmentationClassAug_Visualization/2009_000636.png inflating: SegmentationClassAug_Visualization/2008_002873.png inflating: SegmentationClassAug_Visualization/2008_001353.png inflating: SegmentationClassAug_Visualization/2011_000375.png inflating: SegmentationClassAug_Visualization/2008_005054.png inflating: SegmentationClassAug_Visualization/2011_001567.png inflating: SegmentationClassAug_Visualization/2008_005449.png inflating: SegmentationClassAug_Visualization/2008_000873.png inflating: SegmentationClassAug_Visualization/2008_004347.png inflating: SegmentationClassAug_Visualization/2011_000771.png inflating: SegmentationClassAug_Visualization/2011_000531.png inflating: SegmentationClassAug_Visualization/2009_001313.png inflating: SegmentationClassAug_Visualization/2008_002093.png inflating: SegmentationClassAug_Visualization/2008_003892.png inflating: SegmentationClassAug_Visualization/2010_000002.png inflating: SegmentationClassAug_Visualization/2011_001937.png inflating: SegmentationClassAug_Visualization/2008_001617.png inflating: SegmentationClassAug_Visualization/2009_000197.png inflating: SegmentationClassAug_Visualization/2008_006441.png inflating: SegmentationClassAug_Visualization/2009_000964.png inflating: SegmentationClassAug_Visualization/2008_005705.png inflating: SegmentationClassAug_Visualization/2010_003233.png inflating: SegmentationClassAug_Visualization/2008_005808.png inflating: SegmentationClassAug_Visualization/2011_000344.png inflating: SegmentationClassAug_Visualization/2010_000626.png inflating: SegmentationClassAug_Visualization/2008_008221.png inflating: SegmentationClassAug_Visualization/2009_004436.png inflating: SegmentationClassAug_Visualization/2010_002316.png inflating: SegmentationClassAug_Visualization/2010_004313.png inflating: SegmentationClassAug_Visualization/2011_001510.png inflating: SegmentationClassAug_Visualization/2009_002645.png inflating: SegmentationClassAug_Visualization/2008_007334.png inflating: SegmentationClassAug_Visualization/2008_008145.png inflating: SegmentationClassAug_Visualization/2011_000607.png inflating: SegmentationClassAug_Visualization/2011_000638.png inflating: SegmentationClassAug_Visualization/2010_005681.png inflating: SegmentationClassAug_Visualization/2010_000307.png inflating: SegmentationClassAug_Visualization/2008_004380.png inflating: SegmentationClassAug_Visualization/2011_001789.png inflating: SegmentationClassAug_Visualization/2008_002499.png inflating: SegmentationClassAug_Visualization/2008_003545.png inflating: SegmentationClassAug_Visualization/2010_001185.png inflating: SegmentationClassAug_Visualization/2009_001768.png inflating: SegmentationClassAug_Visualization/2011_002588.png inflating: SegmentationClassAug_Visualization/2007_009251.png inflating: SegmentationClassAug_Visualization/2008_003849.png inflating: SegmentationClassAug_Visualization/2009_000257.png inflating: SegmentationClassAug_Visualization/2010_002815.png inflating: SegmentationClassAug_Visualization/2009_003534.png inflating: SegmentationClassAug_Visualization/2010_002589.png inflating: SegmentationClassAug_Visualization/2009_001253.png inflating: SegmentationClassAug_Visualization/2008_007312.png inflating: SegmentationClassAug_Visualization/2007_001955.png inflating: SegmentationClassAug_Visualization/2009_004974.png inflating: SegmentationClassAug_Visualization/2008_006779.png inflating: SegmentationClassAug_Visualization/2011_001656.png inflating: SegmentationClassAug_Visualization/2008_006072.png inflating: SegmentationClassAug_Visualization/2010_000276.png inflating: SegmentationClassAug_Visualization/2008_003059.png inflating: SegmentationClassAug_Visualization/2011_001775.png inflating: SegmentationClassAug_Visualization/2007_002624.png inflating: SegmentationClassAug_Visualization/2008_006737.png inflating: SegmentationClassAug_Visualization/2010_002192.png inflating: SegmentationClassAug_Visualization/2010_001063.png inflating: SegmentationClassAug_Visualization/2008_003270.png inflating: SegmentationClassAug_Visualization/2008_000116.png inflating: SegmentationClassAug_Visualization/2009_004499.png inflating: SegmentationClassAug_Visualization/2009_000744.png inflating: SegmentationClassAug_Visualization/2011_001801.png inflating: SegmentationClassAug_Visualization/2008_008066.png inflating: SegmentationClassAug_Visualization/2008_004087.png inflating: SegmentationClassAug_Visualization/2008_003208.png inflating: SegmentationClassAug_Visualization/2008_008029.png inflating: SegmentationClassAug_Visualization/2008_007239.png inflating: SegmentationClassAug_Visualization/2008_006844.png inflating: SegmentationClassAug_Visualization/2008_004541.png inflating: SegmentationClassAug_Visualization/2011_001529.png inflating: SegmentationClassAug_Visualization/2009_004794.png inflating: SegmentationClassAug_Visualization/2011_001941.png inflating: SegmentationClassAug_Visualization/2008_002324.png inflating: SegmentationClassAug_Visualization/2011_002504.png inflating: SegmentationClassAug_Visualization/2010_003218.png inflating: SegmentationClassAug_Visualization/2010_002089.png inflating: SegmentationClassAug_Visualization/2011_001895.png inflating: SegmentationClassAug_Visualization/2010_001450.png inflating: SegmentationClassAug_Visualization/2008_007022.png inflating: SegmentationClassAug_Visualization/2010_001247.png inflating: SegmentationClassAug_Visualization/2010_005428.png inflating: SegmentationClassAug_Visualization/2010_001272.png inflating: SegmentationClassAug_Visualization/2010_004890.png inflating: SegmentationClassAug_Visualization/2011_000485.png inflating: SegmentationClassAug_Visualization/2008_004452.png inflating: SegmentationClassAug_Visualization/2011_001264.png inflating: SegmentationClassAug_Visualization/2009_000420.png inflating: SegmentationClassAug_Visualization/2008_006987.png inflating: SegmentationClassAug_Visualization/2010_006050.png inflating: SegmentationClassAug_Visualization/2007_002789.png inflating: SegmentationClassAug_Visualization/2010_000603.png inflating: SegmentationClassAug_Visualization/2008_006616.png inflating: SegmentationClassAug_Visualization/2010_002534.png inflating: SegmentationClassAug_Visualization/2007_002368.png inflating: SegmentationClassAug_Visualization/2011_001150.png inflating: SegmentationClassAug_Visualization/2008_003776.png inflating: SegmentationClassAug_Visualization/2008_000769.png inflating: SegmentationClassAug_Visualization/2008_005873.png inflating: SegmentationClassAug_Visualization/2008_006307.png inflating: SegmentationClassAug_Visualization/2008_007130.png inflating: SegmentationClassAug_Visualization/2008_002590.png inflating: SegmentationClassAug_Visualization/2008_005511.png inflating: SegmentationClassAug_Visualization/2009_000592.png inflating: SegmentationClassAug_Visualization/2008_003609.png inflating: SegmentationClassAug_Visualization/2008_003592.png inflating: SegmentationClassAug_Visualization/2007_001439.png inflating: SegmentationClassAug_Visualization/2008_004263.png inflating: SegmentationClassAug_Visualization/2009_000684.png inflating: SegmentationClassAug_Visualization/2010_001994.png inflating: SegmentationClassAug_Visualization/2007_005911.png inflating: SegmentationClassAug_Visualization/2010_003947.png inflating: SegmentationClassAug_Visualization/2009_000225.png inflating: SegmentationClassAug_Visualization/2010_000722.png inflating: SegmentationClassAug_Visualization/2008_001575.png inflating: SegmentationClassAug_Visualization/2009_000634.png inflating: SegmentationClassAug_Visualization/2009_004374.png inflating: SegmentationClassAug_Visualization/2009_003650.png inflating: SegmentationClassAug_Visualization/2009_001704.png inflating: SegmentationClassAug_Visualization/2010_000671.png inflating: SegmentationClassAug_Visualization/2008_003985.png inflating: SegmentationClassAug_Visualization/2008_007935.png inflating: SegmentationClassAug_Visualization/2010_004253.png inflating: SegmentationClassAug_Visualization/2007_009618.png inflating: SegmentationClassAug_Visualization/2008_006336.png inflating: SegmentationClassAug_Visualization/2008_007112.png inflating: SegmentationClassAug_Visualization/2011_001730.png inflating: SegmentationClassAug_Visualization/2008_004845.png inflating: SegmentationClassAug_Visualization/2008_000511.png inflating: SegmentationClassAug_Visualization/2011_002002.png inflating: SegmentationClassAug_Visualization/2010_001326.png inflating: SegmentationClassAug_Visualization/2009_003543.png inflating: SegmentationClassAug_Visualization/2010_004868.png inflating: SegmentationClassAug_Visualization/2010_004296.png inflating: SegmentationClassAug_Visualization/2009_000308.png inflating: SegmentationClassAug_Visualization/2008_004599.png inflating: SegmentationClassAug_Visualization/2011_002880.png inflating: SegmentationClassAug_Visualization/2011_002940.png inflating: SegmentationClassAug_Visualization/2011_001764.png inflating: SegmentationClassAug_Visualization/2010_005826.png inflating: SegmentationClassAug_Visualization/2007_001960.png inflating: SegmentationClassAug_Visualization/2009_005219.png inflating: SegmentationClassAug_Visualization/2008_005608.png inflating: SegmentationClassAug_Visualization/2010_003988.png inflating: SegmentationClassAug_Visualization/2011_001400.png inflating: SegmentationClassAug_Visualization/2011_002121.png inflating: SegmentationClassAug_Visualization/2007_000032.png inflating: SegmentationClassAug_Visualization/2008_006880.png inflating: SegmentationClassAug_Visualization/2008_002294.png inflating: SegmentationClassAug_Visualization/2011_000094.png inflating: SegmentationClassAug_Visualization/2010_004710.png inflating: SegmentationClassAug_Visualization/2007_007415.png inflating: SegmentationClassAug_Visualization/2009_002325.png inflating: SegmentationClassAug_Visualization/2008_001464.png inflating: SegmentationClassAug_Visualization/2009_002940.png inflating: SegmentationClassAug_Visualization/2009_004507.png inflating: SegmentationClassAug_Visualization/2010_001737.png inflating: SegmentationClassAug_Visualization/2010_005323.png inflating: SegmentationClassAug_Visualization/2009_002110.png inflating: SegmentationClassAug_Visualization/2008_000019.png inflating: SegmentationClassAug_Visualization/2011_000412.png inflating: SegmentationClassAug_Visualization/2007_009435.png inflating: SegmentationClassAug_Visualization/2009_000068.png inflating: SegmentationClassAug_Visualization/2009_000539.png inflating: SegmentationClassAug_Visualization/2010_003220.png inflating: SegmentationClassAug_Visualization/2008_005072.png inflating: SegmentationClassAug_Visualization/2009_001096.png inflating: SegmentationClassAug_Visualization/2008_001296.png inflating: SegmentationClassAug_Visualization/2009_001479.png inflating: SegmentationClassAug_Visualization/2009_002750.png inflating: SegmentationClassAug_Visualization/2010_004008.png inflating: SegmentationClassAug_Visualization/2008_004629.png inflating: SegmentationClassAug_Visualization/2008_004515.png inflating: SegmentationClassAug_Visualization/2009_001673.png inflating: SegmentationClassAug_Visualization/2008_005823.png inflating: SegmentationClassAug_Visualization/2010_000799.png inflating: SegmentationClassAug_Visualization/2008_004706.png inflating: SegmentationClassAug_Visualization/2010_004178.png inflating: SegmentationClassAug_Visualization/2007_006761.png inflating: SegmentationClassAug_Visualization/2008_006764.png inflating: SegmentationClassAug_Visualization/2009_004965.png inflating: SegmentationClassAug_Visualization/2011_001107.png inflating: SegmentationClassAug_Visualization/2009_005231.png inflating: SegmentationClassAug_Visualization/2008_002240.png inflating: SegmentationClassAug_Visualization/2011_001254.png inflating: SegmentationClassAug_Visualization/2009_005177.png inflating: SegmentationClassAug_Visualization/2009_002624.png inflating: SegmentationClassAug_Visualization/2008_008028.png inflating: SegmentationClassAug_Visualization/2008_004230.png inflating: SegmentationClassAug_Visualization/2009_000414.png inflating: SegmentationClassAug_Visualization/2011_002609.png inflating: SegmentationClassAug_Visualization/2010_000644.png inflating: SegmentationClassAug_Visualization/2010_003291.png inflating: SegmentationClassAug_Visualization/2008_004730.png inflating: SegmentationClassAug_Visualization/2010_005480.png inflating: SegmentationClassAug_Visualization/2010_005894.png inflating: SegmentationClassAug_Visualization/2008_005365.png inflating: SegmentationClassAug_Visualization/2010_003509.png inflating: SegmentationClassAug_Visualization/2009_001774.png inflating: SegmentationClassAug_Visualization/2009_002844.png inflating: SegmentationClassAug_Visualization/2008_004774.png inflating: SegmentationClassAug_Visualization/2008_001688.png inflating: SegmentationClassAug_Visualization/2008_005310.png inflating: SegmentationClassAug_Visualization/2010_002366.png inflating: SegmentationClassAug_Visualization/2008_001283.png inflating: SegmentationClassAug_Visualization/2009_005262.png inflating: SegmentationClassAug_Visualization/2009_003951.png inflating: SegmentationClassAug_Visualization/2008_005928.png inflating: SegmentationClassAug_Visualization/2008_003665.png inflating: SegmentationClassAug_Visualization/2010_004180.png inflating: SegmentationClassAug_Visualization/2010_001501.png inflating: SegmentationClassAug_Visualization/2010_005199.png inflating: SegmentationClassAug_Visualization/2008_005360.png inflating: SegmentationClassAug_Visualization/2010_002094.png inflating: SegmentationClassAug_Visualization/2008_003373.png inflating: SegmentationClassAug_Visualization/2011_002811.png inflating: SegmentationClassAug_Visualization/2009_002614.png inflating: SegmentationClassAug_Visualization/2007_003841.png inflating: SegmentationClassAug_Visualization/2008_003688.png inflating: SegmentationClassAug_Visualization/2009_002960.png inflating: SegmentationClassAug_Visualization/2010_001607.png inflating: SegmentationClassAug_Visualization/2008_005319.png inflating: SegmentationClassAug_Visualization/2009_000453.png inflating: SegmentationClassAug_Visualization/2008_007361.png inflating: SegmentationClassAug_Visualization/2008_007095.png inflating: SegmentationClassAug_Visualization/2011_002218.png inflating: SegmentationClassAug_Visualization/2008_000761.png inflating: SegmentationClassAug_Visualization/2009_002271.png inflating: SegmentationClassAug_Visualization/2008_002229.png inflating: SegmentationClassAug_Visualization/2010_004107.png inflating: SegmentationClassAug_Visualization/2011_001629.png inflating: SegmentationClassAug_Visualization/2008_001470.png inflating: SegmentationClassAug_Visualization/2008_005916.png inflating: SegmentationClassAug_Visualization/2008_008679.png inflating: SegmentationClassAug_Visualization/2008_006832.png inflating: SegmentationClassAug_Visualization/2010_000524.png inflating: SegmentationClassAug_Visualization/2010_005736.png inflating: SegmentationClassAug_Visualization/2008_005074.png inflating: SegmentationClassAug_Visualization/2008_007034.png inflating: SegmentationClassAug_Visualization/2009_002845.png inflating: SegmentationClassAug_Visualization/2010_003474.png inflating: SegmentationClassAug_Visualization/2009_001695.png inflating: SegmentationClassAug_Visualization/2010_001929.png inflating: SegmentationClassAug_Visualization/2008_007410.png inflating: SegmentationClassAug_Visualization/2009_003067.png inflating: SegmentationClassAug_Visualization/2011_001229.png inflating: SegmentationClassAug_Visualization/2011_002505.png inflating: SegmentationClassAug_Visualization/2008_006207.png inflating: SegmentationClassAug_Visualization/2009_001431.png inflating: SegmentationClassAug_Visualization/2009_001575.png inflating: SegmentationClassAug_Visualization/2008_003462.png inflating: SegmentationClassAug_Visualization/2010_004422.png inflating: SegmentationClassAug_Visualization/2008_000264.png inflating: SegmentationClassAug_Visualization/2008_001550.png inflating: SegmentationClassAug_Visualization/2008_006747.png inflating: SegmentationClassAug_Visualization/2008_008550.png inflating: SegmentationClassAug_Visualization/2008_006732.png inflating: SegmentationClassAug_Visualization/2007_002597.png inflating: SegmentationClassAug_Visualization/2010_003477.png inflating: SegmentationClassAug_Visualization/2011_000482.png inflating: SegmentationClassAug_Visualization/2009_004562.png inflating: SegmentationClassAug_Visualization/2009_004986.png inflating: SegmentationClassAug_Visualization/2008_005803.png inflating: SegmentationClassAug_Visualization/2008_006716.png inflating: SegmentationClassAug_Visualization/2010_000372.png inflating: SegmentationClassAug_Visualization/2011_000790.png inflating: SegmentationClassAug_Visualization/2011_001544.png inflating: SegmentationClassAug_Visualization/2008_001440.png inflating: SegmentationClassAug_Visualization/2008_005698.png inflating: SegmentationClassAug_Visualization/2008_008263.png inflating: SegmentationClassAug_Visualization/2009_003422.png inflating: SegmentationClassAug_Visualization/2010_002792.png inflating: SegmentationClassAug_Visualization/2008_003094.png inflating: SegmentationClassAug_Visualization/2008_001607.png inflating: SegmentationClassAug_Visualization/2008_000669.png inflating: SegmentationClassAug_Visualization/2009_003488.png inflating: SegmentationClassAug_Visualization/2011_001601.png inflating: SegmentationClassAug_Visualization/2009_001309.png inflating: SegmentationClassAug_Visualization/2010_001956.png inflating: SegmentationClassAug_Visualization/2010_004661.png inflating: SegmentationClassAug_Visualization/2010_003754.png inflating: SegmentationClassAug_Visualization/2009_003784.png inflating: SegmentationClassAug_Visualization/2008_002112.png inflating: SegmentationClassAug_Visualization/2010_005008.png inflating: SegmentationClassAug_Visualization/2007_002024.png inflating: SegmentationClassAug_Visualization/2011_002656.png inflating: SegmentationClassAug_Visualization/2011_001337.png inflating: SegmentationClassAug_Visualization/2010_002242.png inflating: SegmentationClassAug_Visualization/2011_000342.png inflating: SegmentationClassAug_Visualization/2008_000495.png inflating: SegmentationClassAug_Visualization/2008_000311.png inflating: SegmentationClassAug_Visualization/2008_000204.png inflating: SegmentationClassAug_Visualization/2008_001538.png inflating: SegmentationClassAug_Visualization/2011_001557.png inflating: SegmentationClassAug_Visualization/2008_005865.png inflating: SegmentationClassAug_Visualization/2008_004930.png inflating: SegmentationClassAug_Visualization/2010_005960.png inflating: SegmentationClassAug_Visualization/2007_005878.png inflating: SegmentationClassAug_Visualization/2008_000695.png inflating: SegmentationClassAug_Visualization/2011_001975.png inflating: SegmentationClassAug_Visualization/2008_004814.png inflating: SegmentationClassAug_Visualization/2008_001704.png inflating: SegmentationClassAug_Visualization/2007_001587.png inflating: SegmentationClassAug_Visualization/2010_002567.png inflating: SegmentationClassAug_Visualization/2007_001609.png inflating: SegmentationClassAug_Visualization/2010_005060.png inflating: SegmentationClassAug_Visualization/2009_004560.png inflating: SegmentationClassAug_Visualization/2010_004795.png inflating: SegmentationClassAug_Visualization/2008_000924.png inflating: SegmentationClassAug_Visualization/2010_000746.png inflating: SegmentationClassAug_Visualization/2008_003060.png inflating: SegmentationClassAug_Visualization/2010_004475.png inflating: SegmentationClassAug_Visualization/2010_005048.png inflating: SegmentationClassAug_Visualization/2010_004889.png inflating: SegmentationClassAug_Visualization/2010_001511.png inflating: SegmentationClassAug_Visualization/2009_001314.png inflating: SegmentationClassAug_Visualization/2007_004768.png inflating: SegmentationClassAug_Visualization/2008_001911.png inflating: SegmentationClassAug_Visualization/2009_002097.png inflating: SegmentationClassAug_Visualization/2008_007358.png inflating: SegmentationClassAug_Visualization/2010_002023.png inflating: SegmentationClassAug_Visualization/2010_006023.png inflating: SegmentationClassAug_Visualization/2007_005058.png inflating: SegmentationClassAug_Visualization/2008_002405.png inflating: SegmentationClassAug_Visualization/2010_000630.png inflating: SegmentationClassAug_Visualization/2010_003043.png inflating: SegmentationClassAug_Visualization/2010_002582.png inflating: SegmentationClassAug_Visualization/2010_001100.png inflating: SegmentationClassAug_Visualization/2010_000749.png inflating: SegmentationClassAug_Visualization/2010_003640.png inflating: SegmentationClassAug_Visualization/2007_009139.png inflating: SegmentationClassAug_Visualization/2010_005615.png inflating: SegmentationClassAug_Visualization/2010_003102.png inflating: SegmentationClassAug_Visualization/2010_005287.png inflating: SegmentationClassAug_Visualization/2009_004984.png inflating: SegmentationClassAug_Visualization/2009_002519.png inflating: SegmentationClassAug_Visualization/2008_004372.png inflating: SegmentationClassAug_Visualization/2009_004829.png inflating: SegmentationClassAug_Visualization/2009_002042.png inflating: SegmentationClassAug_Visualization/2007_002107.png inflating: SegmentationClassAug_Visualization/2011_002552.png inflating: SegmentationClassAug_Visualization/2008_005342.png inflating: SegmentationClassAug_Visualization/2008_002536.png inflating: SegmentationClassAug_Visualization/2008_007890.png inflating: SegmentationClassAug_Visualization/2010_004770.png inflating: SegmentationClassAug_Visualization/2008_008431.png inflating: SegmentationClassAug_Visualization/2011_001926.png inflating: SegmentationClassAug_Visualization/2008_007069.png inflating: SegmentationClassAug_Visualization/2008_000415.png inflating: SegmentationClassAug_Visualization/2009_003136.png inflating: SegmentationClassAug_Visualization/2009_004988.png inflating: SegmentationClassAug_Visualization/2008_002465.png inflating: SegmentationClassAug_Visualization/2010_002370.png inflating: SegmentationClassAug_Visualization/2008_002047.png inflating: SegmentationClassAug_Visualization/2009_000347.png inflating: SegmentationClassAug_Visualization/2009_000410.png inflating: SegmentationClassAug_Visualization/2009_000593.png inflating: SegmentationClassAug_Visualization/2008_002368.png inflating: SegmentationClassAug_Visualization/2008_003108.png inflating: SegmentationClassAug_Visualization/2009_000164.png inflating: SegmentationClassAug_Visualization/2008_006088.png inflating: SegmentationClassAug_Visualization/2009_003462.png inflating: SegmentationClassAug_Visualization/2008_003110.png inflating: SegmentationClassAug_Visualization/2007_001225.png inflating: SegmentationClassAug_Visualization/2009_003848.png inflating: SegmentationClassAug_Visualization/2009_000151.png inflating: SegmentationClassAug_Visualization/2010_000291.png inflating: SegmentationClassAug_Visualization/2008_004678.png inflating: SegmentationClassAug_Visualization/2011_003044.png inflating: SegmentationClassAug_Visualization/2009_005095.png inflating: SegmentationClassAug_Visualization/2008_006818.png inflating: SegmentationClassAug_Visualization/2008_000457.png inflating: SegmentationClassAug_Visualization/2010_005668.png inflating: SegmentationClassAug_Visualization/2008_000183.png inflating: SegmentationClassAug_Visualization/2010_000473.png inflating: SegmentationClassAug_Visualization/2010_005497.png inflating: SegmentationClassAug_Visualization/2009_001254.png inflating: SegmentationClassAug_Visualization/2010_000296.png inflating: SegmentationClassAug_Visualization/2009_002295.png inflating: SegmentationClassAug_Visualization/2009_002652.png inflating: SegmentationClassAug_Visualization/2008_007950.png inflating: SegmentationClassAug_Visualization/2010_004654.png inflating: SegmentationClassAug_Visualization/2010_000947.png inflating: SegmentationClassAug_Visualization/2009_000389.png inflating: SegmentationClassAug_Visualization/2011_000566.png inflating: SegmentationClassAug_Visualization/2008_003261.png inflating: SegmentationClassAug_Visualization/2011_003159.png inflating: SegmentationClassAug_Visualization/2007_004537.png inflating: SegmentationClassAug_Visualization/2008_002330.png inflating: SegmentationClassAug_Visualization/2010_002046.png inflating: SegmentationClassAug_Visualization/2008_001118.png inflating: SegmentationClassAug_Visualization/2009_004042.png inflating: SegmentationClassAug_Visualization/2008_003637.png inflating: SegmentationClassAug_Visualization/2010_003325.png inflating: SegmentationClassAug_Visualization/2010_003806.png inflating: SegmentationClassAug_Visualization/2010_004856.png inflating: SegmentationClassAug_Visualization/2007_001154.png inflating: SegmentationClassAug_Visualization/2008_001004.png inflating: SegmentationClassAug_Visualization/2011_003168.png inflating: SegmentationClassAug_Visualization/2010_003893.png inflating: SegmentationClassAug_Visualization/2008_005421.png inflating: SegmentationClassAug_Visualization/2011_002385.png inflating: SegmentationClassAug_Visualization/2009_003379.png inflating: SegmentationClassAug_Visualization/2010_005883.png inflating: SegmentationClassAug_Visualization/2009_003694.png inflating: SegmentationClassAug_Visualization/2009_003827.png inflating: SegmentationClassAug_Visualization/2008_003442.png inflating: SegmentationClassAug_Visualization/2009_003271.png inflating: SegmentationClassAug_Visualization/2008_008668.png inflating: SegmentationClassAug_Visualization/2010_003207.png inflating: SegmentationClassAug_Visualization/2010_003017.png inflating: SegmentationClassAug_Visualization/2009_004501.png inflating: SegmentationClassAug_Visualization/2008_007470.png inflating: SegmentationClassAug_Visualization/2010_001739.png inflating: SegmentationClassAug_Visualization/2010_004852.png inflating: SegmentationClassAug_Visualization/2011_000276.png inflating: SegmentationClassAug_Visualization/2008_003997.png inflating: SegmentationClassAug_Visualization/2008_000942.png inflating: SegmentationClassAug_Visualization/2009_003020.png inflating: SegmentationClassAug_Visualization/2007_007203.png inflating: SegmentationClassAug_Visualization/2010_003146.png inflating: SegmentationClassAug_Visualization/2010_005807.png inflating: SegmentationClassAug_Visualization/2007_009331.png inflating: SegmentationClassAug_Visualization/2010_003419.png inflating: SegmentationClassAug_Visualization/2010_002458.png inflating: SegmentationClassAug_Visualization/2011_001028.png inflating: SegmentationClassAug_Visualization/2009_003487.png inflating: SegmentationClassAug_Visualization/2010_005155.png inflating: SegmentationClassAug_Visualization/2010_000321.png inflating: SegmentationClassAug_Visualization/2010_003238.png inflating: SegmentationClassAug_Visualization/2010_002962.png inflating: SegmentationClassAug_Visualization/2008_005552.png inflating: SegmentationClassAug_Visualization/2009_004624.png inflating: SegmentationClassAug_Visualization/2009_002681.png inflating: SegmentationClassAug_Visualization/2011_003124.png inflating: SegmentationClassAug_Visualization/2008_008011.png inflating: SegmentationClassAug_Visualization/2010_002759.png inflating: SegmentationClassAug_Visualization/2008_008377.png inflating: SegmentationClassAug_Visualization/2008_004174.png inflating: SegmentationClassAug_Visualization/2008_000595.png inflating: SegmentationClassAug_Visualization/2009_003075.png inflating: SegmentationClassAug_Visualization/2008_003483.png inflating: SegmentationClassAug_Visualization/2009_004525.png inflating: SegmentationClassAug_Visualization/2010_005492.png inflating: SegmentationClassAug_Visualization/2010_003060.png inflating: SegmentationClassAug_Visualization/2007_004190.png inflating: SegmentationClassAug_Visualization/2010_005572.png inflating: SegmentationClassAug_Visualization/2008_006714.png inflating: SegmentationClassAug_Visualization/2008_002551.png inflating: SegmentationClassAug_Visualization/2010_002413.png inflating: SegmentationClassAug_Visualization/2008_001682.png inflating: SegmentationClassAug_Visualization/2011_002553.png inflating: SegmentationClassAug_Visualization/2011_002965.png inflating: SegmentationClassAug_Visualization/2009_004772.png inflating: SegmentationClassAug_Visualization/2008_006992.png inflating: SegmentationClassAug_Visualization/2009_002052.png inflating: SegmentationClassAug_Visualization/2010_000836.png inflating: SegmentationClassAug_Visualization/2009_004051.png inflating: SegmentationClassAug_Visualization/2009_002715.png inflating: SegmentationClassAug_Visualization/2008_007305.png inflating: SegmentationClassAug_Visualization/2008_001022.png inflating: SegmentationClassAug_Visualization/2010_003949.png inflating: SegmentationClassAug_Visualization/2008_000629.png inflating: SegmentationClassAug_Visualization/2010_003513.png inflating: SegmentationClassAug_Visualization/2008_003265.png inflating: SegmentationClassAug_Visualization/2011_000999.png inflating: SegmentationClassAug_Visualization/2009_003933.png inflating: SegmentationClassAug_Visualization/2008_007014.png inflating: SegmentationClassAug_Visualization/2011_002246.png inflating: SegmentationClassAug_Visualization/2009_003858.png inflating: SegmentationClassAug_Visualization/2010_002482.png inflating: SegmentationClassAug_Visualization/2009_002088.png inflating: SegmentationClassAug_Visualization/2007_006004.png inflating: SegmentationClassAug_Visualization/2008_001410.png inflating: SegmentationClassAug_Visualization/2009_002967.png inflating: SegmentationClassAug_Visualization/2008_006434.png inflating: SegmentationClassAug_Visualization/2010_005405.png inflating: SegmentationClassAug_Visualization/2009_001124.png inflating: SegmentationClassAug_Visualization/2011_001189.png inflating: SegmentationClassAug_Visualization/2010_002597.png inflating: SegmentationClassAug_Visualization/2009_005070.png inflating: SegmentationClassAug_Visualization/2010_002459.png inflating: SegmentationClassAug_Visualization/2008_004961.png inflating: SegmentationClassAug_Visualization/2010_004888.png inflating: SegmentationClassAug_Visualization/2010_001771.png inflating: SegmentationClassAug_Visualization/2010_001885.png inflating: SegmentationClassAug_Visualization/2009_001605.png inflating: SegmentationClassAug_Visualization/2009_005165.png inflating: SegmentationClassAug_Visualization/2010_003911.png inflating: SegmentationClassAug_Visualization/2008_000957.png inflating: SegmentationClassAug_Visualization/2009_001780.png inflating: SegmentationClassAug_Visualization/2008_004982.png inflating: SegmentationClassAug_Visualization/2008_002776.png inflating: SegmentationClassAug_Visualization/2010_002427.png inflating: SegmentationClassAug_Visualization/2008_000191.png inflating: SegmentationClassAug_Visualization/2010_002435.png inflating: SegmentationClassAug_Visualization/2011_000299.png inflating: SegmentationClassAug_Visualization/2010_001337.png inflating: SegmentationClassAug_Visualization/2010_004542.png inflating: SegmentationClassAug_Visualization/2008_001513.png inflating: SegmentationClassAug_Visualization/2008_001914.png inflating: SegmentationClassAug_Visualization/2009_003326.png inflating: SegmentationClassAug_Visualization/2008_003122.png inflating: SegmentationClassAug_Visualization/2009_003369.png inflating: SegmentationClassAug_Visualization/2010_004569.png inflating: SegmentationClassAug_Visualization/2009_001929.png inflating: SegmentationClassAug_Visualization/2011_001536.png inflating: SegmentationClassAug_Visualization/2009_003132.png inflating: SegmentationClassAug_Visualization/2011_002782.png inflating: SegmentationClassAug_Visualization/2008_000408.png inflating: SegmentationClassAug_Visualization/2008_005687.png inflating: SegmentationClassAug_Visualization/2008_007942.png inflating: SegmentationClassAug_Visualization/2010_000272.png inflating: SegmentationClassAug_Visualization/2011_002143.png inflating: SegmentationClassAug_Visualization/2007_007651.png inflating: SegmentationClassAug_Visualization/2009_000390.png inflating: SegmentationClassAug_Visualization/2008_005109.png inflating: SegmentationClassAug_Visualization/2008_002514.png inflating: SegmentationClassAug_Visualization/2009_000090.png inflating: SegmentationClassAug_Visualization/2011_002740.png inflating: SegmentationClassAug_Visualization/2008_000452.png inflating: SegmentationClassAug_Visualization/2008_002904.png inflating: SegmentationClassAug_Visualization/2008_002481.png inflating: SegmentationClassAug_Visualization/2010_002248.png inflating: SegmentationClassAug_Visualization/2008_004950.png inflating: SegmentationClassAug_Visualization/2010_003309.png inflating: SegmentationClassAug_Visualization/2010_005909.png inflating: SegmentationClassAug_Visualization/2010_003491.png inflating: SegmentationClassAug_Visualization/2008_001680.png inflating: SegmentationClassAug_Visualization/2007_002370.png inflating: SegmentationClassAug_Visualization/2009_000597.png inflating: SegmentationClassAug_Visualization/2011_000124.png inflating: SegmentationClassAug_Visualization/2010_005361.png inflating: SegmentationClassAug_Visualization/2011_002279.png inflating: SegmentationClassAug_Visualization/2008_007629.png inflating: SegmentationClassAug_Visualization/2011_000114.png inflating: SegmentationClassAug_Visualization/2010_003821.png inflating: SegmentationClassAug_Visualization/2008_007471.png inflating: SegmentationClassAug_Visualization/2008_004745.png inflating: SegmentationClassAug_Visualization/2010_001229.png inflating: SegmentationClassAug_Visualization/2011_002386.png inflating: SegmentationClassAug_Visualization/2009_002902.png inflating: SegmentationClassAug_Visualization/2008_008696.png inflating: SegmentationClassAug_Visualization/2010_003594.png inflating: SegmentationClassAug_Visualization/2008_004271.png inflating: SegmentationClassAug_Visualization/2010_000800.png inflating: SegmentationClassAug_Visualization/2009_001765.png inflating: SegmentationClassAug_Visualization/2010_003982.png inflating: SegmentationClassAug_Visualization/2008_008675.png inflating: SegmentationClassAug_Visualization/2009_004684.png inflating: SegmentationClassAug_Visualization/2007_000323.png inflating: SegmentationClassAug_Visualization/2011_003041.png inflating: SegmentationClassAug_Visualization/2008_006562.png inflating: SegmentationClassAug_Visualization/2009_002197.png inflating: SegmentationClassAug_Visualization/2007_004705.png inflating: SegmentationClassAug_Visualization/2008_006546.png inflating: SegmentationClassAug_Visualization/2009_000351.png inflating: SegmentationClassAug_Visualization/2010_003264.png inflating: SegmentationClassAug_Visualization/2009_000418.png inflating: SegmentationClassAug_Visualization/2008_004145.png inflating: SegmentationClassAug_Visualization/2010_001652.png inflating: SegmentationClassAug_Visualization/2008_003135.png inflating: SegmentationClassAug_Visualization/2009_000602.png inflating: SegmentationClassAug_Visualization/2009_004179.png inflating: SegmentationClassAug_Visualization/2009_000084.png inflating: SegmentationClassAug_Visualization/2009_004587.png inflating: SegmentationClassAug_Visualization/2007_002426.png inflating: SegmentationClassAug_Visualization/2011_003138.png inflating: SegmentationClassAug_Visualization/2009_003911.png inflating: SegmentationClassAug_Visualization/2010_002580.png inflating: SegmentationClassAug_Visualization/2009_004514.png Archive: list.zip inflating: list/test_id.txt inflating: list/test.txt inflating: list/train_aug.txt inflating: list/train.txt inflating: list/trainval_aug.txt inflating: list/trainval.txt inflating: list/val_id.txt inflating: list/val.txt . %cd /content/DeepLabv3.pytorch/ . /content/DeepLabv3.pytorch . !wget https://cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth -P data/ . --2021-12-19 14:55:23-- https://cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth Resolving cs.jhu.edu (cs.jhu.edu)... 128.220.13.64 Connecting to cs.jhu.edu (cs.jhu.edu)|128.220.13.64|:443... connected. HTTP request sent, awaiting response... 301 Moved Permanently Location: https://www.cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth [following] --2021-12-19 14:55:24-- https://www.cs.jhu.edu/~cxliu/data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth Resolving www.cs.jhu.edu (www.cs.jhu.edu)... 128.220.13.64 Connecting to www.cs.jhu.edu (www.cs.jhu.edu)|128.220.13.64|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 464941444 (443M) Saving to: ‘data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth’ deeplab_resnet101_p 100%[===================&gt;] 443.40M 71.6MB/s in 6.4s 2021-12-19 14:55:30 (69.4 MB/s) - ‘data/deeplab_resnet101_pascal_v3_bn_lr7e-3_epoch50.pth’ saved [464941444/464941444] . Evaluation with Pretrained Model . !python main.py --exp bn_lr7e-3 --epochs 50 --base_lr 0.007 --batch_size 12 --crop_size 400 . eval: 1/1449 eval: 2/1449 eval: 3/1449 eval: 4/1449 eval: 5/1449 eval: 6/1449 eval: 7/1449 eval: 8/1449 eval: 9/1449 eval: 10/1449 eval: 11/1449 eval: 12/1449 eval: 13/1449 eval: 14/1449 eval: 15/1449 eval: 16/1449 eval: 17/1449 eval: 18/1449 eval: 19/1449 eval: 20/1449 eval: 21/1449 eval: 22/1449 eval: 23/1449 eval: 24/1449 eval: 25/1449 eval: 26/1449 eval: 27/1449 eval: 28/1449 eval: 29/1449 eval: 30/1449 eval: 31/1449 eval: 32/1449 eval: 33/1449 eval: 34/1449 eval: 35/1449 eval: 36/1449 eval: 37/1449 eval: 38/1449 eval: 39/1449 eval: 40/1449 eval: 41/1449 eval: 42/1449 eval: 43/1449 eval: 44/1449 eval: 45/1449 eval: 46/1449 eval: 47/1449 eval: 48/1449 eval: 49/1449 eval: 50/1449 eval: 51/1449 eval: 52/1449 eval: 53/1449 eval: 54/1449 eval: 55/1449 eval: 56/1449 eval: 57/1449 eval: 58/1449 eval: 59/1449 eval: 60/1449 eval: 61/1449 eval: 62/1449 eval: 63/1449 eval: 64/1449 eval: 65/1449 eval: 66/1449 eval: 67/1449 eval: 68/1449 eval: 69/1449 eval: 70/1449 eval: 71/1449 eval: 72/1449 eval: 73/1449 eval: 74/1449 eval: 75/1449 eval: 76/1449 eval: 77/1449 eval: 78/1449 eval: 79/1449 eval: 80/1449 eval: 81/1449 eval: 82/1449 eval: 83/1449 eval: 84/1449 eval: 85/1449 eval: 86/1449 eval: 87/1449 eval: 88/1449 eval: 89/1449 eval: 90/1449 eval: 91/1449 eval: 92/1449 eval: 93/1449 eval: 94/1449 eval: 95/1449 eval: 96/1449 eval: 97/1449 eval: 98/1449 eval: 99/1449 eval: 100/1449 eval: 101/1449 eval: 102/1449 eval: 103/1449 eval: 104/1449 eval: 105/1449 eval: 106/1449 eval: 107/1449 eval: 108/1449 eval: 109/1449 eval: 110/1449 eval: 111/1449 eval: 112/1449 eval: 113/1449 eval: 114/1449 eval: 115/1449 eval: 116/1449 eval: 117/1449 eval: 118/1449 eval: 119/1449 eval: 120/1449 eval: 121/1449 eval: 122/1449 eval: 123/1449 eval: 124/1449 eval: 125/1449 eval: 126/1449 eval: 127/1449 eval: 128/1449 eval: 129/1449 eval: 130/1449 eval: 131/1449 eval: 132/1449 eval: 133/1449 eval: 134/1449 eval: 135/1449 eval: 136/1449 eval: 137/1449 eval: 138/1449 eval: 139/1449 eval: 140/1449 eval: 141/1449 eval: 142/1449 eval: 143/1449 eval: 144/1449 eval: 145/1449 eval: 146/1449 eval: 147/1449 eval: 148/1449 eval: 149/1449 eval: 150/1449 eval: 151/1449 eval: 152/1449 eval: 153/1449 eval: 154/1449 eval: 155/1449 eval: 156/1449 eval: 157/1449 eval: 158/1449 eval: 159/1449 eval: 160/1449 eval: 161/1449 eval: 162/1449 eval: 163/1449 eval: 164/1449 eval: 165/1449 eval: 166/1449 eval: 167/1449 eval: 168/1449 eval: 169/1449 eval: 170/1449 eval: 171/1449 eval: 172/1449 eval: 173/1449 eval: 174/1449 eval: 175/1449 eval: 176/1449 eval: 177/1449 eval: 178/1449 eval: 179/1449 eval: 180/1449 eval: 181/1449 eval: 182/1449 eval: 183/1449 eval: 184/1449 eval: 185/1449 eval: 186/1449 eval: 187/1449 eval: 188/1449 eval: 189/1449 eval: 190/1449 eval: 191/1449 eval: 192/1449 eval: 193/1449 eval: 194/1449 eval: 195/1449 eval: 196/1449 eval: 197/1449 eval: 198/1449 eval: 199/1449 eval: 200/1449 eval: 201/1449 eval: 202/1449 eval: 203/1449 eval: 204/1449 eval: 205/1449 eval: 206/1449 eval: 207/1449 eval: 208/1449 eval: 209/1449 eval: 210/1449 eval: 211/1449 eval: 212/1449 eval: 213/1449 eval: 214/1449 eval: 215/1449 eval: 216/1449 eval: 217/1449 eval: 218/1449 eval: 219/1449 eval: 220/1449 eval: 221/1449 eval: 222/1449 eval: 223/1449 eval: 224/1449 eval: 225/1449 eval: 226/1449 eval: 227/1449 eval: 228/1449 eval: 229/1449 eval: 230/1449 eval: 231/1449 eval: 232/1449 eval: 233/1449 eval: 234/1449 eval: 235/1449 eval: 236/1449 eval: 237/1449 eval: 238/1449 eval: 239/1449 eval: 240/1449 eval: 241/1449 eval: 242/1449 eval: 243/1449 eval: 244/1449 eval: 245/1449 eval: 246/1449 eval: 247/1449 eval: 248/1449 eval: 249/1449 eval: 250/1449 eval: 251/1449 eval: 252/1449 eval: 253/1449 eval: 254/1449 eval: 255/1449 eval: 256/1449 eval: 257/1449 eval: 258/1449 eval: 259/1449 eval: 260/1449 eval: 261/1449 eval: 262/1449 eval: 263/1449 eval: 264/1449 eval: 265/1449 eval: 266/1449 eval: 267/1449 eval: 268/1449 eval: 269/1449 eval: 270/1449 eval: 271/1449 eval: 272/1449 eval: 273/1449 eval: 274/1449 eval: 275/1449 eval: 276/1449 eval: 277/1449 eval: 278/1449 eval: 279/1449 eval: 280/1449 eval: 281/1449 eval: 282/1449 eval: 283/1449 eval: 284/1449 eval: 285/1449 eval: 286/1449 eval: 287/1449 eval: 288/1449 eval: 289/1449 eval: 290/1449 eval: 291/1449 eval: 292/1449 eval: 293/1449 eval: 294/1449 eval: 295/1449 eval: 296/1449 eval: 297/1449 eval: 298/1449 eval: 299/1449 eval: 300/1449 eval: 301/1449 eval: 302/1449 eval: 303/1449 eval: 304/1449 eval: 305/1449 eval: 306/1449 eval: 307/1449 eval: 308/1449 eval: 309/1449 eval: 310/1449 eval: 311/1449 eval: 312/1449 eval: 313/1449 eval: 314/1449 eval: 315/1449 eval: 316/1449 eval: 317/1449 eval: 318/1449 eval: 319/1449 eval: 320/1449 eval: 321/1449 eval: 322/1449 eval: 323/1449 eval: 324/1449 eval: 325/1449 eval: 326/1449 eval: 327/1449 eval: 328/1449 eval: 329/1449 eval: 330/1449 eval: 331/1449 eval: 332/1449 eval: 333/1449 eval: 334/1449 eval: 335/1449 eval: 336/1449 eval: 337/1449 eval: 338/1449 eval: 339/1449 eval: 340/1449 eval: 341/1449 eval: 342/1449 eval: 343/1449 eval: 344/1449 eval: 345/1449 eval: 346/1449 eval: 347/1449 eval: 348/1449 eval: 349/1449 eval: 350/1449 eval: 351/1449 eval: 352/1449 eval: 353/1449 eval: 354/1449 eval: 355/1449 eval: 356/1449 eval: 357/1449 eval: 358/1449 eval: 359/1449 eval: 360/1449 eval: 361/1449 eval: 362/1449 eval: 363/1449 eval: 364/1449 eval: 365/1449 eval: 366/1449 eval: 367/1449 eval: 368/1449 eval: 369/1449 eval: 370/1449 eval: 371/1449 eval: 372/1449 eval: 373/1449 eval: 374/1449 eval: 375/1449 eval: 376/1449 eval: 377/1449 eval: 378/1449 eval: 379/1449 eval: 380/1449 eval: 381/1449 eval: 382/1449 eval: 383/1449 eval: 384/1449 eval: 385/1449 eval: 386/1449 eval: 387/1449 eval: 388/1449 eval: 389/1449 eval: 390/1449 eval: 391/1449 eval: 392/1449 eval: 393/1449 eval: 394/1449 eval: 395/1449 eval: 396/1449 eval: 397/1449 eval: 398/1449 eval: 399/1449 eval: 400/1449 eval: 401/1449 eval: 402/1449 eval: 403/1449 eval: 404/1449 eval: 405/1449 eval: 406/1449 eval: 407/1449 eval: 408/1449 eval: 409/1449 eval: 410/1449 eval: 411/1449 eval: 412/1449 eval: 413/1449 eval: 414/1449 eval: 415/1449 eval: 416/1449 eval: 417/1449 eval: 418/1449 eval: 419/1449 eval: 420/1449 eval: 421/1449 eval: 422/1449 eval: 423/1449 eval: 424/1449 eval: 425/1449 eval: 426/1449 eval: 427/1449 eval: 428/1449 eval: 429/1449 eval: 430/1449 eval: 431/1449 eval: 432/1449 eval: 433/1449 eval: 434/1449 eval: 435/1449 eval: 436/1449 eval: 437/1449 eval: 438/1449 eval: 439/1449 eval: 440/1449 eval: 441/1449 eval: 442/1449 eval: 443/1449 eval: 444/1449 eval: 445/1449 eval: 446/1449 eval: 447/1449 eval: 448/1449 eval: 449/1449 eval: 450/1449 eval: 451/1449 eval: 452/1449 eval: 453/1449 eval: 454/1449 eval: 455/1449 eval: 456/1449 eval: 457/1449 eval: 458/1449 eval: 459/1449 eval: 460/1449 eval: 461/1449 eval: 462/1449 eval: 463/1449 eval: 464/1449 eval: 465/1449 eval: 466/1449 eval: 467/1449 eval: 468/1449 eval: 469/1449 eval: 470/1449 eval: 471/1449 eval: 472/1449 eval: 473/1449 eval: 474/1449 eval: 475/1449 eval: 476/1449 eval: 477/1449 eval: 478/1449 eval: 479/1449 eval: 480/1449 eval: 481/1449 eval: 482/1449 eval: 483/1449 eval: 484/1449 eval: 485/1449 eval: 486/1449 eval: 487/1449 eval: 488/1449 eval: 489/1449 eval: 490/1449 eval: 491/1449 eval: 492/1449 eval: 493/1449 eval: 494/1449 eval: 495/1449 eval: 496/1449 eval: 497/1449 eval: 498/1449 eval: 499/1449 eval: 500/1449 eval: 501/1449 eval: 502/1449 eval: 503/1449 eval: 504/1449 eval: 505/1449 eval: 506/1449 eval: 507/1449 eval: 508/1449 eval: 509/1449 eval: 510/1449 eval: 511/1449 eval: 512/1449 eval: 513/1449 eval: 514/1449 eval: 515/1449 eval: 516/1449 eval: 517/1449 eval: 518/1449 eval: 519/1449 eval: 520/1449 eval: 521/1449 eval: 522/1449 eval: 523/1449 eval: 524/1449 eval: 525/1449 eval: 526/1449 eval: 527/1449 eval: 528/1449 eval: 529/1449 eval: 530/1449 eval: 531/1449 eval: 532/1449 eval: 533/1449 eval: 534/1449 eval: 535/1449 eval: 536/1449 eval: 537/1449 eval: 538/1449 eval: 539/1449 eval: 540/1449 eval: 541/1449 eval: 542/1449 eval: 543/1449 eval: 544/1449 eval: 545/1449 eval: 546/1449 eval: 547/1449 eval: 548/1449 eval: 549/1449 eval: 550/1449 eval: 551/1449 eval: 552/1449 eval: 553/1449 eval: 554/1449 eval: 555/1449 eval: 556/1449 eval: 557/1449 eval: 558/1449 eval: 559/1449 eval: 560/1449 eval: 561/1449 eval: 562/1449 eval: 563/1449 eval: 564/1449 eval: 565/1449 eval: 566/1449 eval: 567/1449 eval: 568/1449 eval: 569/1449 eval: 570/1449 eval: 571/1449 eval: 572/1449 eval: 573/1449 eval: 574/1449 eval: 575/1449 eval: 576/1449 eval: 577/1449 eval: 578/1449 eval: 579/1449 eval: 580/1449 eval: 581/1449 eval: 582/1449 eval: 583/1449 eval: 584/1449 eval: 585/1449 eval: 586/1449 eval: 587/1449 eval: 588/1449 eval: 589/1449 eval: 590/1449 eval: 591/1449 eval: 592/1449 eval: 593/1449 eval: 594/1449 eval: 595/1449 eval: 596/1449 eval: 597/1449 eval: 598/1449 eval: 599/1449 eval: 600/1449 eval: 601/1449 eval: 602/1449 eval: 603/1449 eval: 604/1449 eval: 605/1449 eval: 606/1449 eval: 607/1449 eval: 608/1449 eval: 609/1449 eval: 610/1449 eval: 611/1449 eval: 612/1449 eval: 613/1449 eval: 614/1449 eval: 615/1449 eval: 616/1449 eval: 617/1449 eval: 618/1449 eval: 619/1449 eval: 620/1449 eval: 621/1449 eval: 622/1449 eval: 623/1449 eval: 624/1449 eval: 625/1449 eval: 626/1449 eval: 627/1449 eval: 628/1449 eval: 629/1449 eval: 630/1449 eval: 631/1449 eval: 632/1449 eval: 633/1449 eval: 634/1449 eval: 635/1449 eval: 636/1449 eval: 637/1449 eval: 638/1449 eval: 639/1449 eval: 640/1449 eval: 641/1449 eval: 642/1449 eval: 643/1449 eval: 644/1449 eval: 645/1449 eval: 646/1449 eval: 647/1449 eval: 648/1449 eval: 649/1449 eval: 650/1449 eval: 651/1449 eval: 652/1449 eval: 653/1449 eval: 654/1449 eval: 655/1449 eval: 656/1449 eval: 657/1449 eval: 658/1449 eval: 659/1449 eval: 660/1449 eval: 661/1449 eval: 662/1449 eval: 663/1449 eval: 664/1449 eval: 665/1449 eval: 666/1449 eval: 667/1449 eval: 668/1449 eval: 669/1449 eval: 670/1449 eval: 671/1449 eval: 672/1449 eval: 673/1449 eval: 674/1449 eval: 675/1449 eval: 676/1449 eval: 677/1449 eval: 678/1449 eval: 679/1449 eval: 680/1449 eval: 681/1449 eval: 682/1449 eval: 683/1449 eval: 684/1449 eval: 685/1449 eval: 686/1449 eval: 687/1449 eval: 688/1449 eval: 689/1449 eval: 690/1449 eval: 691/1449 eval: 692/1449 eval: 693/1449 eval: 694/1449 eval: 695/1449 eval: 696/1449 eval: 697/1449 eval: 698/1449 eval: 699/1449 eval: 700/1449 eval: 701/1449 eval: 702/1449 eval: 703/1449 eval: 704/1449 eval: 705/1449 eval: 706/1449 eval: 707/1449 eval: 708/1449 eval: 709/1449 eval: 710/1449 eval: 711/1449 eval: 712/1449 eval: 713/1449 eval: 714/1449 eval: 715/1449 eval: 716/1449 eval: 717/1449 eval: 718/1449 eval: 719/1449 eval: 720/1449 eval: 721/1449 eval: 722/1449 eval: 723/1449 eval: 724/1449 eval: 725/1449 eval: 726/1449 eval: 727/1449 eval: 728/1449 eval: 729/1449 eval: 730/1449 eval: 731/1449 eval: 732/1449 eval: 733/1449 eval: 734/1449 eval: 735/1449 eval: 736/1449 eval: 737/1449 eval: 738/1449 eval: 739/1449 eval: 740/1449 eval: 741/1449 eval: 742/1449 eval: 743/1449 eval: 744/1449 eval: 745/1449 eval: 746/1449 eval: 747/1449 eval: 748/1449 eval: 749/1449 eval: 750/1449 eval: 751/1449 eval: 752/1449 eval: 753/1449 eval: 754/1449 eval: 755/1449 eval: 756/1449 eval: 757/1449 eval: 758/1449 eval: 759/1449 eval: 760/1449 eval: 761/1449 eval: 762/1449 eval: 763/1449 eval: 764/1449 eval: 765/1449 eval: 766/1449 eval: 767/1449 eval: 768/1449 eval: 769/1449 eval: 770/1449 eval: 771/1449 eval: 772/1449 eval: 773/1449 eval: 774/1449 eval: 775/1449 eval: 776/1449 eval: 777/1449 eval: 778/1449 eval: 779/1449 eval: 780/1449 eval: 781/1449 eval: 782/1449 eval: 783/1449 eval: 784/1449 eval: 785/1449 eval: 786/1449 eval: 787/1449 eval: 788/1449 eval: 789/1449 eval: 790/1449 eval: 791/1449 eval: 792/1449 eval: 793/1449 eval: 794/1449 eval: 795/1449 eval: 796/1449 eval: 797/1449 eval: 798/1449 eval: 799/1449 eval: 800/1449 eval: 801/1449 eval: 802/1449 eval: 803/1449 eval: 804/1449 eval: 805/1449 eval: 806/1449 eval: 807/1449 eval: 808/1449 eval: 809/1449 eval: 810/1449 eval: 811/1449 eval: 812/1449 eval: 813/1449 eval: 814/1449 eval: 815/1449 eval: 816/1449 eval: 817/1449 eval: 818/1449 eval: 819/1449 eval: 820/1449 eval: 821/1449 eval: 822/1449 eval: 823/1449 eval: 824/1449 eval: 825/1449 eval: 826/1449 eval: 827/1449 eval: 828/1449 eval: 829/1449 eval: 830/1449 eval: 831/1449 eval: 832/1449 eval: 833/1449 eval: 834/1449 eval: 835/1449 eval: 836/1449 eval: 837/1449 eval: 838/1449 eval: 839/1449 eval: 840/1449 eval: 841/1449 eval: 842/1449 eval: 843/1449 eval: 844/1449 eval: 845/1449 eval: 846/1449 eval: 847/1449 eval: 848/1449 eval: 849/1449 eval: 850/1449 eval: 851/1449 eval: 852/1449 eval: 853/1449 eval: 854/1449 eval: 855/1449 eval: 856/1449 eval: 857/1449 eval: 858/1449 eval: 859/1449 eval: 860/1449 eval: 861/1449 eval: 862/1449 eval: 863/1449 eval: 864/1449 eval: 865/1449 eval: 866/1449 eval: 867/1449 eval: 868/1449 eval: 869/1449 eval: 870/1449 eval: 871/1449 eval: 872/1449 eval: 873/1449 eval: 874/1449 eval: 875/1449 eval: 876/1449 eval: 877/1449 eval: 878/1449 eval: 879/1449 eval: 880/1449 eval: 881/1449 eval: 882/1449 eval: 883/1449 eval: 884/1449 eval: 885/1449 eval: 886/1449 eval: 887/1449 eval: 888/1449 eval: 889/1449 eval: 890/1449 eval: 891/1449 eval: 892/1449 eval: 893/1449 eval: 894/1449 eval: 895/1449 eval: 896/1449 eval: 897/1449 eval: 898/1449 eval: 899/1449 eval: 900/1449 eval: 901/1449 eval: 902/1449 eval: 903/1449 eval: 904/1449 eval: 905/1449 eval: 906/1449 eval: 907/1449 eval: 908/1449 eval: 909/1449 eval: 910/1449 eval: 911/1449 eval: 912/1449 eval: 913/1449 eval: 914/1449 eval: 915/1449 eval: 916/1449 eval: 917/1449 eval: 918/1449 eval: 919/1449 eval: 920/1449 eval: 921/1449 eval: 922/1449 eval: 923/1449 eval: 924/1449 eval: 925/1449 eval: 926/1449 eval: 927/1449 eval: 928/1449 eval: 929/1449 eval: 930/1449 eval: 931/1449 eval: 932/1449 eval: 933/1449 eval: 934/1449 eval: 935/1449 eval: 936/1449 eval: 937/1449 eval: 938/1449 eval: 939/1449 eval: 940/1449 eval: 941/1449 eval: 942/1449 eval: 943/1449 eval: 944/1449 eval: 945/1449 eval: 946/1449 eval: 947/1449 eval: 948/1449 eval: 949/1449 eval: 950/1449 eval: 951/1449 eval: 952/1449 eval: 953/1449 eval: 954/1449 eval: 955/1449 eval: 956/1449 eval: 957/1449 eval: 958/1449 eval: 959/1449 eval: 960/1449 eval: 961/1449 eval: 962/1449 eval: 963/1449 eval: 964/1449 eval: 965/1449 eval: 966/1449 eval: 967/1449 eval: 968/1449 eval: 969/1449 eval: 970/1449 eval: 971/1449 eval: 972/1449 eval: 973/1449 eval: 974/1449 eval: 975/1449 eval: 976/1449 eval: 977/1449 eval: 978/1449 eval: 979/1449 eval: 980/1449 eval: 981/1449 eval: 982/1449 eval: 983/1449 eval: 984/1449 eval: 985/1449 eval: 986/1449 eval: 987/1449 eval: 988/1449 eval: 989/1449 eval: 990/1449 eval: 991/1449 eval: 992/1449 eval: 993/1449 eval: 994/1449 eval: 995/1449 eval: 996/1449 eval: 997/1449 eval: 998/1449 eval: 999/1449 eval: 1000/1449 eval: 1001/1449 eval: 1002/1449 eval: 1003/1449 eval: 1004/1449 eval: 1005/1449 eval: 1006/1449 eval: 1007/1449 eval: 1008/1449 eval: 1009/1449 eval: 1010/1449 eval: 1011/1449 eval: 1012/1449 eval: 1013/1449 eval: 1014/1449 eval: 1015/1449 eval: 1016/1449 eval: 1017/1449 eval: 1018/1449 eval: 1019/1449 eval: 1020/1449 eval: 1021/1449 eval: 1022/1449 eval: 1023/1449 eval: 1024/1449 eval: 1025/1449 eval: 1026/1449 eval: 1027/1449 eval: 1028/1449 eval: 1029/1449 eval: 1030/1449 eval: 1031/1449 eval: 1032/1449 eval: 1033/1449 eval: 1034/1449 eval: 1035/1449 eval: 1036/1449 eval: 1037/1449 eval: 1038/1449 eval: 1039/1449 eval: 1040/1449 eval: 1041/1449 eval: 1042/1449 eval: 1043/1449 eval: 1044/1449 eval: 1045/1449 eval: 1046/1449 eval: 1047/1449 eval: 1048/1449 eval: 1049/1449 eval: 1050/1449 eval: 1051/1449 eval: 1052/1449 eval: 1053/1449 eval: 1054/1449 eval: 1055/1449 eval: 1056/1449 eval: 1057/1449 eval: 1058/1449 eval: 1059/1449 eval: 1060/1449 eval: 1061/1449 eval: 1062/1449 eval: 1063/1449 eval: 1064/1449 eval: 1065/1449 eval: 1066/1449 eval: 1067/1449 eval: 1068/1449 eval: 1069/1449 eval: 1070/1449 eval: 1071/1449 eval: 1072/1449 eval: 1073/1449 eval: 1074/1449 eval: 1075/1449 eval: 1076/1449 eval: 1077/1449 eval: 1078/1449 eval: 1079/1449 eval: 1080/1449 eval: 1081/1449 eval: 1082/1449 eval: 1083/1449 eval: 1084/1449 eval: 1085/1449 eval: 1086/1449 eval: 1087/1449 eval: 1088/1449 eval: 1089/1449 eval: 1090/1449 eval: 1091/1449 eval: 1092/1449 eval: 1093/1449 eval: 1094/1449 eval: 1095/1449 eval: 1096/1449 eval: 1097/1449 eval: 1098/1449 eval: 1099/1449 eval: 1100/1449 eval: 1101/1449 eval: 1102/1449 eval: 1103/1449 eval: 1104/1449 eval: 1105/1449 eval: 1106/1449 eval: 1107/1449 eval: 1108/1449 eval: 1109/1449 eval: 1110/1449 eval: 1111/1449 eval: 1112/1449 eval: 1113/1449 eval: 1114/1449 eval: 1115/1449 eval: 1116/1449 eval: 1117/1449 eval: 1118/1449 eval: 1119/1449 eval: 1120/1449 eval: 1121/1449 eval: 1122/1449 eval: 1123/1449 eval: 1124/1449 eval: 1125/1449 eval: 1126/1449 eval: 1127/1449 eval: 1128/1449 eval: 1129/1449 eval: 1130/1449 eval: 1131/1449 eval: 1132/1449 eval: 1133/1449 eval: 1134/1449 eval: 1135/1449 eval: 1136/1449 eval: 1137/1449 eval: 1138/1449 eval: 1139/1449 eval: 1140/1449 eval: 1141/1449 eval: 1142/1449 eval: 1143/1449 eval: 1144/1449 eval: 1145/1449 eval: 1146/1449 eval: 1147/1449 eval: 1148/1449 eval: 1149/1449 eval: 1150/1449 eval: 1151/1449 eval: 1152/1449 eval: 1153/1449 eval: 1154/1449 eval: 1155/1449 eval: 1156/1449 eval: 1157/1449 eval: 1158/1449 eval: 1159/1449 eval: 1160/1449 eval: 1161/1449 eval: 1162/1449 eval: 1163/1449 eval: 1164/1449 eval: 1165/1449 eval: 1166/1449 eval: 1167/1449 eval: 1168/1449 eval: 1169/1449 eval: 1170/1449 eval: 1171/1449 eval: 1172/1449 eval: 1173/1449 eval: 1174/1449 eval: 1175/1449 eval: 1176/1449 eval: 1177/1449 eval: 1178/1449 eval: 1179/1449 eval: 1180/1449 eval: 1181/1449 eval: 1182/1449 eval: 1183/1449 eval: 1184/1449 eval: 1185/1449 eval: 1186/1449 eval: 1187/1449 eval: 1188/1449 eval: 1189/1449 eval: 1190/1449 eval: 1191/1449 eval: 1192/1449 eval: 1193/1449 eval: 1194/1449 eval: 1195/1449 eval: 1196/1449 eval: 1197/1449 eval: 1198/1449 eval: 1199/1449 eval: 1200/1449 eval: 1201/1449 eval: 1202/1449 eval: 1203/1449 eval: 1204/1449 eval: 1205/1449 eval: 1206/1449 eval: 1207/1449 eval: 1208/1449 eval: 1209/1449 eval: 1210/1449 eval: 1211/1449 eval: 1212/1449 eval: 1213/1449 eval: 1214/1449 eval: 1215/1449 eval: 1216/1449 eval: 1217/1449 eval: 1218/1449 eval: 1219/1449 eval: 1220/1449 eval: 1221/1449 eval: 1222/1449 eval: 1223/1449 eval: 1224/1449 eval: 1225/1449 eval: 1226/1449 eval: 1227/1449 eval: 1228/1449 eval: 1229/1449 eval: 1230/1449 eval: 1231/1449 eval: 1232/1449 eval: 1233/1449 eval: 1234/1449 eval: 1235/1449 eval: 1236/1449 eval: 1237/1449 eval: 1238/1449 eval: 1239/1449 eval: 1240/1449 eval: 1241/1449 eval: 1242/1449 eval: 1243/1449 eval: 1244/1449 eval: 1245/1449 eval: 1246/1449 eval: 1247/1449 eval: 1248/1449 eval: 1249/1449 eval: 1250/1449 eval: 1251/1449 eval: 1252/1449 eval: 1253/1449 eval: 1254/1449 eval: 1255/1449 eval: 1256/1449 eval: 1257/1449 eval: 1258/1449 eval: 1259/1449 eval: 1260/1449 eval: 1261/1449 eval: 1262/1449 eval: 1263/1449 eval: 1264/1449 eval: 1265/1449 eval: 1266/1449 eval: 1267/1449 eval: 1268/1449 eval: 1269/1449 eval: 1270/1449 eval: 1271/1449 eval: 1272/1449 eval: 1273/1449 eval: 1274/1449 eval: 1275/1449 eval: 1276/1449 eval: 1277/1449 eval: 1278/1449 eval: 1279/1449 eval: 1280/1449 eval: 1281/1449 eval: 1282/1449 eval: 1283/1449 eval: 1284/1449 eval: 1285/1449 eval: 1286/1449 eval: 1287/1449 eval: 1288/1449 eval: 1289/1449 eval: 1290/1449 eval: 1291/1449 eval: 1292/1449 eval: 1293/1449 eval: 1294/1449 eval: 1295/1449 eval: 1296/1449 eval: 1297/1449 eval: 1298/1449 eval: 1299/1449 eval: 1300/1449 eval: 1301/1449 eval: 1302/1449 eval: 1303/1449 eval: 1304/1449 eval: 1305/1449 eval: 1306/1449 eval: 1307/1449 eval: 1308/1449 eval: 1309/1449 eval: 1310/1449 eval: 1311/1449 eval: 1312/1449 eval: 1313/1449 eval: 1314/1449 eval: 1315/1449 eval: 1316/1449 eval: 1317/1449 eval: 1318/1449 eval: 1319/1449 eval: 1320/1449 eval: 1321/1449 eval: 1322/1449 eval: 1323/1449 eval: 1324/1449 eval: 1325/1449 eval: 1326/1449 eval: 1327/1449 eval: 1328/1449 eval: 1329/1449 eval: 1330/1449 eval: 1331/1449 eval: 1332/1449 eval: 1333/1449 eval: 1334/1449 eval: 1335/1449 eval: 1336/1449 eval: 1337/1449 eval: 1338/1449 eval: 1339/1449 eval: 1340/1449 eval: 1341/1449 eval: 1342/1449 eval: 1343/1449 eval: 1344/1449 eval: 1345/1449 eval: 1346/1449 eval: 1347/1449 eval: 1348/1449 eval: 1349/1449 eval: 1350/1449 eval: 1351/1449 eval: 1352/1449 eval: 1353/1449 eval: 1354/1449 eval: 1355/1449 eval: 1356/1449 eval: 1357/1449 eval: 1358/1449 eval: 1359/1449 eval: 1360/1449 eval: 1361/1449 eval: 1362/1449 eval: 1363/1449 eval: 1364/1449 eval: 1365/1449 eval: 1366/1449 eval: 1367/1449 eval: 1368/1449 eval: 1369/1449 eval: 1370/1449 eval: 1371/1449 eval: 1372/1449 eval: 1373/1449 eval: 1374/1449 eval: 1375/1449 eval: 1376/1449 eval: 1377/1449 eval: 1378/1449 eval: 1379/1449 eval: 1380/1449 eval: 1381/1449 eval: 1382/1449 eval: 1383/1449 eval: 1384/1449 eval: 1385/1449 eval: 1386/1449 eval: 1387/1449 eval: 1388/1449 eval: 1389/1449 eval: 1390/1449 eval: 1391/1449 eval: 1392/1449 eval: 1393/1449 eval: 1394/1449 eval: 1395/1449 eval: 1396/1449 eval: 1397/1449 eval: 1398/1449 eval: 1399/1449 eval: 1400/1449 eval: 1401/1449 eval: 1402/1449 eval: 1403/1449 eval: 1404/1449 eval: 1405/1449 eval: 1406/1449 eval: 1407/1449 eval: 1408/1449 eval: 1409/1449 eval: 1410/1449 eval: 1411/1449 eval: 1412/1449 eval: 1413/1449 eval: 1414/1449 eval: 1415/1449 eval: 1416/1449 eval: 1417/1449 eval: 1418/1449 eval: 1419/1449 eval: 1420/1449 eval: 1421/1449 eval: 1422/1449 eval: 1423/1449 eval: 1424/1449 eval: 1425/1449 eval: 1426/1449 eval: 1427/1449 eval: 1428/1449 eval: 1429/1449 eval: 1430/1449 eval: 1431/1449 eval: 1432/1449 eval: 1433/1449 eval: 1434/1449 eval: 1435/1449 eval: 1436/1449 eval: 1437/1449 eval: 1438/1449 eval: 1439/1449 eval: 1440/1449 eval: 1441/1449 eval: 1442/1449 eval: 1443/1449 eval: 1444/1449 eval: 1445/1449 eval: 1446/1449 eval: 1447/1449 eval: 1448/1449 eval: 1449/1449 IoU background: 90.52 IoU aeroplane: 74.30 IoU bicycle: 34.93 IoU bird: 78.07 IoU boat: 61.26 IoU bottle: 78.60 IoU bus: 89.25 IoU car: 84.26 IoU cat: 86.32 IoU chair: 37.38 IoU cow: 78.74 IoU diningtable: 56.57 IoU dog: 81.94 IoU horse: 75.09 IoU motorbike: 77.93 IoU person: 78.73 IoU potted-plant: 56.44 IoU sheep: 77.17 IoU sofa: 48.24 IoU train: 77.77 IoU tv/monitor: 73.73 Mean IoU: 71.30 . %cd /content/DeepLabv3.pytorch/data . /content/DeepLabv3.pytorch/data . !mkdir val . import cv2 import matplotlib.pyplot as plt import os %matplotlib inline . default_dir = &#39;/content/DeepLabv3.pytorch/data&#39; img = cv2.imread(os.path.join(default_dir, &#39;VOCdevkit/VOC2012/JPEGImages/2007_000033.jpg&#39;)) img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) print(&#39;img shape:&#39;, img.shape) plt.figure(figsize=(8, 8)) plt.imshow(img_rgb) plt.show() . img shape: (366, 500, 3) . img = cv2.imread(os.path.join(default_dir, &#39;VOCdevkit/VOC2012/SegmentationObject/2007_000033.png&#39;)) img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) print(&#39;img shape:&#39;, img.shape) plt.figure(figsize=(8, 8)) plt.imshow(img_rgb) plt.show() . img shape: (366, 500, 3) . img = cv2.imread(os.path.join(default_dir, &#39;val/2007_000033.png&#39;)) img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) print(&#39;img shape:&#39;, img.shape) plt.figure(figsize=(8, 8)) plt.imshow(img_rgb) plt.show() . img shape: (513, 513, 3) . default_dir = &#39;/content/DeepLabv3.pytorch/data&#39; img1 = cv2.imread(os.path.join(default_dir, &#39;VOCdevkit/VOC2012/JPEGImages/2007_001423.jpg&#39;)) img2 = cv2.imread(os.path.join(default_dir, &#39;VOCdevkit/VOC2012/SegmentationObject/2007_001423.png&#39;)) img3 = cv2.imread(os.path.join(default_dir, &#39;val/2007_001423.png&#39;)) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) img3_rgb = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB) plt.figure(figsize=(20, 20)) ax1 = plt.subplot(1, 3, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) ax2 = plt.subplot(1, 3, 2) ax2.imshow(img2_rgb) plt.xlabel(&quot;LABEL&quot;, size = 15) ax3 = plt.subplot(1, 3, 3) ax3 = plt.imshow(img3_rgb) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . Covid_Mask_fail . %cd /content/DeepLabv3.pytorch . /content/DeepLabv3.pytorch . !cp -r /content/drive/MyDrive/deeplab/covid_mask.py /content/DeepLabv3.pytorch/covid_mask.py !cp -r /content/drive/MyDrive/deeplab/main.py /content/DeepLabv3.pytorch/main.py . !cp -r /content/drive/MyDrive/archive /content/DeepLabv3.pytorch/data/archive . Learning . !python main.py --train --dataset covid_mask --exp bn_lr7e-3 --epochs 50 --base_lr 0.01 --batch_size 12 . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) epoch: 1 iter: 1/19 lr: 0.010000 loss: 15.2245 (15.2245) epoch: 1 iter: 2/19 lr: 0.009991 loss: 13.2159 (15.2044) epoch: 1 iter: 3/19 lr: 0.009981 loss: 10.9538 (15.1619) epoch: 1 iter: 4/19 lr: 0.009972 loss: 8.0630 (15.0909) epoch: 1 iter: 5/19 lr: 0.009962 loss: 5.2944 (14.9930) epoch: 1 iter: 6/19 lr: 0.009953 loss: 2.7867 (14.8709) epoch: 1 iter: 7/19 lr: 0.009943 loss: 1.9044 (14.7412) epoch: 1 iter: 8/19 lr: 0.009934 loss: 1.4536 (14.6083) epoch: 1 iter: 9/19 lr: 0.009924 loss: 1.5100 (14.4774) epoch: 1 iter: 10/19 lr: 0.009915 loss: 0.2143 (14.3347) epoch: 1 iter: 11/19 lr: 0.009905 loss: 0.8923 (14.2003) epoch: 1 iter: 12/19 lr: 0.009896 loss: 0.8881 (14.0672) epoch: 1 iter: 13/19 lr: 0.009886 loss: 0.3701 (13.9302) epoch: 1 iter: 14/19 lr: 0.009877 loss: 1.7686 (13.8086) epoch: 1 iter: 15/19 lr: 0.009867 loss: 0.5533 (13.6760) epoch: 1 iter: 16/19 lr: 0.009858 loss: 0.6899 (13.5462) epoch: 1 iter: 17/19 lr: 0.009848 loss: 0.2701 (13.4134) epoch: 1 iter: 18/19 lr: 0.009839 loss: 1.0436 (13.2897) epoch: 1 iter: 19/19 lr: 0.009829 loss: 0.4200 (13.1610) epoch: 2 iter: 1/19 lr: 0.009820 loss: 0.1569 (13.0310) epoch: 2 iter: 2/19 lr: 0.009810 loss: 1.0914 (12.9116) epoch: 2 iter: 3/19 lr: 0.009801 loss: 0.6643 (12.7891) epoch: 2 iter: 4/19 lr: 0.009791 loss: 0.4255 (12.6655) epoch: 2 iter: 5/19 lr: 0.009782 loss: 0.7735 (12.5466) epoch: 2 iter: 6/19 lr: 0.009772 loss: 0.4610 (12.4257) epoch: 2 iter: 7/19 lr: 0.009763 loss: 0.3469 (12.3049) epoch: 2 iter: 8/19 lr: 0.009753 loss: 0.0357 (12.1822) epoch: 2 iter: 9/19 lr: 0.009744 loss: 0.1131 (12.0615) epoch: 2 iter: 10/19 lr: 0.009734 loss: 0.2484 (11.9434) epoch: 2 iter: 11/19 lr: 0.009725 loss: 2.9087 (11.8531) epoch: 2 iter: 12/19 lr: 0.009715 loss: 0.5176 (11.7397) epoch: 2 iter: 13/19 lr: 0.009706 loss: 1.0973 (11.6333) epoch: 2 iter: 14/19 lr: 0.009696 loss: 0.9879 (11.5268) epoch: 2 iter: 15/19 lr: 0.009687 loss: 0.7725 (11.4193) epoch: 2 iter: 16/19 lr: 0.009677 loss: 0.9894 (11.3150) epoch: 2 iter: 17/19 lr: 0.009668 loss: 1.1890 (11.2137) epoch: 2 iter: 18/19 lr: 0.009658 loss: 0.7147 (11.1087) epoch: 2 iter: 19/19 lr: 0.009649 loss: 0.2814 (11.0005) epoch: 3 iter: 1/19 lr: 0.009639 loss: 1.5025 (10.9055) epoch: 3 iter: 2/19 lr: 0.009630 loss: 0.5968 (10.8024) epoch: 3 iter: 3/19 lr: 0.009620 loss: 0.7898 (10.7023) epoch: 3 iter: 4/19 lr: 0.009611 loss: 0.6763 (10.6020) epoch: 3 iter: 5/19 lr: 0.009601 loss: 0.0961 (10.4970) epoch: 3 iter: 6/19 lr: 0.009592 loss: 0.1924 (10.3939) epoch: 3 iter: 7/19 lr: 0.009582 loss: 0.4080 (10.2940) epoch: 3 iter: 8/19 lr: 0.009573 loss: 0.6136 (10.1972) epoch: 3 iter: 9/19 lr: 0.009563 loss: 1.0189 (10.1055) epoch: 3 iter: 10/19 lr: 0.009554 loss: 0.1702 (10.0061) epoch: 3 iter: 11/19 lr: 0.009544 loss: 0.4307 (9.9104) epoch: 3 iter: 12/19 lr: 0.009535 loss: 0.3931 (9.8152) epoch: 3 iter: 13/19 lr: 0.009525 loss: 0.2406 (9.7194) epoch: 3 iter: 14/19 lr: 0.009516 loss: 0.2887 (9.6251) epoch: 3 iter: 15/19 lr: 0.009506 loss: 0.5203 (9.5341) epoch: 3 iter: 16/19 lr: 0.009496 loss: 0.0307 (9.4390) epoch: 3 iter: 17/19 lr: 0.009487 loss: 0.9888 (9.3545) epoch: 3 iter: 18/19 lr: 0.009477 loss: 0.3966 (9.2650) epoch: 3 iter: 19/19 lr: 0.009468 loss: 1.4238 (9.1866) epoch: 4 iter: 1/19 lr: 0.009458 loss: 0.1650 (9.0963) epoch: 4 iter: 2/19 lr: 0.009449 loss: 0.2387 (9.0078) epoch: 4 iter: 3/19 lr: 0.009439 loss: 0.2442 (8.9201) epoch: 4 iter: 4/19 lr: 0.009430 loss: 0.5014 (8.8359) epoch: 4 iter: 5/19 lr: 0.009420 loss: 0.8270 (8.7558) epoch: 4 iter: 6/19 lr: 0.009411 loss: 1.4397 (8.6827) epoch: 4 iter: 7/19 lr: 0.009401 loss: 0.2289 (8.5981) epoch: 4 iter: 8/19 lr: 0.009392 loss: 0.7591 (8.5198) epoch: 4 iter: 9/19 lr: 0.009382 loss: 0.2191 (8.4368) epoch: 4 iter: 10/19 lr: 0.009373 loss: 0.4202 (8.3566) epoch: 4 iter: 11/19 lr: 0.009363 loss: 1.3883 (8.2869) epoch: 4 iter: 12/19 lr: 0.009353 loss: 0.9507 (8.2135) epoch: 4 iter: 13/19 lr: 0.009344 loss: 0.3494 (8.1349) epoch: 4 iter: 14/19 lr: 0.009334 loss: 0.1297 (8.0548) epoch: 4 iter: 15/19 lr: 0.009325 loss: 0.1791 (7.9761) epoch: 4 iter: 16/19 lr: 0.009315 loss: 0.2965 (7.8993) epoch: 4 iter: 17/19 lr: 0.009306 loss: 0.6407 (7.8267) epoch: 4 iter: 18/19 lr: 0.009296 loss: 0.8283 (7.7567) epoch: 4 iter: 19/19 lr: 0.009287 loss: 0.2501 (7.6817) epoch: 5 iter: 1/19 lr: 0.009277 loss: 0.2660 (7.6075) epoch: 5 iter: 2/19 lr: 0.009267 loss: 0.1228 (7.5327) epoch: 5 iter: 3/19 lr: 0.009258 loss: 1.1648 (7.4690) epoch: 5 iter: 4/19 lr: 0.009248 loss: 0.3380 (7.3977) epoch: 5 iter: 5/19 lr: 0.009239 loss: 0.0671 (7.3244) epoch: 5 iter: 6/19 lr: 0.009229 loss: 0.2628 (7.2537) epoch: 5 iter: 7/19 lr: 0.009220 loss: 0.0687 (7.1819) epoch: 5 iter: 8/19 lr: 0.009210 loss: 0.1124 (7.1112) epoch: 5 iter: 9/19 lr: 0.009201 loss: 0.2150 (7.0422) epoch: 5 iter: 10/19 lr: 0.009191 loss: 0.2437 (6.9743) epoch: 5 iter: 11/19 lr: 0.009181 loss: 0.1832 (6.9063) epoch: 5 iter: 12/19 lr: 0.009172 loss: 0.2193 (6.8395) epoch: 5 iter: 13/19 lr: 0.009162 loss: 0.0629 (6.7717) epoch: 5 iter: 14/19 lr: 0.009153 loss: 0.1649 (6.7056) epoch: 5 iter: 15/19 lr: 0.009143 loss: 0.5039 (6.6436) epoch: 5 iter: 16/19 lr: 0.009134 loss: 0.3088 (6.5803) epoch: 5 iter: 17/19 lr: 0.009124 loss: 0.2256 (6.5167) epoch: 5 iter: 18/19 lr: 0.009114 loss: 0.1924 (6.4535) epoch: 5 iter: 19/19 lr: 0.009105 loss: 0.0865 (6.3898) epoch: 6 iter: 1/19 lr: 0.009095 loss: 0.4359 (6.3303) epoch: 6 iter: 2/19 lr: 0.009086 loss: 0.4103 (6.2711) epoch: 6 iter: 3/19 lr: 0.009076 loss: 0.3432 (6.2118) epoch: 6 iter: 4/19 lr: 0.009067 loss: 0.0989 (6.1507) epoch: 6 iter: 5/19 lr: 0.009057 loss: 0.1451 (6.0906) epoch: 6 iter: 6/19 lr: 0.009047 loss: 0.2161 (6.0319) epoch: 6 iter: 7/19 lr: 0.009038 loss: 0.3078 (5.9746) epoch: 6 iter: 8/19 lr: 0.009028 loss: 0.3763 (5.9186) epoch: 6 iter: 9/19 lr: 0.009019 loss: 0.4330 (5.8638) epoch: 6 iter: 10/19 lr: 0.009009 loss: 0.2361 (5.8075) epoch: 6 iter: 11/19 lr: 0.009000 loss: 0.2609 (5.7520) epoch: 6 iter: 12/19 lr: 0.008990 loss: 0.4464 (5.6990) epoch: 6 iter: 13/19 lr: 0.008980 loss: 0.1253 (5.6432) epoch: 6 iter: 14/19 lr: 0.008971 loss: 0.0822 (5.5876) epoch: 6 iter: 15/19 lr: 0.008961 loss: 0.2551 (5.5343) epoch: 6 iter: 16/19 lr: 0.008952 loss: 0.2036 (5.4810) epoch: 6 iter: 17/19 lr: 0.008942 loss: 0.1208 (5.4274) epoch: 6 iter: 18/19 lr: 0.008932 loss: 0.1098 (5.3742) epoch: 6 iter: 19/19 lr: 0.008923 loss: 0.1257 (5.3217) epoch: 7 iter: 1/19 lr: 0.008913 loss: 0.1195 (5.2697) epoch: 7 iter: 2/19 lr: 0.008904 loss: 0.1448 (5.2185) epoch: 7 iter: 3/19 lr: 0.008894 loss: 0.0841 (5.1671) epoch: 7 iter: 4/19 lr: 0.008884 loss: 0.1948 (5.1174) epoch: 7 iter: 5/19 lr: 0.008875 loss: 0.4205 (5.0704) epoch: 7 iter: 6/19 lr: 0.008865 loss: 0.2394 (5.0221) epoch: 7 iter: 7/19 lr: 0.008856 loss: 0.0398 (4.9723) epoch: 7 iter: 8/19 lr: 0.008846 loss: 0.0215 (4.9228) epoch: 7 iter: 9/19 lr: 0.008836 loss: 0.7019 (4.8806) epoch: 7 iter: 10/19 lr: 0.008827 loss: 0.2013 (4.8338) epoch: 7 iter: 11/19 lr: 0.008817 loss: 0.1881 (4.7873) epoch: 7 iter: 12/19 lr: 0.008808 loss: 0.2710 (4.7422) epoch: 7 iter: 13/19 lr: 0.008798 loss: 0.0738 (4.6955) epoch: 7 iter: 14/19 lr: 0.008788 loss: 0.1372 (4.6499) epoch: 7 iter: 15/19 lr: 0.008779 loss: 0.1616 (4.6050) epoch: 7 iter: 16/19 lr: 0.008769 loss: 0.1370 (4.5603) epoch: 7 iter: 17/19 lr: 0.008760 loss: 0.1398 (4.5161) epoch: 7 iter: 18/19 lr: 0.008750 loss: 0.6299 (4.4773) epoch: 7 iter: 19/19 lr: 0.008740 loss: 0.2051 (4.4346) epoch: 8 iter: 1/19 lr: 0.008731 loss: 0.1855 (4.3921) epoch: 8 iter: 2/19 lr: 0.008721 loss: 0.1284 (4.3494) epoch: 8 iter: 3/19 lr: 0.008711 loss: 0.4682 (4.3106) epoch: 8 iter: 4/19 lr: 0.008702 loss: 0.8156 (4.2757) epoch: 8 iter: 5/19 lr: 0.008692 loss: 0.1266 (4.2342) epoch: 8 iter: 6/19 lr: 0.008683 loss: 0.7549 (4.1994) epoch: 8 iter: 7/19 lr: 0.008673 loss: 0.1737 (4.1591) epoch: 8 iter: 8/19 lr: 0.008663 loss: 0.2852 (4.1204) epoch: 8 iter: 9/19 lr: 0.008654 loss: 0.1384 (4.0806) epoch: 8 iter: 10/19 lr: 0.008644 loss: 0.1264 (4.0410) epoch: 8 iter: 11/19 lr: 0.008634 loss: 0.1482 (4.0021) epoch: 8 iter: 12/19 lr: 0.008625 loss: 0.1080 (3.9632) epoch: 8 iter: 13/19 lr: 0.008615 loss: 0.1583 (3.9251) epoch: 8 iter: 14/19 lr: 0.008606 loss: 0.3738 (3.8896) epoch: 8 iter: 15/19 lr: 0.008596 loss: 0.1895 (3.8526) epoch: 8 iter: 16/19 lr: 0.008586 loss: 0.5792 (3.8199) epoch: 8 iter: 17/19 lr: 0.008577 loss: 0.6436 (3.7881) epoch: 8 iter: 18/19 lr: 0.008567 loss: 0.1664 (3.7519) epoch: 8 iter: 19/19 lr: 0.008557 loss: 0.0533 (3.7149) epoch: 9 iter: 1/19 lr: 0.008548 loss: 0.1741 (3.6795) epoch: 9 iter: 2/19 lr: 0.008538 loss: 0.1713 (3.6444) epoch: 9 iter: 3/19 lr: 0.008528 loss: 0.1025 (3.6090) epoch: 9 iter: 4/19 lr: 0.008519 loss: 0.6246 (3.5791) epoch: 9 iter: 5/19 lr: 0.008509 loss: 0.0611 (3.5440) epoch: 9 iter: 6/19 lr: 0.008500 loss: 0.0866 (3.5094) epoch: 9 iter: 7/19 lr: 0.008490 loss: 0.7771 (3.4821) epoch: 9 iter: 8/19 lr: 0.008480 loss: 0.1686 (3.4489) epoch: 9 iter: 9/19 lr: 0.008471 loss: 0.4408 (3.4188) epoch: 9 iter: 10/19 lr: 0.008461 loss: 0.1951 (3.3866) epoch: 9 iter: 11/19 lr: 0.008451 loss: 0.2208 (3.3549) epoch: 9 iter: 12/19 lr: 0.008442 loss: 0.4192 (3.3256) epoch: 9 iter: 13/19 lr: 0.008432 loss: 0.2034 (3.2944) epoch: 9 iter: 14/19 lr: 0.008422 loss: 0.4451 (3.2659) epoch: 9 iter: 15/19 lr: 0.008413 loss: 0.1568 (3.2348) epoch: 9 iter: 16/19 lr: 0.008403 loss: 0.0570 (3.2030) epoch: 9 iter: 17/19 lr: 0.008393 loss: 0.0722 (3.1717) epoch: 9 iter: 18/19 lr: 0.008384 loss: 0.3971 (3.1440) epoch: 9 iter: 19/19 lr: 0.008374 loss: 0.1793 (3.1143) epoch: 10 iter: 1/19 lr: 0.008364 loss: 0.3092 (3.0863) epoch: 10 iter: 2/19 lr: 0.008355 loss: 0.1464 (3.0569) epoch: 10 iter: 3/19 lr: 0.008345 loss: 0.0964 (3.0273) epoch: 10 iter: 4/19 lr: 0.008335 loss: 0.0794 (2.9978) epoch: 10 iter: 5/19 lr: 0.008326 loss: 0.3378 (2.9712) epoch: 10 iter: 6/19 lr: 0.008316 loss: 0.5607 (2.9471) epoch: 10 iter: 7/19 lr: 0.008306 loss: 0.2041 (2.9196) epoch: 10 iter: 8/19 lr: 0.008297 loss: 0.0933 (2.8914) epoch: 10 iter: 9/19 lr: 0.008287 loss: 0.3153 (2.8656) epoch: 10 iter: 10/19 lr: 0.008277 loss: 0.0960 (2.8379) epoch: 10 iter: 11/19 lr: 0.008268 loss: 0.2414 (2.8120) epoch: 10 iter: 12/19 lr: 0.008258 loss: 0.1424 (2.7853) epoch: 10 iter: 13/19 lr: 0.008248 loss: 0.1130 (2.7585) epoch: 10 iter: 14/19 lr: 0.008239 loss: 0.4252 (2.7352) epoch: 10 iter: 15/19 lr: 0.008229 loss: 0.1805 (2.7097) epoch: 10 iter: 16/19 lr: 0.008219 loss: 0.1627 (2.6842) epoch: 10 iter: 17/19 lr: 0.008210 loss: 0.2498 (2.6598) epoch: 10 iter: 18/19 lr: 0.008200 loss: 0.2020 (2.6353) epoch: 10 iter: 19/19 lr: 0.008190 loss: 0.2187 (2.6111) epoch: 11 iter: 1/19 lr: 0.008181 loss: 0.3883 (2.5889) epoch: 11 iter: 2/19 lr: 0.008171 loss: 0.1548 (2.5645) epoch: 11 iter: 3/19 lr: 0.008161 loss: 0.6846 (2.5457) epoch: 11 iter: 4/19 lr: 0.008151 loss: 0.5562 (2.5258) epoch: 11 iter: 5/19 lr: 0.008142 loss: 0.2940 (2.5035) epoch: 11 iter: 6/19 lr: 0.008132 loss: 0.2843 (2.4813) epoch: 11 iter: 7/19 lr: 0.008122 loss: 0.1770 (2.4583) epoch: 11 iter: 8/19 lr: 0.008113 loss: 0.2559 (2.4363) epoch: 11 iter: 9/19 lr: 0.008103 loss: 0.1813 (2.4137) epoch: 11 iter: 10/19 lr: 0.008093 loss: 0.1356 (2.3909) epoch: 11 iter: 11/19 lr: 0.008084 loss: 0.1354 (2.3684) epoch: 11 iter: 12/19 lr: 0.008074 loss: 0.2392 (2.3471) epoch: 11 iter: 13/19 lr: 0.008064 loss: 0.2166 (2.3258) epoch: 11 iter: 14/19 lr: 0.008054 loss: 0.4872 (2.3074) epoch: 11 iter: 15/19 lr: 0.008045 loss: 0.1279 (2.2856) epoch: 11 iter: 16/19 lr: 0.008035 loss: 0.1752 (2.2645) epoch: 11 iter: 17/19 lr: 0.008025 loss: 0.2238 (2.2441) epoch: 11 iter: 18/19 lr: 0.008016 loss: 0.1852 (2.2235) epoch: 11 iter: 19/19 lr: 0.008006 loss: 0.3195 (2.2045) epoch: 12 iter: 1/19 lr: 0.007996 loss: 0.1076 (2.1835) epoch: 12 iter: 2/19 lr: 0.007987 loss: 0.2866 (2.1645) epoch: 12 iter: 3/19 lr: 0.007977 loss: 0.0799 (2.1437) epoch: 12 iter: 4/19 lr: 0.007967 loss: 0.4235 (2.1265) epoch: 12 iter: 5/19 lr: 0.007957 loss: 0.0632 (2.1058) epoch: 12 iter: 6/19 lr: 0.007948 loss: 0.1180 (2.0860) epoch: 12 iter: 7/19 lr: 0.007938 loss: 0.1137 (2.0662) epoch: 12 iter: 8/19 lr: 0.007928 loss: 0.0892 (2.0465) epoch: 12 iter: 9/19 lr: 0.007918 loss: 0.2712 (2.0287) epoch: 12 iter: 10/19 lr: 0.007909 loss: 0.1731 (2.0102) epoch: 12 iter: 11/19 lr: 0.007899 loss: 0.2168 (1.9922) epoch: 12 iter: 12/19 lr: 0.007889 loss: 0.4791 (1.9771) epoch: 12 iter: 13/19 lr: 0.007880 loss: 0.2777 (1.9601) epoch: 12 iter: 14/19 lr: 0.007870 loss: 0.1262 (1.9418) epoch: 12 iter: 15/19 lr: 0.007860 loss: 0.9134 (1.9315) epoch: 12 iter: 16/19 lr: 0.007850 loss: 0.3162 (1.9153) epoch: 12 iter: 17/19 lr: 0.007841 loss: 0.0981 (1.8972) epoch: 12 iter: 18/19 lr: 0.007831 loss: 0.2194 (1.8804) epoch: 12 iter: 19/19 lr: 0.007821 loss: 0.2713 (1.8643) epoch: 13 iter: 1/19 lr: 0.007811 loss: 0.5505 (1.8511) epoch: 13 iter: 2/19 lr: 0.007802 loss: 1.3894 (1.8465) epoch: 13 iter: 3/19 lr: 0.007792 loss: 0.2283 (1.8303) epoch: 13 iter: 4/19 lr: 0.007782 loss: 0.1594 (1.8136) epoch: 13 iter: 5/19 lr: 0.007773 loss: 0.1834 (1.7973) epoch: 13 iter: 6/19 lr: 0.007763 loss: 0.2001 (1.7814) epoch: 13 iter: 7/19 lr: 0.007753 loss: 0.2495 (1.7660) epoch: 13 iter: 8/19 lr: 0.007743 loss: 0.1317 (1.7497) epoch: 13 iter: 9/19 lr: 0.007734 loss: 0.2623 (1.7348) epoch: 13 iter: 10/19 lr: 0.007724 loss: 0.0819 (1.7183) epoch: 13 iter: 11/19 lr: 0.007714 loss: 0.1912 (1.7030) epoch: 13 iter: 12/19 lr: 0.007704 loss: 0.4704 (1.6907) epoch: 13 iter: 13/19 lr: 0.007695 loss: 0.2825 (1.6766) epoch: 13 iter: 14/19 lr: 0.007685 loss: 1.3076 (1.6729) epoch: 13 iter: 15/19 lr: 0.007675 loss: 0.1979 (1.6582) epoch: 13 iter: 16/19 lr: 0.007665 loss: 0.3339 (1.6449) epoch: 13 iter: 17/19 lr: 0.007655 loss: 0.2221 (1.6307) epoch: 13 iter: 18/19 lr: 0.007646 loss: 0.1282 (1.6157) epoch: 13 iter: 19/19 lr: 0.007636 loss: 0.0467 (1.6000) epoch: 14 iter: 1/19 lr: 0.007626 loss: 0.1307 (1.5853) epoch: 14 iter: 2/19 lr: 0.007616 loss: 0.1520 (1.5710) epoch: 14 iter: 3/19 lr: 0.007607 loss: 0.1562 (1.5568) epoch: 14 iter: 4/19 lr: 0.007597 loss: 0.4730 (1.5460) epoch: 14 iter: 5/19 lr: 0.007587 loss: 0.0767 (1.5313) epoch: 14 iter: 6/19 lr: 0.007577 loss: 0.3260 (1.5192) epoch: 14 iter: 7/19 lr: 0.007568 loss: 0.1839 (1.5059) epoch: 14 iter: 8/19 lr: 0.007558 loss: 0.1554 (1.4924) epoch: 14 iter: 9/19 lr: 0.007548 loss: 0.2119 (1.4796) epoch: 14 iter: 10/19 lr: 0.007538 loss: 0.1627 (1.4664) epoch: 14 iter: 11/19 lr: 0.007529 loss: 0.1596 (1.4533) epoch: 14 iter: 12/19 lr: 0.007519 loss: 0.2480 (1.4413) epoch: 14 iter: 13/19 lr: 0.007509 loss: 0.1436 (1.4283) epoch: 14 iter: 14/19 lr: 0.007499 loss: 0.1171 (1.4152) epoch: 14 iter: 15/19 lr: 0.007489 loss: 0.0903 (1.4019) epoch: 14 iter: 16/19 lr: 0.007480 loss: 0.2210 (1.3901) epoch: 14 iter: 17/19 lr: 0.007470 loss: 0.6217 (1.3824) epoch: 14 iter: 18/19 lr: 0.007460 loss: 0.2459 (1.3711) epoch: 14 iter: 19/19 lr: 0.007450 loss: 0.2246 (1.3596) epoch: 15 iter: 1/19 lr: 0.007440 loss: 0.2369 (1.3484) epoch: 15 iter: 2/19 lr: 0.007431 loss: 0.1695 (1.3366) epoch: 15 iter: 3/19 lr: 0.007421 loss: 0.1698 (1.3249) epoch: 15 iter: 4/19 lr: 0.007411 loss: 0.2493 (1.3142) epoch: 15 iter: 5/19 lr: 0.007401 loss: 0.5079 (1.3061) epoch: 15 iter: 6/19 lr: 0.007391 loss: 0.0788 (1.2938) epoch: 15 iter: 7/19 lr: 0.007382 loss: 0.1770 (1.2827) epoch: 15 iter: 8/19 lr: 0.007372 loss: 0.0564 (1.2704) epoch: 15 iter: 9/19 lr: 0.007362 loss: 0.1801 (1.2595) epoch: 15 iter: 10/19 lr: 0.007352 loss: 0.2046 (1.2490) epoch: 15 iter: 11/19 lr: 0.007342 loss: 0.2571 (1.2390) epoch: 15 iter: 12/19 lr: 0.007333 loss: 0.1079 (1.2277) epoch: 15 iter: 13/19 lr: 0.007323 loss: 0.2309 (1.2178) epoch: 15 iter: 14/19 lr: 0.007313 loss: 0.2385 (1.2080) epoch: 15 iter: 15/19 lr: 0.007303 loss: 0.3756 (1.1996) epoch: 15 iter: 16/19 lr: 0.007293 loss: 0.8648 (1.1963) epoch: 15 iter: 17/19 lr: 0.007284 loss: 0.2334 (1.1867) epoch: 15 iter: 18/19 lr: 0.007274 loss: 0.1574 (1.1764) epoch: 15 iter: 19/19 lr: 0.007264 loss: 1.2344 (1.1770) epoch: 16 iter: 1/19 lr: 0.007254 loss: 0.1231 (1.1664) epoch: 16 iter: 2/19 lr: 0.007244 loss: 0.1839 (1.1566) epoch: 16 iter: 3/19 lr: 0.007235 loss: 0.7476 (1.1525) epoch: 16 iter: 4/19 lr: 0.007225 loss: 0.2794 (1.1438) epoch: 16 iter: 5/19 lr: 0.007215 loss: 0.2469 (1.1348) epoch: 16 iter: 6/19 lr: 0.007205 loss: 0.2549 (1.1260) epoch: 16 iter: 7/19 lr: 0.007195 loss: 0.2032 (1.1168) epoch: 16 iter: 8/19 lr: 0.007185 loss: 0.2491 (1.1081) epoch: 16 iter: 9/19 lr: 0.007176 loss: 0.2005 (1.0990) epoch: 16 iter: 10/19 lr: 0.007166 loss: 0.2653 (1.0907) epoch: 16 iter: 11/19 lr: 0.007156 loss: 0.2068 (1.0818) epoch: 16 iter: 12/19 lr: 0.007146 loss: 0.1547 (1.0726) epoch: 16 iter: 13/19 lr: 0.007136 loss: 0.1165 (1.0630) epoch: 16 iter: 14/19 lr: 0.007126 loss: 0.1242 (1.0536) epoch: 16 iter: 15/19 lr: 0.007117 loss: 0.8803 (1.0519) epoch: 16 iter: 16/19 lr: 0.007107 loss: 0.1587 (1.0430) epoch: 16 iter: 17/19 lr: 0.007097 loss: 0.0860 (1.0334) epoch: 16 iter: 18/19 lr: 0.007087 loss: 0.1189 (1.0242) epoch: 16 iter: 19/19 lr: 0.007077 loss: 0.0690 (1.0147) epoch: 17 iter: 1/19 lr: 0.007067 loss: 0.0665 (1.0052) epoch: 17 iter: 2/19 lr: 0.007058 loss: 0.2562 (0.9977) epoch: 17 iter: 3/19 lr: 0.007048 loss: 0.1516 (0.9893) epoch: 17 iter: 4/19 lr: 0.007038 loss: 0.0914 (0.9803) epoch: 17 iter: 5/19 lr: 0.007028 loss: 0.1636 (0.9721) epoch: 17 iter: 6/19 lr: 0.007018 loss: 0.0499 (0.9629) epoch: 17 iter: 7/19 lr: 0.007008 loss: 0.1356 (0.9546) epoch: 17 iter: 8/19 lr: 0.006998 loss: 0.2348 (0.9474) epoch: 17 iter: 9/19 lr: 0.006989 loss: 0.1793 (0.9397) epoch: 17 iter: 10/19 lr: 0.006979 loss: 0.2944 (0.9333) epoch: 17 iter: 11/19 lr: 0.006969 loss: 0.1492 (0.9254) epoch: 17 iter: 12/19 lr: 0.006959 loss: 0.3021 (0.9192) epoch: 17 iter: 13/19 lr: 0.006949 loss: 0.5289 (0.9153) epoch: 17 iter: 14/19 lr: 0.006939 loss: 0.0799 (0.9070) epoch: 17 iter: 15/19 lr: 0.006929 loss: 0.0860 (0.8987) epoch: 17 iter: 16/19 lr: 0.006920 loss: 0.2243 (0.8920) epoch: 17 iter: 17/19 lr: 0.006910 loss: 0.2206 (0.8853) epoch: 17 iter: 18/19 lr: 0.006900 loss: 0.3210 (0.8796) epoch: 17 iter: 19/19 lr: 0.006890 loss: 0.0416 (0.8713) epoch: 18 iter: 1/19 lr: 0.006880 loss: 0.1099 (0.8637) epoch: 18 iter: 2/19 lr: 0.006870 loss: 0.1910 (0.8569) epoch: 18 iter: 3/19 lr: 0.006860 loss: 0.3116 (0.8515) epoch: 18 iter: 4/19 lr: 0.006850 loss: 0.1757 (0.8447) epoch: 18 iter: 5/19 lr: 0.006841 loss: 0.0808 (0.8371) epoch: 18 iter: 6/19 lr: 0.006831 loss: 0.2187 (0.8309) epoch: 18 iter: 7/19 lr: 0.006821 loss: 0.1898 (0.8245) epoch: 18 iter: 8/19 lr: 0.006811 loss: 0.0461 (0.8167) epoch: 18 iter: 9/19 lr: 0.006801 loss: 0.0841 (0.8094) epoch: 18 iter: 10/19 lr: 0.006791 loss: 0.2318 (0.8036) epoch: 18 iter: 11/19 lr: 0.006781 loss: 0.5625 (0.8012) epoch: 18 iter: 12/19 lr: 0.006771 loss: 0.1801 (0.7950) epoch: 18 iter: 13/19 lr: 0.006761 loss: 0.1002 (0.7880) epoch: 18 iter: 14/19 lr: 0.006752 loss: 0.1061 (0.7812) epoch: 18 iter: 15/19 lr: 0.006742 loss: 0.1925 (0.7753) epoch: 18 iter: 16/19 lr: 0.006732 loss: 0.2518 (0.7701) epoch: 18 iter: 17/19 lr: 0.006722 loss: 0.1194 (0.7636) epoch: 18 iter: 18/19 lr: 0.006712 loss: 0.0684 (0.7566) epoch: 18 iter: 19/19 lr: 0.006702 loss: 0.0767 (0.7498) epoch: 19 iter: 1/19 lr: 0.006692 loss: 0.0719 (0.7430) epoch: 19 iter: 2/19 lr: 0.006682 loss: 0.1071 (0.7367) epoch: 19 iter: 3/19 lr: 0.006672 loss: 0.3072 (0.7324) epoch: 19 iter: 4/19 lr: 0.006662 loss: 0.1973 (0.7270) epoch: 19 iter: 5/19 lr: 0.006652 loss: 0.2023 (0.7218) epoch: 19 iter: 6/19 lr: 0.006643 loss: 0.1408 (0.7160) epoch: 19 iter: 7/19 lr: 0.006633 loss: 0.0802 (0.7096) epoch: 19 iter: 8/19 lr: 0.006623 loss: 0.2953 (0.7055) epoch: 19 iter: 9/19 lr: 0.006613 loss: 0.2561 (0.7010) epoch: 19 iter: 10/19 lr: 0.006603 loss: 0.0790 (0.6948) epoch: 19 iter: 11/19 lr: 0.006593 loss: 0.3506 (0.6913) epoch: 19 iter: 12/19 lr: 0.006583 loss: 0.0881 (0.6853) epoch: 19 iter: 13/19 lr: 0.006573 loss: 0.1046 (0.6795) epoch: 19 iter: 14/19 lr: 0.006563 loss: 0.0999 (0.6737) epoch: 19 iter: 15/19 lr: 0.006553 loss: 0.0767 (0.6677) epoch: 19 iter: 16/19 lr: 0.006543 loss: 0.4115 (0.6652) epoch: 19 iter: 17/19 lr: 0.006533 loss: 0.1397 (0.6599) epoch: 19 iter: 18/19 lr: 0.006523 loss: 0.1159 (0.6545) epoch: 19 iter: 19/19 lr: 0.006514 loss: 0.1390 (0.6493) epoch: 20 iter: 1/19 lr: 0.006504 loss: 0.3851 (0.6467) epoch: 20 iter: 2/19 lr: 0.006494 loss: 0.2578 (0.6428) epoch: 20 iter: 3/19 lr: 0.006484 loss: 0.1474 (0.6378) epoch: 20 iter: 4/19 lr: 0.006474 loss: 0.1320 (0.6328) epoch: 20 iter: 5/19 lr: 0.006464 loss: 0.0553 (0.6270) epoch: 20 iter: 6/19 lr: 0.006454 loss: 0.1363 (0.6221) epoch: 20 iter: 7/19 lr: 0.006444 loss: 0.4587 (0.6205) epoch: 20 iter: 8/19 lr: 0.006434 loss: 0.1070 (0.6153) epoch: 20 iter: 9/19 lr: 0.006424 loss: 0.2560 (0.6117) epoch: 20 iter: 10/19 lr: 0.006414 loss: 0.0810 (0.6064) epoch: 20 iter: 11/19 lr: 0.006404 loss: 0.2180 (0.6025) epoch: 20 iter: 12/19 lr: 0.006394 loss: 0.2553 (0.5991) epoch: 20 iter: 13/19 lr: 0.006384 loss: 0.0425 (0.5935) epoch: 20 iter: 14/19 lr: 0.006374 loss: 0.2039 (0.5896) epoch: 20 iter: 15/19 lr: 0.006364 loss: 0.1905 (0.5856) epoch: 20 iter: 16/19 lr: 0.006354 loss: 0.1644 (0.5814) epoch: 20 iter: 17/19 lr: 0.006344 loss: 0.0639 (0.5762) epoch: 20 iter: 18/19 lr: 0.006334 loss: 0.1002 (0.5715) epoch: 20 iter: 19/19 lr: 0.006324 loss: 0.1968 (0.5677) epoch: 21 iter: 1/19 lr: 0.006314 loss: 0.2925 (0.5650) epoch: 21 iter: 2/19 lr: 0.006304 loss: 0.1296 (0.5606) epoch: 21 iter: 3/19 lr: 0.006295 loss: 0.1088 (0.5561) epoch: 21 iter: 4/19 lr: 0.006285 loss: 0.0974 (0.5515) epoch: 21 iter: 5/19 lr: 0.006275 loss: 0.0690 (0.5467) epoch: 21 iter: 6/19 lr: 0.006265 loss: 0.0884 (0.5421) epoch: 21 iter: 7/19 lr: 0.006255 loss: 0.3433 (0.5401) epoch: 21 iter: 8/19 lr: 0.006245 loss: 0.1653 (0.5364) epoch: 21 iter: 9/19 lr: 0.006235 loss: 0.1487 (0.5325) epoch: 21 iter: 10/19 lr: 0.006225 loss: 0.0968 (0.5281) epoch: 21 iter: 11/19 lr: 0.006215 loss: 0.1781 (0.5246) epoch: 21 iter: 12/19 lr: 0.006205 loss: 0.2983 (0.5224) epoch: 21 iter: 13/19 lr: 0.006195 loss: 0.1511 (0.5186) epoch: 21 iter: 14/19 lr: 0.006185 loss: 0.2234 (0.5157) epoch: 21 iter: 15/19 lr: 0.006175 loss: 0.0873 (0.5114) epoch: 21 iter: 16/19 lr: 0.006165 loss: 0.2485 (0.5088) epoch: 21 iter: 17/19 lr: 0.006155 loss: 0.3335 (0.5070) epoch: 21 iter: 18/19 lr: 0.006145 loss: 0.1779 (0.5037) epoch: 21 iter: 19/19 lr: 0.006135 loss: 0.6865 (0.5056) epoch: 22 iter: 1/19 lr: 0.006125 loss: 0.1482 (0.5020) epoch: 22 iter: 2/19 lr: 0.006115 loss: 0.0415 (0.4974) epoch: 22 iter: 3/19 lr: 0.006105 loss: 0.1778 (0.4942) epoch: 22 iter: 4/19 lr: 0.006095 loss: 0.1595 (0.4908) epoch: 22 iter: 5/19 lr: 0.006085 loss: 0.3335 (0.4893) epoch: 22 iter: 6/19 lr: 0.006075 loss: 0.1799 (0.4862) epoch: 22 iter: 7/19 lr: 0.006065 loss: 0.1031 (0.4823) epoch: 22 iter: 8/19 lr: 0.006055 loss: 0.1052 (0.4786) epoch: 22 iter: 9/19 lr: 0.006045 loss: 0.1856 (0.4756) epoch: 22 iter: 10/19 lr: 0.006035 loss: 0.0812 (0.4717) epoch: 22 iter: 11/19 lr: 0.006025 loss: 0.1705 (0.4687) epoch: 22 iter: 12/19 lr: 0.006015 loss: 0.0938 (0.4649) epoch: 22 iter: 13/19 lr: 0.006005 loss: 0.2170 (0.4625) epoch: 22 iter: 14/19 lr: 0.005994 loss: 0.4691 (0.4625) epoch: 22 iter: 15/19 lr: 0.005984 loss: 0.0914 (0.4588) epoch: 22 iter: 16/19 lr: 0.005974 loss: 0.2532 (0.4568) epoch: 22 iter: 17/19 lr: 0.005964 loss: 0.0874 (0.4531) epoch: 22 iter: 18/19 lr: 0.005954 loss: 0.0572 (0.4491) epoch: 22 iter: 19/19 lr: 0.005944 loss: 0.1165 (0.4458) epoch: 23 iter: 1/19 lr: 0.005934 loss: 0.1206 (0.4425) epoch: 23 iter: 2/19 lr: 0.005924 loss: 0.9623 (0.4477) epoch: 23 iter: 3/19 lr: 0.005914 loss: 0.1873 (0.4451) epoch: 23 iter: 4/19 lr: 0.005904 loss: 0.1176 (0.4418) epoch: 23 iter: 5/19 lr: 0.005894 loss: 0.0720 (0.4381) epoch: 23 iter: 6/19 lr: 0.005884 loss: 0.1145 (0.4349) epoch: 23 iter: 7/19 lr: 0.005874 loss: 0.0876 (0.4314) epoch: 23 iter: 8/19 lr: 0.005864 loss: 0.0922 (0.4280) epoch: 23 iter: 9/19 lr: 0.005854 loss: 0.3412 (0.4272) epoch: 23 iter: 10/19 lr: 0.005844 loss: 0.1183 (0.4241) epoch: 23 iter: 11/19 lr: 0.005834 loss: 0.2435 (0.4223) epoch: 23 iter: 12/19 lr: 0.005824 loss: 0.1186 (0.4192) epoch: 23 iter: 13/19 lr: 0.005814 loss: 0.2215 (0.4173) epoch: 23 iter: 14/19 lr: 0.005804 loss: 0.2994 (0.4161) epoch: 23 iter: 15/19 lr: 0.005794 loss: 0.2484 (0.4144) epoch: 23 iter: 16/19 lr: 0.005783 loss: 0.1947 (0.4122) epoch: 23 iter: 17/19 lr: 0.005773 loss: 0.1817 (0.4099) epoch: 23 iter: 18/19 lr: 0.005763 loss: 0.2129 (0.4079) epoch: 23 iter: 19/19 lr: 0.005753 loss: 0.3023 (0.4069) epoch: 24 iter: 1/19 lr: 0.005743 loss: 0.1301 (0.4041) epoch: 24 iter: 2/19 lr: 0.005733 loss: 0.2369 (0.4024) epoch: 24 iter: 3/19 lr: 0.005723 loss: 0.2815 (0.4012) epoch: 24 iter: 4/19 lr: 0.005713 loss: 0.1237 (0.3985) epoch: 24 iter: 5/19 lr: 0.005703 loss: 0.3606 (0.3981) epoch: 24 iter: 6/19 lr: 0.005693 loss: 0.2945 (0.3970) epoch: 24 iter: 7/19 lr: 0.005683 loss: 0.4066 (0.3971) epoch: 24 iter: 8/19 lr: 0.005673 loss: 0.0866 (0.3940) epoch: 24 iter: 9/19 lr: 0.005663 loss: 0.1648 (0.3917) epoch: 24 iter: 10/19 lr: 0.005652 loss: 0.1122 (0.3889) epoch: 24 iter: 11/19 lr: 0.005642 loss: 0.4662 (0.3897) epoch: 24 iter: 12/19 lr: 0.005632 loss: 0.2030 (0.3879) epoch: 24 iter: 13/19 lr: 0.005622 loss: 0.2023 (0.3860) epoch: 24 iter: 14/19 lr: 0.005612 loss: 0.0911 (0.3830) epoch: 24 iter: 15/19 lr: 0.005602 loss: 0.1594 (0.3808) epoch: 24 iter: 16/19 lr: 0.005592 loss: 0.0960 (0.3780) epoch: 24 iter: 17/19 lr: 0.005582 loss: 0.1000 (0.3752) epoch: 24 iter: 18/19 lr: 0.005572 loss: 0.3208 (0.3746) epoch: 24 iter: 19/19 lr: 0.005562 loss: 0.1004 (0.3719) epoch: 25 iter: 1/19 lr: 0.005551 loss: 0.2180 (0.3704) epoch: 25 iter: 2/19 lr: 0.005541 loss: 0.0837 (0.3675) epoch: 25 iter: 3/19 lr: 0.005531 loss: 0.2339 (0.3662) epoch: 25 iter: 4/19 lr: 0.005521 loss: 0.1556 (0.3641) epoch: 25 iter: 5/19 lr: 0.005511 loss: 0.4231 (0.3646) epoch: 25 iter: 6/19 lr: 0.005501 loss: 0.1705 (0.3627) epoch: 25 iter: 7/19 lr: 0.005491 loss: 0.0856 (0.3599) epoch: 25 iter: 8/19 lr: 0.005481 loss: 0.2002 (0.3583) epoch: 25 iter: 9/19 lr: 0.005470 loss: 0.0468 (0.3552) epoch: 25 iter: 10/19 lr: 0.005460 loss: 0.0858 (0.3525) epoch: 25 iter: 11/19 lr: 0.005450 loss: 0.1305 (0.3503) epoch: 25 iter: 12/19 lr: 0.005440 loss: 0.1411 (0.3482) epoch: 25 iter: 13/19 lr: 0.005430 loss: 0.1340 (0.3461) epoch: 25 iter: 14/19 lr: 0.005420 loss: 0.2004 (0.3446) epoch: 25 iter: 15/19 lr: 0.005410 loss: 0.2732 (0.3439) epoch: 25 iter: 16/19 lr: 0.005399 loss: 0.1853 (0.3423) epoch: 25 iter: 17/19 lr: 0.005389 loss: 0.1338 (0.3402) epoch: 25 iter: 18/19 lr: 0.005379 loss: 0.1191 (0.3380) epoch: 25 iter: 19/19 lr: 0.005369 loss: 0.3440 (0.3381) epoch: 26 iter: 1/19 lr: 0.005359 loss: 0.1845 (0.3365) epoch: 26 iter: 2/19 lr: 0.005349 loss: 0.1036 (0.3342) epoch: 26 iter: 3/19 lr: 0.005339 loss: 0.0961 (0.3318) epoch: 26 iter: 4/19 lr: 0.005328 loss: 0.8897 (0.3374) epoch: 26 iter: 5/19 lr: 0.005318 loss: 0.2587 (0.3366) epoch: 26 iter: 6/19 lr: 0.005308 loss: 0.1175 (0.3344) epoch: 26 iter: 7/19 lr: 0.005298 loss: 0.0841 (0.3319) epoch: 26 iter: 8/19 lr: 0.005288 loss: 0.0972 (0.3296) epoch: 26 iter: 9/19 lr: 0.005278 loss: 0.2515 (0.3288) epoch: 26 iter: 10/19 lr: 0.005267 loss: 0.3779 (0.3293) epoch: 26 iter: 11/19 lr: 0.005257 loss: 0.1952 (0.3279) epoch: 26 iter: 12/19 lr: 0.005247 loss: 0.1454 (0.3261) epoch: 26 iter: 13/19 lr: 0.005237 loss: 0.2270 (0.3251) epoch: 26 iter: 14/19 lr: 0.005227 loss: 0.2132 (0.3240) epoch: 26 iter: 15/19 lr: 0.005217 loss: 0.1666 (0.3224) epoch: 26 iter: 16/19 lr: 0.005206 loss: 0.0783 (0.3200) epoch: 26 iter: 17/19 lr: 0.005196 loss: 0.1908 (0.3187) epoch: 26 iter: 18/19 lr: 0.005186 loss: 0.1568 (0.3171) epoch: 26 iter: 19/19 lr: 0.005176 loss: 0.1531 (0.3154) epoch: 27 iter: 1/19 lr: 0.005166 loss: 0.1465 (0.3138) epoch: 27 iter: 2/19 lr: 0.005155 loss: 0.2607 (0.3132) epoch: 27 iter: 3/19 lr: 0.005145 loss: 0.1088 (0.3112) epoch: 27 iter: 4/19 lr: 0.005135 loss: 0.1051 (0.3091) epoch: 27 iter: 5/19 lr: 0.005125 loss: 0.2200 (0.3082) epoch: 27 iter: 6/19 lr: 0.005115 loss: 0.1878 (0.3070) epoch: 27 iter: 7/19 lr: 0.005104 loss: 0.1118 (0.3051) epoch: 27 iter: 8/19 lr: 0.005094 loss: 0.1060 (0.3031) epoch: 27 iter: 9/19 lr: 0.005084 loss: 0.1203 (0.3013) epoch: 27 iter: 10/19 lr: 0.005074 loss: 0.1187 (0.2994) epoch: 27 iter: 11/19 lr: 0.005063 loss: 0.1423 (0.2979) epoch: 27 iter: 12/19 lr: 0.005053 loss: 0.1646 (0.2965) epoch: 27 iter: 13/19 lr: 0.005043 loss: 0.2409 (0.2960) epoch: 27 iter: 14/19 lr: 0.005033 loss: 0.1536 (0.2945) epoch: 27 iter: 15/19 lr: 0.005023 loss: 0.2234 (0.2938) epoch: 27 iter: 16/19 lr: 0.005012 loss: 0.0791 (0.2917) epoch: 27 iter: 17/19 lr: 0.005002 loss: 0.2717 (0.2915) epoch: 27 iter: 18/19 lr: 0.004992 loss: 0.0991 (0.2896) epoch: 27 iter: 19/19 lr: 0.004982 loss: 0.0836 (0.2875) epoch: 28 iter: 1/19 lr: 0.004971 loss: 0.1395 (0.2860) epoch: 28 iter: 2/19 lr: 0.004961 loss: 0.1479 (0.2846) epoch: 28 iter: 3/19 lr: 0.004951 loss: 0.1530 (0.2833) epoch: 28 iter: 4/19 lr: 0.004941 loss: 0.0819 (0.2813) epoch: 28 iter: 5/19 lr: 0.004930 loss: 0.1830 (0.2803) epoch: 28 iter: 6/19 lr: 0.004920 loss: 0.0930 (0.2785) epoch: 28 iter: 7/19 lr: 0.004910 loss: 0.2901 (0.2786) epoch: 28 iter: 8/19 lr: 0.004900 loss: 0.1469 (0.2773) epoch: 28 iter: 9/19 lr: 0.004889 loss: 0.4389 (0.2789) epoch: 28 iter: 10/19 lr: 0.004879 loss: 0.0488 (0.2766) epoch: 28 iter: 11/19 lr: 0.004869 loss: 0.1482 (0.2753) epoch: 28 iter: 12/19 lr: 0.004859 loss: 0.0811 (0.2733) epoch: 28 iter: 13/19 lr: 0.004848 loss: 0.4289 (0.2749) epoch: 28 iter: 14/19 lr: 0.004838 loss: 0.3659 (0.2758) epoch: 28 iter: 15/19 lr: 0.004828 loss: 0.1079 (0.2741) epoch: 28 iter: 16/19 lr: 0.004818 loss: 0.1874 (0.2733) epoch: 28 iter: 17/19 lr: 0.004807 loss: 0.0771 (0.2713) epoch: 28 iter: 18/19 lr: 0.004797 loss: 0.2267 (0.2709) epoch: 28 iter: 19/19 lr: 0.004787 loss: 0.2452 (0.2706) epoch: 29 iter: 1/19 lr: 0.004776 loss: 0.2782 (0.2707) epoch: 29 iter: 2/19 lr: 0.004766 loss: 0.2847 (0.2708) epoch: 29 iter: 3/19 lr: 0.004756 loss: 0.1832 (0.2699) epoch: 29 iter: 4/19 lr: 0.004746 loss: 0.2219 (0.2695) epoch: 29 iter: 5/19 lr: 0.004735 loss: 0.3877 (0.2706) epoch: 29 iter: 6/19 lr: 0.004725 loss: 0.0551 (0.2685) epoch: 29 iter: 7/19 lr: 0.004715 loss: 0.0958 (0.2668) epoch: 29 iter: 8/19 lr: 0.004704 loss: 0.2740 (0.2668) epoch: 29 iter: 9/19 lr: 0.004694 loss: 0.1210 (0.2654) epoch: 29 iter: 10/19 lr: 0.004684 loss: 0.1514 (0.2642) epoch: 29 iter: 11/19 lr: 0.004674 loss: 0.1493 (0.2631) epoch: 29 iter: 12/19 lr: 0.004663 loss: 0.0752 (0.2612) epoch: 29 iter: 13/19 lr: 0.004653 loss: 0.2357 (0.2609) epoch: 29 iter: 14/19 lr: 0.004643 loss: 0.2539 (0.2609) epoch: 29 iter: 15/19 lr: 0.004632 loss: 0.2115 (0.2604) epoch: 29 iter: 16/19 lr: 0.004622 loss: 0.0887 (0.2587) epoch: 29 iter: 17/19 lr: 0.004612 loss: 0.0954 (0.2570) epoch: 29 iter: 18/19 lr: 0.004601 loss: 0.1215 (0.2557) epoch: 29 iter: 19/19 lr: 0.004591 loss: 0.1794 (0.2549) epoch: 30 iter: 1/19 lr: 0.004581 loss: 0.1354 (0.2537) epoch: 30 iter: 2/19 lr: 0.004570 loss: 0.1255 (0.2524) epoch: 30 iter: 3/19 lr: 0.004560 loss: 0.0519 (0.2504) epoch: 30 iter: 4/19 lr: 0.004550 loss: 0.2872 (0.2508) epoch: 30 iter: 5/19 lr: 0.004539 loss: 0.0342 (0.2486) epoch: 30 iter: 6/19 lr: 0.004529 loss: 0.0775 (0.2469) epoch: 30 iter: 7/19 lr: 0.004519 loss: 0.3194 (0.2476) epoch: 30 iter: 8/19 lr: 0.004508 loss: 0.0724 (0.2459) epoch: 30 iter: 9/19 lr: 0.004498 loss: 0.1529 (0.2450) epoch: 30 iter: 10/19 lr: 0.004488 loss: 0.2352 (0.2449) epoch: 30 iter: 11/19 lr: 0.004477 loss: 0.0859 (0.2433) epoch: 30 iter: 12/19 lr: 0.004467 loss: 0.2009 (0.2429) epoch: 30 iter: 13/19 lr: 0.004456 loss: 0.0752 (0.2412) epoch: 30 iter: 14/19 lr: 0.004446 loss: 0.1131 (0.2399) epoch: 30 iter: 15/19 lr: 0.004436 loss: 0.5097 (0.2426) epoch: 30 iter: 16/19 lr: 0.004425 loss: 0.1116 (0.2413) epoch: 30 iter: 17/19 lr: 0.004415 loss: 0.1904 (0.2408) epoch: 30 iter: 18/19 lr: 0.004405 loss: 0.2566 (0.2409) epoch: 30 iter: 19/19 lr: 0.004394 loss: 0.1146 (0.2397) epoch: 31 iter: 1/19 lr: 0.004384 loss: 0.2562 (0.2398) epoch: 31 iter: 2/19 lr: 0.004373 loss: 0.0826 (0.2383) epoch: 31 iter: 3/19 lr: 0.004363 loss: 0.2227 (0.2381) epoch: 31 iter: 4/19 lr: 0.004353 loss: 0.3684 (0.2394) epoch: 31 iter: 5/19 lr: 0.004342 loss: 0.0877 (0.2379) epoch: 31 iter: 6/19 lr: 0.004332 loss: 0.2810 (0.2383) epoch: 31 iter: 7/19 lr: 0.004321 loss: 0.1131 (0.2371) epoch: 31 iter: 8/19 lr: 0.004311 loss: 0.3746 (0.2384) epoch: 31 iter: 9/19 lr: 0.004301 loss: 0.1675 (0.2377) epoch: 31 iter: 10/19 lr: 0.004290 loss: 0.0889 (0.2363) epoch: 31 iter: 11/19 lr: 0.004280 loss: 0.0777 (0.2347) epoch: 31 iter: 12/19 lr: 0.004269 loss: 0.0688 (0.2330) epoch: 31 iter: 13/19 lr: 0.004259 loss: 0.0758 (0.2314) epoch: 31 iter: 14/19 lr: 0.004249 loss: 0.1173 (0.2303) epoch: 31 iter: 15/19 lr: 0.004238 loss: 0.2272 (0.2303) epoch: 31 iter: 16/19 lr: 0.004228 loss: 0.1006 (0.2290) epoch: 31 iter: 17/19 lr: 0.004217 loss: 0.2885 (0.2296) epoch: 31 iter: 18/19 lr: 0.004207 loss: 0.1449 (0.2287) epoch: 31 iter: 19/19 lr: 0.004196 loss: 0.1618 (0.2280) epoch: 32 iter: 1/19 lr: 0.004186 loss: 0.1362 (0.2271) epoch: 32 iter: 2/19 lr: 0.004176 loss: 0.1805 (0.2267) epoch: 32 iter: 3/19 lr: 0.004165 loss: 0.2241 (0.2266) epoch: 32 iter: 4/19 lr: 0.004155 loss: 0.1656 (0.2260) epoch: 32 iter: 5/19 lr: 0.004144 loss: 0.1306 (0.2251) epoch: 32 iter: 6/19 lr: 0.004134 loss: 0.1465 (0.2243) epoch: 32 iter: 7/19 lr: 0.004123 loss: 0.1942 (0.2240) epoch: 32 iter: 8/19 lr: 0.004113 loss: 0.1670 (0.2234) epoch: 32 iter: 9/19 lr: 0.004102 loss: 0.4822 (0.2260) epoch: 32 iter: 10/19 lr: 0.004092 loss: 0.5788 (0.2295) epoch: 32 iter: 11/19 lr: 0.004082 loss: 0.2517 (0.2298) epoch: 32 iter: 12/19 lr: 0.004071 loss: 0.1397 (0.2289) epoch: 32 iter: 13/19 lr: 0.004061 loss: 0.1602 (0.2282) epoch: 32 iter: 14/19 lr: 0.004050 loss: 0.2939 (0.2288) epoch: 32 iter: 15/19 lr: 0.004040 loss: 0.1150 (0.2277) epoch: 32 iter: 16/19 lr: 0.004029 loss: 0.1648 (0.2271) epoch: 32 iter: 17/19 lr: 0.004019 loss: 0.0759 (0.2255) epoch: 32 iter: 18/19 lr: 0.004008 loss: 0.0641 (0.2239) epoch: 32 iter: 19/19 lr: 0.003998 loss: 0.0671 (0.2224) epoch: 33 iter: 1/19 lr: 0.003987 loss: 0.1869 (0.2220) epoch: 33 iter: 2/19 lr: 0.003977 loss: 0.1216 (0.2210) epoch: 33 iter: 3/19 lr: 0.003966 loss: 0.1448 (0.2202) epoch: 33 iter: 4/19 lr: 0.003956 loss: 0.0831 (0.2189) epoch: 33 iter: 5/19 lr: 0.003945 loss: 0.1670 (0.2184) epoch: 33 iter: 6/19 lr: 0.003935 loss: 0.1876 (0.2180) epoch: 33 iter: 7/19 lr: 0.003924 loss: 0.2276 (0.2181) epoch: 33 iter: 8/19 lr: 0.003914 loss: 0.2990 (0.2189) epoch: 33 iter: 9/19 lr: 0.003903 loss: 0.5942 (0.2227) epoch: 33 iter: 10/19 lr: 0.003893 loss: 0.5877 (0.2264) epoch: 33 iter: 11/19 lr: 0.003882 loss: 0.1262 (0.2254) epoch: 33 iter: 12/19 lr: 0.003872 loss: 0.1644 (0.2247) epoch: 33 iter: 13/19 lr: 0.003861 loss: 0.1816 (0.2243) epoch: 33 iter: 14/19 lr: 0.003851 loss: 0.3508 (0.2256) epoch: 33 iter: 15/19 lr: 0.003840 loss: 0.2017 (0.2253) epoch: 33 iter: 16/19 lr: 0.003829 loss: 0.1461 (0.2245) epoch: 33 iter: 17/19 lr: 0.003819 loss: 0.1194 (0.2235) epoch: 33 iter: 18/19 lr: 0.003808 loss: 0.1160 (0.2224) epoch: 33 iter: 19/19 lr: 0.003798 loss: 0.0930 (0.2211) epoch: 34 iter: 1/19 lr: 0.003787 loss: 0.0660 (0.2196) epoch: 34 iter: 2/19 lr: 0.003777 loss: 0.1397 (0.2188) epoch: 34 iter: 3/19 lr: 0.003766 loss: 0.1694 (0.2183) epoch: 34 iter: 4/19 lr: 0.003756 loss: 0.0908 (0.2170) epoch: 34 iter: 5/19 lr: 0.003745 loss: 0.1923 (0.2168) epoch: 34 iter: 6/19 lr: 0.003735 loss: 0.0857 (0.2154) epoch: 34 iter: 7/19 lr: 0.003724 loss: 0.0914 (0.2142) epoch: 34 iter: 8/19 lr: 0.003713 loss: 0.0919 (0.2130) epoch: 34 iter: 9/19 lr: 0.003703 loss: 0.0671 (0.2115) epoch: 34 iter: 10/19 lr: 0.003692 loss: 0.0438 (0.2098) epoch: 34 iter: 11/19 lr: 0.003682 loss: 0.1188 (0.2089) epoch: 34 iter: 12/19 lr: 0.003671 loss: 0.1061 (0.2079) epoch: 34 iter: 13/19 lr: 0.003660 loss: 0.0759 (0.2066) epoch: 34 iter: 14/19 lr: 0.003650 loss: 0.0910 (0.2054) epoch: 34 iter: 15/19 lr: 0.003639 loss: 0.2127 (0.2055) epoch: 34 iter: 16/19 lr: 0.003629 loss: 0.1235 (0.2047) epoch: 34 iter: 17/19 lr: 0.003618 loss: 0.1650 (0.2043) epoch: 34 iter: 18/19 lr: 0.003607 loss: 0.3724 (0.2060) epoch: 34 iter: 19/19 lr: 0.003597 loss: 0.2937 (0.2068) epoch: 35 iter: 1/19 lr: 0.003586 loss: 0.2050 (0.2068) epoch: 35 iter: 2/19 lr: 0.003576 loss: 0.0977 (0.2057) epoch: 35 iter: 3/19 lr: 0.003565 loss: 0.3878 (0.2076) epoch: 35 iter: 4/19 lr: 0.003554 loss: 0.7629 (0.2131) epoch: 35 iter: 5/19 lr: 0.003544 loss: 0.0755 (0.2117) epoch: 35 iter: 6/19 lr: 0.003533 loss: 0.3709 (0.2133) epoch: 35 iter: 7/19 lr: 0.003522 loss: 0.1349 (0.2125) epoch: 35 iter: 8/19 lr: 0.003512 loss: 0.0966 (0.2114) epoch: 35 iter: 9/19 lr: 0.003501 loss: 0.1201 (0.2105) epoch: 35 iter: 10/19 lr: 0.003491 loss: 0.1465 (0.2098) epoch: 35 iter: 11/19 lr: 0.003480 loss: 0.1760 (0.2095) epoch: 35 iter: 12/19 lr: 0.003469 loss: 0.2627 (0.2100) epoch: 35 iter: 13/19 lr: 0.003459 loss: 0.3105 (0.2110) epoch: 35 iter: 14/19 lr: 0.003448 loss: 0.1384 (0.2103) epoch: 35 iter: 15/19 lr: 0.003437 loss: 0.1066 (0.2093) epoch: 35 iter: 16/19 lr: 0.003427 loss: 0.2258 (0.2094) epoch: 35 iter: 17/19 lr: 0.003416 loss: 0.0786 (0.2081) epoch: 35 iter: 18/19 lr: 0.003405 loss: 0.0579 (0.2066) epoch: 35 iter: 19/19 lr: 0.003395 loss: 0.5553 (0.2101) epoch: 36 iter: 1/19 lr: 0.003384 loss: 0.1121 (0.2091) epoch: 36 iter: 2/19 lr: 0.003373 loss: 0.0777 (0.2078) epoch: 36 iter: 3/19 lr: 0.003362 loss: 0.1797 (0.2075) epoch: 36 iter: 4/19 lr: 0.003352 loss: 0.1133 (0.2066) epoch: 36 iter: 5/19 lr: 0.003341 loss: 0.2444 (0.2070) epoch: 36 iter: 6/19 lr: 0.003330 loss: 0.2439 (0.2073) epoch: 36 iter: 7/19 lr: 0.003320 loss: 0.4231 (0.2095) epoch: 36 iter: 8/19 lr: 0.003309 loss: 0.1314 (0.2087) epoch: 36 iter: 9/19 lr: 0.003298 loss: 0.0719 (0.2073) epoch: 36 iter: 10/19 lr: 0.003288 loss: 0.2164 (0.2074) epoch: 36 iter: 11/19 lr: 0.003277 loss: 0.4099 (0.2095) epoch: 36 iter: 12/19 lr: 0.003266 loss: 0.2947 (0.2103) epoch: 36 iter: 13/19 lr: 0.003255 loss: 0.1078 (0.2093) epoch: 36 iter: 14/19 lr: 0.003245 loss: 0.0567 (0.2078) epoch: 36 iter: 15/19 lr: 0.003234 loss: 0.1709 (0.2074) epoch: 36 iter: 16/19 lr: 0.003223 loss: 0.1288 (0.2066) epoch: 36 iter: 17/19 lr: 0.003212 loss: 0.0949 (0.2055) epoch: 36 iter: 18/19 lr: 0.003202 loss: 0.2520 (0.2060) epoch: 36 iter: 19/19 lr: 0.003191 loss: 0.1299 (0.2052) epoch: 37 iter: 1/19 lr: 0.003180 loss: 0.0636 (0.2038) epoch: 37 iter: 2/19 lr: 0.003169 loss: 0.0623 (0.2024) epoch: 37 iter: 3/19 lr: 0.003159 loss: 0.0787 (0.2011) epoch: 37 iter: 4/19 lr: 0.003148 loss: 0.1514 (0.2006) epoch: 37 iter: 5/19 lr: 0.003137 loss: 0.1649 (0.2003) epoch: 37 iter: 6/19 lr: 0.003126 loss: 0.1446 (0.1997) epoch: 37 iter: 7/19 lr: 0.003115 loss: 0.1875 (0.1996) epoch: 37 iter: 8/19 lr: 0.003105 loss: 0.1329 (0.1989) epoch: 37 iter: 9/19 lr: 0.003094 loss: 0.1590 (0.1985) epoch: 37 iter: 10/19 lr: 0.003083 loss: 0.1201 (0.1977) epoch: 37 iter: 11/19 lr: 0.003072 loss: 0.2236 (0.1980) epoch: 37 iter: 12/19 lr: 0.003062 loss: 0.1614 (0.1976) epoch: 37 iter: 13/19 lr: 0.003051 loss: 0.0779 (0.1964) epoch: 37 iter: 14/19 lr: 0.003040 loss: 0.1868 (0.1963) epoch: 37 iter: 15/19 lr: 0.003029 loss: 0.1039 (0.1954) epoch: 37 iter: 16/19 lr: 0.003018 loss: 0.1447 (0.1949) epoch: 37 iter: 17/19 lr: 0.003007 loss: 0.0800 (0.1938) epoch: 37 iter: 18/19 lr: 0.002997 loss: 0.1938 (0.1938) epoch: 37 iter: 19/19 lr: 0.002986 loss: 0.0287 (0.1921) epoch: 38 iter: 1/19 lr: 0.002975 loss: 0.2776 (0.1930) epoch: 38 iter: 2/19 lr: 0.002964 loss: 0.1394 (0.1924) epoch: 38 iter: 3/19 lr: 0.002953 loss: 0.5736 (0.1962) epoch: 38 iter: 4/19 lr: 0.002942 loss: 0.1902 (0.1962) epoch: 38 iter: 5/19 lr: 0.002932 loss: 0.0841 (0.1951) epoch: 38 iter: 6/19 lr: 0.002921 loss: 0.1571 (0.1947) epoch: 38 iter: 7/19 lr: 0.002910 loss: 0.0959 (0.1937) epoch: 38 iter: 8/19 lr: 0.002899 loss: 0.0913 (0.1927) epoch: 38 iter: 9/19 lr: 0.002888 loss: 0.5516 (0.1963) epoch: 38 iter: 10/19 lr: 0.002877 loss: 0.1409 (0.1957) epoch: 38 iter: 11/19 lr: 0.002866 loss: 0.0812 (0.1946) epoch: 38 iter: 12/19 lr: 0.002855 loss: 0.1836 (0.1945) epoch: 38 iter: 13/19 lr: 0.002845 loss: 0.2031 (0.1945) epoch: 38 iter: 14/19 lr: 0.002834 loss: 0.0425 (0.1930) epoch: 38 iter: 15/19 lr: 0.002823 loss: 0.1009 (0.1921) epoch: 38 iter: 16/19 lr: 0.002812 loss: 0.0637 (0.1908) epoch: 38 iter: 17/19 lr: 0.002801 loss: 0.1813 (0.1907) epoch: 38 iter: 18/19 lr: 0.002790 loss: 0.1166 (0.1900) epoch: 38 iter: 19/19 lr: 0.002779 loss: 0.3648 (0.1917) epoch: 39 iter: 1/19 lr: 0.002768 loss: 0.0971 (0.1908) epoch: 39 iter: 2/19 lr: 0.002757 loss: 0.1294 (0.1902) epoch: 39 iter: 3/19 lr: 0.002746 loss: 0.2251 (0.1905) epoch: 39 iter: 4/19 lr: 0.002735 loss: 0.1428 (0.1900) epoch: 39 iter: 5/19 lr: 0.002724 loss: 0.1579 (0.1897) epoch: 39 iter: 6/19 lr: 0.002713 loss: 0.2820 (0.1906) epoch: 39 iter: 7/19 lr: 0.002703 loss: 0.2372 (0.1911) epoch: 39 iter: 8/19 lr: 0.002692 loss: 0.0850 (0.1900) epoch: 39 iter: 9/19 lr: 0.002681 loss: 0.2687 (0.1908) epoch: 39 iter: 10/19 lr: 0.002670 loss: 0.3212 (0.1921) epoch: 39 iter: 11/19 lr: 0.002659 loss: 0.1624 (0.1918) epoch: 39 iter: 12/19 lr: 0.002648 loss: 0.1733 (0.1917) epoch: 39 iter: 13/19 lr: 0.002637 loss: 0.1765 (0.1915) epoch: 39 iter: 14/19 lr: 0.002626 loss: 0.0536 (0.1901) epoch: 39 iter: 15/19 lr: 0.002615 loss: 0.2532 (0.1908) epoch: 39 iter: 16/19 lr: 0.002604 loss: 0.1602 (0.1904) epoch: 39 iter: 17/19 lr: 0.002593 loss: 0.1637 (0.1902) epoch: 39 iter: 18/19 lr: 0.002582 loss: 0.0724 (0.1890) epoch: 39 iter: 19/19 lr: 0.002571 loss: 0.0782 (0.1879) epoch: 40 iter: 1/19 lr: 0.002560 loss: 0.1114 (0.1871) epoch: 40 iter: 2/19 lr: 0.002549 loss: 0.2003 (0.1873) epoch: 40 iter: 3/19 lr: 0.002538 loss: 0.1177 (0.1866) epoch: 40 iter: 4/19 lr: 0.002527 loss: 0.1211 (0.1859) epoch: 40 iter: 5/19 lr: 0.002516 loss: 0.1031 (0.1851) epoch: 40 iter: 6/19 lr: 0.002504 loss: 0.2560 (0.1858) epoch: 40 iter: 7/19 lr: 0.002493 loss: 0.1035 (0.1850) epoch: 40 iter: 8/19 lr: 0.002482 loss: 0.0688 (0.1838) epoch: 40 iter: 9/19 lr: 0.002471 loss: 0.0709 (0.1827) epoch: 40 iter: 10/19 lr: 0.002460 loss: 0.1587 (0.1824) epoch: 40 iter: 11/19 lr: 0.002449 loss: 0.1230 (0.1818) epoch: 40 iter: 12/19 lr: 0.002438 loss: 0.0474 (0.1805) epoch: 40 iter: 13/19 lr: 0.002427 loss: 0.2488 (0.1812) epoch: 40 iter: 14/19 lr: 0.002416 loss: 0.1521 (0.1809) epoch: 40 iter: 15/19 lr: 0.002405 loss: 0.1279 (0.1804) epoch: 40 iter: 16/19 lr: 0.002394 loss: 0.2157 (0.1807) epoch: 40 iter: 17/19 lr: 0.002383 loss: 0.1139 (0.1800) epoch: 40 iter: 18/19 lr: 0.002371 loss: 0.3431 (0.1817) epoch: 40 iter: 19/19 lr: 0.002360 loss: 0.0941 (0.1808) epoch: 41 iter: 1/19 lr: 0.002349 loss: 0.6349 (0.1853) epoch: 41 iter: 2/19 lr: 0.002338 loss: 0.1122 (0.1846) epoch: 41 iter: 3/19 lr: 0.002327 loss: 0.2238 (0.1850) epoch: 41 iter: 4/19 lr: 0.002316 loss: 0.1137 (0.1843) epoch: 41 iter: 5/19 lr: 0.002305 loss: 0.2522 (0.1850) epoch: 41 iter: 6/19 lr: 0.002294 loss: 0.2727 (0.1858) epoch: 41 iter: 7/19 lr: 0.002282 loss: 0.1571 (0.1856) epoch: 41 iter: 8/19 lr: 0.002271 loss: 0.1153 (0.1849) epoch: 41 iter: 9/19 lr: 0.002260 loss: 0.0935 (0.1839) epoch: 41 iter: 10/19 lr: 0.002249 loss: 0.2301 (0.1844) epoch: 41 iter: 11/19 lr: 0.002238 loss: 0.0825 (0.1834) epoch: 41 iter: 12/19 lr: 0.002226 loss: 0.1234 (0.1828) epoch: 41 iter: 13/19 lr: 0.002215 loss: 0.1819 (0.1828) epoch: 41 iter: 14/19 lr: 0.002204 loss: 0.2593 (0.1835) epoch: 41 iter: 15/19 lr: 0.002193 loss: 0.1722 (0.1834) epoch: 41 iter: 16/19 lr: 0.002182 loss: 0.1740 (0.1833) epoch: 41 iter: 17/19 lr: 0.002170 loss: 0.2112 (0.1836) epoch: 41 iter: 18/19 lr: 0.002159 loss: 0.1235 (0.1830) epoch: 41 iter: 19/19 lr: 0.002148 loss: 0.2113 (0.1833) epoch: 42 iter: 1/19 lr: 0.002137 loss: 0.2715 (0.1842) epoch: 42 iter: 2/19 lr: 0.002125 loss: 0.0823 (0.1832) epoch: 42 iter: 3/19 lr: 0.002114 loss: 0.0700 (0.1820) epoch: 42 iter: 4/19 lr: 0.002103 loss: 0.1175 (0.1814) epoch: 42 iter: 5/19 lr: 0.002092 loss: 0.1572 (0.1811) epoch: 42 iter: 6/19 lr: 0.002080 loss: 0.0843 (0.1802) epoch: 42 iter: 7/19 lr: 0.002069 loss: 0.1373 (0.1797) epoch: 42 iter: 8/19 lr: 0.002058 loss: 0.0538 (0.1785) epoch: 42 iter: 9/19 lr: 0.002047 loss: 0.1377 (0.1781) epoch: 42 iter: 10/19 lr: 0.002035 loss: 0.1156 (0.1774) epoch: 42 iter: 11/19 lr: 0.002024 loss: 0.0718 (0.1764) epoch: 42 iter: 12/19 lr: 0.002013 loss: 0.1373 (0.1760) epoch: 42 iter: 13/19 lr: 0.002001 loss: 0.2008 (0.1762) epoch: 42 iter: 14/19 lr: 0.001990 loss: 0.2519 (0.1770) epoch: 42 iter: 15/19 lr: 0.001979 loss: 0.4685 (0.1799) epoch: 42 iter: 16/19 lr: 0.001967 loss: 0.1093 (0.1792) epoch: 42 iter: 17/19 lr: 0.001956 loss: 0.1786 (0.1792) epoch: 42 iter: 18/19 lr: 0.001945 loss: 0.1953 (0.1794) epoch: 42 iter: 19/19 lr: 0.001933 loss: 0.1513 (0.1791) epoch: 43 iter: 1/19 lr: 0.001922 loss: 0.1660 (0.1790) epoch: 43 iter: 2/19 lr: 0.001910 loss: 0.1665 (0.1788) epoch: 43 iter: 3/19 lr: 0.001899 loss: 0.2822 (0.1799) epoch: 43 iter: 4/19 lr: 0.001888 loss: 0.0420 (0.1785) epoch: 43 iter: 5/19 lr: 0.001876 loss: 0.2954 (0.1797) epoch: 43 iter: 6/19 lr: 0.001865 loss: 0.1324 (0.1792) epoch: 43 iter: 7/19 lr: 0.001853 loss: 0.1085 (0.1785) epoch: 43 iter: 8/19 lr: 0.001842 loss: 0.0929 (0.1776) epoch: 43 iter: 9/19 lr: 0.001831 loss: 0.1524 (0.1774) epoch: 43 iter: 10/19 lr: 0.001819 loss: 0.0852 (0.1764) epoch: 43 iter: 11/19 lr: 0.001808 loss: 0.2376 (0.1771) epoch: 43 iter: 12/19 lr: 0.001796 loss: 0.0814 (0.1761) epoch: 43 iter: 13/19 lr: 0.001785 loss: 0.4410 (0.1788) epoch: 43 iter: 14/19 lr: 0.001773 loss: 0.1376 (0.1783) epoch: 43 iter: 15/19 lr: 0.001762 loss: 0.1275 (0.1778) epoch: 43 iter: 16/19 lr: 0.001750 loss: 0.1034 (0.1771) epoch: 43 iter: 17/19 lr: 0.001739 loss: 0.0966 (0.1763) epoch: 43 iter: 18/19 lr: 0.001727 loss: 0.1175 (0.1757) epoch: 43 iter: 19/19 lr: 0.001716 loss: 0.2130 (0.1761) epoch: 44 iter: 1/19 lr: 0.001704 loss: 0.2266 (0.1766) epoch: 44 iter: 2/19 lr: 0.001693 loss: 0.1721 (0.1765) epoch: 44 iter: 3/19 lr: 0.001681 loss: 0.1009 (0.1758) epoch: 44 iter: 4/19 lr: 0.001670 loss: 0.1302 (0.1753) epoch: 44 iter: 5/19 lr: 0.001658 loss: 0.0731 (0.1743) epoch: 44 iter: 6/19 lr: 0.001646 loss: 0.2391 (0.1749) epoch: 44 iter: 7/19 lr: 0.001635 loss: 0.2204 (0.1754) epoch: 44 iter: 8/19 lr: 0.001623 loss: 0.0562 (0.1742) epoch: 44 iter: 9/19 lr: 0.001612 loss: 0.1235 (0.1737) epoch: 44 iter: 10/19 lr: 0.001600 loss: 0.2241 (0.1742) epoch: 44 iter: 11/19 lr: 0.001588 loss: 0.2676 (0.1751) epoch: 44 iter: 12/19 lr: 0.001577 loss: 0.1153 (0.1745) epoch: 44 iter: 13/19 lr: 0.001565 loss: 0.1634 (0.1744) epoch: 44 iter: 14/19 lr: 0.001554 loss: 0.1892 (0.1746) epoch: 44 iter: 15/19 lr: 0.001542 loss: 0.2315 (0.1751) epoch: 44 iter: 16/19 lr: 0.001530 loss: 0.1751 (0.1751) epoch: 44 iter: 17/19 lr: 0.001519 loss: 0.1841 (0.1752) epoch: 44 iter: 18/19 lr: 0.001507 loss: 0.1490 (0.1750) epoch: 44 iter: 19/19 lr: 0.001495 loss: 0.3812 (0.1770) epoch: 45 iter: 1/19 lr: 0.001483 loss: 0.1878 (0.1771) epoch: 45 iter: 2/19 lr: 0.001472 loss: 0.4774 (0.1801) epoch: 45 iter: 3/19 lr: 0.001460 loss: 0.3009 (0.1814) epoch: 45 iter: 4/19 lr: 0.001448 loss: 0.1467 (0.1810) epoch: 45 iter: 5/19 lr: 0.001436 loss: 0.0828 (0.1800) epoch: 45 iter: 6/19 lr: 0.001425 loss: 0.1585 (0.1798) epoch: 45 iter: 7/19 lr: 0.001413 loss: 0.1244 (0.1793) epoch: 45 iter: 8/19 lr: 0.001401 loss: 0.0747 (0.1782) epoch: 45 iter: 9/19 lr: 0.001389 loss: 0.1410 (0.1778) epoch: 45 iter: 10/19 lr: 0.001378 loss: 0.1624 (0.1777) epoch: 45 iter: 11/19 lr: 0.001366 loss: 0.0636 (0.1765) epoch: 45 iter: 12/19 lr: 0.001354 loss: 0.1948 (0.1767) epoch: 45 iter: 13/19 lr: 0.001342 loss: 0.2037 (0.1770) epoch: 45 iter: 14/19 lr: 0.001330 loss: 0.2031 (0.1773) epoch: 45 iter: 15/19 lr: 0.001318 loss: 0.1236 (0.1767) epoch: 45 iter: 16/19 lr: 0.001307 loss: 0.3351 (0.1783) epoch: 45 iter: 17/19 lr: 0.001295 loss: 0.1880 (0.1784) epoch: 45 iter: 18/19 lr: 0.001283 loss: 0.0844 (0.1775) epoch: 45 iter: 19/19 lr: 0.001271 loss: 0.0873 (0.1766) epoch: 46 iter: 1/19 lr: 0.001259 loss: 0.2024 (0.1768) epoch: 46 iter: 2/19 lr: 0.001247 loss: 0.3192 (0.1782) epoch: 46 iter: 3/19 lr: 0.001235 loss: 0.1181 (0.1776) epoch: 46 iter: 4/19 lr: 0.001223 loss: 0.1291 (0.1772) epoch: 46 iter: 5/19 lr: 0.001211 loss: 0.3033 (0.1784) epoch: 46 iter: 6/19 lr: 0.001199 loss: 0.1008 (0.1776) epoch: 46 iter: 7/19 lr: 0.001187 loss: 0.3095 (0.1790) epoch: 46 iter: 8/19 lr: 0.001175 loss: 0.0578 (0.1777) epoch: 46 iter: 9/19 lr: 0.001163 loss: 0.2200 (0.1782) epoch: 46 iter: 10/19 lr: 0.001151 loss: 0.0823 (0.1772) epoch: 46 iter: 11/19 lr: 0.001139 loss: 0.3515 (0.1789) epoch: 46 iter: 12/19 lr: 0.001127 loss: 0.1263 (0.1784) epoch: 46 iter: 13/19 lr: 0.001115 loss: 0.1243 (0.1779) epoch: 46 iter: 14/19 lr: 0.001103 loss: 0.1631 (0.1777) epoch: 46 iter: 15/19 lr: 0.001091 loss: 0.0801 (0.1768) epoch: 46 iter: 16/19 lr: 0.001079 loss: 0.1973 (0.1770) epoch: 46 iter: 17/19 lr: 0.001066 loss: 0.1662 (0.1769) epoch: 46 iter: 18/19 lr: 0.001054 loss: 0.2754 (0.1778) epoch: 46 iter: 19/19 lr: 0.001042 loss: 0.1838 (0.1779) epoch: 47 iter: 1/19 lr: 0.001030 loss: 0.0894 (0.1770) epoch: 47 iter: 2/19 lr: 0.001018 loss: 0.1498 (0.1767) epoch: 47 iter: 3/19 lr: 0.001005 loss: 0.1738 (0.1767) epoch: 47 iter: 4/19 lr: 0.000993 loss: 0.2496 (0.1774) epoch: 47 iter: 5/19 lr: 0.000981 loss: 0.1220 (0.1769) epoch: 47 iter: 6/19 lr: 0.000969 loss: 0.1314 (0.1764) epoch: 47 iter: 7/19 lr: 0.000956 loss: 0.1702 (0.1764) epoch: 47 iter: 8/19 lr: 0.000944 loss: 0.1153 (0.1758) epoch: 47 iter: 9/19 lr: 0.000932 loss: 0.1816 (0.1758) epoch: 47 iter: 10/19 lr: 0.000919 loss: 0.0712 (0.1748) epoch: 47 iter: 11/19 lr: 0.000907 loss: 0.0754 (0.1738) epoch: 47 iter: 12/19 lr: 0.000895 loss: 0.2786 (0.1748) epoch: 47 iter: 13/19 lr: 0.000882 loss: 0.2232 (0.1753) epoch: 47 iter: 14/19 lr: 0.000870 loss: 0.1140 (0.1747) epoch: 47 iter: 15/19 lr: 0.000857 loss: 0.1622 (0.1746) epoch: 47 iter: 16/19 lr: 0.000845 loss: 0.1704 (0.1745) epoch: 47 iter: 17/19 lr: 0.000833 loss: 0.8035 (0.1808) epoch: 47 iter: 18/19 lr: 0.000820 loss: 0.2174 (0.1812) epoch: 47 iter: 19/19 lr: 0.000807 loss: 0.1743 (0.1811) epoch: 48 iter: 1/19 lr: 0.000795 loss: 0.1639 (0.1809) epoch: 48 iter: 2/19 lr: 0.000782 loss: 0.2193 (0.1813) epoch: 48 iter: 3/19 lr: 0.000770 loss: 0.1220 (0.1807) epoch: 48 iter: 4/19 lr: 0.000757 loss: 0.1802 (0.1807) epoch: 48 iter: 5/19 lr: 0.000745 loss: 0.1416 (0.1803) epoch: 48 iter: 6/19 lr: 0.000732 loss: 0.0650 (0.1792) epoch: 48 iter: 7/19 lr: 0.000719 loss: 0.1184 (0.1786) epoch: 48 iter: 8/19 lr: 0.000707 loss: 0.1257 (0.1780) epoch: 48 iter: 9/19 lr: 0.000694 loss: 0.1136 (0.1774) epoch: 48 iter: 10/19 lr: 0.000681 loss: 0.1840 (0.1775) epoch: 48 iter: 11/19 lr: 0.000668 loss: 0.2919 (0.1786) epoch: 48 iter: 12/19 lr: 0.000655 loss: 0.0714 (0.1775) epoch: 48 iter: 13/19 lr: 0.000643 loss: 0.1472 (0.1772) epoch: 48 iter: 14/19 lr: 0.000630 loss: 0.2380 (0.1778) epoch: 48 iter: 15/19 lr: 0.000617 loss: 0.2421 (0.1785) epoch: 48 iter: 16/19 lr: 0.000604 loss: 0.4226 (0.1809) epoch: 48 iter: 17/19 lr: 0.000591 loss: 0.0677 (0.1798) epoch: 48 iter: 18/19 lr: 0.000578 loss: 0.0849 (0.1788) epoch: 48 iter: 19/19 lr: 0.000565 loss: 0.0676 (0.1777) epoch: 49 iter: 1/19 lr: 0.000552 loss: 0.0842 (0.1768) epoch: 49 iter: 2/19 lr: 0.000539 loss: 0.2245 (0.1773) epoch: 49 iter: 3/19 lr: 0.000526 loss: 0.1868 (0.1774) epoch: 49 iter: 4/19 lr: 0.000513 loss: 0.1249 (0.1768) epoch: 49 iter: 5/19 lr: 0.000499 loss: 0.1465 (0.1765) epoch: 49 iter: 6/19 lr: 0.000486 loss: 0.0992 (0.1758) epoch: 49 iter: 7/19 lr: 0.000473 loss: 0.1373 (0.1754) epoch: 49 iter: 8/19 lr: 0.000459 loss: 0.0742 (0.1744) epoch: 49 iter: 9/19 lr: 0.000446 loss: 0.0755 (0.1734) epoch: 49 iter: 10/19 lr: 0.000433 loss: 0.1908 (0.1736) epoch: 49 iter: 11/19 lr: 0.000419 loss: 0.1623 (0.1734) epoch: 49 iter: 12/19 lr: 0.000406 loss: 0.1356 (0.1731) epoch: 49 iter: 13/19 lr: 0.000392 loss: 0.2628 (0.1740) epoch: 49 iter: 14/19 lr: 0.000379 loss: 0.1347 (0.1736) epoch: 49 iter: 15/19 lr: 0.000365 loss: 0.0978 (0.1728) epoch: 49 iter: 16/19 lr: 0.000351 loss: 0.1270 (0.1724) epoch: 49 iter: 17/19 lr: 0.000337 loss: 0.1710 (0.1723) epoch: 49 iter: 18/19 lr: 0.000324 loss: 0.1072 (0.1717) epoch: 49 iter: 19/19 lr: 0.000310 loss: 0.0290 (0.1703) epoch: 50 iter: 1/19 lr: 0.000296 loss: 0.0973 (0.1695) epoch: 50 iter: 2/19 lr: 0.000282 loss: 0.0835 (0.1687) epoch: 50 iter: 3/19 lr: 0.000268 loss: 0.1358 (0.1683) epoch: 50 iter: 4/19 lr: 0.000253 loss: 0.1795 (0.1685) epoch: 50 iter: 5/19 lr: 0.000239 loss: 0.0752 (0.1675) epoch: 50 iter: 6/19 lr: 0.000225 loss: 0.1452 (0.1673) epoch: 50 iter: 7/19 lr: 0.000210 loss: 0.0959 (0.1666) epoch: 50 iter: 8/19 lr: 0.000196 loss: 0.1525 (0.1664) epoch: 50 iter: 9/19 lr: 0.000181 loss: 0.2731 (0.1675) epoch: 50 iter: 10/19 lr: 0.000166 loss: 0.1323 (0.1672) epoch: 50 iter: 11/19 lr: 0.000151 loss: 0.2021 (0.1675) epoch: 50 iter: 12/19 lr: 0.000136 loss: 0.1027 (0.1669) epoch: 50 iter: 13/19 lr: 0.000120 loss: 0.1223 (0.1664) epoch: 50 iter: 14/19 lr: 0.000105 loss: 0.0660 (0.1654) epoch: 50 iter: 15/19 lr: 0.000089 loss: 0.1154 (0.1649) epoch: 50 iter: 16/19 lr: 0.000073 loss: 0.0772 (0.1640) epoch: 50 iter: 17/19 lr: 0.000056 loss: 0.2633 (0.1650) epoch: 50 iter: 18/19 lr: 0.000039 loss: 0.1368 (0.1647) epoch: 50 iter: 19/19 lr: 0.000021 loss: 0.0579 (0.1637) . Checking . !pwd . /content/DeepLabv3.pytorch . !cp -r /content/DeepLabv3.pytorch/data/archive/images /content/DeepLabv3.pytorch/data/val . !python main.py --dataset covid_mask --exp bn_lr7e-3 --epochs 1 --base_lr 0.007 --batch_size 12 . eval: 1/222 eval: 2/222 eval: 3/222 eval: 4/222 eval: 5/222 eval: 6/222 eval: 7/222 eval: 8/222 eval: 9/222 eval: 10/222 eval: 11/222 eval: 12/222 eval: 13/222 eval: 14/222 eval: 15/222 eval: 16/222 eval: 17/222 eval: 18/222 eval: 19/222 eval: 20/222 eval: 21/222 eval: 22/222 eval: 23/222 eval: 24/222 eval: 25/222 eval: 26/222 eval: 27/222 eval: 28/222 eval: 29/222 eval: 30/222 eval: 31/222 eval: 32/222 eval: 33/222 eval: 34/222 eval: 35/222 eval: 36/222 eval: 37/222 eval: 38/222 eval: 39/222 eval: 40/222 eval: 41/222 eval: 42/222 eval: 43/222 eval: 44/222 eval: 45/222 eval: 46/222 eval: 47/222 eval: 48/222 eval: 49/222 eval: 50/222 eval: 51/222 eval: 52/222 eval: 53/222 eval: 54/222 eval: 55/222 eval: 56/222 eval: 57/222 eval: 58/222 eval: 59/222 eval: 60/222 eval: 61/222 eval: 62/222 eval: 63/222 eval: 64/222 eval: 65/222 eval: 66/222 eval: 67/222 eval: 68/222 eval: 69/222 eval: 70/222 eval: 71/222 eval: 72/222 eval: 73/222 eval: 74/222 eval: 75/222 eval: 76/222 eval: 77/222 eval: 78/222 eval: 79/222 eval: 80/222 eval: 81/222 eval: 82/222 eval: 83/222 eval: 84/222 eval: 85/222 eval: 86/222 eval: 87/222 eval: 88/222 eval: 89/222 eval: 90/222 eval: 91/222 eval: 92/222 eval: 93/222 eval: 94/222 eval: 95/222 eval: 96/222 eval: 97/222 eval: 98/222 eval: 99/222 eval: 100/222 eval: 101/222 eval: 102/222 eval: 103/222 eval: 104/222 eval: 105/222 eval: 106/222 eval: 107/222 eval: 108/222 eval: 109/222 eval: 110/222 eval: 111/222 eval: 112/222 eval: 113/222 eval: 114/222 eval: 115/222 eval: 116/222 eval: 117/222 eval: 118/222 eval: 119/222 eval: 120/222 eval: 121/222 eval: 122/222 eval: 123/222 eval: 124/222 eval: 125/222 eval: 126/222 eval: 127/222 eval: 128/222 eval: 129/222 eval: 130/222 eval: 131/222 eval: 132/222 eval: 133/222 eval: 134/222 eval: 135/222 eval: 136/222 eval: 137/222 eval: 138/222 eval: 139/222 eval: 140/222 eval: 141/222 eval: 142/222 eval: 143/222 eval: 144/222 eval: 145/222 eval: 146/222 eval: 147/222 eval: 148/222 eval: 149/222 eval: 150/222 eval: 151/222 eval: 152/222 eval: 153/222 eval: 154/222 eval: 155/222 eval: 156/222 eval: 157/222 eval: 158/222 eval: 159/222 eval: 160/222 eval: 161/222 eval: 162/222 eval: 163/222 eval: 164/222 eval: 165/222 eval: 166/222 eval: 167/222 eval: 168/222 eval: 169/222 eval: 170/222 eval: 171/222 eval: 172/222 eval: 173/222 eval: 174/222 eval: 175/222 eval: 176/222 eval: 177/222 eval: 178/222 eval: 179/222 eval: 180/222 eval: 181/222 eval: 182/222 eval: 183/222 eval: 184/222 eval: 185/222 eval: 186/222 eval: 187/222 eval: 188/222 eval: 189/222 eval: 190/222 eval: 191/222 eval: 192/222 eval: 193/222 eval: 194/222 eval: 195/222 eval: 196/222 eval: 197/222 eval: 198/222 eval: 199/222 eval: 200/222 eval: 201/222 eval: 202/222 eval: 203/222 eval: 204/222 eval: 205/222 eval: 206/222 eval: 207/222 eval: 208/222 eval: 209/222 eval: 210/222 eval: 211/222 eval: 212/222 eval: 213/222 eval: 214/222 eval: 215/222 eval: 216/222 eval: 217/222 eval: 218/222 eval: 219/222 eval: 220/222 eval: 221/222 eval: 222/222 . import cv2 import matplotlib.pyplot as plt import os %matplotlib inline img1 = cv2.imread(&#39;/content/DeepLabv3.pytorch/data/val/mouth-guard-5060809_1920.jpg&#39;) img2 = cv2.imread(&#39;/content/DeepLabv3.pytorch/data/archive/masks/mouth-guard-5060809_1920.png&#39;) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) print(&#39;img shape:&#39;, img1.shape) print(&#39;img shape:&#39;, img2.shape) plt.figure(figsize=(16, 16)) ax1 = plt.subplot(1, 2, 1) plt.imshow(img1_rgb) ax2 = plt.subplot(1, 2, 2) plt.imshow(img2_rgb) plt.show() . img shape: (1280, 1920, 3) . ? . img = cv2.imread(&#39;/content/DeepLabv3.pytorch/data/val/mouth-guard-5060809_1920.png&#39;) img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) print(&#39;img shape:&#39;, img.shape) plt.figure(figsize=(8, 8)) plt.imshow(img_rgb) plt.show() . img shape: (513, 513, 3) . &#45796;&#47476;&#48064;&#47492;&#12601; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . !cp -r /content/drive/MyDrive/archive /content/archive . %cd /content/archive . /content/archive . from torchvision import models deeplab = models.segmentation.deeplabv3_resnet101(pretrained=True).eval() . from PIL import Image import matplotlib.pyplot as plt import torch img = Image.open(&#39;/content/archive/images/pexels-anna-tarazevich-6027374.jpg&#39;) plt.imshow(img); plt.show() . import torchvision.transforms as T from PIL import Image transform = T.Compose([T.Resize(400), T.CenterCrop(513), T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]) # Resize, Centercrop 등 ToTensor 를 제외한 수치들은 Image 에 맞게 바꿔주시면 됩니다. inp = transform(img).unsqueeze(0) . out = deeplab(inp)[&#39;out&#39;] print (out.shape) . torch.Size([1, 21, 513, 513]) . import numpy as np om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy() print (om.shape) print (np.unique(om)) . (513, 513) [ 0 15] . def decode_segmap(image, nc=21): label_colors = np.array([(0, 0, 0), # 0=background # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128), # 6=bus, 7=car, 8=cat, 9=chair, 10=cow (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0), # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128), # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)]) r = np.zeros_like(image).astype(np.uint8) g = np.zeros_like(image).astype(np.uint8) b = np.zeros_like(image).astype(np.uint8) for l in range(0, nc): idx = image == l r[idx] = label_colors[l, 0] g[idx] = label_colors[l, 1] b[idx] = label_colors[l, 2] rgb = np.stack([r, g, b], axis=2) return rgb . rgb = decode_segmap(om) plt.imshow(rgb); plt.show() . Covid_Mask_Success . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . !git clone https://github.com/msminhas93/DeepLabv3FineTuning.git . Cloning into &#39;DeepLabv3FineTuning&#39;... remote: Enumerating objects: 349, done. remote: Counting objects: 100% (9/9), done. remote: Compressing objects: 100% (9/9), done. remote: Total 349 (delta 2), reused 3 (delta 0), pack-reused 340 Receiving objects: 100% (349/349), 4.09 MiB | 16.68 MiB/s, done. Resolving deltas: 100% (23/23), done. . %cd /content . /content . %cd /content/DeepLabv3FineTuning . /content/DeepLabv3FineTuning . !mkdir archive2 . !cp -r /content/drive/MyDrive/archive2 /content/DeepLabv3FineTuning/archive2 . !python main.py --data-directory archive2 --exp_directory val . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Epoch 1/20 - 100% 45/45 [03:59&lt;00:00, 5.32s/it] Train Loss: 0.0559 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.21s/it] Test Loss: 0.0478 {&#39;epoch&#39;: 1, &#39;Train_loss&#39;: 0.0558532178401947, &#39;Test_loss&#39;: 0.047845929861068726, &#39;Train_f1_score&#39;: 0.2545670008855428, &#39;Train_auroc&#39;: 0.8076701355014818, &#39;Test_f1_score&#39;: 0.23780841380159087, &#39;Test_auroc&#39;: 0.5692131854981063} Epoch 2/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:58&lt;00:00, 5.29s/it] Train Loss: 0.0275 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.20s/it] Test Loss: 0.0149 {&#39;epoch&#39;: 2, &#39;Train_loss&#39;: 0.02753956988453865, &#39;Test_loss&#39;: 0.014873748645186424, &#39;Train_f1_score&#39;: 0.3773094303808951, &#39;Train_auroc&#39;: 0.9332748667218229, &#39;Test_f1_score&#39;: 0.34869785060010905, &#39;Test_auroc&#39;: 0.8534060257007386} Epoch 3/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0098 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:23&lt;00:00, 2.18s/it] Test Loss: 0.0084 {&#39;epoch&#39;: 3, &#39;Train_loss&#39;: 0.00975809246301651, &#39;Test_loss&#39;: 0.00838901475071907, &#39;Train_f1_score&#39;: 0.48202951135927063, &#39;Train_auroc&#39;: 0.959489085382832, &#39;Test_f1_score&#39;: 0.598758118279879, &#39;Test_auroc&#39;: 0.9016274371675398} Epoch 4/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0059 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:23&lt;00:00, 2.16s/it] Test Loss: 0.0021 {&#39;epoch&#39;: 4, &#39;Train_loss&#39;: 0.005904238671064377, &#39;Test_loss&#39;: 0.002097081858664751, &#39;Train_f1_score&#39;: 0.5851498370008555, &#39;Train_auroc&#39;: 0.9677172265900211, &#39;Test_f1_score&#39;: 0.662186206328654, &#39;Test_auroc&#39;: 0.9131762077695114} Epoch 5/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0065 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.18s/it] Test Loss: 0.0171 {&#39;epoch&#39;: 5, &#39;Train_loss&#39;: 0.006461464334279299, &#39;Test_loss&#39;: 0.017088839784264565, &#39;Train_f1_score&#39;: 0.6380660650859429, &#39;Train_auroc&#39;: 0.9706738202371625, &#39;Test_f1_score&#39;: 0.6605795633118537, &#39;Test_auroc&#39;: 0.9143367924995155} Epoch 6/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0054 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:23&lt;00:00, 2.18s/it] Test Loss: 0.0020 {&#39;epoch&#39;: 6, &#39;Train_loss&#39;: 0.005387309938669205, &#39;Test_loss&#39;: 0.002003094647079706, &#39;Train_f1_score&#39;: 0.7079781215504763, &#39;Train_auroc&#39;: 0.9730275603244208, &#39;Test_f1_score&#39;: 0.6376990288752256, &#39;Test_auroc&#39;: 0.9142978496412647} Epoch 7/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0036 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:23&lt;00:00, 2.16s/it] Test Loss: 0.0026 {&#39;epoch&#39;: 7, &#39;Train_loss&#39;: 0.0036153539549559355, &#39;Test_loss&#39;: 0.002612881362438202, &#39;Train_f1_score&#39;: 0.7242810165301242, &#39;Train_auroc&#39;: 0.974296071644299, &#39;Test_f1_score&#39;: 0.6620514749108094, &#39;Test_auroc&#39;: 0.9144852769039286} Epoch 8/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0066 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.18s/it] Test Loss: 0.0022 {&#39;epoch&#39;: 8, &#39;Train_loss&#39;: 0.006613801699131727, &#39;Test_loss&#39;: 0.0022073332220315933, &#39;Train_f1_score&#39;: 0.7603517537855357, &#39;Train_auroc&#39;: 0.9744154032878953, &#39;Test_f1_score&#39;: 0.7349772897043357, &#39;Test_auroc&#39;: 0.9153795479946009} Epoch 9/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0035 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.18s/it] Test Loss: 0.0026 {&#39;epoch&#39;: 9, &#39;Train_loss&#39;: 0.00352257932536304, &#39;Test_loss&#39;: 0.0025799251161515713, &#39;Train_f1_score&#39;: 0.8033791372304683, &#39;Train_auroc&#39;: 0.9755416985066906, &#39;Test_f1_score&#39;: 0.7742687339746038, &#39;Test_auroc&#39;: 0.9152778716348723} Epoch 10/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0045 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0023 {&#39;epoch&#39;: 10, &#39;Train_loss&#39;: 0.004523678682744503, &#39;Test_loss&#39;: 0.002294049598276615, &#39;Train_f1_score&#39;: 0.8086864748587834, &#39;Train_auroc&#39;: 0.9758005086729932, &#39;Test_f1_score&#39;: 0.7771937967163122, &#39;Test_auroc&#39;: 0.9155933086227938} Epoch 11/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0044 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:23&lt;00:00, 2.18s/it] Test Loss: 0.0015 {&#39;epoch&#39;: 11, &#39;Train_loss&#39;: 0.004359075333923101, &#39;Test_loss&#39;: 0.00148110743612051, &#39;Train_f1_score&#39;: 0.8189728376001135, &#39;Train_auroc&#39;: 0.9760666951594961, &#39;Test_f1_score&#39;: 0.7487498299633873, &#39;Test_auroc&#39;: 0.9155920035636588} Epoch 12/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0022 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.18s/it] Test Loss: 0.0042 {&#39;epoch&#39;: 12, &#39;Train_loss&#39;: 0.00220703799277544, &#39;Test_loss&#39;: 0.004197620786726475, &#39;Train_f1_score&#39;: 0.8364125111752605, &#39;Train_auroc&#39;: 0.9764361434186626, &#39;Test_f1_score&#39;: 0.7972990761552733, &#39;Test_auroc&#39;: 0.9143758163820276} Epoch 13/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0045 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0025 {&#39;epoch&#39;: 13, &#39;Train_loss&#39;: 0.004544214345514774, &#39;Test_loss&#39;: 0.00248390669003129, &#39;Train_f1_score&#39;: 0.8145136248841697, &#39;Train_auroc&#39;: 0.9761663836110377, &#39;Test_f1_score&#39;: 0.7641746945740667, &#39;Test_auroc&#39;: 0.9157572228095877} Epoch 14/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0021 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.18s/it] Test Loss: 0.0019 {&#39;epoch&#39;: 14, &#39;Train_loss&#39;: 0.0021202731877565384, &#39;Test_loss&#39;: 0.0018618772737681866, &#39;Train_f1_score&#39;: 0.8463463881282745, &#39;Train_auroc&#39;: 0.9763691452304694, &#39;Test_f1_score&#39;: 0.8078065532721941, &#39;Test_auroc&#39;: 0.915704178888095} Epoch 15/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0023 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0018 {&#39;epoch&#39;: 15, &#39;Train_loss&#39;: 0.0023305551148951054, &#39;Test_loss&#39;: 0.0018446161411702633, &#39;Train_f1_score&#39;: 0.8433033967696616, &#39;Train_auroc&#39;: 0.9766012468529055, &#39;Test_f1_score&#39;: 0.8155434124212841, &#39;Test_auroc&#39;: 0.9158584313268533} Epoch 16/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0016 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0013 {&#39;epoch&#39;: 16, &#39;Train_loss&#39;: 0.0015778380911797285, &#39;Test_loss&#39;: 0.0013395079877227545, &#39;Train_f1_score&#39;: 0.8640393598277673, &#39;Train_auroc&#39;: 0.9768124533233159, &#39;Test_f1_score&#39;: 0.8121123877837831, &#39;Test_auroc&#39;: 0.9154184716712012} Epoch 17/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0019 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0024 {&#39;epoch&#39;: 17, &#39;Train_loss&#39;: 0.0018582935445010662, &#39;Test_loss&#39;: 0.002368517220020294, &#39;Train_f1_score&#39;: 0.8636502857910306, &#39;Train_auroc&#39;: 0.9769784925034648, &#39;Test_f1_score&#39;: 0.8087692266019841, &#39;Test_auroc&#39;: 0.915905892270252} Epoch 18/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.27s/it] Train Loss: 0.0021 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0088 {&#39;epoch&#39;: 18, &#39;Train_loss&#39;: 0.0021403669379651546, &#39;Test_loss&#39;: 0.008790221065282822, &#39;Train_f1_score&#39;: 0.8637440568239145, &#39;Train_auroc&#39;: 0.9770660843158886, &#39;Test_f1_score&#39;: 0.8143079205680945, &#39;Test_auroc&#39;: 0.9160954067243341} Epoch 19/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0026 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0020 {&#39;epoch&#39;: 19, &#39;Train_loss&#39;: 0.002574247308075428, &#39;Test_loss&#39;: 0.0020015176851302385, &#39;Train_f1_score&#39;: 0.8774504930182717, &#39;Train_auroc&#39;: 0.977102641728701, &#39;Test_f1_score&#39;: 0.8286490195044509, &#39;Test_auroc&#39;: 0.9159689330687774} Epoch 20/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 45/45 [03:57&lt;00:00, 5.28s/it] Train Loss: 0.0012 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 11/11 [00:24&lt;00:00, 2.19s/it] Test Loss: 0.0027 {&#39;epoch&#39;: 20, &#39;Train_loss&#39;: 0.0012150346301496029, &#39;Test_loss&#39;: 0.0026821549981832504, &#39;Train_f1_score&#39;: 0.8659291530438553, &#39;Train_auroc&#39;: 0.9770601403815509, &#39;Test_f1_score&#39;: 0.8170721323939998, &#39;Test_auroc&#39;: 0.9159602667377084} Training complete in 87m 26s Lowest Loss: 0.001340 . import torch import numpy as np import cv2 model = torch.load(&quot;/content/DeepLabv3FineTuning/val/weights.pt&quot;) model.eval() #import urllib #url, filename = (&quot;file:///home/rhobincu/gitroot/DeepLabv3FineTuning/CrackForest/Images/092.jpg&quot;, &quot;092.jpg&quot;) #try: urllib.URLopener().retrieve(url, filename) #except: urllib.request.urlretrieve(url, filename) filename = &#39;/content/DeepLabv3FineTuning/archive2/Images/nurse-4962034_1920 (Custom).jpg&#39; # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) import matplotlib.pyplot as plt plt.imshow(r) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F26D64B4ED0&gt; Using GPU! [[ 2.2541602 2.2541602 2.2541602 ... 7.1043715 7.1043715 7.1043715 ] [ 2.2541602 2.2541602 2.2541602 ... 7.1043715 7.1043715 7.1043715 ] [ 2.2541602 2.2541602 2.2541602 ... 7.1043715 7.1043715 7.1043715 ] ... [-0.52397954 -0.52397954 -0.52397954 ... -1.3202007 -1.3202007 -1.3202007 ] [-0.52397954 -0.52397954 -0.52397954 ... -1.3202007 -1.3202007 -1.3202007 ] [-0.52397954 -0.52397954 -0.52397954 ... -1.3202007 -1.3202007 -1.3202007 ]] 7621312.0 . import cv2 filename = &#39;/content/DeepLabv3FineTuning/archive2/Images/pexels-極逵勻筠剋-_棘_棘克龜戟-2324837 (Custom).jpg&#39; img1 = cv2.imread(filename) img2 = cv2.imread(&#39;/content/DeepLabv3FineTuning/archive2/Masks/pexels-極逵勻筠剋-_棘_棘克龜戟-2324837 (Custom).png&#39;) img1_rgb = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB) img2_rgb = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB) plt.figure(figsize=(15, 15)) ax1 = plt.subplot(1, 3, 1) ax1.imshow(img1_rgb) plt.xlabel(&quot;INPUT&quot;, size = 15) ax2 = plt.subplot(1, 3, 2) ax2.imshow(img2_rgb) plt.xlabel(&quot;LABEL&quot;, size = 15) # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) ax3 = plt.subplot(1, 3, 3) ax3 = plt.imshow(r) plt.xlabel(&quot;OUTPUT&quot;, size = 15) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F26D0DE70D0&gt; Using GPU! [[ 2.7625597 2.7625597 2.7625597 ... 0.6204884 0.6204884 0.6204884] [ 2.7625597 2.7625597 2.7625597 ... 0.6204884 0.6204884 0.6204884] [ 2.7625597 2.7625597 2.7625597 ... 0.6204884 0.6204884 0.6204884] ... [-3.5775788 -3.5775788 -3.5775788 ... 3.5316634 3.5316634 3.5316634] [-3.5775788 -3.5775788 -3.5775788 ... 3.5316634 3.5316634 3.5316634] [-3.5775788 -3.5775788 -3.5775788 ... 3.5316634 3.5316634 3.5316634]] 6805480.5 . crack&#51060; &#46104;&#45716;&#51648; &#48512;&#53552; &#54869;&#51064; &#54644;&#50556; &#54624; &#46319; . 되긴 함 | . %cd /content/DeepLabv3FineTuning . /content/DeepLabv3FineTuning . !python main.py --data-directory CrackForest --exp_directory CFExp . /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) Epoch 1/20 - 100% 24/24 [01:12&lt;00:00, 3.03s/it] Train Loss: 0.0364 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0461 {&#39;epoch&#39;: 1, &#39;Train_loss&#39;: 0.03641420230269432, &#39;Test_loss&#39;: 0.04607094079256058, &#39;Train_f1_score&#39;: 0.054846423883562395, &#39;Train_auroc&#39;: 0.5663054422091399, &#39;Test_f1_score&#39;: 0.01356463793212177, &#39;Test_auroc&#39;: 0.4021435138943125} Epoch 2/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.04s/it] Train Loss: 0.0279 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0161 {&#39;epoch&#39;: 2, &#39;Train_loss&#39;: 0.02786177769303322, &#39;Test_loss&#39;: 0.016071867197752, &#39;Train_f1_score&#39;: 0.08347586912162694, &#39;Train_auroc&#39;: 0.6896203827960143, &#39;Test_f1_score&#39;: 0.07015544058122772, &#39;Test_auroc&#39;: 0.6407583609519486} Epoch 3/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.02s/it] Train Loss: 0.0177 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.21s/it] Test Loss: 0.0213 {&#39;epoch&#39;: 3, &#39;Train_loss&#39;: 0.01770329475402832, &#39;Test_loss&#39;: 0.021320365369319916, &#39;Train_f1_score&#39;: 0.1264257985352043, &#39;Train_auroc&#39;: 0.7990749242847827, &#39;Test_f1_score&#39;: 0.179176931373783, &#39;Test_auroc&#39;: 0.7638337068942815} Epoch 4/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.02s/it] Train Loss: 0.0296 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0101 {&#39;epoch&#39;: 4, &#39;Train_loss&#39;: 0.029597271233797073, &#39;Test_loss&#39;: 0.010104403831064701, &#39;Train_f1_score&#39;: 0.1806005747528704, &#39;Train_auroc&#39;: 0.8476549886135349, &#39;Test_f1_score&#39;: 0.17344265458497823, &#39;Test_auroc&#39;: 0.7922656742706151} Epoch 5/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0161 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0087 {&#39;epoch&#39;: 5, &#39;Train_loss&#39;: 0.016051875427365303, &#39;Test_loss&#39;: 0.00871072243899107, &#39;Train_f1_score&#39;: 0.21523014238787827, &#39;Train_auroc&#39;: 0.8732268475421424, &#39;Test_f1_score&#39;: 0.24044212042221605, &#39;Test_auroc&#39;: 0.8029833679038354} Epoch 6/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:13&lt;00:00, 3.07s/it] Train Loss: 0.0114 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0070 {&#39;epoch&#39;: 6, &#39;Train_loss&#39;: 0.01135195977985859, &#39;Test_loss&#39;: 0.007030495908111334, &#39;Train_f1_score&#39;: 0.23787246696463782, &#39;Train_auroc&#39;: 0.8910430316102053, &#39;Test_f1_score&#39;: 0.26121431879345164, &#39;Test_auroc&#39;: 0.808470460676416} Epoch 7/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.03s/it] Train Loss: 0.0276 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.19s/it] Test Loss: 0.0118 {&#39;epoch&#39;: 7, &#39;Train_loss&#39;: 0.027572475373744965, &#39;Test_loss&#39;: 0.011759184300899506, &#39;Train_f1_score&#39;: 0.2698861740153685, &#39;Train_auroc&#39;: 0.9000451068911747, &#39;Test_f1_score&#39;: 0.29079985609012665, &#39;Test_auroc&#39;: 0.7921001523983954} Epoch 8/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0104 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.19s/it] Test Loss: 0.0099 {&#39;epoch&#39;: 8, &#39;Train_loss&#39;: 0.010358788073062897, &#39;Test_loss&#39;: 0.009920933283865452, &#39;Train_f1_score&#39;: 0.290971676787778, &#39;Train_auroc&#39;: 0.9100962128169019, &#39;Test_f1_score&#39;: 0.2996428472776724, &#39;Test_auroc&#39;: 0.8183586652030385} Epoch 9/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0127 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0073 {&#39;epoch&#39;: 9, &#39;Train_loss&#39;: 0.01272133644670248, &#39;Test_loss&#39;: 0.007258118130266666, &#39;Train_f1_score&#39;: 0.3098554145131152, &#39;Train_auroc&#39;: 0.9158514517351243, &#39;Test_f1_score&#39;: 0.30074806513227875, &#39;Test_auroc&#39;: 0.8273134011633577} Epoch 10/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0099 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0186 {&#39;epoch&#39;: 10, &#39;Train_loss&#39;: 0.009870304726064205, &#39;Test_loss&#39;: 0.018645979464054108, &#39;Train_f1_score&#39;: 0.32037760736527576, &#39;Train_auroc&#39;: 0.9203475610386761, &#39;Test_f1_score&#39;: 0.3006550110563932, &#39;Test_auroc&#39;: 0.8272859917882867} Epoch 11/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0193 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0069 {&#39;epoch&#39;: 11, &#39;Train_loss&#39;: 0.019272182136774063, &#39;Test_loss&#39;: 0.0068652997724711895, &#39;Train_f1_score&#39;: 0.340209753180169, &#39;Train_auroc&#39;: 0.9257411436280363, &#39;Test_f1_score&#39;: 0.29515990308327716, &#39;Test_auroc&#39;: 0.8291728426064849} Epoch 12/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0120 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.18s/it] Test Loss: 0.0072 {&#39;epoch&#39;: 12, &#39;Train_loss&#39;: 0.011974063701927662, &#39;Test_loss&#39;: 0.007168004754930735, &#39;Train_f1_score&#39;: 0.3461091032900889, &#39;Train_auroc&#39;: 0.92519729591772, &#39;Test_f1_score&#39;: 0.30844929254930387, &#39;Test_auroc&#39;: 0.8355698944536033} Epoch 13/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0098 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0086 {&#39;epoch&#39;: 13, &#39;Train_loss&#39;: 0.009785499423742294, &#39;Test_loss&#39;: 0.008560686372220516, &#39;Train_f1_score&#39;: 0.3559770064511662, &#39;Train_auroc&#39;: 0.9304539505669039, &#39;Test_f1_score&#39;: 0.2761958112476465, &#39;Test_auroc&#39;: 0.8365514442258722} Epoch 14/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.02s/it] Train Loss: 0.0139 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0162 {&#39;epoch&#39;: 14, &#39;Train_loss&#39;: 0.013885611668229103, &#39;Test_loss&#39;: 0.01620718464255333, &#39;Train_f1_score&#39;: 0.3567318770632889, &#39;Train_auroc&#39;: 0.9303158350959804, &#39;Test_f1_score&#39;: 0.3049634973903451, &#39;Test_auroc&#39;: 0.8317187176241267} Epoch 15/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.02s/it] Train Loss: 0.0117 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0069 {&#39;epoch&#39;: 15, &#39;Train_loss&#39;: 0.01167416200041771, &#39;Test_loss&#39;: 0.006866126321256161, &#39;Train_f1_score&#39;: 0.37259281669360605, &#39;Train_auroc&#39;: 0.9341292765115997, &#39;Test_f1_score&#39;: 0.3190617899072231, &#39;Test_auroc&#39;: 0.831729455061243} Epoch 16/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.02s/it] Train Loss: 0.0123 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.20s/it] Test Loss: 0.0064 {&#39;epoch&#39;: 16, &#39;Train_loss&#39;: 0.0122584979981184, &#39;Test_loss&#39;: 0.006374400574713945, &#39;Train_f1_score&#39;: 0.38173928486809316, &#39;Train_auroc&#39;: 0.9366575879957318, &#39;Test_f1_score&#39;: 0.34171426296326246, &#39;Test_auroc&#39;: 0.8352430350036191} Epoch 17/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0096 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.18s/it] Test Loss: 0.0107 {&#39;epoch&#39;: 17, &#39;Train_loss&#39;: 0.009648161940276623, &#39;Test_loss&#39;: 0.010659500025212765, &#39;Train_f1_score&#39;: 0.3923900038889913, &#39;Train_auroc&#39;: 0.9379839849082607, &#39;Test_f1_score&#39;: 0.33346624496549626, &#39;Test_auroc&#39;: 0.8333429949196116} Epoch 18/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0087 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:06&lt;00:00, 1.16s/it] Test Loss: 0.0067 {&#39;epoch&#39;: 18, &#39;Train_loss&#39;: 0.008744689635932446, &#39;Test_loss&#39;: 0.006720169447362423, &#39;Train_f1_score&#39;: 0.3902035305407743, &#39;Train_auroc&#39;: 0.9363833115704652, &#39;Test_f1_score&#39;: 0.34246518335066334, &#39;Test_auroc&#39;: 0.8344072264014298} Epoch 19/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0117 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.19s/it] Test Loss: 0.0079 {&#39;epoch&#39;: 19, &#39;Train_loss&#39;: 0.01167882326990366, &#39;Test_loss&#39;: 0.007933910004794598, &#39;Train_f1_score&#39;: 0.3905962647671534, &#39;Train_auroc&#39;: 0.9365460107544883, &#39;Test_f1_score&#39;: 0.3319154650945707, &#39;Test_auroc&#39;: 0.8295564345789856} Epoch 20/20 - /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 24/24 [01:12&lt;00:00, 3.01s/it] Train Loss: 0.0235 /usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary. cpuset_checked)) 100% 6/6 [00:07&lt;00:00, 1.19s/it] Test Loss: 0.0098 {&#39;epoch&#39;: 20, &#39;Train_loss&#39;: 0.02354367822408676, &#39;Test_loss&#39;: 0.009758192114531994, &#39;Train_f1_score&#39;: 0.4097223641417578, &#39;Train_auroc&#39;: 0.9390610174925973, &#39;Test_f1_score&#39;: 0.3487654086436266, &#39;Test_auroc&#39;: 0.8392464458388973} Training complete in 26m 47s Lowest Loss: 0.006374 . Crack Dataset . import torch import numpy as np import cv2 model = torch.load(&quot;/content/DeepLabv3FineTuning/CFExp/weights.pt&quot;) model.eval() filename = &#39;/content/DeepLabv3FineTuning/CrackForest/Images/002.jpg&#39; # sample execution (requires torchvision) from PIL import Image from torchvision import transforms input_image = Image.open(filename) print(input_image) preprocess = transforms.Compose([ transforms.ToTensor() ]) input_tensor = preprocess(input_image) input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model # move the input and model to GPU for speed if available if torch.cuda.is_available(): print(&#39;Using GPU!&#39;) input_batch = input_batch.to(&#39;cuda&#39;) model.to(&#39;cuda&#39;) with torch.no_grad(): output = model(input_batch)[&#39;out&#39;][0] output_predictions = output[0] # plot the semantic segmentation predictions of 21 classes in each color img_size = input_image.size data = output_predictions.cpu().numpy() * 255 print(data) print(data.sum()) r = Image.fromarray(data).resize(img_size) import matplotlib.pyplot as plt plt.imshow(r) plt.show() . &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x320 at 0x7F26D1AA8550&gt; Using GPU! [[-0.07728636 -0.07728636 -0.07728636 ... -1.2099057 -1.2099057 -1.2099057 ] [-0.07728636 -0.07728636 -0.07728636 ... -1.2099057 -1.2099057 -1.2099057 ] [-0.07728636 -0.07728636 -0.07728636 ... -1.2099057 -1.2099057 -1.2099057 ] ... [24.869083 24.869083 24.869083 ... 2.0871072 2.0871072 2.0871072 ] [24.869083 24.869083 24.869083 ... 2.0871072 2.0871072 2.0871072 ] [24.869083 24.869083 24.869083 ... 2.0871072 2.0871072 2.0871072 ]] 3871240.5 . img = Image.open(&#39;/content/DeepLabv3FineTuning/CrackForest/Images/002.jpg&#39;) plt.imshow(img); plt.show() .",
            "url": "https://raukrauk.github.io/ML-DL/boaz/deeplearning/2021/12/24/Deeplab_V3_Learning.html",
            "relUrl": "/boaz/deeplearning/2021/12/24/Deeplab_V3_Learning.html",
            "date": " • Dec 24, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "[Kaggle 설문놀이] Kaggle Machine Learning & Data Science Survey",
            "content": "from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . from google.colab import files files.upload() !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 /root/.kaggle/kaggle.json . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . !kaggle competitions download -c kaggle-survey-2021 . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading kaggle_survey_2021_methodology.pdf to /content 0% 0.00/55.2k [00:00&lt;?, ?B/s] 100% 55.2k/55.2k [00:00&lt;00:00, 76.1MB/s] Downloading kaggle_survey_2021_answer_choices.pdf to /content 0% 0.00/158k [00:00&lt;?, ?B/s] 100% 158k/158k [00:00&lt;00:00, 51.2MB/s] Downloading kaggle_survey_2021_responses.csv.zip to /content 0% 0.00/2.86M [00:00&lt;?, ?B/s] 100% 2.86M/2.86M [00:00&lt;00:00, 89.5MB/s] . import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd import seaborn as sns . !ls . drive kaggle_survey_2021_methodology.pdf kaggle.json kaggle_survey_2021_responses.csv.zip kaggle_survey_2021_answer_choices.pdf sample_data . !unzip kaggle_survey_2021_responses.csv.zip . Archive: kaggle_survey_2021_responses.csv.zip inflating: kaggle_survey_2021_responses.csv . response = pd.read_csv(&quot;kaggle_survey_2021_responses.csv&quot;) . /usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,195,201,285,286,287,288,289,290,291,292) have mixed types.Specify dtype option on import or set low_memory=False. interactivity=interactivity, compiler=compiler, result=result) . response.head() . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 Q7_Part_4 Q7_Part_5 Q7_Part_6 Q7_Part_7 Q7_Part_8 Q7_Part_9 Q7_Part_10 Q7_Part_11 Q7_Part_12 Q7_OTHER Q8 Q9_Part_1 Q9_Part_2 Q9_Part_3 Q9_Part_4 Q9_Part_5 Q9_Part_6 Q9_Part_7 Q9_Part_8 Q9_Part_9 Q9_Part_10 Q9_Part_11 Q9_Part_12 Q9_OTHER Q10_Part_1 Q10_Part_2 Q10_Part_3 Q10_Part_4 Q10_Part_5 Q10_Part_6 ... Q34_B_Part_6 Q34_B_Part_7 Q34_B_Part_8 Q34_B_Part_9 Q34_B_Part_10 Q34_B_Part_11 Q34_B_Part_12 Q34_B_Part_13 Q34_B_Part_14 Q34_B_Part_15 Q34_B_Part_16 Q34_B_OTHER Q36_B_Part_1 Q36_B_Part_2 Q36_B_Part_3 Q36_B_Part_4 Q36_B_Part_5 Q36_B_Part_6 Q36_B_Part_7 Q36_B_OTHER Q37_B_Part_1 Q37_B_Part_2 Q37_B_Part_3 Q37_B_Part_4 Q37_B_Part_5 Q37_B_Part_6 Q37_B_Part_7 Q37_B_OTHER Q38_B_Part_1 Q38_B_Part_2 Q38_B_Part_3 Q38_B_Part_4 Q38_B_Part_5 Q38_B_Part_6 Q38_B_Part_7 Q38_B_Part_8 Q38_B_Part_9 Q38_B_Part_10 Q38_B_Part_11 Q38_B_OTHER . 0 Duration (in seconds) | What is your age (# years)? | What is your gender? - Selected Choice | In which country do you currently reside? | What is the highest level of formal education ... | Select the title most similar to your current ... | For how many years have you been writing code ... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming languages do you use on a reg... | What programming language would you recommend ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following integrated development ... | Which of the following hosted notebook product... | Which of the following hosted notebook product... | Which of the following hosted notebook product... | Which of the following hosted notebook product... | Which of the following hosted notebook product... | Which of the following hosted notebook product... | ... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which of the following business intelligence t... | Which categories of automated machine learning... | Which categories of automated machine learning... | Which categories of automated machine learning... | Which categories of automated machine learning... | Which categories of automated machine learning... | Which categories of automated machine learning... | Which categories of automated machine learning... | Which categories of automated machine learning... | Which specific automated machine learning tool... | Which specific automated machine learning tool... | Which specific automated machine learning tool... | Which specific automated machine learning tool... | Which specific automated machine learning tool... | Which specific automated machine learning tool... | Which specific automated machine learning tool... | Which specific automated machine learning tool... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | In the next 2 years, do you hope to become mor... | . 1 910 | 50-54 | Man | India | Bachelor’s degree | Other | 5-10 years | Python | R | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Python | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Vim / Emacs | NaN | NaN | NaN | NaN | NaN | Colab Notebooks | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 784 | 50-54 | Man | Indonesia | Master’s degree | Program/Project Manager | 20+ years | NaN | NaN | SQL | C | C++ | Java | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Python | NaN | NaN | NaN | NaN | NaN | NaN | Notepad++ | NaN | NaN | NaN | Jupyter Notebook | NaN | NaN | Kaggle Notebooks | Colab Notebooks | NaN | NaN | NaN | NaN | ... | NaN | NaN | Qlik | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Automated model selection (e.g. auto-sklearn, ... | NaN | NaN | NaN | NaN | NaN | Google Cloud AutoML | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | None | NaN | . 3 924 | 22-24 | Man | Pakistan | Master’s degree | Software Engineer | 1-3 years | Python | NaN | NaN | NaN | C++ | Java | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Python | NaN | NaN | NaN | NaN | PyCharm | NaN | NaN | NaN | NaN | NaN | Jupyter Notebook | NaN | Other | Kaggle Notebooks | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Automated model selection (e.g. auto-sklearn, ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | DataRobot AutoML | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | TensorBoard | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 575 | 45-49 | Man | Mexico | Doctoral degree | Research Scientist | 20+ years | Python | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Python | NaN | NaN | NaN | NaN | NaN | Spyder | NaN | NaN | NaN | NaN | Jupyter Notebook | NaN | NaN | NaN | Colab Notebooks | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | None | NaN | NaN | NaN | NaN | NaN | NaN | NaN | None | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | None | NaN | . 5 rows × 369 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; response.describe() . Time from Start to Finish (seconds) Q1 Q2 Q3 Q4 Q5 Q6 Q7_Part_1 Q7_Part_2 Q7_Part_3 Q7_Part_4 Q7_Part_5 Q7_Part_6 Q7_Part_7 Q7_Part_8 Q7_Part_9 Q7_Part_10 Q7_Part_11 Q7_Part_12 Q7_OTHER Q8 Q9_Part_1 Q9_Part_2 Q9_Part_3 Q9_Part_4 Q9_Part_5 Q9_Part_6 Q9_Part_7 Q9_Part_8 Q9_Part_9 Q9_Part_10 Q9_Part_11 Q9_Part_12 Q9_OTHER Q10_Part_1 Q10_Part_2 Q10_Part_3 Q10_Part_4 Q10_Part_5 Q10_Part_6 ... Q34_B_Part_6 Q34_B_Part_7 Q34_B_Part_8 Q34_B_Part_9 Q34_B_Part_10 Q34_B_Part_11 Q34_B_Part_12 Q34_B_Part_13 Q34_B_Part_14 Q34_B_Part_15 Q34_B_Part_16 Q34_B_OTHER Q36_B_Part_1 Q36_B_Part_2 Q36_B_Part_3 Q36_B_Part_4 Q36_B_Part_5 Q36_B_Part_6 Q36_B_Part_7 Q36_B_OTHER Q37_B_Part_1 Q37_B_Part_2 Q37_B_Part_3 Q37_B_Part_4 Q37_B_Part_5 Q37_B_Part_6 Q37_B_Part_7 Q37_B_OTHER Q38_B_Part_1 Q38_B_Part_2 Q38_B_Part_3 Q38_B_Part_4 Q38_B_Part_5 Q38_B_Part_6 Q38_B_Part_7 Q38_B_Part_8 Q38_B_Part_9 Q38_B_Part_10 Q38_B_Part_11 Q38_B_OTHER . count 25974 | 25974 | 25974 | 25974 | 25974 | 25974 | 25974 | 21861 | 5335 | 10757 | 4710 | 5536 | 4770 | 4333 | 306 | 243 | 2217 | 2936 | 320 | 2576 | 24941 | 5489 | 4772 | 4111 | 10041 | 7469 | 3795 | 3938 | 2840 | 1647 | 2204 | 16234 | 527 | 1492 | 9508 | 9793 | 1063 | 210 | 1771 | 176 | ... | 1265 | 1700 | 784 | 277 | 269 | 527 | 316 | 1344 | 1696 | 263 | 3057 | 293 | 3497 | 3649 | 4762 | 2184 | 3332 | 4753 | 3222 | 324 | 4818 | 1388 | 1650 | 1546 | 2684 | 3153 | 1399 | 281 | 1277 | 1584 | 634 | 592 | 4240 | 730 | 738 | 1021 | 667 | 2748 | 4543 | 378 | . unique 5410 | 12 | 6 | 67 | 8 | 16 | 8 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 14 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . top 484 | 25-29 | Man | India | Master’s degree | Student | 1-3 years | Python | R | SQL | C | C++ | Java | Javascript | Julia | Swift | Bash | MATLAB | None | Other | Python | Jupyter (JupyterLab, Jupyter Notebooks, etc) | RStudio | Visual Studio | Visual Studio Code (VSCode) | PyCharm | Spyder | Notepad++ | Sublime Text | Vim / Emacs | MATLAB | Jupyter Notebook | None | Other | Kaggle Notebooks | Colab Notebooks | Azure Notebooks | Paperspace / Gradient | Binder / JupyterHub | Code Ocean | ... | Salesforce | Tableau CRM | Qlik | Domo | TIBCO Spotfire | Alteryx | Sisense | SAP Analytics Cloud | Microsoft Azure Synapse | Thoughtspot | None | Other | Automated data augmentation (e.g. imgaug, albu... | Automated feature engineering/selection (e.g. ... | Automated model selection (e.g. auto-sklearn, ... | Automated model architecture searches (e.g. da... | Automated hyperparameter tuning (e.g. hyperopt... | Automation of full ML pipelines (e.g. Google C... | None | Other | Google Cloud AutoML | H2O Driverless AI | Databricks AutoML | DataRobot AutoML | Amazon Sagemaker Autopilot | Azure Automated Machine Learning | None | Other | Neptune.ai | Weights &amp; Biases | Comet.ml | Sacred + Omniboard | TensorBoard | Guild.ai | Polyaxon | ClearML | Domino Model Monitor | MLflow | None | Other | . freq 42 | 4931 | 20598 | 7434 | 10132 | 6804 | 7874 | 21860 | 5334 | 10756 | 4709 | 5535 | 4769 | 4332 | 305 | 242 | 2216 | 2935 | 319 | 2575 | 20213 | 5488 | 4771 | 4110 | 10040 | 7468 | 3794 | 3937 | 2839 | 1646 | 2203 | 16233 | 526 | 1491 | 9507 | 9792 | 1062 | 209 | 1770 | 175 | ... | 1264 | 1699 | 783 | 276 | 268 | 526 | 315 | 1343 | 1695 | 262 | 3056 | 292 | 3496 | 3648 | 4761 | 2183 | 3331 | 4752 | 3221 | 323 | 4817 | 1387 | 1649 | 1545 | 2683 | 3152 | 1398 | 280 | 1276 | 1583 | 633 | 591 | 4239 | 729 | 737 | 1020 | 666 | 2747 | 4542 | 377 | . 4 rows × 369 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(5, 8), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 1) gs.update(wspace=1.5, hspace=1.1) ##########COUNTRY########## temp = pd.DataFrame(response.groupby(&#39;Q3&#39;, dropna=False).size()).reset_index() temp.columns = [&#39;country&#39;, &#39;amount&#39;] temp[&#39;pct&#39;] = temp[&#39;amount&#39;]*100/temp.amount.sum() temp = temp.sort_values(&#39;amount&#39;, ascending=False).reset_index() temp = temp.query(&quot;amount&gt;1&quot;) temp.loc[1, &#39;country&#39;] = &#39;USA&#39; temp.loc[8, &#39;country&#39;] = &#39;UK&#39; temp.loc[27, &#39;country&#39;] = &#39;Iran&#39; for i in range(31,66): temp.at[i,&#39;country&#39;] = &#39;Other&#39; temp = temp.groupby(&#39;country&#39;).sum() temp = temp.query(&quot;country!=&#39;Other&#39;&quot;) temp = temp.query(&quot;pct&gt;1&quot;) temp = temp.sort_values(&#39;amount&#39;, ascending=False).reset_index() temp = temp[0:20] background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(20)] color_map[0] = &quot;#F2A154&quot; #F2A154 79B4B7 sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[0, 0]) ax0.set_facecolor(background_color) ax0.set(xlim=(0, 40)) ax0.set(ylim=(0, 20)) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0_sns = sns.barplot(ax=ax0, y=temp[&#39;country&#39;], x=temp[&#39;pct&#39;], zorder=2, linewidth=0, orient=&#39;h&#39;, saturation=0.9, alpha=0.7) ax0_sns.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#EEEEEE&#39;, linewidth=0.2) #format axis ax0_sns.set_xlabel(&quot;Percentage&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0_sns.set_ylabel(&quot;Country&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0_sns.tick_params(labelsize=3, width=0.3, length=1) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1-1.5, &#39;Top 20 Countries&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1-0.8, &#39;Q3: In which country do you currently reside?&#39;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) # data label i=0 for p in ax0.patches: value = f&#39;{p.get_width():.1f}%&#39; if i&lt;7: x1 = -1 else: x1 = .8 x = p.get_x() + p.get_width() + x1 i+=1 y = p.get_y() + p.get_height() / 2 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2, weight=&#39;semibold&#39;) #x_format = ticker.PercentFormatter() #ax0.xaxis.set_major_formatter(x_format) plt.show() . &#54620;&#51473;&#51068; &#51064;&#44036;&#46308;&#51012; &#49332;&#54196;&#48372;&#51088; . kor = response[response.Q3 == &quot;South Korea&quot;] jpn = response[response.Q3 == &quot;Japan&quot;] chn = response[response.Q3 == &quot;China&quot;] comb = pd.concat([kor,jpn,chn], axis = 0) . print(&quot;한국인 응답자 수:&quot;,len(kor),&quot;명&quot;) print(&quot;중국인 응답자 수:&quot;,len(jpn),&quot;명&quot;) print(&quot;일본인 응답자 수:&quot;,len(chn),&quot;명&quot;) . 한국인 응답자 수: 359 명 중국인 응답자 수: 921 명 일본인 응답자 수: 814 명 . &#49457;&#48324;&#51008; &#45224;&#49457;&#51060; &#51228;&#51068; &#47566;&#51020; . Man | Woman | Nonbinary | Prefer not to say(선택하고 싶지 않음) | Prefer to self-describe(?) | . plt.figure(figsize = (8,7)) sns.countplot(kor.Q2) plt.xticks(rotation = 45) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . (array([0, 1, 2]), &lt;a list of 3 Text major ticklabel objects&gt;) . &#54617;&#47141; . temp = kor.groupby(&#39;Q4&#39;, dropna=False).size().reset_index() temp.columns = [&#39;education&#39;, &#39;amount&#39;] temp = temp.query(&quot;amount&gt;1&quot;) temp.loc[4, &#39;education&#39;] = &#39;High school&#39; temp.loc[6, &#39;education&#39;] = &#39;Some college&#39; temp[&#39;pct&#39;] = temp[&#39;amount&#39;]*100/temp.amount.sum() temp = temp.sort_values(&#39;amount&#39;, ascending=False) plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(3, 5), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 1) gs.update(wspace=1.5, hspace=1.1) background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(7)] color_map[0] = &quot;#F2A154&quot; sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[1, 0]) ax0.set_facecolor(background_color) ax0.set(xlim=(0, 45)) ax0.set(ylim=(0, 7)) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0_sns = sns.barplot(ax=ax0, y=temp[&#39;education&#39;], x=temp[&#39;pct&#39;], zorder=2, linewidth=0, orient=&#39;h&#39;, saturation=0.9, alpha=0.7) ax0_sns.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#EEEEEE&#39;, linewidth=0.2) #format axis ax0_sns.set_xlabel(&quot;Percentage&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0_sns.set_ylabel(&quot;Education&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0_sns.tick_params(labelsize=3, width=0.3, length=1) #title x0, x1 = ax0_sns.get_xlim() y0, y1 = ax0_sns.get_ylim() ax0_sns.text(x0, y1-1.2, &#39;Higher Education&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0_sns.text(x0, y1-0.6, &#39;What is the highest level of formal education that you have attained or plan to attain within the next 2 years?&#39;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) # data labe i=0 for p in ax0.patches: value = f&#39;{p.get_width():.1f}%&#39; if i&lt;4: x1 = -1.5 else: x1 = 1.2 x = p.get_x() + p.get_width() + x1 i+=1 y = p.get_y() + p.get_height() / 2 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=3, weight=&#39;semibold&#39;) #x_format = ticker.PercentFormatter() #ax0.xaxis.set_major_formatter(x_format) plt.show() . plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(4, 6), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 1) gs.update(wspace=1.5, hspace=1.1) ##########AGE########## temp = kor.groupby(&#39;Q1&#39;, dropna=False).size().reset_index() temp.columns = [&#39;answer&#39;, &#39;amount&#39;] temp[&#39;pct&#39;] = temp[&#39;amount&#39;]*100/temp.amount.sum() #temp = temp.sort_values(&#39;pct&#39;, ascending=False) background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(75)] color_map[0] = &quot;#F2A154&quot; sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[1, 0]) ax0.set_facecolor(background_color) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0 = sns.barplot(ax=ax0, y=temp[&#39;pct&#39;], x=temp[&#39;answer&#39;], zorder=2, linewidth=0, saturation=1, alpha=0.7) fillwidth = np.array([-0.5,2.5]) ax0.fill_between(fillwidth, np.max(temp[&#39;pct&#39;]+2), color=&#39;#FFE699&#39;, alpha=0.5, zorder=2, linewidth=0) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#EEEEEE&#39;, lw=0.3) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1+3, &#39;Age&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1+1.8, &#39;What is your age (# years)?&#39;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) # data label i=0 for p in ax0.patches: value = f&#39;{p.get_height():.1f}%&#39; if i&lt;8: y1 = -.6 else: y1 = .5 i+=1 x = p.get_x() + p.get_width() / 2 y = p.get_y() + p.get_height() + y1 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2.5, weight=&#39;semibold&#39;) #format axis ax0.set_xlabel(&quot;Age&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.set_ylabel(&quot;Percentage&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.tick_params(labelsize=3, width=0.3, length=1) #plt.xticks(rotation=90) plt.show() . plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(4, 6), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 1) gs.update(wspace=1.5, hspace=1.1) ##########AGEGENDER########## temp = pd.DataFrame(comb.groupby([&quot;Q1&quot;, &quot;Q2&quot;], dropna=False).size()) temp = temp.query(&quot;Q2==&#39;Man&#39; | Q2==&#39;Woman&#39;&quot;) temp = temp.reset_index() temp.columns = [&#39;age&#39;, &#39;gender&#39;, &#39;amount&#39;] temp[&#39;pct&#39;] = temp[&#39;amount&#39;]*100/temp.amount.sum() background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(75)] color_map[0] = &quot;#F2A154&quot; sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[1, 0]) ax0.set_facecolor(background_color) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0 = sns.barplot(ax=ax0, y=temp[&#39;pct&#39;], x=temp[&#39;age&#39;], hue=temp[&#39;gender&#39;], zorder=2, linewidth=0, saturation=0.9, alpha=0.7, animated=True) fillwidth1 = np.array([-0.5,2.5]) ax0.fill_between(fillwidth1, 16, color=&#39;#7C9473&#39;, alpha=0.3, zorder=2, linewidth=0) fillwidth2 = np.array([2.5,6.5]) ax0.fill_between(fillwidth2, 12, color=&#39;#CFDAC8&#39;, alpha=0.3, zorder=2, linewidth=0) fillwidth3 = np.array([6.5,10.5]) ax0.fill_between(fillwidth3, 4, color=&#39;#E8EAE6&#39;, alpha=0.3, zorder=2, linewidth=0) yline = 1 plt.axvline(-0.5, linewidth=0.3, ymax=yline, linestyle=&#39;dashed&#39;, color=&#39;#90AACB&#39;, alpha=0.5) plt.axvline(2.5, linewidth=0.3, ymax=yline, linestyle=&#39;dashed&#39;, color=&#39;#90AACB&#39;, alpha=0.5) plt.axvline(6.5, linewidth=0.3, ymax=yline*.75, linestyle=&#39;dashed&#39;, color=&#39;#90AACB&#39;, alpha=0.5) plt.axvline(10.5, linewidth=0.3, ymax=yline*.25, linestyle=&#39;dashed&#39;, color=&#39;#90AACB&#39;, alpha=0.5) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#EEEEEE&#39;, lw=0.3) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1+2, &#39;Age by Gender&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1+1, &#39;What is your age (# years)?&#39;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) # data label i=0 for p in ax0.patches: value = f&#39;{p.get_height():.1f}%&#39; if i&lt;8: y1 = -.6 else: y1 = .5 i+=1 x = p.get_x() + p.get_width() / 2 y = p.get_y() + p.get_height() + y1 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2, weight=&#39;semibold&#39;) #legend ax0.legend(loc=&quot;upper right&quot;, prop={&#39;size&#39;: 2}, frameon=False, ncol=5, title_fontsize=3, title=&#39;Gender&#39;) #format axis ax0.set_xlabel(&quot;Age&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.set_ylabel(&quot;Percentage&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.tick_params(labelsize=3, width=0.3, length=1) plt.show() . posx and posy should be finite values posx and posy should be finite values . Python | R | SQL | C | C++ | Java | Javascript | Julia | Swift | Bash | MATLAB | None | Other | . col = 7 result=pd.DataFrame() for col in range(col, col+13): choice = kor.iloc[0:1, col:col+1].to_string() lang = choice[choice.find(&#39;ce -&#39;)+5:len(choice)] grouped = kor.iloc[:, -1].groupby(kor.iloc[:, col]) data = {&quot;language&quot;: lang, &quot;amount&quot;: grouped.size()[0]} result = result.append(data, ignore_index=True) total = result[&#39;amount&#39;].sum() result = result.query(&quot;language!=&#39;None&#39; &amp; language!=&#39;Other&#39;&quot;) result[&#39;pct&#39;] = result[&#39;amount&#39;]*100/total temp = result temp = temp.sort_values(&#39;pct&#39;, ascending=False) plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(3, 8), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 1) gs.update(wspace=1.5, hspace=1.1) background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(75)] color_map[0] = &quot;#F2A154&quot; sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[1, 0]) ax0.set_facecolor(background_color) ax0.set(xlim=(0, 40)) ax0.set(ylim=(0, 11)) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0 = sns.barplot(ax=ax0, y=temp[&#39;language&#39;], x=temp[&#39;pct&#39;], zorder=2, linewidth=0, orient=&#39;h&#39;, saturation=0.9, alpha=0.7) ax0.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#CDD0CB&#39;, linewidth=0.3, alpha=0.5) ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#E8EAE6&#39;, linewidth=10.5, alpha=0.7) #format axis ax0.set_xlabel(&quot;Percentage&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.set_ylabel(&quot;Programming Language&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.tick_params(labelsize=3, width=0.3, length=1) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1-1.5, &#39;Programming Language&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1-1, &#39;What programming languages do you use on a regular basis?&#39;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) # data label i=0 for p in ax0.patches: value = f&#39;{p.get_width():.1f}%&#39; if i&lt;7: x1 = -1.2 else: x1 = 1.2 i+=1 x = p.get_x() + p.get_width() + x1 y = p.get_y() + p.get_height() / 2 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2.5, weight=&#39;semibold&#39;) plt.show() . temp = pd.DataFrame(.groupby(&#39;Q5&#39;, dropna=False).size()).reset_index() temp.columns = [&#39;jobtitle&#39;, &#39;amount&#39;] temp = temp.query(&quot;amount&gt;1&quot;) temp = temp.query(&quot;jobtitle!=&#39;Other&#39;&quot;) temp[&#39;pct&#39;] = temp[&#39;amount&#39;]*100/temp.amount.sum() temp = temp.sort_values(&#39;amount&#39;, ascending=False).reset_index() temp = temp.drop(&#39;index&#39;, axis=1) plt.rcParams[&#39;figure.dpi&#39;] = 400 fig = plt.figure(figsize=(6, 6), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 2) gs.update(wspace=0.2, hspace=1.5) background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(14)] color_map[0] = &quot;#F2A154&quot; sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[0, 0]) ax0.set(xlim=(-1, 14)) ax0.set(ylim=(-2, 35)) ax0.set_facecolor(background_color) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0.scatter(x=temp[&#39;jobtitle&#39;], y=temp[&#39;pct&#39;], s=40, color=color_map, zorder=3, alpha=0.7, lw=0.3) ax0.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#E8EAE6&#39;, linewidth=7.2, alpha=0.7) ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#CDD0CB&#39;, linewidth=0.2, alpha=0.5) #data label y = temp[&#39;pct&#39;] z = temp[&#39;jobtitle&#39;] n = temp[&#39;pct&#39;] for i, txt in enumerate(n): txt = f&#39;{y[i]:.1f}%&#39; ax0.annotate(txt, (z[i], y[i]), ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=1.8, weight=&#39;semibold&#39;, color=&quot;#FBFBFB&quot;) #format axis ax0.set_xlabel(&quot;Job Title&quot;,fontsize=2, weight=&#39;semibold&#39;) ax0.set_ylabel(&quot;Percentage&quot;,fontsize=2, weight=&#39;semibold&#39;) ax0.tick_params(labelsize=2, width=0.3, length=1) plt.setp( ax0.xaxis.get_majorticklabels(), rotation=45, ha=&quot;right&quot; ) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1+3.5, &#39;Job Title&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1+1.5, &#39;Select the title most similar to your current role (or most recent title if retired)&#39;, fontsize=2, ha=&#39;left&#39;, va=&#39;top&#39;) plt.show() . ValueError Traceback (most recent call last) &lt;ipython-input-53-01cb5d936e59&gt; in &lt;module&gt;() 25 26 #graph &gt; 27 ax0.scatter(x=temp[&#39;jobtitle&#39;], y=temp[&#39;pct&#39;], s=40, color=color_map, zorder=3, alpha=0.7, lw=0.3) 28 ax0.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#E8EAE6&#39;, linewidth=7.2, alpha=0.7) 29 ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#CDD0CB&#39;, linewidth=0.2, alpha=0.5) /usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs) 1563 def inner(ax, *args, data=None, **kwargs): 1564 if data is None: -&gt; 1565 return func(ax, *map(sanitize_sequence, args), **kwargs) 1566 1567 bound = new_sig.bind(ax, *args, **kwargs) /usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs) 356 f&#34;%(removal)s. If any parameter follows {name!r}, they &#34; 357 f&#34;should be pass as keyword, not positionally.&#34;) --&gt; 358 return func(*args, **kwargs) 359 360 return wrapper /usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs) 4401 self._parse_scatter_color_args( 4402 c, edgecolors, kwargs, x.size, -&gt; 4403 get_next_color_func=self._get_patches_for_fill.get_next_color) 4404 4405 if plotnonfinite and colors is None: /usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py in _parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func) 4250 # NB: remember that a single color is also acceptable. 4251 # Besides *colors* will be an empty array if c == &#39;none&#39;. -&gt; 4252 raise invalid_shape_exception(len(colors), xsize) 4253 else: 4254 colors = None # use cmap, norm after collection is created ValueError: &#39;c&#39; argument has 14 elements, which is inconsistent with &#39;x&#39; and &#39;y&#39; with size 12. . # responses_data[responses_data.columns[34]][0] temp = pd.DataFrame(kor.groupby([&quot;Q6&quot;, &quot;Q2&quot;], dropna=False).size()) temp = temp.query(&quot;Q2==&#39;Man&#39; | Q2==&#39;Woman&#39;&quot;) temp = temp.reset_index() temp.columns = [&#39;experience&#39;, &#39;gender&#39;, &#39;amount&#39;] temp[&#39;pct&#39;] = temp[&#39;amount&#39;]*100/temp.amount.sum() custom_dict = {&#39;I have never written code&#39;: 0, &#39;&lt; 1 years&#39;: 1, &#39;1-3 years&#39;: 2, &#39;3-5 years&#39;: 3, &#39;5-10 years&#39;: 4, &#39;10-20 years&#39;: 5, &#39;20+ years&#39;: 6, &#39;Never&#39;: 7} #temp = temp.sort_values(by=[&#39;experience&#39;], key=lambda x: x.map(custom_dict)) temp = temp.sort_values(by=[&#39;experience&#39;], ascending=False, key=lambda x: x.map(custom_dict)).reset_index() #temp1 = temp.query(&quot;gender==&#39;Man&#39;&quot;) plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(8, 2), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(1, 2) gs.update(wspace=0.3, hspace=1) background_color = &quot;#FEFBF3&quot; color_map1 = [&quot;#90AACB&quot; for _ in range(7)] color_map1[4] = &quot;#E26A2C&quot; sns.set_palette(sns.color_palette(color_map1)) ax0 = fig.add_subplot(gs[0, 0]) ax0.set_facecolor(background_color) ax0.set(xlim=(-2, 30)) ax0.set(ylim=(-1, 7)) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph temp1 = temp.query(&quot;gender==&#39;Man&#39;&quot;) ax0.scatter(x=temp1[&#39;pct&#39;], y=temp1[&#39;experience&#39;], s=63, color=color_map1, zorder=3, linewidth=0, alpha=0.9) ax0.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#CDD0CB&#39;, linewidth=0.3, alpha=0.5) ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#E8EAE6&#39;, linewidth=8, alpha=0.7) color_map2 = [&quot;#ADC2A9&quot; for _ in range(7)] color_map2[4] = &quot;#FDA65D&quot; temp2 = temp.query(&quot;gender==&#39;Woman&#39;&quot;) ax0.scatter(x=temp2[&#39;pct&#39;], y=temp2[&#39;experience&#39;], s=60, color=color_map2, zorder=3, linewidth=0.3, alpha=0.7) #data label y = temp[&#39;experience&#39;] z = temp[&#39;pct&#39;] n = temp[&#39;pct&#39;] for i, txt in enumerate(n): label = f&#39;{txt:.1f}%&#39; ax0.annotate(label, (z[i], y[i]), ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2, weight=&#39;semibold&#39;) #red_patch = mpatches.Patch(color=&#39;#90AACB&#39;, label=&#39;Man&#39;) #blue_patch = mpatches.Patch(color=&#39;#ADC2A9&#39;, label=&#39;Woman&#39;) #ax0.legend(handles=[red_patch, blue_patch], loc=&#39;upper right&#39;, frameon=False, fontsize=3, ncol=2) #format axis ax0.set_ylabel(&quot;Writting Code Experience&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.tick_params(labelsize=3, width=0.3, length=1) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1+1.2, &#39;Writting Code Experience&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1+0.8, &quot;For how many years have you been writing code and/or programming?&quot;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) ax0.text(x0+15, y0-1.5, &quot;Percentage&quot;, fontsize=3, ha=&#39;center&#39;, va=&#39;center&#39;, weight=&#39;bold&#39;) plt.show() . ValueError Traceback (most recent call last) &lt;ipython-input-54-42e46c6aa592&gt; in &lt;module&gt;() 38 color_map2[4] = &#34;#FDA65D&#34; 39 temp2 = temp.query(&#34;gender==&#39;Woman&#39;&#34;) &gt; 40 ax0.scatter(x=temp2[&#39;pct&#39;], y=temp2[&#39;experience&#39;], s=60, color=color_map2, zorder=3, linewidth=0.3, alpha=0.7) 41 42 #data label /usr/local/lib/python3.7/dist-packages/matplotlib/__init__.py in inner(ax, data, *args, **kwargs) 1563 def inner(ax, *args, data=None, **kwargs): 1564 if data is None: -&gt; 1565 return func(ax, *map(sanitize_sequence, args), **kwargs) 1566 1567 bound = new_sig.bind(ax, *args, **kwargs) /usr/local/lib/python3.7/dist-packages/matplotlib/cbook/deprecation.py in wrapper(*args, **kwargs) 356 f&#34;%(removal)s. If any parameter follows {name!r}, they &#34; 357 f&#34;should be pass as keyword, not positionally.&#34;) --&gt; 358 return func(*args, **kwargs) 359 360 return wrapper /usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs) 4401 self._parse_scatter_color_args( 4402 c, edgecolors, kwargs, x.size, -&gt; 4403 get_next_color_func=self._get_patches_for_fill.get_next_color) 4404 4405 if plotnonfinite and colors is None: /usr/local/lib/python3.7/dist-packages/matplotlib/axes/_axes.py in _parse_scatter_color_args(c, edgecolors, kwargs, xsize, get_next_color_func) 4250 # NB: remember that a single color is also acceptable. 4251 # Besides *colors* will be an empty array if c == &#39;none&#39;. -&gt; 4252 raise invalid_shape_exception(len(colors), xsize) 4253 else: 4254 colors = None # use cmap, norm after collection is created ValueError: &#39;c&#39; argument has 7 elements, which is inconsistent with &#39;x&#39; and &#39;y&#39; with size 6. . # Which of the following hosted notebook products do you use on a regular basis? col = 34 result=pd.DataFrame() for col in range(col, col+17): choice = kor.iloc[0:1, col:col+1].to_string() question = choice[choice.find(&#39;ce -&#39;)+5:len(choice)] grouped = kor.iloc[:, -1].groupby(kor.iloc[:, col]) data = {&quot;question&quot;: question, &quot;amount&quot;: grouped.size()[0]} result = result.append(data, ignore_index=True) result = result.query(&quot;question!=&#39;None&#39; &amp; question!=&#39;Other&#39;&quot;) result[&#39;pct&#39;] = result[&#39;amount&#39;]*100/result.amount.sum() result = result.sort_values(&#39;pct&#39;, ascending=False).reset_index() result = result.drop(&#39;index&#39;, axis=1) temp=result plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(3, 9), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 1) gs.update(wspace=1.5, hspace=1.1) background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(75)] color_map[0] = &quot;#F2A154&quot; sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[1, 0]) ax0.set_facecolor(background_color) ax0.set(xlim=(0, 40)) ax0.set(ylim=(0, 11)) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0 = sns.barplot(ax=ax0, y=temp[&#39;question&#39;], x=temp[&#39;pct&#39;], zorder=2, linewidth=0, orient=&#39;h&#39;, saturation=0.9, alpha=0.7) ax0.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#CDD0CB&#39;, linewidth=0.3, alpha=0.5) ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#E8EAE6&#39;, linewidth=9, alpha=0.7) #format axis ax0.set_xlabel(&quot;Percentage&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.set_ylabel(&quot;Notebooks&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.tick_params(labelsize=3, width=0.3, length=1) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1-1.5, &#39;Data Science Notebooks&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1-1, &#39;Which of the following hosted notebook products do you use on a regular basis?&#39;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) # data label i=0 for p in ax0.patches: value = f&#39;{p.get_width():.1f}%&#39; if i&lt;7: x1 = -1.2 else: x1 = 1.2 i+=1 x = p.get_x() + p.get_width() + x1 y = p.get_y() + p.get_height() / 2 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2.5, weight=&#39;semibold&#39;) plt.show() . IndexError Traceback (most recent call last) &lt;ipython-input-55-8145acaf69d0&gt; in &lt;module&gt;() 7 question = choice[choice.find(&#39;ce -&#39;)+5:len(choice)] 8 grouped = kor.iloc[:, -1].groupby(kor.iloc[:, col]) -&gt; 9 data = {&#34;question&#34;: question, &#34;amount&#34;: grouped.size()[0]} 10 result = result.append(data, ignore_index=True) 11 /usr/local/lib/python3.7/dist-packages/pandas/core/series.py in __getitem__(self, key) 877 878 if is_integer(key) and self.index._should_fallback_to_positional(): --&gt; 879 return self._values[key] 880 881 elif key_is_scalar: IndexError: index 0 is out of bounds for axis 0 with size 0 . col = 233 result=pd.DataFrame() for col in range(col, col+10): choice = comb.iloc[0:1, col:col+1].to_string() answer = choice[choice.find(&#39;ce -&#39;)+5:len(choice)] grouped = comb.iloc[:, -1].groupby(comb.iloc[:, col]) data = {&quot;answer&quot;: answer, &quot;amount&quot;: grouped.size()[0]} result = result.append(data, ignore_index=True) total = result[&#39;amount&#39;].sum() result = result.query(&quot;answer!=&#39;No / None&#39; and answer!=&#39;None&#39; and answer!=&#39;Other&#39; and answer!=&#39;I do not share my work publicly&#39;&quot;) result[&#39;pct&#39;] = result[&#39;amount&#39;]*100/total temp = result temp = temp.sort_values(&#39;pct&#39;, ascending=False) plt.rcParams[&#39;figure.dpi&#39;] = 300 fig = plt.figure(figsize=(3, 8), facecolor=&#39;#FEFBF3&#39;) gs = fig.add_gridspec(2, 1) gs.update(wspace=1.5, hspace=1.1) background_color = &quot;#FEFBF3&quot; color_map = [&quot;#90AACB&quot; for _ in range(75)] color_map[0] = &quot;#F2A154&quot; sns.set_palette(sns.color_palette(color_map)) ax0 = fig.add_subplot(gs[1, 0]) ax0.set_facecolor(background_color) ax0.set(xlim=(0, 40)) ax0.set(ylim=(0, 11)) for s in [&quot;right&quot;, &quot;top&quot;]: ax0.spines[s].set_visible(False) #graph ax0 = sns.barplot(ax=ax0, y=temp[&#39;answer&#39;], x=temp[&#39;pct&#39;], zorder=2, linewidth=0, orient=&#39;h&#39;, saturation=0.9, alpha=0.7) ax0.grid(which=&#39;major&#39;, axis=&#39;x&#39;, zorder=0, color=&#39;#CDD0CB&#39;, linewidth=0.3, alpha=0.5) ax0.grid(which=&#39;major&#39;, axis=&#39;y&#39;, zorder=0, color=&#39;#E8EAE6&#39;, linewidth=14, alpha=0.7) #format axis ax0.set_xlabel(&quot;Percentage&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.set_ylabel(&quot;Platforms&quot;,fontsize=3, weight=&#39;semibold&#39;) ax0.tick_params(labelsize=3, width=0.3, length=1) #title x0, x1 = ax0.get_xlim() y0, y1 = ax0.get_ylim() ax0.text(x0, y1-1, &#39;Share EDA&#39;, fontsize=4, ha=&#39;left&#39;, va=&#39;top&#39;, weight=&#39;bold&#39;) ax0.text(x0, y1-0.6, &#39;Where do you publicly share your data analysis or machine learning applications?&#39;, fontsize=3, ha=&#39;left&#39;, va=&#39;top&#39;) # data label i=0 for p in ax0.patches: value = f&#39;{p.get_width():.1f}%&#39; if i&lt;5: x1 = -1.2 else: x1 = 1.2 i+=1 x = p.get_x() + p.get_width() + x1 y = p.get_y() + p.get_height() / 2 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2.5, weight=&#39;semibold&#39;) x_format = ticker.PercentFormatter() ax0.xaxis.set_major_formatter(x_format) plt.show() . NameError Traceback (most recent call last) &lt;ipython-input-48-270b0ee08f3a&gt; in &lt;module&gt;() 64 ax0.text(x, y, value, ha=&#39;center&#39;, va=&#39;center&#39;, fontsize=2.5, weight=&#39;semibold&#39;) 65 &gt; 66 x_format = ticker.PercentFormatter() 67 ax0.xaxis.set_major_formatter(x_format) 68 NameError: name &#39;ticker&#39; is not defined .",
            "url": "https://raukrauk.github.io/ML-DL/ssuda/eda/2021/12/19/Kaggle_Machine_Learning_&_Data_Science_Survey.html",
            "relUrl": "/ssuda/eda/2021/12/19/Kaggle_Machine_Learning_&_Data_Science_Survey.html",
            "date": " • Dec 19, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "[Kaggle] Otto Group",
            "content": "Data fields . id - an anonymous id unique to a product | feat_1, feat_2, ..., feat_93 - the various features of a product | target - the class of a product | . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . from google.colab import files files.upload() !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 /root/.kaggle/kaggle.json . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . !kaggle competitions download -c otto-group-product-classification-challenge . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sampleSubmission.csv.zip to /content 0% 0.00/369k [00:00&lt;?, ?B/s] 100% 369k/369k [00:00&lt;00:00, 51.7MB/s] Downloading test.csv.zip to /content 0% 0.00/4.00M [00:00&lt;?, ?B/s] 100% 4.00M/4.00M [00:00&lt;00:00, 65.4MB/s] Downloading train.csv.zip to /content 0% 0.00/1.69M [00:00&lt;?, ?B/s] 100% 1.69M/1.69M [00:00&lt;00:00, 53.9MB/s] . !unzip sampleSubmission.csv.zip !unzip test.csv.zip !unzip train.csv.zip . Archive: sampleSubmission.csv.zip inflating: sampleSubmission.csv Archive: test.csv.zip inflating: test.csv Archive: train.csv.zip inflating: train.csv . CODING . import matplotlib.pyplot as plt import seaborn as sns import numpy as np import pandas as pd . train = pd.read_csv(&#39;train.csv&#39;) train.head(7) . id feat_1 feat_2 feat_3 feat_4 feat_5 feat_6 feat_7 feat_8 feat_9 feat_10 feat_11 feat_12 feat_13 feat_14 feat_15 feat_16 feat_17 feat_18 feat_19 feat_20 feat_21 feat_22 feat_23 feat_24 feat_25 feat_26 feat_27 feat_28 feat_29 feat_30 feat_31 feat_32 feat_33 feat_34 feat_35 feat_36 feat_37 feat_38 feat_39 ... feat_55 feat_56 feat_57 feat_58 feat_59 feat_60 feat_61 feat_62 feat_63 feat_64 feat_65 feat_66 feat_67 feat_68 feat_69 feat_70 feat_71 feat_72 feat_73 feat_74 feat_75 feat_76 feat_77 feat_78 feat_79 feat_80 feat_81 feat_82 feat_83 feat_84 feat_85 feat_86 feat_87 feat_88 feat_89 feat_90 feat_91 feat_92 feat_93 target . 0 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | 4 | 1 | 1 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 2 | 0 | 0 | 11 | 0 | 1 | 1 | 0 | 1 | 0 | 7 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 1 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | ... | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 2 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 3 4 | 1 | 0 | 0 | 1 | 6 | 1 | 5 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 2 | 2 | 0 | 0 | 0 | 58 | 0 | 10 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | ... | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 1 | 5 | 0 | 0 | 4 | 0 | 0 | 2 | 1 | 0 | 1 | 0 | 0 | 1 | 1 | 2 | 2 | 0 | 22 | 0 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | Class_1 | . 4 5 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | Class_1 | . 5 6 | 2 | 1 | 0 | 0 | 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 6 | 0 | 0 | 2 | 0 | 0 | 0 | 5 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 5 | 0 | 1 | 0 | 1 | 4 | 2 | 6 | 0 | 2 | 4 | 2 | 0 | 0 | 1 | 0 | 2 | 0 | 4 | 3 | 0 | 0 | 0 | 0 | 1 | 0 | 3 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | Class_1 | . 6 7 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 2 | ... | 2 | 0 | 0 | 0 | 0 | 2 | 0 | 1 | 0 | 3 | 1 | 0 | 1 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | Class_1 | . 7 rows × 95 columns . test = pd.read_csv(&#39;test.csv&#39;) test.head(7) . id feat_1 feat_2 feat_3 feat_4 feat_5 feat_6 feat_7 feat_8 feat_9 feat_10 feat_11 feat_12 feat_13 feat_14 feat_15 feat_16 feat_17 feat_18 feat_19 feat_20 feat_21 feat_22 feat_23 feat_24 feat_25 feat_26 feat_27 feat_28 feat_29 feat_30 feat_31 feat_32 feat_33 feat_34 feat_35 feat_36 feat_37 feat_38 feat_39 ... feat_54 feat_55 feat_56 feat_57 feat_58 feat_59 feat_60 feat_61 feat_62 feat_63 feat_64 feat_65 feat_66 feat_67 feat_68 feat_69 feat_70 feat_71 feat_72 feat_73 feat_74 feat_75 feat_76 feat_77 feat_78 feat_79 feat_80 feat_81 feat_82 feat_83 feat_84 feat_85 feat_86 feat_87 feat_88 feat_89 feat_90 feat_91 feat_92 feat_93 . 0 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 3 | 2 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 3 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 1 | 20 | 0 | 0 | 0 | 0 | 0 | . 1 2 | 2 | 2 | 14 | 16 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 2 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 4 | 0 | 4 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 8 | 0 | 0 | 0 | 0 | ... | 24 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 4 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 2 | 0 | . 2 3 | 0 | 1 | 12 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 1 | 0 | 0 | 0 | 7 | 0 | 2 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 1 | 1 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 2 | 0 | ... | 10 | 1 | 0 | 2 | 0 | 0 | 1 | 6 | 1 | 1 | 0 | 0 | 1 | 1 | 1 | 2 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 0 | 2 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 1 | . 3 4 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 21 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 1 | 0 | ... | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 0 | 0 | 2 | 0 | 0 | 0 | 0 | 6 | 0 | 8 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 3 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 5 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 2 | 0 | 3 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 0 | 0 | 0 | 0 | 3 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 0 | 0 | 0 | 0 | 0 | 4 | 0 | 5 | 16 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 0 | 0 | . 5 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 17 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 4 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 0 | 0 | 7 | 0 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | . 6 7 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 | 0 | 0 | 1 | 0 | 4 | 0 | 1 | 0 | 4 | 0 | 2 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 2 | 0 | 2 | 1 | 1 | ... | 1 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 2 | 0 | 0 | 2 | 0 | 0 | 0 | 2 | 2 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 2 | 0 | 3 | 0 | 0 | 1 | 0 | 18 | 1 | 0 | 0 | . 7 rows × 94 columns . sns.countplot(x = train.target) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f06e3eb6790&gt; . class_to_order = dict() order_to_class = dict() for idx, col in enumerate(train.target.unique()): order_to_class[idx] = col class_to_order[col] = idx train[&quot;target_ord&quot;] = train[&quot;target&quot;].map(class_to_order).astype(&quot;int16&quot;) feature_columns = [col for col in train.columns if col.startswith(&quot;feat_&quot;)] target_column = [&quot;target_ord&quot;] . order_to_class . {0: &#39;Class_1&#39;, 1: &#39;Class_2&#39;, 2: &#39;Class_3&#39;, 3: &#39;Class_4&#39;, 4: &#39;Class_5&#39;, 5: &#39;Class_6&#39;, 6: &#39;Class_7&#39;, 7: &#39;Class_8&#39;, 8: &#39;Class_9&#39;} . class_to_order . {&#39;Class_1&#39;: 0, &#39;Class_2&#39;: 1, &#39;Class_3&#39;: 2, &#39;Class_4&#39;: 3, &#39;Class_5&#39;: 4, &#39;Class_6&#39;: 5, &#39;Class_7&#39;: 6, &#39;Class_8&#39;: 7, &#39;Class_9&#39;: 8} . Skewness . skew = [] for i in train[feature_columns].columns: skew.append(train[str(i)].skew()) skew_df = pd.DataFrame({&#39;Feature&#39;: train[feature_columns].columns, &#39;Skewness&#39;: skew}) skew_df.plot(kind=&#39;bar&#39;,figsize=(18,10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fe6009a9850&gt; . Quantile Transformer . We are now going to apply the QuantileTransformer from scikit-learn. I first used StandardScaler but found that there was no change in the skew value of the features. | QuantileTransformer는 각 형상의 확률 밀도 함수가 균등 또는 가우스 분포에 매핑되도록 비선형 변환을 적용한다. 이 경우, 특이치를 포함한 모든 데이터는 [0,1] 범위의 균등 분포에 매핑되어 특이치와 inlier를 구분할 수 없게 됩니다. | 기본적으로 1000개 분위를 사용하여 데이터를 &#39;균등분포&#39; 시킵니다.Robust처럼 이상치에 민감X, 0~1사이로 압축합니다. -&gt; 이후 output_distribution=&#39;normal&#39;을 적용하면 정규분포형태의 값으로 나타난다. . | RobustScaler와 QuantileTransformer는 훈련 세트에서 특이치를 추가하거나 제거하면 거의 동일한 변환을 얻을 수 있다는 점에서 특이치에 강하다. 그러나 RobustScaler와 반대로 QuantileTransformer는 또한 이상치를 사전 정의된 범위 경계(0 및 1)로 설정하여 자동으로 축소한다. 이로 인해 극단값의 포화 아티팩트가 발생할 수 있습니다. . | . from sklearn.preprocessing import QuantileTransformer train[feature_columns] = QuantileTransformer(copy=False, output_distribution=&#39;normal&#39;).fit_transform(train[feature_columns]) test[feature_columns] = QuantileTransformer(copy=False, output_distribution=&#39;normal&#39;).fit_transform(test[feature_columns]) . skew = [] for i in train[feature_columns].columns: skew.append(train[str(i)].skew()) skew_df = pd.DataFrame({&#39;Feature&#39;: train[feature_columns].columns, &#39;Skewness&#39;: skew}) skew_df.plot(kind=&#39;bar&#39;,figsize=(18,10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f966e19c9d0&gt; . skew_feats = train[feature_columns].skew().sort_values(ascending=False) skewness = pd.DataFrame({&#39;Skew&#39;: skew_feats}) skewness = skewness[abs(skewness) &gt; 3.75].dropna() skewed_features = skewness.index.values.tolist() skewed_features . [&#39;feat_6&#39;, &#39;feat_84&#39;, &#39;feat_51&#39;, &#39;feat_5&#39;, &#39;feat_81&#39;, &#39;feat_77&#39;] . train_new = train.drop(skewed_features, axis = 1) train_new . id feat_1 feat_2 feat_3 feat_4 feat_7 feat_8 feat_9 feat_10 feat_11 feat_12 feat_13 feat_14 feat_15 feat_16 feat_17 feat_18 feat_19 feat_20 feat_21 feat_22 feat_23 feat_24 feat_25 feat_26 feat_27 feat_28 feat_29 feat_30 feat_31 feat_32 feat_33 feat_34 feat_35 feat_36 feat_37 feat_38 feat_39 feat_40 feat_41 ... feat_53 feat_54 feat_55 feat_56 feat_57 feat_58 feat_59 feat_60 feat_61 feat_62 feat_63 feat_64 feat_65 feat_66 feat_67 feat_68 feat_69 feat_70 feat_71 feat_72 feat_73 feat_74 feat_75 feat_76 feat_78 feat_79 feat_80 feat_82 feat_83 feat_85 feat_86 feat_87 feat_88 feat_89 feat_90 feat_91 feat_92 feat_93 target target_ord . 0 1 | 1.174387 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.779924 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.644369 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.651824 | -5.199338 | 0.833781 | 0.152391 | 1.004131 | -5.199338 | -5.199338 | 1.934489 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.983573 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.304482 | -5.199338 | ... | -5.199338 | 0.287442 | -5.199338 | -5.199338 | 1.649237 | -5.199338 | -5.199338 | 1.780005 | -5.199338 | 0.434861 | 1.541796 | -5.199338 | 1.312981 | -5.199338 | 1.199766 | -5.199338 | -5.199338 | -5.199338 | 1.207532 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.721914 | 0.969427 | -5.199338 | -5.199338 | 1.027247 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | Class_1 | 0 | . 1 2 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.869846 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.310017 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.223287 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.266860 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.642543 | -5.199338 | -5.199338 | -0.084155 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.615945 | 1.072973 | -5.199338 | 0.997920 | 1.467860 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | Class_1 | 0 | . 2 3 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.869846 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.202347 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.644086 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | 1.157061 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.046576 | -5.199338 | -5.199338 | 1.027247 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | Class_1 | 0 | . 3 4 | 1.174387 | -5.199338 | -5.199338 | 0.959440 | 2.432017 | -5.199338 | -5.199338 | 1.355761 | 0.779924 | -5.199338 | 1.128144 | -5.199338 | -5.199338 | 0.253606 | 1.202347 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.295363 | 0.636387 | 1.340200 | -5.199338 | -5.199338 | -5.199338 | 3.365547 | -5.199338 | 2.325972 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.634747 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | 0.803958 | 1.157061 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.926403 | 0.819656 | 0.888309 | -5.199338 | -5.199338 | 1.562721 | -5.199338 | -5.199338 | 1.615945 | 1.072973 | -5.199338 | 0.997920 | -5.199338 | 1.365257 | 0.969427 | 1.851734 | -5.199338 | -5.199338 | 0.485069 | 1.644369 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | Class_1 | 0 | . 4 5 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.074445 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -0.118204 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.192071 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.434861 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.275302 | -5.199338 | -5.199338 | 2.004234 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.096521 | -5.199338 | -5.199338 | -5.199338 | 1.027247 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.255743 | -5.199338 | -5.199338 | -5.199338 | Class_1 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 61873 61874 | 1.174387 | -5.199338 | -5.199338 | 0.959440 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.529558 | 1.824556 | -5.199338 | -5.199338 | 2.144011 | -5.199338 | 1.310017 | -5.199338 | -5.199338 | -5.199338 | 2.386340 | -5.199338 | 0.610484 | 1.602210 | 1.004131 | -5.199338 | -5.199338 | 3.718764 | 1.664082 | -5.199338 | 1.337127 | 1.554270 | 0.912774 | 0.983573 | 0.893908 | 1.780005 | 0.884593 | -5.199338 | 0.703922 | 1.786156 | ... | 2.456904 | 1.340200 | 2.023292 | 1.421556 | -5.199338 | -5.199338 | -5.199338 | 0.897655 | -5.199338 | 2.947533 | -5.199338 | 0.642543 | 2.611712 | 2.877846 | 0.496390 | -5.199338 | -5.199338 | 1.324958 | -5.199338 | 0.800496 | 1.166919 | 1.475269 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.027247 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.575613 | -5.199338 | Class_9 | 8 | . 61874 61875 | 1.994971 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.951050 | 0.683178 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.636387 | -5.199338 | -5.199338 | -5.199338 | 1.425011 | -5.199338 | -5.199338 | -5.199338 | 1.171890 | 0.912774 | -5.199338 | 1.327982 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.192071 | ... | 1.700222 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.434861 | 2.096521 | -5.199338 | -5.199338 | 0.819656 | 0.888309 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.575483 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.953493 | -5.199338 | -5.199338 | 1.497997 | -5.199338 | -5.199338 | 1.035790 | -5.199338 | Class_9 | 8 | . 61875 61876 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.639539 | 1.033647 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.349483 | -5.199338 | -5.199338 | -5.199338 | 0.814401 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -0.118204 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.554270 | -5.199338 | -5.199338 | 1.327982 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.597412 | -5.199338 | -5.199338 | 1.435477 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.063775 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.228607 | -5.199338 | -5.199338 | -5.199338 | 1.381364 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.275302 | 1.121060 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | Class_9 | 8 | . 61876 61877 | 1.174387 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.955471 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.303168 | 0.152391 | -5.199338 | -5.199338 | 1.337127 | 1.934489 | -5.199338 | -5.199338 | 0.965420 | 0.644086 | -5.199338 | -5.199338 | 1.902873 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.192071 | ... | 1.700222 | 0.287442 | -5.199338 | -5.199338 | 1.244778 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.882739 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.046576 | -5.199338 | -5.199338 | -5.199338 | 1.207532 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.014571 | -5.199338 | 2.169694 | 3.121095 | -5.199338 | Class_9 | 8 | . 61877 61878 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.824556 | -5.199338 | 0.746351 | 0.253606 | 1.202347 | 0.814401 | -5.199338 | -5.199338 | -5.199338 | 1.471555 | -5.199338 | 0.303168 | 0.152391 | -5.199338 | -5.199338 | -5.199338 | 2.635514 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.327982 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.144011 | ... | 1.700222 | 1.104747 | 1.157061 | -5.199338 | 1.244778 | -5.199338 | -5.199338 | 1.387908 | -5.199338 | 1.435477 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.566990 | 1.644369 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.773922 | 1.037937 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.280981 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.575613 | -5.199338 | Class_9 | 8 | . 61878 rows × 90 columns . Train_test &#48516;&#47532; . from sklearn.model_selection import train_test_split from sklearn.metrics import log_loss X_train, X_valid, y_train, y_valid = train_test_split( train_new.drop([&#39;id&#39;, &#39;target&#39;, &#39;target_ord&#39;], axis = 1), train_new[target_column], test_size = 0.275, random_state = 7, stratify = train_new[target_column] ) . KNN . from sklearn.neighbors import KNeighborsClassifier knc = KNeighborsClassifier(n_neighbors = 25, weights = &#39;distance&#39;) knc.fit(X_train, y_train) yhat = knc.predict(X_valid) . /usr/local/lib/python3.7/dist-packages/sklearn/neighbors/_classification.py:198: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel(). return self._fit(X, y) . from sklearn.metrics import classification_report, confusion_matrix result = confusion_matrix(y_valid, yhat) print(&quot;Confusion Matrix:&quot;) print(result) . Confusion Matrix: [[ 120 50 5 0 45 44 3 103 160] [ 2 3891 461 11 36 9 10 8 6] [ 0 1234 928 11 3 6 7 10 2] [ 1 452 129 123 10 14 5 2 4] [ 0 29 3 0 719 1 0 0 1] [ 11 124 12 5 48 3514 21 72 80] [ 11 195 95 1 33 59 320 55 12] [ 14 113 8 0 54 152 7 1913 67] [ 8 129 7 0 39 47 5 45 1083]] . result1 = classification_report(y_valid, yhat) print(&quot;Classification Report:&quot;) print(result1) . Classification Report: precision recall f1-score support 0 0.72 0.23 0.34 530 1 0.63 0.88 0.73 4434 2 0.56 0.42 0.48 2201 3 0.81 0.17 0.28 740 4 0.73 0.95 0.83 753 5 0.91 0.90 0.91 3887 6 0.85 0.41 0.55 781 7 0.87 0.82 0.84 2328 8 0.77 0.79 0.78 1363 accuracy 0.74 17017 macro avg 0.76 0.62 0.64 17017 weighted avg 0.75 0.74 0.72 17017 . yhat_KNN = knc.predict_proba(X_valid) logloss_KNN = log_loss(y_valid, yhat_KNN) print(&#39;Log loss using KNN classifier:&#39;, logloss_KNN) . Log loss using KNN classifier: 1.2113082274705085 . Support Vector Machine . from sklearn import svm svm = svm.SVC(kernel = &#39;rbf&#39;, probability = True, random_state = 7) svm.fit(X_train, y_train) yhat = svm.predict(X_valid) yhat_svm = svm.predict_proba(X_valid) logloss_svm = log_loss(y_valid, yhat_svm) print(&#39;Logloss using Support Vector Machines:&#39;, logloss_svm) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py:985: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) . cnf_matrix = confusion_matrix(y_valid, yhat, labels = train_new.target_ord.unique().tolist()) plt.figure() plot_confusion_matrix(cnf_matrix, classes = train_new.target.unique().tolist()) . print(&#39;Classification Report:&#39;) print(classification_report(y_valid, yhat)) . Catboost . pip install catboost . Collecting catboost Downloading catboost-1.0.3-cp37-none-manylinux1_x86_64.whl (76.3 MB) |████████████████████████████████| 76.3 MB 1.4 MB/s Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (3.0.6) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (1.3.3) Installing collected packages: catboost Successfully installed catboost-1.0.3 . from catboost import CatBoostClassifier . CBC_params = { &#39;iterations&#39;: 5000, &#39;od_wait&#39;: 250, &#39;use_best_model&#39;: True, &#39;loss_function&#39;: &#39;MultiClass&#39;, &#39;eval_metric&#39;: &#39;MultiClass&#39;, &#39;leaf_estimation_method&#39;: &#39;Newton&#39;, &#39;bootstrap_type&#39;: &#39;Bernoulli&#39;, &#39;subsample&#39;: 0.5, &#39;learning_rate&#39;: 0.08, &#39;l2_leaf_reg&#39;: 0.4, #L2 Regularization &#39;random_strength&#39;: 10, #amount of randomness to use for scoring splits when tree structure is selected &#39;depth&#39;: 7, #Tree depth &#39;min_data_in_leaf&#39;: 3, #minimum number of training samples in a leaf &#39;leaf_estimation_iterations&#39;: 4, #Earlier = 7 &#39;task_type&#39;: &#39;GPU&#39;, &#39;border_count&#39;: 128, #Number of splits for numerical features } . cbc = CatBoostClassifier(**CBC_params) cbc.fit(X_train, y_train, eval_set = [(X_valid, y_valid)], early_stopping_rounds = 100, ) . 0: learn: 2.0121432 test: 2.0116864 best: 2.0116864 (0) total: 56.9ms remaining: 4m 44s 1: learn: 1.8642747 test: 1.8644735 best: 1.8644735 (1) total: 98.4ms remaining: 4m 5s 2: learn: 1.7692236 test: 1.7704969 best: 1.7704969 (2) total: 146ms remaining: 4m 2s 3: learn: 1.6593988 test: 1.6598242 best: 1.6598242 (3) total: 185ms remaining: 3m 51s 4: learn: 1.5631640 test: 1.5640647 best: 1.5640647 (4) total: 224ms remaining: 3m 43s 5: learn: 1.4930563 test: 1.4943294 best: 1.4943294 (5) total: 264ms remaining: 3m 39s 6: learn: 1.4217600 test: 1.4245219 best: 1.4245219 (6) total: 303ms remaining: 3m 36s 7: learn: 1.3784399 test: 1.3820308 best: 1.3820308 (7) total: 343ms remaining: 3m 33s 8: learn: 1.3193321 test: 1.3238905 best: 1.3238905 (8) total: 379ms remaining: 3m 30s 9: learn: 1.2804642 test: 1.2863043 best: 1.2863043 (9) total: 426ms remaining: 3m 32s 10: learn: 1.2379976 test: 1.2440289 best: 1.2440289 (10) total: 471ms remaining: 3m 33s 11: learn: 1.2071237 test: 1.2131275 best: 1.2131275 (11) total: 506ms remaining: 3m 30s 12: learn: 1.1747567 test: 1.1818489 best: 1.1818489 (12) total: 544ms remaining: 3m 28s 13: learn: 1.1476924 test: 1.1557281 best: 1.1557281 (13) total: 579ms remaining: 3m 26s 14: learn: 1.1191370 test: 1.1278348 best: 1.1278348 (14) total: 617ms remaining: 3m 24s 15: learn: 1.0911632 test: 1.1003257 best: 1.1003257 (15) total: 647ms remaining: 3m 21s 16: learn: 1.0701273 test: 1.0794967 best: 1.0794967 (16) total: 692ms remaining: 3m 22s 17: learn: 1.0470832 test: 1.0564591 best: 1.0564591 (17) total: 731ms remaining: 3m 22s 18: learn: 1.0294548 test: 1.0395015 best: 1.0395015 (18) total: 765ms remaining: 3m 20s 19: learn: 1.0122759 test: 1.0230065 best: 1.0230065 (19) total: 800ms remaining: 3m 19s 20: learn: 0.9965889 test: 1.0079739 best: 1.0079739 (20) total: 833ms remaining: 3m 17s 21: learn: 0.9800706 test: 0.9917076 best: 0.9917076 (21) total: 866ms remaining: 3m 16s 22: learn: 0.9685396 test: 0.9801578 best: 0.9801578 (22) total: 904ms remaining: 3m 15s 23: learn: 0.9513502 test: 0.9638739 best: 0.9638739 (23) total: 940ms remaining: 3m 14s 24: learn: 0.9392962 test: 0.9522362 best: 0.9522362 (24) total: 973ms remaining: 3m 13s 25: learn: 0.9281457 test: 0.9418182 best: 0.9418182 (25) total: 1.01s remaining: 3m 12s 26: learn: 0.9160768 test: 0.9304367 best: 0.9304367 (26) total: 1.05s remaining: 3m 12s 27: learn: 0.9070015 test: 0.9216445 best: 0.9216445 (27) total: 1.08s remaining: 3m 11s 28: learn: 0.8991063 test: 0.9141311 best: 0.9141311 (28) total: 1.11s remaining: 3m 11s 29: learn: 0.8885228 test: 0.9039827 best: 0.9039827 (29) total: 1.16s remaining: 3m 12s 30: learn: 0.8760427 test: 0.8922666 best: 0.8922666 (30) total: 1.2s remaining: 3m 12s 31: learn: 0.8686766 test: 0.8850108 best: 0.8850108 (31) total: 1.23s remaining: 3m 11s 32: learn: 0.8595038 test: 0.8759866 best: 0.8759866 (32) total: 1.27s remaining: 3m 10s 33: learn: 0.8506861 test: 0.8677174 best: 0.8677174 (33) total: 1.32s remaining: 3m 12s 34: learn: 0.8429729 test: 0.8599030 best: 0.8599030 (34) total: 1.35s remaining: 3m 12s 35: learn: 0.8337739 test: 0.8511419 best: 0.8511419 (35) total: 1.39s remaining: 3m 11s 36: learn: 0.8272128 test: 0.8445066 best: 0.8445066 (36) total: 1.42s remaining: 3m 10s 37: learn: 0.8218700 test: 0.8394683 best: 0.8394683 (37) total: 1.45s remaining: 3m 9s 38: learn: 0.8136234 test: 0.8318700 best: 0.8318700 (38) total: 1.49s remaining: 3m 9s 39: learn: 0.8069180 test: 0.8253550 best: 0.8253550 (39) total: 1.53s remaining: 3m 9s 40: learn: 0.7992706 test: 0.8181263 best: 0.8181263 (40) total: 1.56s remaining: 3m 8s 41: learn: 0.7920182 test: 0.8114673 best: 0.8114673 (41) total: 1.59s remaining: 3m 8s 42: learn: 0.7863240 test: 0.8055931 best: 0.8055931 (42) total: 1.62s remaining: 3m 7s 43: learn: 0.7801634 test: 0.8000369 best: 0.8000369 (43) total: 1.66s remaining: 3m 6s 44: learn: 0.7733525 test: 0.7940273 best: 0.7940273 (44) total: 1.69s remaining: 3m 6s 45: learn: 0.7694524 test: 0.7906393 best: 0.7906393 (45) total: 1.73s remaining: 3m 5s 46: learn: 0.7649377 test: 0.7865212 best: 0.7865212 (46) total: 1.76s remaining: 3m 5s 47: learn: 0.7603833 test: 0.7825274 best: 0.7825274 (47) total: 1.79s remaining: 3m 4s 48: learn: 0.7559769 test: 0.7786019 best: 0.7786019 (48) total: 1.82s remaining: 3m 4s 49: learn: 0.7497580 test: 0.7727930 best: 0.7727930 (49) total: 1.86s remaining: 3m 4s 50: learn: 0.7434602 test: 0.7671380 best: 0.7671380 (50) total: 1.9s remaining: 3m 4s 51: learn: 0.7400290 test: 0.7640101 best: 0.7640101 (51) total: 1.94s remaining: 3m 4s 52: learn: 0.7344471 test: 0.7584137 best: 0.7584137 (52) total: 1.97s remaining: 3m 4s 53: learn: 0.7341302 test: 0.7581168 best: 0.7581168 (53) total: 1.98s remaining: 3m 1s 54: learn: 0.7289813 test: 0.7536093 best: 0.7536093 (54) total: 2.02s remaining: 3m 1s 55: learn: 0.7253654 test: 0.7502907 best: 0.7502907 (55) total: 2.05s remaining: 3m 56: learn: 0.7209865 test: 0.7463499 best: 0.7463499 (56) total: 2.08s remaining: 3m 57: learn: 0.7166652 test: 0.7421067 best: 0.7421067 (57) total: 2.11s remaining: 2m 59s 58: learn: 0.7130299 test: 0.7390447 best: 0.7390447 (58) total: 2.17s remaining: 3m 1s 59: learn: 0.7097933 test: 0.7362404 best: 0.7362404 (59) total: 2.2s remaining: 3m 1s 60: learn: 0.7065591 test: 0.7335150 best: 0.7335150 (60) total: 2.24s remaining: 3m 1s 61: learn: 0.7023363 test: 0.7299950 best: 0.7299950 (61) total: 2.27s remaining: 3m 62: learn: 0.6998706 test: 0.7276943 best: 0.7276943 (62) total: 2.31s remaining: 3m 1s 63: learn: 0.6969317 test: 0.7251061 best: 0.7251061 (63) total: 2.34s remaining: 3m 64: learn: 0.6948621 test: 0.7231549 best: 0.7231549 (64) total: 2.38s remaining: 3m 65: learn: 0.6908905 test: 0.7197813 best: 0.7197813 (65) total: 2.42s remaining: 3m 66: learn: 0.6880602 test: 0.7172392 best: 0.7172392 (66) total: 2.45s remaining: 3m 67: learn: 0.6833511 test: 0.7132968 best: 0.7132968 (67) total: 2.49s remaining: 3m 68: learn: 0.6809719 test: 0.7111947 best: 0.7111947 (68) total: 2.52s remaining: 3m 69: learn: 0.6782949 test: 0.7089711 best: 0.7089711 (69) total: 2.55s remaining: 2m 59s 70: learn: 0.6757091 test: 0.7064934 best: 0.7064934 (70) total: 2.58s remaining: 2m 59s 71: learn: 0.6707557 test: 0.7024831 best: 0.7024831 (71) total: 2.63s remaining: 2m 59s 72: learn: 0.6671530 test: 0.6990870 best: 0.6990870 (72) total: 2.66s remaining: 2m 59s 73: learn: 0.6638417 test: 0.6962883 best: 0.6962883 (73) total: 2.7s remaining: 2m 59s 74: learn: 0.6596053 test: 0.6923730 best: 0.6923730 (74) total: 2.73s remaining: 2m 59s 75: learn: 0.6572036 test: 0.6904115 best: 0.6904115 (75) total: 2.77s remaining: 2m 59s 76: learn: 0.6546028 test: 0.6881029 best: 0.6881029 (76) total: 2.81s remaining: 2m 59s 77: learn: 0.6542434 test: 0.6877023 best: 0.6877023 (77) total: 2.82s remaining: 2m 57s 78: learn: 0.6516202 test: 0.6854925 best: 0.6854925 (78) total: 2.85s remaining: 2m 57s 79: learn: 0.6497794 test: 0.6839601 best: 0.6839601 (79) total: 2.89s remaining: 2m 57s 80: learn: 0.6467890 test: 0.6811995 best: 0.6811995 (80) total: 2.93s remaining: 2m 57s 81: learn: 0.6442167 test: 0.6792093 best: 0.6792093 (81) total: 2.96s remaining: 2m 57s 82: learn: 0.6421361 test: 0.6773587 best: 0.6773587 (82) total: 3s remaining: 2m 57s 83: learn: 0.6421276 test: 0.6773451 best: 0.6773451 (83) total: 3.01s remaining: 2m 56s 84: learn: 0.6396174 test: 0.6749977 best: 0.6749977 (84) total: 3.04s remaining: 2m 55s 85: learn: 0.6372447 test: 0.6732176 best: 0.6732176 (85) total: 3.08s remaining: 2m 55s 86: learn: 0.6348631 test: 0.6710037 best: 0.6710037 (86) total: 3.11s remaining: 2m 55s 87: learn: 0.6320489 test: 0.6689277 best: 0.6689277 (87) total: 3.16s remaining: 2m 56s 88: learn: 0.6295096 test: 0.6667155 best: 0.6667155 (88) total: 3.2s remaining: 2m 56s 89: learn: 0.6272812 test: 0.6651545 best: 0.6651545 (89) total: 3.23s remaining: 2m 56s 90: learn: 0.6256676 test: 0.6635536 best: 0.6635536 (90) total: 3.26s remaining: 2m 56s 91: learn: 0.6239243 test: 0.6619434 best: 0.6619434 (91) total: 3.3s remaining: 2m 56s 92: learn: 0.6236777 test: 0.6616300 best: 0.6616300 (92) total: 3.31s remaining: 2m 54s 93: learn: 0.6221258 test: 0.6603529 best: 0.6603529 (93) total: 3.35s remaining: 2m 54s 94: learn: 0.6197131 test: 0.6582380 best: 0.6582380 (94) total: 3.39s remaining: 2m 55s 95: learn: 0.6178380 test: 0.6568946 best: 0.6568946 (95) total: 3.42s remaining: 2m 54s 96: learn: 0.6162087 test: 0.6553937 best: 0.6553937 (96) total: 3.46s remaining: 2m 54s 97: learn: 0.6139316 test: 0.6536377 best: 0.6536377 (97) total: 3.49s remaining: 2m 54s 98: learn: 0.6119039 test: 0.6524657 best: 0.6524657 (98) total: 3.53s remaining: 2m 54s 99: learn: 0.6094595 test: 0.6505799 best: 0.6505799 (99) total: 3.57s remaining: 2m 54s 100: learn: 0.6062709 test: 0.6480045 best: 0.6480045 (100) total: 3.6s remaining: 2m 54s 101: learn: 0.6042665 test: 0.6465189 best: 0.6465189 (101) total: 3.65s remaining: 2m 55s 102: learn: 0.6023895 test: 0.6451948 best: 0.6451948 (102) total: 3.68s remaining: 2m 55s 103: learn: 0.6004429 test: 0.6434599 best: 0.6434599 (103) total: 3.71s remaining: 2m 54s 104: learn: 0.5986547 test: 0.6420228 best: 0.6420228 (104) total: 3.76s remaining: 2m 55s 105: learn: 0.5966829 test: 0.6403374 best: 0.6403374 (105) total: 3.79s remaining: 2m 55s 106: learn: 0.5948533 test: 0.6392265 best: 0.6392265 (106) total: 3.82s remaining: 2m 54s 107: learn: 0.5933957 test: 0.6380039 best: 0.6380039 (107) total: 3.86s remaining: 2m 55s 108: learn: 0.5914159 test: 0.6364295 best: 0.6364295 (108) total: 3.9s remaining: 2m 54s 109: learn: 0.5889872 test: 0.6345290 best: 0.6345290 (109) total: 3.94s remaining: 2m 55s 110: learn: 0.5876745 test: 0.6334521 best: 0.6334521 (110) total: 3.97s remaining: 2m 54s 111: learn: 0.5863361 test: 0.6324006 best: 0.6324006 (111) total: 4s remaining: 2m 54s 112: learn: 0.5848493 test: 0.6313520 best: 0.6313520 (112) total: 4.04s remaining: 2m 54s 113: learn: 0.5834709 test: 0.6302314 best: 0.6302314 (113) total: 4.07s remaining: 2m 54s 114: learn: 0.5831505 test: 0.6299114 best: 0.6299114 (114) total: 4.09s remaining: 2m 53s 115: learn: 0.5812015 test: 0.6285281 best: 0.6285281 (115) total: 4.12s remaining: 2m 53s 116: learn: 0.5795246 test: 0.6272714 best: 0.6272714 (116) total: 4.17s remaining: 2m 53s 117: learn: 0.5778684 test: 0.6261956 best: 0.6261956 (117) total: 4.21s remaining: 2m 54s 118: learn: 0.5756968 test: 0.6245244 best: 0.6245244 (118) total: 4.24s remaining: 2m 54s 119: learn: 0.5743139 test: 0.6234191 best: 0.6234191 (119) total: 4.28s remaining: 2m 53s 120: learn: 0.5737222 test: 0.6227750 best: 0.6227750 (120) total: 4.31s remaining: 2m 53s 121: learn: 0.5736372 test: 0.6226974 best: 0.6226974 (121) total: 4.32s remaining: 2m 52s 122: learn: 0.5721428 test: 0.6216431 best: 0.6216431 (122) total: 4.36s remaining: 2m 52s 123: learn: 0.5707763 test: 0.6207303 best: 0.6207303 (123) total: 4.39s remaining: 2m 52s 124: learn: 0.5707740 test: 0.6207257 best: 0.6207257 (124) total: 4.4s remaining: 2m 51s 125: learn: 0.5687831 test: 0.6192347 best: 0.6192347 (125) total: 4.44s remaining: 2m 51s 126: learn: 0.5676017 test: 0.6182873 best: 0.6182873 (126) total: 4.47s remaining: 2m 51s 127: learn: 0.5663032 test: 0.6171227 best: 0.6171227 (127) total: 4.5s remaining: 2m 51s 128: learn: 0.5646069 test: 0.6158330 best: 0.6158330 (128) total: 4.54s remaining: 2m 51s 129: learn: 0.5636998 test: 0.6148911 best: 0.6148911 (129) total: 4.58s remaining: 2m 51s 130: learn: 0.5624051 test: 0.6138233 best: 0.6138233 (130) total: 4.61s remaining: 2m 51s 131: learn: 0.5613818 test: 0.6131759 best: 0.6131759 (131) total: 4.65s remaining: 2m 51s 132: learn: 0.5605742 test: 0.6123939 best: 0.6123939 (132) total: 4.68s remaining: 2m 51s 133: learn: 0.5593603 test: 0.6115568 best: 0.6115568 (133) total: 4.71s remaining: 2m 51s 134: learn: 0.5584147 test: 0.6107653 best: 0.6107653 (134) total: 4.74s remaining: 2m 50s 135: learn: 0.5570475 test: 0.6097534 best: 0.6097534 (135) total: 4.77s remaining: 2m 50s 136: learn: 0.5558403 test: 0.6089931 best: 0.6089931 (136) total: 4.81s remaining: 2m 50s 137: learn: 0.5544492 test: 0.6081794 best: 0.6081794 (137) total: 4.84s remaining: 2m 50s 138: learn: 0.5532240 test: 0.6073256 best: 0.6073256 (138) total: 4.88s remaining: 2m 50s 139: learn: 0.5515358 test: 0.6065015 best: 0.6065015 (139) total: 4.92s remaining: 2m 50s 140: learn: 0.5501964 test: 0.6054399 best: 0.6054399 (140) total: 4.95s remaining: 2m 50s 141: learn: 0.5486938 test: 0.6044356 best: 0.6044356 (141) total: 4.98s remaining: 2m 50s 142: learn: 0.5476147 test: 0.6035673 best: 0.6035673 (142) total: 5.01s remaining: 2m 50s 143: learn: 0.5465622 test: 0.6029090 best: 0.6029090 (143) total: 5.05s remaining: 2m 50s 144: learn: 0.5455535 test: 0.6022399 best: 0.6022399 (144) total: 5.08s remaining: 2m 50s 145: learn: 0.5444757 test: 0.6014578 best: 0.6014578 (145) total: 5.12s remaining: 2m 50s 146: learn: 0.5437233 test: 0.6007998 best: 0.6007998 (146) total: 5.16s remaining: 2m 50s 147: learn: 0.5423331 test: 0.5997900 best: 0.5997900 (147) total: 5.2s remaining: 2m 50s 148: learn: 0.5410182 test: 0.5989750 best: 0.5989750 (148) total: 5.23s remaining: 2m 50s 149: learn: 0.5399777 test: 0.5982267 best: 0.5982267 (149) total: 5.26s remaining: 2m 50s 150: learn: 0.5389165 test: 0.5974243 best: 0.5974243 (150) total: 5.3s remaining: 2m 50s 151: learn: 0.5376710 test: 0.5964955 best: 0.5964955 (151) total: 5.34s remaining: 2m 50s 152: learn: 0.5358124 test: 0.5951009 best: 0.5951009 (152) total: 5.37s remaining: 2m 50s 153: learn: 0.5344128 test: 0.5942038 best: 0.5942038 (153) total: 5.41s remaining: 2m 50s 154: learn: 0.5330765 test: 0.5933830 best: 0.5933830 (154) total: 5.44s remaining: 2m 50s 155: learn: 0.5315852 test: 0.5925192 best: 0.5925192 (155) total: 5.48s remaining: 2m 50s 156: learn: 0.5299229 test: 0.5916346 best: 0.5916346 (156) total: 5.52s remaining: 2m 50s 157: learn: 0.5288326 test: 0.5908632 best: 0.5908632 (157) total: 5.56s remaining: 2m 50s 158: learn: 0.5271844 test: 0.5898123 best: 0.5898123 (158) total: 5.59s remaining: 2m 50s 159: learn: 0.5262675 test: 0.5892568 best: 0.5892568 (159) total: 5.63s remaining: 2m 50s 160: learn: 0.5253779 test: 0.5884786 best: 0.5884786 (160) total: 5.66s remaining: 2m 49s 161: learn: 0.5241472 test: 0.5875002 best: 0.5875002 (161) total: 5.69s remaining: 2m 50s 162: learn: 0.5231664 test: 0.5869448 best: 0.5869448 (162) total: 5.73s remaining: 2m 50s 163: learn: 0.5221628 test: 0.5863588 best: 0.5863588 (163) total: 5.77s remaining: 2m 50s 164: learn: 0.5206537 test: 0.5851733 best: 0.5851733 (164) total: 5.81s remaining: 2m 50s 165: learn: 0.5191180 test: 0.5842061 best: 0.5842061 (165) total: 5.84s remaining: 2m 50s 166: learn: 0.5178837 test: 0.5832502 best: 0.5832502 (166) total: 5.88s remaining: 2m 50s 167: learn: 0.5166296 test: 0.5824438 best: 0.5824438 (167) total: 5.91s remaining: 2m 50s 168: learn: 0.5157156 test: 0.5816713 best: 0.5816713 (168) total: 5.95s remaining: 2m 50s 169: learn: 0.5145971 test: 0.5809234 best: 0.5809234 (169) total: 5.98s remaining: 2m 49s 170: learn: 0.5138367 test: 0.5804650 best: 0.5804650 (170) total: 6.04s remaining: 2m 50s 171: learn: 0.5131483 test: 0.5800046 best: 0.5800046 (171) total: 6.07s remaining: 2m 50s 172: learn: 0.5118819 test: 0.5792092 best: 0.5792092 (172) total: 6.1s remaining: 2m 50s 173: learn: 0.5107730 test: 0.5784109 best: 0.5784109 (173) total: 6.13s remaining: 2m 50s 174: learn: 0.5097368 test: 0.5776254 best: 0.5776254 (174) total: 6.18s remaining: 2m 50s 175: learn: 0.5085089 test: 0.5769211 best: 0.5769211 (175) total: 6.22s remaining: 2m 50s 176: learn: 0.5074244 test: 0.5761918 best: 0.5761918 (176) total: 6.25s remaining: 2m 50s 177: learn: 0.5067543 test: 0.5755818 best: 0.5755818 (177) total: 6.29s remaining: 2m 50s 178: learn: 0.5061261 test: 0.5750411 best: 0.5750411 (178) total: 6.32s remaining: 2m 50s 179: learn: 0.5050547 test: 0.5744768 best: 0.5744768 (179) total: 6.35s remaining: 2m 50s 180: learn: 0.5031563 test: 0.5733049 best: 0.5733049 (180) total: 6.4s remaining: 2m 50s 181: learn: 0.5015976 test: 0.5724944 best: 0.5724944 (181) total: 6.44s remaining: 2m 50s 182: learn: 0.5008278 test: 0.5720639 best: 0.5720639 (182) total: 6.47s remaining: 2m 50s 183: learn: 0.5000759 test: 0.5715841 best: 0.5715841 (183) total: 6.5s remaining: 2m 50s 184: learn: 0.4987197 test: 0.5711314 best: 0.5711314 (184) total: 6.54s remaining: 2m 50s 185: learn: 0.4980068 test: 0.5707429 best: 0.5707429 (185) total: 6.57s remaining: 2m 50s 186: learn: 0.4971863 test: 0.5704456 best: 0.5704456 (186) total: 6.61s remaining: 2m 50s 187: learn: 0.4961213 test: 0.5697591 best: 0.5697591 (187) total: 6.65s remaining: 2m 50s 188: learn: 0.4951608 test: 0.5691988 best: 0.5691988 (188) total: 6.69s remaining: 2m 50s 189: learn: 0.4942653 test: 0.5686429 best: 0.5686429 (189) total: 6.72s remaining: 2m 50s 190: learn: 0.4931535 test: 0.5681213 best: 0.5681213 (190) total: 6.76s remaining: 2m 50s 191: learn: 0.4921740 test: 0.5672729 best: 0.5672729 (191) total: 6.79s remaining: 2m 50s 192: learn: 0.4913135 test: 0.5667303 best: 0.5667303 (192) total: 6.83s remaining: 2m 50s 193: learn: 0.4904351 test: 0.5661645 best: 0.5661645 (193) total: 6.87s remaining: 2m 50s 194: learn: 0.4901841 test: 0.5660258 best: 0.5660258 (194) total: 6.9s remaining: 2m 49s 195: learn: 0.4893543 test: 0.5654649 best: 0.5654649 (195) total: 6.93s remaining: 2m 49s 196: learn: 0.4887633 test: 0.5648160 best: 0.5648160 (196) total: 6.96s remaining: 2m 49s 197: learn: 0.4878215 test: 0.5643368 best: 0.5643368 (197) total: 7s remaining: 2m 49s 198: learn: 0.4874833 test: 0.5641746 best: 0.5641746 (198) total: 7.02s remaining: 2m 49s 199: learn: 0.4868631 test: 0.5638278 best: 0.5638278 (199) total: 7.06s remaining: 2m 49s 200: learn: 0.4860840 test: 0.5634638 best: 0.5634638 (200) total: 7.09s remaining: 2m 49s 201: learn: 0.4850630 test: 0.5629276 best: 0.5629276 (201) total: 7.13s remaining: 2m 49s 202: learn: 0.4846152 test: 0.5625314 best: 0.5625314 (202) total: 7.16s remaining: 2m 49s 203: learn: 0.4837837 test: 0.5619608 best: 0.5619608 (203) total: 7.2s remaining: 2m 49s 204: learn: 0.4827972 test: 0.5614537 best: 0.5614537 (204) total: 7.23s remaining: 2m 49s 205: learn: 0.4821546 test: 0.5609987 best: 0.5609987 (205) total: 7.27s remaining: 2m 49s 206: learn: 0.4815571 test: 0.5606446 best: 0.5606446 (206) total: 7.3s remaining: 2m 49s 207: learn: 0.4810360 test: 0.5603035 best: 0.5603035 (207) total: 7.34s remaining: 2m 49s 208: learn: 0.4801175 test: 0.5597878 best: 0.5597878 (208) total: 7.37s remaining: 2m 48s 209: learn: 0.4792780 test: 0.5593443 best: 0.5593443 (209) total: 7.41s remaining: 2m 48s 210: learn: 0.4786309 test: 0.5590840 best: 0.5590840 (210) total: 7.44s remaining: 2m 48s 211: learn: 0.4778132 test: 0.5587343 best: 0.5587343 (211) total: 7.47s remaining: 2m 48s 212: learn: 0.4769064 test: 0.5585394 best: 0.5585394 (212) total: 7.51s remaining: 2m 48s 213: learn: 0.4760850 test: 0.5581012 best: 0.5581012 (213) total: 7.54s remaining: 2m 48s 214: learn: 0.4754689 test: 0.5576828 best: 0.5576828 (214) total: 7.58s remaining: 2m 48s 215: learn: 0.4746073 test: 0.5574775 best: 0.5574775 (215) total: 7.62s remaining: 2m 48s 216: learn: 0.4741063 test: 0.5571526 best: 0.5571526 (216) total: 7.65s remaining: 2m 48s 217: learn: 0.4734831 test: 0.5568498 best: 0.5568498 (217) total: 7.68s remaining: 2m 48s 218: learn: 0.4725752 test: 0.5564258 best: 0.5564258 (218) total: 7.71s remaining: 2m 48s 219: learn: 0.4719817 test: 0.5561745 best: 0.5561745 (219) total: 7.75s remaining: 2m 48s 220: learn: 0.4715739 test: 0.5558728 best: 0.5558728 (220) total: 7.78s remaining: 2m 48s 221: learn: 0.4705307 test: 0.5555031 best: 0.5555031 (221) total: 7.82s remaining: 2m 48s 222: learn: 0.4698973 test: 0.5551354 best: 0.5551354 (222) total: 7.85s remaining: 2m 48s 223: learn: 0.4691608 test: 0.5548348 best: 0.5548348 (223) total: 7.89s remaining: 2m 48s 224: learn: 0.4682270 test: 0.5543548 best: 0.5543548 (224) total: 7.92s remaining: 2m 48s 225: learn: 0.4673677 test: 0.5537963 best: 0.5537963 (225) total: 7.96s remaining: 2m 48s 226: learn: 0.4667611 test: 0.5533989 best: 0.5533989 (226) total: 7.99s remaining: 2m 47s 227: learn: 0.4660443 test: 0.5529879 best: 0.5529879 (227) total: 8.02s remaining: 2m 47s 228: learn: 0.4653217 test: 0.5526490 best: 0.5526490 (228) total: 8.05s remaining: 2m 47s 229: learn: 0.4645091 test: 0.5523045 best: 0.5523045 (229) total: 8.09s remaining: 2m 47s 230: learn: 0.4637354 test: 0.5519965 best: 0.5519965 (230) total: 8.13s remaining: 2m 47s 231: learn: 0.4627772 test: 0.5514374 best: 0.5514374 (231) total: 8.17s remaining: 2m 47s 232: learn: 0.4621508 test: 0.5511736 best: 0.5511736 (232) total: 8.21s remaining: 2m 47s 233: learn: 0.4615326 test: 0.5507849 best: 0.5507849 (233) total: 8.24s remaining: 2m 47s 234: learn: 0.4609914 test: 0.5504650 best: 0.5504650 (234) total: 8.27s remaining: 2m 47s 235: learn: 0.4601563 test: 0.5502211 best: 0.5502211 (235) total: 8.31s remaining: 2m 47s 236: learn: 0.4595527 test: 0.5498994 best: 0.5498994 (236) total: 8.35s remaining: 2m 47s 237: learn: 0.4589367 test: 0.5496052 best: 0.5496052 (237) total: 8.38s remaining: 2m 47s 238: learn: 0.4581873 test: 0.5491254 best: 0.5491254 (238) total: 8.41s remaining: 2m 47s 239: learn: 0.4577540 test: 0.5488763 best: 0.5488763 (239) total: 8.44s remaining: 2m 47s 240: learn: 0.4569586 test: 0.5484039 best: 0.5484039 (240) total: 8.48s remaining: 2m 47s 241: learn: 0.4564730 test: 0.5481702 best: 0.5481702 (241) total: 8.51s remaining: 2m 47s 242: learn: 0.4559679 test: 0.5478365 best: 0.5478365 (242) total: 8.54s remaining: 2m 47s 243: learn: 0.4553664 test: 0.5475122 best: 0.5475122 (243) total: 8.57s remaining: 2m 47s 244: learn: 0.4546216 test: 0.5472263 best: 0.5472263 (244) total: 8.61s remaining: 2m 47s 245: learn: 0.4540809 test: 0.5471072 best: 0.5471072 (245) total: 8.64s remaining: 2m 47s 246: learn: 0.4532212 test: 0.5467615 best: 0.5467615 (246) total: 8.68s remaining: 2m 46s 247: learn: 0.4524666 test: 0.5465053 best: 0.5465053 (247) total: 8.71s remaining: 2m 46s 248: learn: 0.4517627 test: 0.5462523 best: 0.5462523 (248) total: 8.76s remaining: 2m 47s 249: learn: 0.4511129 test: 0.5458757 best: 0.5458757 (249) total: 8.8s remaining: 2m 47s 250: learn: 0.4505067 test: 0.5457351 best: 0.5457351 (250) total: 8.84s remaining: 2m 47s 251: learn: 0.4498144 test: 0.5454709 best: 0.5454709 (251) total: 8.87s remaining: 2m 47s 252: learn: 0.4494945 test: 0.5452605 best: 0.5452605 (252) total: 8.9s remaining: 2m 47s 253: learn: 0.4483763 test: 0.5447930 best: 0.5447930 (253) total: 8.94s remaining: 2m 47s 254: learn: 0.4479369 test: 0.5444404 best: 0.5444404 (254) total: 8.97s remaining: 2m 47s 255: learn: 0.4474586 test: 0.5442014 best: 0.5442014 (255) total: 9.01s remaining: 2m 46s 256: learn: 0.4468620 test: 0.5438844 best: 0.5438844 (256) total: 9.04s remaining: 2m 46s 257: learn: 0.4464399 test: 0.5435752 best: 0.5435752 (257) total: 9.07s remaining: 2m 46s 258: learn: 0.4455402 test: 0.5430828 best: 0.5430828 (258) total: 9.11s remaining: 2m 46s 259: learn: 0.4444056 test: 0.5425518 best: 0.5425518 (259) total: 9.14s remaining: 2m 46s 260: learn: 0.4439010 test: 0.5423188 best: 0.5423188 (260) total: 9.19s remaining: 2m 46s 261: learn: 0.4430291 test: 0.5418296 best: 0.5418296 (261) total: 9.22s remaining: 2m 46s 262: learn: 0.4425650 test: 0.5415644 best: 0.5415644 (262) total: 9.26s remaining: 2m 46s 263: learn: 0.4422645 test: 0.5413603 best: 0.5413603 (263) total: 9.29s remaining: 2m 46s 264: learn: 0.4414013 test: 0.5410366 best: 0.5410366 (264) total: 9.33s remaining: 2m 46s 265: learn: 0.4406750 test: 0.5407514 best: 0.5407514 (265) total: 9.36s remaining: 2m 46s 266: learn: 0.4400322 test: 0.5404104 best: 0.5404104 (266) total: 9.4s remaining: 2m 46s 267: learn: 0.4395827 test: 0.5400724 best: 0.5400724 (267) total: 9.43s remaining: 2m 46s 268: learn: 0.4390092 test: 0.5397576 best: 0.5397576 (268) total: 9.47s remaining: 2m 46s 269: learn: 0.4384767 test: 0.5395930 best: 0.5395930 (269) total: 9.51s remaining: 2m 46s 270: learn: 0.4376949 test: 0.5392127 best: 0.5392127 (270) total: 9.54s remaining: 2m 46s 271: learn: 0.4371999 test: 0.5390682 best: 0.5390682 (271) total: 9.58s remaining: 2m 46s 272: learn: 0.4367913 test: 0.5388308 best: 0.5388308 (272) total: 9.61s remaining: 2m 46s 273: learn: 0.4363735 test: 0.5386817 best: 0.5386817 (273) total: 9.64s remaining: 2m 46s 274: learn: 0.4354698 test: 0.5384022 best: 0.5384022 (274) total: 9.68s remaining: 2m 46s 275: learn: 0.4350016 test: 0.5381590 best: 0.5381590 (275) total: 9.71s remaining: 2m 46s 276: learn: 0.4345560 test: 0.5379736 best: 0.5379736 (276) total: 9.74s remaining: 2m 46s 277: learn: 0.4341620 test: 0.5378270 best: 0.5378270 (277) total: 9.78s remaining: 2m 46s 278: learn: 0.4335073 test: 0.5376077 best: 0.5376077 (278) total: 9.81s remaining: 2m 46s 279: learn: 0.4330307 test: 0.5375284 best: 0.5375284 (279) total: 9.85s remaining: 2m 46s 280: learn: 0.4326489 test: 0.5372873 best: 0.5372873 (280) total: 9.88s remaining: 2m 45s 281: learn: 0.4318554 test: 0.5368975 best: 0.5368975 (281) total: 9.92s remaining: 2m 45s 282: learn: 0.4312729 test: 0.5367098 best: 0.5367098 (282) total: 9.95s remaining: 2m 45s 283: learn: 0.4309665 test: 0.5365543 best: 0.5365543 (283) total: 9.98s remaining: 2m 45s 284: learn: 0.4302827 test: 0.5362082 best: 0.5362082 (284) total: 10s remaining: 2m 45s 285: learn: 0.4297559 test: 0.5360636 best: 0.5360636 (285) total: 10.1s remaining: 2m 45s 286: learn: 0.4295421 test: 0.5359103 best: 0.5359103 (286) total: 10.1s remaining: 2m 45s 287: learn: 0.4290250 test: 0.5358053 best: 0.5358053 (287) total: 10.1s remaining: 2m 45s 288: learn: 0.4283514 test: 0.5355213 best: 0.5355213 (288) total: 10.2s remaining: 2m 45s 289: learn: 0.4274631 test: 0.5351721 best: 0.5351721 (289) total: 10.2s remaining: 2m 45s 290: learn: 0.4269333 test: 0.5348127 best: 0.5348127 (290) total: 10.2s remaining: 2m 45s 291: learn: 0.4267325 test: 0.5346562 best: 0.5346562 (291) total: 10.3s remaining: 2m 45s 292: learn: 0.4262465 test: 0.5344907 best: 0.5344907 (292) total: 10.3s remaining: 2m 45s 293: learn: 0.4258685 test: 0.5343034 best: 0.5343034 (293) total: 10.3s remaining: 2m 45s 294: learn: 0.4254865 test: 0.5341027 best: 0.5341027 (294) total: 10.4s remaining: 2m 45s 295: learn: 0.4251043 test: 0.5338961 best: 0.5338961 (295) total: 10.4s remaining: 2m 45s 296: learn: 0.4247596 test: 0.5337800 best: 0.5337800 (296) total: 10.4s remaining: 2m 45s 297: learn: 0.4245049 test: 0.5336977 best: 0.5336977 (297) total: 10.5s remaining: 2m 45s 298: learn: 0.4238523 test: 0.5332961 best: 0.5332961 (298) total: 10.5s remaining: 2m 44s 299: learn: 0.4232181 test: 0.5331499 best: 0.5331499 (299) total: 10.5s remaining: 2m 44s 300: learn: 0.4228171 test: 0.5329317 best: 0.5329317 (300) total: 10.6s remaining: 2m 44s 301: learn: 0.4222153 test: 0.5326283 best: 0.5326283 (301) total: 10.6s remaining: 2m 44s 302: learn: 0.4214834 test: 0.5324478 best: 0.5324478 (302) total: 10.6s remaining: 2m 44s 303: learn: 0.4209493 test: 0.5322320 best: 0.5322320 (303) total: 10.7s remaining: 2m 44s 304: learn: 0.4201933 test: 0.5317265 best: 0.5317265 (304) total: 10.7s remaining: 2m 44s 305: learn: 0.4197234 test: 0.5316336 best: 0.5316336 (305) total: 10.7s remaining: 2m 44s 306: learn: 0.4191964 test: 0.5312654 best: 0.5312654 (306) total: 10.8s remaining: 2m 44s 307: learn: 0.4184174 test: 0.5310244 best: 0.5310244 (307) total: 10.8s remaining: 2m 44s 308: learn: 0.4181590 test: 0.5308606 best: 0.5308606 (308) total: 10.8s remaining: 2m 44s 309: learn: 0.4175969 test: 0.5305985 best: 0.5305985 (309) total: 10.9s remaining: 2m 44s 310: learn: 0.4170204 test: 0.5304243 best: 0.5304243 (310) total: 10.9s remaining: 2m 44s 311: learn: 0.4164306 test: 0.5302348 best: 0.5302348 (311) total: 10.9s remaining: 2m 44s 312: learn: 0.4159929 test: 0.5299953 best: 0.5299953 (312) total: 11s remaining: 2m 44s 313: learn: 0.4155989 test: 0.5298942 best: 0.5298942 (313) total: 11s remaining: 2m 44s 314: learn: 0.4151963 test: 0.5297970 best: 0.5297970 (314) total: 11s remaining: 2m 44s 315: learn: 0.4147770 test: 0.5296659 best: 0.5296659 (315) total: 11.1s remaining: 2m 44s 316: learn: 0.4143702 test: 0.5295207 best: 0.5295207 (316) total: 11.1s remaining: 2m 44s 317: learn: 0.4136664 test: 0.5290230 best: 0.5290230 (317) total: 11.1s remaining: 2m 44s 318: learn: 0.4133045 test: 0.5288498 best: 0.5288498 (318) total: 11.2s remaining: 2m 44s 319: learn: 0.4129579 test: 0.5285962 best: 0.5285962 (319) total: 11.2s remaining: 2m 44s 320: learn: 0.4121078 test: 0.5282325 best: 0.5282325 (320) total: 11.3s remaining: 2m 44s 321: learn: 0.4116004 test: 0.5281230 best: 0.5281230 (321) total: 11.3s remaining: 2m 44s 322: learn: 0.4110242 test: 0.5278655 best: 0.5278655 (322) total: 11.3s remaining: 2m 44s 323: learn: 0.4103166 test: 0.5276270 best: 0.5276270 (323) total: 11.4s remaining: 2m 44s 324: learn: 0.4097844 test: 0.5272042 best: 0.5272042 (324) total: 11.4s remaining: 2m 44s 325: learn: 0.4091532 test: 0.5270324 best: 0.5270324 (325) total: 11.4s remaining: 2m 44s 326: learn: 0.4087594 test: 0.5267304 best: 0.5267304 (326) total: 11.5s remaining: 2m 43s 327: learn: 0.4083862 test: 0.5264589 best: 0.5264589 (327) total: 11.5s remaining: 2m 43s 328: learn: 0.4078721 test: 0.5261641 best: 0.5261641 (328) total: 11.5s remaining: 2m 43s 329: learn: 0.4072899 test: 0.5260813 best: 0.5260813 (329) total: 11.6s remaining: 2m 43s 330: learn: 0.4066907 test: 0.5258263 best: 0.5258263 (330) total: 11.6s remaining: 2m 43s 331: learn: 0.4060412 test: 0.5256177 best: 0.5256177 (331) total: 11.6s remaining: 2m 43s 332: learn: 0.4056499 test: 0.5254181 best: 0.5254181 (332) total: 11.7s remaining: 2m 43s 333: learn: 0.4051948 test: 0.5252162 best: 0.5252162 (333) total: 11.7s remaining: 2m 43s 334: learn: 0.4049738 test: 0.5250619 best: 0.5250619 (334) total: 11.8s remaining: 2m 43s 335: learn: 0.4046534 test: 0.5249727 best: 0.5249727 (335) total: 11.8s remaining: 2m 43s 336: learn: 0.4040502 test: 0.5246802 best: 0.5246802 (336) total: 11.8s remaining: 2m 43s 337: learn: 0.4038366 test: 0.5245152 best: 0.5245152 (337) total: 11.8s remaining: 2m 43s 338: learn: 0.4030823 test: 0.5244918 best: 0.5244918 (338) total: 11.9s remaining: 2m 43s 339: learn: 0.4026505 test: 0.5243173 best: 0.5243173 (339) total: 11.9s remaining: 2m 43s 340: learn: 0.4022784 test: 0.5240696 best: 0.5240696 (340) total: 12s remaining: 2m 43s 341: learn: 0.4017406 test: 0.5238796 best: 0.5238796 (341) total: 12s remaining: 2m 43s 342: learn: 0.4012258 test: 0.5237034 best: 0.5237034 (342) total: 12s remaining: 2m 43s 343: learn: 0.4007517 test: 0.5235955 best: 0.5235955 (343) total: 12.1s remaining: 2m 43s 344: learn: 0.4003605 test: 0.5234015 best: 0.5234015 (344) total: 12.1s remaining: 2m 43s 345: learn: 0.4000775 test: 0.5233239 best: 0.5233239 (345) total: 12.1s remaining: 2m 43s 346: learn: 0.3998604 test: 0.5233167 best: 0.5233167 (346) total: 12.2s remaining: 2m 43s 347: learn: 0.3994218 test: 0.5231643 best: 0.5231643 (347) total: 12.2s remaining: 2m 43s 348: learn: 0.3991059 test: 0.5229259 best: 0.5229259 (348) total: 12.3s remaining: 2m 43s 349: learn: 0.3986747 test: 0.5227779 best: 0.5227779 (349) total: 12.3s remaining: 2m 43s 350: learn: 0.3977483 test: 0.5222175 best: 0.5222175 (350) total: 12.3s remaining: 2m 43s 351: learn: 0.3973839 test: 0.5221667 best: 0.5221667 (351) total: 12.4s remaining: 2m 43s 352: learn: 0.3970024 test: 0.5219808 best: 0.5219808 (352) total: 12.4s remaining: 2m 43s 353: learn: 0.3967234 test: 0.5218158 best: 0.5218158 (353) total: 12.4s remaining: 2m 43s 354: learn: 0.3964325 test: 0.5216490 best: 0.5216490 (354) total: 12.5s remaining: 2m 42s 355: learn: 0.3959961 test: 0.5214724 best: 0.5214724 (355) total: 12.5s remaining: 2m 42s 356: learn: 0.3957665 test: 0.5213696 best: 0.5213696 (356) total: 12.5s remaining: 2m 42s 357: learn: 0.3951690 test: 0.5211102 best: 0.5211102 (357) total: 12.5s remaining: 2m 42s 358: learn: 0.3945653 test: 0.5208911 best: 0.5208911 (358) total: 12.6s remaining: 2m 42s 359: learn: 0.3941301 test: 0.5206968 best: 0.5206968 (359) total: 12.6s remaining: 2m 42s 360: learn: 0.3937129 test: 0.5205540 best: 0.5205540 (360) total: 12.7s remaining: 2m 42s 361: learn: 0.3932344 test: 0.5202555 best: 0.5202555 (361) total: 12.7s remaining: 2m 42s 362: learn: 0.3930283 test: 0.5202100 best: 0.5202100 (362) total: 12.7s remaining: 2m 42s 363: learn: 0.3924523 test: 0.5199796 best: 0.5199796 (363) total: 12.7s remaining: 2m 42s 364: learn: 0.3918342 test: 0.5197703 best: 0.5197703 (364) total: 12.8s remaining: 2m 42s 365: learn: 0.3915004 test: 0.5196553 best: 0.5196553 (365) total: 12.8s remaining: 2m 42s 366: learn: 0.3909649 test: 0.5195202 best: 0.5195202 (366) total: 12.9s remaining: 2m 42s 367: learn: 0.3903750 test: 0.5193972 best: 0.5193972 (367) total: 12.9s remaining: 2m 42s 368: learn: 0.3898014 test: 0.5192192 best: 0.5192192 (368) total: 12.9s remaining: 2m 42s 369: learn: 0.3894294 test: 0.5190480 best: 0.5190480 (369) total: 13s remaining: 2m 42s 370: learn: 0.3890367 test: 0.5189377 best: 0.5189377 (370) total: 13s remaining: 2m 42s 371: learn: 0.3884306 test: 0.5187323 best: 0.5187323 (371) total: 13s remaining: 2m 42s 372: learn: 0.3878970 test: 0.5185903 best: 0.5185903 (372) total: 13.1s remaining: 2m 42s 373: learn: 0.3874603 test: 0.5185561 best: 0.5185561 (373) total: 13.1s remaining: 2m 42s 374: learn: 0.3868631 test: 0.5184066 best: 0.5184066 (374) total: 13.1s remaining: 2m 42s 375: learn: 0.3863001 test: 0.5181693 best: 0.5181693 (375) total: 13.2s remaining: 2m 41s 376: learn: 0.3861314 test: 0.5180101 best: 0.5180101 (376) total: 13.2s remaining: 2m 42s 377: learn: 0.3855968 test: 0.5177668 best: 0.5177668 (377) total: 13.3s remaining: 2m 42s 378: learn: 0.3852136 test: 0.5175732 best: 0.5175732 (378) total: 13.3s remaining: 2m 41s 379: learn: 0.3850284 test: 0.5174774 best: 0.5174774 (379) total: 13.3s remaining: 2m 41s 380: learn: 0.3846514 test: 0.5174029 best: 0.5174029 (380) total: 13.3s remaining: 2m 41s 381: learn: 0.3841872 test: 0.5172143 best: 0.5172143 (381) total: 13.4s remaining: 2m 41s 382: learn: 0.3836293 test: 0.5170129 best: 0.5170129 (382) total: 13.4s remaining: 2m 41s 383: learn: 0.3833107 test: 0.5169287 best: 0.5169287 (383) total: 13.4s remaining: 2m 41s 384: learn: 0.3829597 test: 0.5168536 best: 0.5168536 (384) total: 13.5s remaining: 2m 41s 385: learn: 0.3826087 test: 0.5168108 best: 0.5168108 (385) total: 13.5s remaining: 2m 41s 386: learn: 0.3821681 test: 0.5166896 best: 0.5166896 (386) total: 13.5s remaining: 2m 41s 387: learn: 0.3817458 test: 0.5166282 best: 0.5166282 (387) total: 13.6s remaining: 2m 41s 388: learn: 0.3813038 test: 0.5164032 best: 0.5164032 (388) total: 13.6s remaining: 2m 41s 389: learn: 0.3810966 test: 0.5163247 best: 0.5163247 (389) total: 13.7s remaining: 2m 41s 390: learn: 0.3807042 test: 0.5162395 best: 0.5162395 (390) total: 13.7s remaining: 2m 41s 391: learn: 0.3804826 test: 0.5161646 best: 0.5161646 (391) total: 13.7s remaining: 2m 41s 392: learn: 0.3802903 test: 0.5160683 best: 0.5160683 (392) total: 13.8s remaining: 2m 41s 393: learn: 0.3797468 test: 0.5159605 best: 0.5159605 (393) total: 13.8s remaining: 2m 41s 394: learn: 0.3790643 test: 0.5157550 best: 0.5157550 (394) total: 13.8s remaining: 2m 41s 395: learn: 0.3787575 test: 0.5156045 best: 0.5156045 (395) total: 13.9s remaining: 2m 41s 396: learn: 0.3785489 test: 0.5154812 best: 0.5154812 (396) total: 13.9s remaining: 2m 41s 397: learn: 0.3783416 test: 0.5154494 best: 0.5154494 (397) total: 13.9s remaining: 2m 41s 398: learn: 0.3779345 test: 0.5152891 best: 0.5152891 (398) total: 14s remaining: 2m 41s 399: learn: 0.3776215 test: 0.5151847 best: 0.5151847 (399) total: 14s remaining: 2m 40s 400: learn: 0.3770221 test: 0.5150989 best: 0.5150989 (400) total: 14s remaining: 2m 40s 401: learn: 0.3765497 test: 0.5149024 best: 0.5149024 (401) total: 14.1s remaining: 2m 40s 402: learn: 0.3760453 test: 0.5146997 best: 0.5146997 (402) total: 14.1s remaining: 2m 40s 403: learn: 0.3754911 test: 0.5145777 best: 0.5145777 (403) total: 14.1s remaining: 2m 40s 404: learn: 0.3751623 test: 0.5144820 best: 0.5144820 (404) total: 14.2s remaining: 2m 40s 405: learn: 0.3744620 test: 0.5143383 best: 0.5143383 (405) total: 14.2s remaining: 2m 41s 406: learn: 0.3742088 test: 0.5142789 best: 0.5142789 (406) total: 14.3s remaining: 2m 40s 407: learn: 0.3739672 test: 0.5141324 best: 0.5141324 (407) total: 14.3s remaining: 2m 40s 408: learn: 0.3737850 test: 0.5140912 best: 0.5140912 (408) total: 14.3s remaining: 2m 40s 409: learn: 0.3731291 test: 0.5137511 best: 0.5137511 (409) total: 14.4s remaining: 2m 40s 410: learn: 0.3727523 test: 0.5136936 best: 0.5136936 (410) total: 14.4s remaining: 2m 40s 411: learn: 0.3723754 test: 0.5135317 best: 0.5135317 (411) total: 14.4s remaining: 2m 40s 412: learn: 0.3719555 test: 0.5133783 best: 0.5133783 (412) total: 14.5s remaining: 2m 40s 413: learn: 0.3711343 test: 0.5130950 best: 0.5130950 (413) total: 14.5s remaining: 2m 40s 414: learn: 0.3709236 test: 0.5130582 best: 0.5130582 (414) total: 14.5s remaining: 2m 40s 415: learn: 0.3703056 test: 0.5130262 best: 0.5130262 (415) total: 14.6s remaining: 2m 40s 416: learn: 0.3699472 test: 0.5129571 best: 0.5129571 (416) total: 14.6s remaining: 2m 40s 417: learn: 0.3693426 test: 0.5127577 best: 0.5127577 (417) total: 14.6s remaining: 2m 40s 418: learn: 0.3688799 test: 0.5125569 best: 0.5125569 (418) total: 14.7s remaining: 2m 40s 419: learn: 0.3683514 test: 0.5123493 best: 0.5123493 (419) total: 14.7s remaining: 2m 40s 420: learn: 0.3677151 test: 0.5121051 best: 0.5121051 (420) total: 14.7s remaining: 2m 40s 421: learn: 0.3673484 test: 0.5119874 best: 0.5119874 (421) total: 14.8s remaining: 2m 40s 422: learn: 0.3668416 test: 0.5119487 best: 0.5119487 (422) total: 14.8s remaining: 2m 40s 423: learn: 0.3665516 test: 0.5118738 best: 0.5118738 (423) total: 14.8s remaining: 2m 40s 424: learn: 0.3664245 test: 0.5118215 best: 0.5118215 (424) total: 14.9s remaining: 2m 40s 425: learn: 0.3659536 test: 0.5116526 best: 0.5116526 (425) total: 14.9s remaining: 2m 40s 426: learn: 0.3656985 test: 0.5115303 best: 0.5115303 (426) total: 14.9s remaining: 2m 39s 427: learn: 0.3652653 test: 0.5113952 best: 0.5113952 (427) total: 15s remaining: 2m 39s 428: learn: 0.3650020 test: 0.5113694 best: 0.5113694 (428) total: 15s remaining: 2m 39s 429: learn: 0.3647058 test: 0.5111545 best: 0.5111545 (429) total: 15s remaining: 2m 39s 430: learn: 0.3643480 test: 0.5110953 best: 0.5110953 (430) total: 15.1s remaining: 2m 39s 431: learn: 0.3638819 test: 0.5110466 best: 0.5110466 (431) total: 15.1s remaining: 2m 39s 432: learn: 0.3635590 test: 0.5109735 best: 0.5109735 (432) total: 15.1s remaining: 2m 39s 433: learn: 0.3633678 test: 0.5109179 best: 0.5109179 (433) total: 15.2s remaining: 2m 39s 434: learn: 0.3628215 test: 0.5106905 best: 0.5106905 (434) total: 15.2s remaining: 2m 39s 435: learn: 0.3623841 test: 0.5105352 best: 0.5105352 (435) total: 15.2s remaining: 2m 39s 436: learn: 0.3615756 test: 0.5103624 best: 0.5103624 (436) total: 15.3s remaining: 2m 39s 437: learn: 0.3612376 test: 0.5101833 best: 0.5101833 (437) total: 15.3s remaining: 2m 39s 438: learn: 0.3609498 test: 0.5100857 best: 0.5100857 (438) total: 15.3s remaining: 2m 39s 439: learn: 0.3605937 test: 0.5099793 best: 0.5099793 (439) total: 15.4s remaining: 2m 39s 440: learn: 0.3603183 test: 0.5099147 best: 0.5099147 (440) total: 15.4s remaining: 2m 39s 441: learn: 0.3599937 test: 0.5098095 best: 0.5098095 (441) total: 15.5s remaining: 2m 39s 442: learn: 0.3596253 test: 0.5096292 best: 0.5096292 (442) total: 15.5s remaining: 2m 39s 443: learn: 0.3590174 test: 0.5093845 best: 0.5093845 (443) total: 15.5s remaining: 2m 39s 444: learn: 0.3586877 test: 0.5092447 best: 0.5092447 (444) total: 15.5s remaining: 2m 39s 445: learn: 0.3583509 test: 0.5092327 best: 0.5092327 (445) total: 15.6s remaining: 2m 39s 446: learn: 0.3579733 test: 0.5091874 best: 0.5091874 (446) total: 15.6s remaining: 2m 39s 447: learn: 0.3574679 test: 0.5091464 best: 0.5091464 (447) total: 15.7s remaining: 2m 39s 448: learn: 0.3572355 test: 0.5089353 best: 0.5089353 (448) total: 15.7s remaining: 2m 38s 449: learn: 0.3566693 test: 0.5086358 best: 0.5086358 (449) total: 15.7s remaining: 2m 38s 450: learn: 0.3564201 test: 0.5086034 best: 0.5086034 (450) total: 15.8s remaining: 2m 38s 451: learn: 0.3561524 test: 0.5085350 best: 0.5085350 (451) total: 15.8s remaining: 2m 38s 452: learn: 0.3557497 test: 0.5084485 best: 0.5084485 (452) total: 15.8s remaining: 2m 38s 453: learn: 0.3554335 test: 0.5084635 best: 0.5084485 (452) total: 15.9s remaining: 2m 38s 454: learn: 0.3550132 test: 0.5084471 best: 0.5084471 (454) total: 15.9s remaining: 2m 38s 455: learn: 0.3544140 test: 0.5083549 best: 0.5083549 (455) total: 15.9s remaining: 2m 38s 456: learn: 0.3539372 test: 0.5083497 best: 0.5083497 (456) total: 16s remaining: 2m 38s 457: learn: 0.3532325 test: 0.5080955 best: 0.5080955 (457) total: 16s remaining: 2m 38s 458: learn: 0.3530327 test: 0.5080614 best: 0.5080614 (458) total: 16s remaining: 2m 38s 459: learn: 0.3529277 test: 0.5080535 best: 0.5080535 (459) total: 16.1s remaining: 2m 38s 460: learn: 0.3525870 test: 0.5079371 best: 0.5079371 (460) total: 16.1s remaining: 2m 38s 461: learn: 0.3521106 test: 0.5079409 best: 0.5079371 (460) total: 16.1s remaining: 2m 38s 462: learn: 0.3517972 test: 0.5079083 best: 0.5079083 (462) total: 16.2s remaining: 2m 38s 463: learn: 0.3514798 test: 0.5078005 best: 0.5078005 (463) total: 16.2s remaining: 2m 38s 464: learn: 0.3512357 test: 0.5077781 best: 0.5077781 (464) total: 16.3s remaining: 2m 38s 465: learn: 0.3508989 test: 0.5076377 best: 0.5076377 (465) total: 16.3s remaining: 2m 38s 466: learn: 0.3504544 test: 0.5074377 best: 0.5074377 (466) total: 16.3s remaining: 2m 38s 467: learn: 0.3501087 test: 0.5072332 best: 0.5072332 (467) total: 16.4s remaining: 2m 38s 468: learn: 0.3498303 test: 0.5071408 best: 0.5071408 (468) total: 16.4s remaining: 2m 38s 469: learn: 0.3495730 test: 0.5070036 best: 0.5070036 (469) total: 16.4s remaining: 2m 38s 470: learn: 0.3490558 test: 0.5067888 best: 0.5067888 (470) total: 16.5s remaining: 2m 38s 471: learn: 0.3488921 test: 0.5066341 best: 0.5066341 (471) total: 16.5s remaining: 2m 38s 472: learn: 0.3485945 test: 0.5066132 best: 0.5066132 (472) total: 16.5s remaining: 2m 38s 473: learn: 0.3480971 test: 0.5065268 best: 0.5065268 (473) total: 16.6s remaining: 2m 38s 474: learn: 0.3475660 test: 0.5062927 best: 0.5062927 (474) total: 16.6s remaining: 2m 38s 475: learn: 0.3471549 test: 0.5062463 best: 0.5062463 (475) total: 16.6s remaining: 2m 38s 476: learn: 0.3470009 test: 0.5062239 best: 0.5062239 (476) total: 16.7s remaining: 2m 37s 477: learn: 0.3468046 test: 0.5060899 best: 0.5060899 (477) total: 16.7s remaining: 2m 37s 478: learn: 0.3465504 test: 0.5059589 best: 0.5059589 (478) total: 16.7s remaining: 2m 37s 479: learn: 0.3462390 test: 0.5058523 best: 0.5058523 (479) total: 16.8s remaining: 2m 37s 480: learn: 0.3461506 test: 0.5058105 best: 0.5058105 (480) total: 16.8s remaining: 2m 37s 481: learn: 0.3459605 test: 0.5058253 best: 0.5058105 (480) total: 16.8s remaining: 2m 37s 482: learn: 0.3455859 test: 0.5057860 best: 0.5057860 (482) total: 16.9s remaining: 2m 37s 483: learn: 0.3452683 test: 0.5057664 best: 0.5057664 (483) total: 16.9s remaining: 2m 37s 484: learn: 0.3449873 test: 0.5056723 best: 0.5056723 (484) total: 16.9s remaining: 2m 37s 485: learn: 0.3447685 test: 0.5055397 best: 0.5055397 (485) total: 16.9s remaining: 2m 37s 486: learn: 0.3444783 test: 0.5054610 best: 0.5054610 (486) total: 17s remaining: 2m 37s 487: learn: 0.3440002 test: 0.5051387 best: 0.5051387 (487) total: 17s remaining: 2m 37s 488: learn: 0.3437371 test: 0.5051081 best: 0.5051081 (488) total: 17s remaining: 2m 37s 489: learn: 0.3433404 test: 0.5050783 best: 0.5050783 (489) total: 17.1s remaining: 2m 37s 490: learn: 0.3430080 test: 0.5050195 best: 0.5050195 (490) total: 17.1s remaining: 2m 37s 491: learn: 0.3428427 test: 0.5049681 best: 0.5049681 (491) total: 17.1s remaining: 2m 37s 492: learn: 0.3424494 test: 0.5047978 best: 0.5047978 (492) total: 17.2s remaining: 2m 37s 493: learn: 0.3421171 test: 0.5046742 best: 0.5046742 (493) total: 17.2s remaining: 2m 37s 494: learn: 0.3418226 test: 0.5046670 best: 0.5046670 (494) total: 17.3s remaining: 2m 37s 495: learn: 0.3416057 test: 0.5046591 best: 0.5046591 (495) total: 17.3s remaining: 2m 36s 496: learn: 0.3408481 test: 0.5045361 best: 0.5045361 (496) total: 17.3s remaining: 2m 36s 497: learn: 0.3406364 test: 0.5044298 best: 0.5044298 (497) total: 17.4s remaining: 2m 36s 498: learn: 0.3404010 test: 0.5043412 best: 0.5043412 (498) total: 17.4s remaining: 2m 36s 499: learn: 0.3401570 test: 0.5043247 best: 0.5043247 (499) total: 17.4s remaining: 2m 36s 500: learn: 0.3399586 test: 0.5042810 best: 0.5042810 (500) total: 17.4s remaining: 2m 36s 501: learn: 0.3397422 test: 0.5041442 best: 0.5041442 (501) total: 17.5s remaining: 2m 36s 502: learn: 0.3395593 test: 0.5040475 best: 0.5040475 (502) total: 17.5s remaining: 2m 36s 503: learn: 0.3393317 test: 0.5039646 best: 0.5039646 (503) total: 17.5s remaining: 2m 36s 504: learn: 0.3388578 test: 0.5038532 best: 0.5038532 (504) total: 17.6s remaining: 2m 36s 505: learn: 0.3385872 test: 0.5037697 best: 0.5037697 (505) total: 17.6s remaining: 2m 36s 506: learn: 0.3383813 test: 0.5037946 best: 0.5037697 (505) total: 17.6s remaining: 2m 36s 507: learn: 0.3381711 test: 0.5037981 best: 0.5037697 (505) total: 17.7s remaining: 2m 36s 508: learn: 0.3380041 test: 0.5036295 best: 0.5036295 (508) total: 17.7s remaining: 2m 36s 509: learn: 0.3377711 test: 0.5035958 best: 0.5035958 (509) total: 17.7s remaining: 2m 36s 510: learn: 0.3375755 test: 0.5035287 best: 0.5035287 (510) total: 17.8s remaining: 2m 36s 511: learn: 0.3371750 test: 0.5034150 best: 0.5034150 (511) total: 17.8s remaining: 2m 36s 512: learn: 0.3369911 test: 0.5033806 best: 0.5033806 (512) total: 17.8s remaining: 2m 36s 513: learn: 0.3365727 test: 0.5033472 best: 0.5033472 (513) total: 17.9s remaining: 2m 36s 514: learn: 0.3364231 test: 0.5033309 best: 0.5033309 (514) total: 17.9s remaining: 2m 35s 515: learn: 0.3360431 test: 0.5033043 best: 0.5033043 (515) total: 17.9s remaining: 2m 35s 516: learn: 0.3357804 test: 0.5032950 best: 0.5032950 (516) total: 18s remaining: 2m 35s 517: learn: 0.3355433 test: 0.5031769 best: 0.5031769 (517) total: 18s remaining: 2m 35s 518: learn: 0.3352469 test: 0.5031133 best: 0.5031133 (518) total: 18s remaining: 2m 35s 519: learn: 0.3351118 test: 0.5030983 best: 0.5030983 (519) total: 18.1s remaining: 2m 35s 520: learn: 0.3348611 test: 0.5030955 best: 0.5030955 (520) total: 18.1s remaining: 2m 35s 521: learn: 0.3343357 test: 0.5029965 best: 0.5029965 (521) total: 18.1s remaining: 2m 35s 522: learn: 0.3341436 test: 0.5029520 best: 0.5029520 (522) total: 18.2s remaining: 2m 35s 523: learn: 0.3337646 test: 0.5028064 best: 0.5028064 (523) total: 18.2s remaining: 2m 35s 524: learn: 0.3335358 test: 0.5026239 best: 0.5026239 (524) total: 18.3s remaining: 2m 35s 525: learn: 0.3332860 test: 0.5025513 best: 0.5025513 (525) total: 18.3s remaining: 2m 35s 526: learn: 0.3331849 test: 0.5025137 best: 0.5025137 (526) total: 18.3s remaining: 2m 35s 527: learn: 0.3329004 test: 0.5023807 best: 0.5023807 (527) total: 18.3s remaining: 2m 35s 528: learn: 0.3327820 test: 0.5023632 best: 0.5023632 (528) total: 18.4s remaining: 2m 35s 529: learn: 0.3326040 test: 0.5023216 best: 0.5023216 (529) total: 18.4s remaining: 2m 35s 530: learn: 0.3321604 test: 0.5021234 best: 0.5021234 (530) total: 18.4s remaining: 2m 35s 531: learn: 0.3320319 test: 0.5020861 best: 0.5020861 (531) total: 18.5s remaining: 2m 35s 532: learn: 0.3314007 test: 0.5018955 best: 0.5018955 (532) total: 18.5s remaining: 2m 35s 533: learn: 0.3310703 test: 0.5017181 best: 0.5017181 (533) total: 18.5s remaining: 2m 35s 534: learn: 0.3308835 test: 0.5017273 best: 0.5017181 (533) total: 18.6s remaining: 2m 35s 535: learn: 0.3306610 test: 0.5016119 best: 0.5016119 (535) total: 18.6s remaining: 2m 34s 536: learn: 0.3303379 test: 0.5016018 best: 0.5016018 (536) total: 18.6s remaining: 2m 34s 537: learn: 0.3301068 test: 0.5015429 best: 0.5015429 (537) total: 18.7s remaining: 2m 34s 538: learn: 0.3296229 test: 0.5014985 best: 0.5014985 (538) total: 18.7s remaining: 2m 34s 539: learn: 0.3292384 test: 0.5015065 best: 0.5014985 (538) total: 18.7s remaining: 2m 34s 540: learn: 0.3291198 test: 0.5014471 best: 0.5014471 (540) total: 18.8s remaining: 2m 34s 541: learn: 0.3287838 test: 0.5014219 best: 0.5014219 (541) total: 18.8s remaining: 2m 34s 542: learn: 0.3284530 test: 0.5014182 best: 0.5014182 (542) total: 18.8s remaining: 2m 34s 543: learn: 0.3282663 test: 0.5014231 best: 0.5014182 (542) total: 18.9s remaining: 2m 34s 544: learn: 0.3279398 test: 0.5013408 best: 0.5013408 (544) total: 18.9s remaining: 2m 34s 545: learn: 0.3274198 test: 0.5012944 best: 0.5012944 (545) total: 18.9s remaining: 2m 34s 546: learn: 0.3271979 test: 0.5012680 best: 0.5012680 (546) total: 19s remaining: 2m 34s 547: learn: 0.3269277 test: 0.5012434 best: 0.5012434 (547) total: 19s remaining: 2m 34s 548: learn: 0.3264676 test: 0.5010885 best: 0.5010885 (548) total: 19s remaining: 2m 34s 549: learn: 0.3261269 test: 0.5009864 best: 0.5009864 (549) total: 19.1s remaining: 2m 34s 550: learn: 0.3258453 test: 0.5009183 best: 0.5009183 (550) total: 19.1s remaining: 2m 34s 551: learn: 0.3255185 test: 0.5007525 best: 0.5007525 (551) total: 19.1s remaining: 2m 34s 552: learn: 0.3252612 test: 0.5006586 best: 0.5006586 (552) total: 19.2s remaining: 2m 34s 553: learn: 0.3248920 test: 0.5004176 best: 0.5004176 (553) total: 19.2s remaining: 2m 34s 554: learn: 0.3246565 test: 0.5004822 best: 0.5004176 (553) total: 19.2s remaining: 2m 34s 555: learn: 0.3242247 test: 0.5003897 best: 0.5003897 (555) total: 19.3s remaining: 2m 34s 556: learn: 0.3237627 test: 0.5002685 best: 0.5002685 (556) total: 19.3s remaining: 2m 34s 557: learn: 0.3232105 test: 0.5001438 best: 0.5001438 (557) total: 19.4s remaining: 2m 34s 558: learn: 0.3228879 test: 0.5000986 best: 0.5000986 (558) total: 19.4s remaining: 2m 34s 559: learn: 0.3225995 test: 0.5000791 best: 0.5000791 (559) total: 19.4s remaining: 2m 33s 560: learn: 0.3223708 test: 0.5001753 best: 0.5000791 (559) total: 19.4s remaining: 2m 33s 561: learn: 0.3220483 test: 0.5001312 best: 0.5000791 (559) total: 19.5s remaining: 2m 33s 562: learn: 0.3216494 test: 0.5000091 best: 0.5000091 (562) total: 19.5s remaining: 2m 33s 563: learn: 0.3213638 test: 0.4999166 best: 0.4999166 (563) total: 19.6s remaining: 2m 33s 564: learn: 0.3209796 test: 0.4998479 best: 0.4998479 (564) total: 19.6s remaining: 2m 33s 565: learn: 0.3207340 test: 0.4998049 best: 0.4998049 (565) total: 19.6s remaining: 2m 33s 566: learn: 0.3203580 test: 0.4997446 best: 0.4997446 (566) total: 19.7s remaining: 2m 33s 567: learn: 0.3199245 test: 0.4997510 best: 0.4997446 (566) total: 19.7s remaining: 2m 33s 568: learn: 0.3197848 test: 0.4997661 best: 0.4997446 (566) total: 19.7s remaining: 2m 33s 569: learn: 0.3193994 test: 0.4996828 best: 0.4996828 (569) total: 19.8s remaining: 2m 33s 570: learn: 0.3192313 test: 0.4996564 best: 0.4996564 (570) total: 19.8s remaining: 2m 33s 571: learn: 0.3188010 test: 0.4995505 best: 0.4995505 (571) total: 19.8s remaining: 2m 33s 572: learn: 0.3185572 test: 0.4995660 best: 0.4995505 (571) total: 19.9s remaining: 2m 33s 573: learn: 0.3181961 test: 0.4995388 best: 0.4995388 (573) total: 19.9s remaining: 2m 33s 574: learn: 0.3178343 test: 0.4994857 best: 0.4994857 (574) total: 19.9s remaining: 2m 33s 575: learn: 0.3174546 test: 0.4994775 best: 0.4994775 (575) total: 20s remaining: 2m 33s 576: learn: 0.3171327 test: 0.4992732 best: 0.4992732 (576) total: 20s remaining: 2m 33s 577: learn: 0.3170113 test: 0.4992270 best: 0.4992270 (577) total: 20s remaining: 2m 33s 578: learn: 0.3168624 test: 0.4991759 best: 0.4991759 (578) total: 20.1s remaining: 2m 33s 579: learn: 0.3163031 test: 0.4990471 best: 0.4990471 (579) total: 20.1s remaining: 2m 33s 580: learn: 0.3158580 test: 0.4990065 best: 0.4990065 (580) total: 20.1s remaining: 2m 33s 581: learn: 0.3155478 test: 0.4989793 best: 0.4989793 (581) total: 20.2s remaining: 2m 32s 582: learn: 0.3151112 test: 0.4988527 best: 0.4988527 (582) total: 20.2s remaining: 2m 33s 583: learn: 0.3150069 test: 0.4988217 best: 0.4988217 (583) total: 20.2s remaining: 2m 33s 584: learn: 0.3148239 test: 0.4988028 best: 0.4988028 (584) total: 20.3s remaining: 2m 32s 585: learn: 0.3144323 test: 0.4987558 best: 0.4987558 (585) total: 20.3s remaining: 2m 32s 586: learn: 0.3141802 test: 0.4986515 best: 0.4986515 (586) total: 20.3s remaining: 2m 32s 587: learn: 0.3140591 test: 0.4986458 best: 0.4986458 (587) total: 20.4s remaining: 2m 32s 588: learn: 0.3139664 test: 0.4986485 best: 0.4986458 (587) total: 20.4s remaining: 2m 32s 589: learn: 0.3135777 test: 0.4985766 best: 0.4985766 (589) total: 20.4s remaining: 2m 32s 590: learn: 0.3132525 test: 0.4985016 best: 0.4985016 (590) total: 20.5s remaining: 2m 32s 591: learn: 0.3130460 test: 0.4983981 best: 0.4983981 (591) total: 20.5s remaining: 2m 32s 592: learn: 0.3127393 test: 0.4982938 best: 0.4982938 (592) total: 20.5s remaining: 2m 32s 593: learn: 0.3125950 test: 0.4982599 best: 0.4982599 (593) total: 20.6s remaining: 2m 32s 594: learn: 0.3123885 test: 0.4982149 best: 0.4982149 (594) total: 20.6s remaining: 2m 32s 595: learn: 0.3120993 test: 0.4982260 best: 0.4982149 (594) total: 20.6s remaining: 2m 32s 596: learn: 0.3117949 test: 0.4981414 best: 0.4981414 (596) total: 20.7s remaining: 2m 32s 597: learn: 0.3115816 test: 0.4981133 best: 0.4981133 (597) total: 20.7s remaining: 2m 32s 598: learn: 0.3112241 test: 0.4981295 best: 0.4981133 (597) total: 20.7s remaining: 2m 32s 599: learn: 0.3110166 test: 0.4981159 best: 0.4981133 (597) total: 20.8s remaining: 2m 32s 600: learn: 0.3105955 test: 0.4981437 best: 0.4981133 (597) total: 20.8s remaining: 2m 32s 601: learn: 0.3102196 test: 0.4980855 best: 0.4980855 (601) total: 20.8s remaining: 2m 32s 602: learn: 0.3101170 test: 0.4980784 best: 0.4980784 (602) total: 20.9s remaining: 2m 32s 603: learn: 0.3098579 test: 0.4978976 best: 0.4978976 (603) total: 20.9s remaining: 2m 32s 604: learn: 0.3095515 test: 0.4977879 best: 0.4977879 (604) total: 20.9s remaining: 2m 32s 605: learn: 0.3093590 test: 0.4977583 best: 0.4977583 (605) total: 21s remaining: 2m 32s 606: learn: 0.3089180 test: 0.4976881 best: 0.4976881 (606) total: 21s remaining: 2m 32s 607: learn: 0.3087544 test: 0.4977848 best: 0.4976881 (606) total: 21s remaining: 2m 32s 608: learn: 0.3084842 test: 0.4977320 best: 0.4976881 (606) total: 21.1s remaining: 2m 31s 609: learn: 0.3082251 test: 0.4976569 best: 0.4976569 (609) total: 21.1s remaining: 2m 31s 610: learn: 0.3080273 test: 0.4976372 best: 0.4976372 (610) total: 21.1s remaining: 2m 31s 611: learn: 0.3078095 test: 0.4976638 best: 0.4976372 (610) total: 21.2s remaining: 2m 31s 612: learn: 0.3076009 test: 0.4976815 best: 0.4976372 (610) total: 21.2s remaining: 2m 31s 613: learn: 0.3073095 test: 0.4977782 best: 0.4976372 (610) total: 21.3s remaining: 2m 31s 614: learn: 0.3070467 test: 0.4977181 best: 0.4976372 (610) total: 21.3s remaining: 2m 31s 615: learn: 0.3066129 test: 0.4976494 best: 0.4976372 (610) total: 21.3s remaining: 2m 31s 616: learn: 0.3062191 test: 0.4976315 best: 0.4976315 (616) total: 21.4s remaining: 2m 31s 617: learn: 0.3060375 test: 0.4974678 best: 0.4974678 (617) total: 21.4s remaining: 2m 31s 618: learn: 0.3056171 test: 0.4973081 best: 0.4973081 (618) total: 21.4s remaining: 2m 31s 619: learn: 0.3053711 test: 0.4971934 best: 0.4971934 (619) total: 21.5s remaining: 2m 31s 620: learn: 0.3050059 test: 0.4971241 best: 0.4971241 (620) total: 21.5s remaining: 2m 31s 621: learn: 0.3046991 test: 0.4970505 best: 0.4970505 (621) total: 21.5s remaining: 2m 31s 622: learn: 0.3044359 test: 0.4969707 best: 0.4969707 (622) total: 21.6s remaining: 2m 31s 623: learn: 0.3039277 test: 0.4969506 best: 0.4969506 (623) total: 21.6s remaining: 2m 31s 624: learn: 0.3036633 test: 0.4968971 best: 0.4968971 (624) total: 21.6s remaining: 2m 31s 625: learn: 0.3033602 test: 0.4968110 best: 0.4968110 (625) total: 21.7s remaining: 2m 31s 626: learn: 0.3029977 test: 0.4967506 best: 0.4967506 (626) total: 21.7s remaining: 2m 31s 627: learn: 0.3028024 test: 0.4967170 best: 0.4967170 (627) total: 21.8s remaining: 2m 31s 628: learn: 0.3025088 test: 0.4965876 best: 0.4965876 (628) total: 21.8s remaining: 2m 31s 629: learn: 0.3022282 test: 0.4965964 best: 0.4965876 (628) total: 21.8s remaining: 2m 31s 630: learn: 0.3019389 test: 0.4964865 best: 0.4964865 (630) total: 21.9s remaining: 2m 31s 631: learn: 0.3017589 test: 0.4964870 best: 0.4964865 (630) total: 21.9s remaining: 2m 31s 632: learn: 0.3014651 test: 0.4964724 best: 0.4964724 (632) total: 21.9s remaining: 2m 31s 633: learn: 0.3011478 test: 0.4964389 best: 0.4964389 (633) total: 22s remaining: 2m 31s 634: learn: 0.3007239 test: 0.4962798 best: 0.4962798 (634) total: 22s remaining: 2m 31s 635: learn: 0.3004192 test: 0.4963169 best: 0.4962798 (634) total: 22s remaining: 2m 31s 636: learn: 0.3002759 test: 0.4963341 best: 0.4962798 (634) total: 22.1s remaining: 2m 31s 637: learn: 0.2998897 test: 0.4961614 best: 0.4961614 (637) total: 22.1s remaining: 2m 31s 638: learn: 0.2995146 test: 0.4960511 best: 0.4960511 (638) total: 22.1s remaining: 2m 30s 639: learn: 0.2994154 test: 0.4960411 best: 0.4960411 (639) total: 22.2s remaining: 2m 30s 640: learn: 0.2990775 test: 0.4959282 best: 0.4959282 (640) total: 22.2s remaining: 2m 30s 641: learn: 0.2989304 test: 0.4959013 best: 0.4959013 (641) total: 22.2s remaining: 2m 30s 642: learn: 0.2987006 test: 0.4957808 best: 0.4957808 (642) total: 22.3s remaining: 2m 30s 643: learn: 0.2983518 test: 0.4956733 best: 0.4956733 (643) total: 22.3s remaining: 2m 30s 644: learn: 0.2982543 test: 0.4955925 best: 0.4955925 (644) total: 22.3s remaining: 2m 30s 645: learn: 0.2979780 test: 0.4954589 best: 0.4954589 (645) total: 22.4s remaining: 2m 30s 646: learn: 0.2976976 test: 0.4954906 best: 0.4954589 (645) total: 22.4s remaining: 2m 30s 647: learn: 0.2975599 test: 0.4954266 best: 0.4954266 (647) total: 22.4s remaining: 2m 30s 648: learn: 0.2970687 test: 0.4953288 best: 0.4953288 (648) total: 22.5s remaining: 2m 30s 649: learn: 0.2968891 test: 0.4954319 best: 0.4953288 (648) total: 22.5s remaining: 2m 30s 650: learn: 0.2967368 test: 0.4953720 best: 0.4953288 (648) total: 22.5s remaining: 2m 30s 651: learn: 0.2965960 test: 0.4954443 best: 0.4953288 (648) total: 22.6s remaining: 2m 30s 652: learn: 0.2962900 test: 0.4954875 best: 0.4953288 (648) total: 22.6s remaining: 2m 30s 653: learn: 0.2961682 test: 0.4954147 best: 0.4953288 (648) total: 22.6s remaining: 2m 30s 654: learn: 0.2958379 test: 0.4953502 best: 0.4953288 (648) total: 22.7s remaining: 2m 30s 655: learn: 0.2956257 test: 0.4952133 best: 0.4952133 (655) total: 22.7s remaining: 2m 30s 656: learn: 0.2953005 test: 0.4951936 best: 0.4951936 (656) total: 22.7s remaining: 2m 30s 657: learn: 0.2951110 test: 0.4951570 best: 0.4951570 (657) total: 22.8s remaining: 2m 30s 658: learn: 0.2946143 test: 0.4949826 best: 0.4949826 (658) total: 22.8s remaining: 2m 30s 659: learn: 0.2943853 test: 0.4949466 best: 0.4949466 (659) total: 22.8s remaining: 2m 30s 660: learn: 0.2941594 test: 0.4948963 best: 0.4948963 (660) total: 22.9s remaining: 2m 30s 661: learn: 0.2938667 test: 0.4947920 best: 0.4947920 (661) total: 22.9s remaining: 2m 30s 662: learn: 0.2936040 test: 0.4947652 best: 0.4947652 (662) total: 22.9s remaining: 2m 30s 663: learn: 0.2932500 test: 0.4946569 best: 0.4946569 (663) total: 23s remaining: 2m 30s 664: learn: 0.2930901 test: 0.4946128 best: 0.4946128 (664) total: 23s remaining: 2m 29s 665: learn: 0.2927975 test: 0.4946107 best: 0.4946107 (665) total: 23s remaining: 2m 29s 666: learn: 0.2924591 test: 0.4945735 best: 0.4945735 (666) total: 23.1s remaining: 2m 29s 667: learn: 0.2922171 test: 0.4946254 best: 0.4945735 (666) total: 23.1s remaining: 2m 29s 668: learn: 0.2919828 test: 0.4946240 best: 0.4945735 (666) total: 23.1s remaining: 2m 29s 669: learn: 0.2917669 test: 0.4945784 best: 0.4945735 (666) total: 23.2s remaining: 2m 29s 670: learn: 0.2916411 test: 0.4945728 best: 0.4945728 (670) total: 23.2s remaining: 2m 29s 671: learn: 0.2915147 test: 0.4945192 best: 0.4945192 (671) total: 23.2s remaining: 2m 29s 672: learn: 0.2913001 test: 0.4944925 best: 0.4944925 (672) total: 23.3s remaining: 2m 29s 673: learn: 0.2911770 test: 0.4944957 best: 0.4944925 (672) total: 23.3s remaining: 2m 29s 674: learn: 0.2908693 test: 0.4943765 best: 0.4943765 (674) total: 23.3s remaining: 2m 29s 675: learn: 0.2903913 test: 0.4941713 best: 0.4941713 (675) total: 23.4s remaining: 2m 29s 676: learn: 0.2900031 test: 0.4941741 best: 0.4941713 (675) total: 23.4s remaining: 2m 29s 677: learn: 0.2898180 test: 0.4941920 best: 0.4941713 (675) total: 23.4s remaining: 2m 29s 678: learn: 0.2895600 test: 0.4942349 best: 0.4941713 (675) total: 23.5s remaining: 2m 29s 679: learn: 0.2893500 test: 0.4943002 best: 0.4941713 (675) total: 23.5s remaining: 2m 29s 680: learn: 0.2891705 test: 0.4942660 best: 0.4941713 (675) total: 23.5s remaining: 2m 29s 681: learn: 0.2888939 test: 0.4941760 best: 0.4941713 (675) total: 23.6s remaining: 2m 29s 682: learn: 0.2886384 test: 0.4941334 best: 0.4941334 (682) total: 23.6s remaining: 2m 29s 683: learn: 0.2882642 test: 0.4939632 best: 0.4939632 (683) total: 23.6s remaining: 2m 29s 684: learn: 0.2881192 test: 0.4939170 best: 0.4939170 (684) total: 23.7s remaining: 2m 29s 685: learn: 0.2878809 test: 0.4939107 best: 0.4939107 (685) total: 23.7s remaining: 2m 29s 686: learn: 0.2876442 test: 0.4939261 best: 0.4939107 (685) total: 23.7s remaining: 2m 29s 687: learn: 0.2872755 test: 0.4939227 best: 0.4939107 (685) total: 23.8s remaining: 2m 28s 688: learn: 0.2870175 test: 0.4937726 best: 0.4937726 (688) total: 23.8s remaining: 2m 28s 689: learn: 0.2867794 test: 0.4938528 best: 0.4937726 (688) total: 23.8s remaining: 2m 28s 690: learn: 0.2862302 test: 0.4936692 best: 0.4936692 (690) total: 23.9s remaining: 2m 28s 691: learn: 0.2858849 test: 0.4936164 best: 0.4936164 (691) total: 23.9s remaining: 2m 28s 692: learn: 0.2857419 test: 0.4936212 best: 0.4936164 (691) total: 23.9s remaining: 2m 28s 693: learn: 0.2854235 test: 0.4935745 best: 0.4935745 (693) total: 24s remaining: 2m 28s 694: learn: 0.2852746 test: 0.4935177 best: 0.4935177 (694) total: 24s remaining: 2m 28s 695: learn: 0.2851502 test: 0.4934173 best: 0.4934173 (695) total: 24s remaining: 2m 28s 696: learn: 0.2849355 test: 0.4934210 best: 0.4934173 (695) total: 24.1s remaining: 2m 28s 697: learn: 0.2847048 test: 0.4934208 best: 0.4934173 (695) total: 24.1s remaining: 2m 28s 698: learn: 0.2844235 test: 0.4932865 best: 0.4932865 (698) total: 24.1s remaining: 2m 28s 699: learn: 0.2841740 test: 0.4931998 best: 0.4931998 (699) total: 24.2s remaining: 2m 28s 700: learn: 0.2840385 test: 0.4931663 best: 0.4931663 (700) total: 24.2s remaining: 2m 28s 701: learn: 0.2836211 test: 0.4932080 best: 0.4931663 (700) total: 24.2s remaining: 2m 28s 702: learn: 0.2832004 test: 0.4932076 best: 0.4931663 (700) total: 24.3s remaining: 2m 28s 703: learn: 0.2827228 test: 0.4930672 best: 0.4930672 (703) total: 24.3s remaining: 2m 28s 704: learn: 0.2824444 test: 0.4930721 best: 0.4930672 (703) total: 24.4s remaining: 2m 28s 705: learn: 0.2821820 test: 0.4930125 best: 0.4930125 (705) total: 24.4s remaining: 2m 28s 706: learn: 0.2818706 test: 0.4930178 best: 0.4930125 (705) total: 24.4s remaining: 2m 28s 707: learn: 0.2815043 test: 0.4929139 best: 0.4929139 (707) total: 24.5s remaining: 2m 28s 708: learn: 0.2813406 test: 0.4929603 best: 0.4929139 (707) total: 24.5s remaining: 2m 28s 709: learn: 0.2809902 test: 0.4929294 best: 0.4929139 (707) total: 24.5s remaining: 2m 28s 710: learn: 0.2808117 test: 0.4927819 best: 0.4927819 (710) total: 24.6s remaining: 2m 28s 711: learn: 0.2806990 test: 0.4927294 best: 0.4927294 (711) total: 24.6s remaining: 2m 28s 712: learn: 0.2802826 test: 0.4927216 best: 0.4927216 (712) total: 24.6s remaining: 2m 28s 713: learn: 0.2801076 test: 0.4927055 best: 0.4927055 (713) total: 24.7s remaining: 2m 28s 714: learn: 0.2798737 test: 0.4926443 best: 0.4926443 (714) total: 24.7s remaining: 2m 27s 715: learn: 0.2797041 test: 0.4926673 best: 0.4926443 (714) total: 24.7s remaining: 2m 28s 716: learn: 0.2795122 test: 0.4926868 best: 0.4926443 (714) total: 24.8s remaining: 2m 27s 717: learn: 0.2793360 test: 0.4927034 best: 0.4926443 (714) total: 24.8s remaining: 2m 27s 718: learn: 0.2791118 test: 0.4927077 best: 0.4926443 (714) total: 24.8s remaining: 2m 27s 719: learn: 0.2788153 test: 0.4927368 best: 0.4926443 (714) total: 24.9s remaining: 2m 27s 720: learn: 0.2785479 test: 0.4927278 best: 0.4926443 (714) total: 24.9s remaining: 2m 27s 721: learn: 0.2782488 test: 0.4926107 best: 0.4926107 (721) total: 24.9s remaining: 2m 27s 722: learn: 0.2778821 test: 0.4925590 best: 0.4925590 (722) total: 25s remaining: 2m 27s 723: learn: 0.2775780 test: 0.4925492 best: 0.4925492 (723) total: 25s remaining: 2m 27s 724: learn: 0.2771166 test: 0.4924987 best: 0.4924987 (724) total: 25.1s remaining: 2m 27s 725: learn: 0.2769843 test: 0.4924651 best: 0.4924651 (725) total: 25.1s remaining: 2m 27s 726: learn: 0.2768031 test: 0.4924416 best: 0.4924416 (726) total: 25.1s remaining: 2m 27s 727: learn: 0.2765439 test: 0.4923553 best: 0.4923553 (727) total: 25.1s remaining: 2m 27s 728: learn: 0.2762906 test: 0.4924031 best: 0.4923553 (727) total: 25.2s remaining: 2m 27s 729: learn: 0.2761386 test: 0.4924144 best: 0.4923553 (727) total: 25.2s remaining: 2m 27s 730: learn: 0.2758354 test: 0.4924478 best: 0.4923553 (727) total: 25.3s remaining: 2m 27s 731: learn: 0.2757052 test: 0.4924417 best: 0.4923553 (727) total: 25.3s remaining: 2m 27s 732: learn: 0.2755088 test: 0.4923492 best: 0.4923492 (732) total: 25.3s remaining: 2m 27s 733: learn: 0.2752859 test: 0.4923761 best: 0.4923492 (732) total: 25.4s remaining: 2m 27s 734: learn: 0.2749366 test: 0.4924214 best: 0.4923492 (732) total: 25.4s remaining: 2m 27s 735: learn: 0.2746816 test: 0.4923170 best: 0.4923170 (735) total: 25.4s remaining: 2m 27s 736: learn: 0.2745468 test: 0.4922739 best: 0.4922739 (736) total: 25.5s remaining: 2m 27s 737: learn: 0.2742013 test: 0.4922990 best: 0.4922739 (736) total: 25.5s remaining: 2m 27s 738: learn: 0.2739300 test: 0.4922782 best: 0.4922739 (736) total: 25.5s remaining: 2m 27s 739: learn: 0.2737395 test: 0.4923371 best: 0.4922739 (736) total: 25.6s remaining: 2m 27s 740: learn: 0.2735181 test: 0.4922818 best: 0.4922739 (736) total: 25.6s remaining: 2m 27s 741: learn: 0.2732474 test: 0.4922791 best: 0.4922739 (736) total: 25.6s remaining: 2m 27s 742: learn: 0.2729963 test: 0.4922140 best: 0.4922140 (742) total: 25.7s remaining: 2m 27s 743: learn: 0.2727857 test: 0.4920640 best: 0.4920640 (743) total: 25.7s remaining: 2m 27s 744: learn: 0.2726182 test: 0.4919782 best: 0.4919782 (744) total: 25.7s remaining: 2m 27s 745: learn: 0.2723404 test: 0.4919430 best: 0.4919430 (745) total: 25.8s remaining: 2m 26s 746: learn: 0.2721065 test: 0.4919327 best: 0.4919327 (746) total: 25.8s remaining: 2m 26s 747: learn: 0.2719951 test: 0.4918789 best: 0.4918789 (747) total: 25.8s remaining: 2m 26s 748: learn: 0.2718246 test: 0.4918470 best: 0.4918470 (748) total: 25.9s remaining: 2m 26s 749: learn: 0.2716697 test: 0.4918373 best: 0.4918373 (749) total: 25.9s remaining: 2m 26s 750: learn: 0.2714772 test: 0.4917833 best: 0.4917833 (750) total: 25.9s remaining: 2m 26s 751: learn: 0.2712159 test: 0.4918157 best: 0.4917833 (750) total: 26s remaining: 2m 26s 752: learn: 0.2708023 test: 0.4917231 best: 0.4917231 (752) total: 26s remaining: 2m 26s 753: learn: 0.2704620 test: 0.4916741 best: 0.4916741 (753) total: 26s remaining: 2m 26s 754: learn: 0.2702719 test: 0.4916163 best: 0.4916163 (754) total: 26.1s remaining: 2m 26s 755: learn: 0.2699809 test: 0.4915446 best: 0.4915446 (755) total: 26.1s remaining: 2m 26s 756: learn: 0.2699056 test: 0.4915732 best: 0.4915446 (755) total: 26.1s remaining: 2m 26s 757: learn: 0.2697243 test: 0.4915631 best: 0.4915446 (755) total: 26.2s remaining: 2m 26s 758: learn: 0.2696107 test: 0.4915454 best: 0.4915446 (755) total: 26.2s remaining: 2m 26s 759: learn: 0.2694829 test: 0.4915834 best: 0.4915446 (755) total: 26.2s remaining: 2m 26s 760: learn: 0.2692084 test: 0.4915004 best: 0.4915004 (760) total: 26.3s remaining: 2m 26s 761: learn: 0.2691305 test: 0.4915004 best: 0.4915004 (760) total: 26.3s remaining: 2m 26s 762: learn: 0.2688567 test: 0.4914839 best: 0.4914839 (762) total: 26.4s remaining: 2m 26s 763: learn: 0.2685142 test: 0.4914181 best: 0.4914181 (763) total: 26.4s remaining: 2m 26s 764: learn: 0.2682690 test: 0.4915011 best: 0.4914181 (763) total: 26.4s remaining: 2m 26s 765: learn: 0.2681305 test: 0.4915627 best: 0.4914181 (763) total: 26.5s remaining: 2m 26s 766: learn: 0.2678833 test: 0.4915632 best: 0.4914181 (763) total: 26.5s remaining: 2m 26s 767: learn: 0.2677390 test: 0.4915884 best: 0.4914181 (763) total: 26.5s remaining: 2m 26s 768: learn: 0.2673737 test: 0.4914004 best: 0.4914004 (768) total: 26.6s remaining: 2m 26s 769: learn: 0.2671946 test: 0.4913226 best: 0.4913226 (769) total: 26.6s remaining: 2m 26s 770: learn: 0.2668785 test: 0.4911949 best: 0.4911949 (770) total: 26.6s remaining: 2m 26s 771: learn: 0.2667773 test: 0.4911977 best: 0.4911949 (770) total: 26.6s remaining: 2m 25s 772: learn: 0.2664484 test: 0.4911933 best: 0.4911933 (772) total: 26.7s remaining: 2m 25s 773: learn: 0.2663282 test: 0.4911758 best: 0.4911758 (773) total: 26.7s remaining: 2m 25s 774: learn: 0.2660447 test: 0.4912628 best: 0.4911758 (773) total: 26.8s remaining: 2m 25s 775: learn: 0.2659047 test: 0.4912054 best: 0.4911758 (773) total: 26.8s remaining: 2m 25s 776: learn: 0.2657889 test: 0.4911429 best: 0.4911429 (776) total: 26.8s remaining: 2m 25s 777: learn: 0.2656147 test: 0.4911329 best: 0.4911329 (777) total: 26.8s remaining: 2m 25s 778: learn: 0.2653435 test: 0.4911065 best: 0.4911065 (778) total: 26.9s remaining: 2m 25s 779: learn: 0.2651462 test: 0.4910555 best: 0.4910555 (779) total: 26.9s remaining: 2m 25s 780: learn: 0.2649711 test: 0.4910372 best: 0.4910372 (780) total: 26.9s remaining: 2m 25s 781: learn: 0.2648206 test: 0.4910384 best: 0.4910372 (780) total: 27s remaining: 2m 25s 782: learn: 0.2645030 test: 0.4910135 best: 0.4910135 (782) total: 27s remaining: 2m 25s 783: learn: 0.2641808 test: 0.4910216 best: 0.4910135 (782) total: 27s remaining: 2m 25s 784: learn: 0.2639813 test: 0.4909282 best: 0.4909282 (784) total: 27.1s remaining: 2m 25s 785: learn: 0.2637955 test: 0.4909517 best: 0.4909282 (784) total: 27.1s remaining: 2m 25s 786: learn: 0.2636691 test: 0.4909308 best: 0.4909282 (784) total: 27.1s remaining: 2m 25s 787: learn: 0.2634787 test: 0.4909895 best: 0.4909282 (784) total: 27.2s remaining: 2m 25s 788: learn: 0.2632271 test: 0.4909417 best: 0.4909282 (784) total: 27.2s remaining: 2m 25s 789: learn: 0.2630014 test: 0.4909567 best: 0.4909282 (784) total: 27.3s remaining: 2m 25s 790: learn: 0.2625620 test: 0.4910338 best: 0.4909282 (784) total: 27.3s remaining: 2m 25s 791: learn: 0.2623368 test: 0.4910762 best: 0.4909282 (784) total: 27.3s remaining: 2m 25s 792: learn: 0.2621335 test: 0.4910292 best: 0.4909282 (784) total: 27.4s remaining: 2m 25s 793: learn: 0.2619385 test: 0.4909420 best: 0.4909282 (784) total: 27.4s remaining: 2m 25s 794: learn: 0.2614991 test: 0.4909163 best: 0.4909163 (794) total: 27.4s remaining: 2m 25s 795: learn: 0.2612450 test: 0.4909294 best: 0.4909163 (794) total: 27.5s remaining: 2m 25s 796: learn: 0.2610542 test: 0.4908192 best: 0.4908192 (796) total: 27.5s remaining: 2m 24s 797: learn: 0.2608908 test: 0.4907676 best: 0.4907676 (797) total: 27.5s remaining: 2m 24s 798: learn: 0.2605107 test: 0.4907778 best: 0.4907676 (797) total: 27.6s remaining: 2m 24s 799: learn: 0.2603777 test: 0.4908349 best: 0.4907676 (797) total: 27.6s remaining: 2m 24s 800: learn: 0.2602066 test: 0.4908700 best: 0.4907676 (797) total: 27.6s remaining: 2m 24s 801: learn: 0.2598250 test: 0.4909253 best: 0.4907676 (797) total: 27.7s remaining: 2m 24s 802: learn: 0.2596827 test: 0.4908250 best: 0.4907676 (797) total: 27.7s remaining: 2m 24s 803: learn: 0.2595364 test: 0.4908332 best: 0.4907676 (797) total: 27.7s remaining: 2m 24s 804: learn: 0.2591751 test: 0.4907721 best: 0.4907676 (797) total: 27.8s remaining: 2m 24s 805: learn: 0.2590287 test: 0.4907459 best: 0.4907459 (805) total: 27.8s remaining: 2m 24s 806: learn: 0.2587923 test: 0.4907151 best: 0.4907151 (806) total: 27.8s remaining: 2m 24s 807: learn: 0.2586548 test: 0.4906745 best: 0.4906745 (807) total: 27.9s remaining: 2m 24s 808: learn: 0.2585227 test: 0.4906114 best: 0.4906114 (808) total: 27.9s remaining: 2m 24s 809: learn: 0.2583299 test: 0.4905112 best: 0.4905112 (809) total: 27.9s remaining: 2m 24s 810: learn: 0.2580520 test: 0.4905260 best: 0.4905112 (809) total: 28s remaining: 2m 24s 811: learn: 0.2578109 test: 0.4904579 best: 0.4904579 (811) total: 28s remaining: 2m 24s 812: learn: 0.2576158 test: 0.4905003 best: 0.4904579 (811) total: 28s remaining: 2m 24s 813: learn: 0.2571189 test: 0.4904363 best: 0.4904363 (813) total: 28s remaining: 2m 24s 814: learn: 0.2569923 test: 0.4904218 best: 0.4904218 (814) total: 28.1s remaining: 2m 24s 815: learn: 0.2568170 test: 0.4905209 best: 0.4904218 (814) total: 28.1s remaining: 2m 24s 816: learn: 0.2566252 test: 0.4905590 best: 0.4904218 (814) total: 28.2s remaining: 2m 24s 817: learn: 0.2563934 test: 0.4905354 best: 0.4904218 (814) total: 28.2s remaining: 2m 24s 818: learn: 0.2560387 test: 0.4905886 best: 0.4904218 (814) total: 28.2s remaining: 2m 24s 819: learn: 0.2558179 test: 0.4906477 best: 0.4904218 (814) total: 28.3s remaining: 2m 24s 820: learn: 0.2553885 test: 0.4906891 best: 0.4904218 (814) total: 28.3s remaining: 2m 24s 821: learn: 0.2551104 test: 0.4907485 best: 0.4904218 (814) total: 28.3s remaining: 2m 24s 822: learn: 0.2548529 test: 0.4907340 best: 0.4904218 (814) total: 28.4s remaining: 2m 24s 823: learn: 0.2546667 test: 0.4906734 best: 0.4904218 (814) total: 28.4s remaining: 2m 23s 824: learn: 0.2545689 test: 0.4906788 best: 0.4904218 (814) total: 28.4s remaining: 2m 23s 825: learn: 0.2543666 test: 0.4905955 best: 0.4904218 (814) total: 28.5s remaining: 2m 23s 826: learn: 0.2542205 test: 0.4906638 best: 0.4904218 (814) total: 28.5s remaining: 2m 23s 827: learn: 0.2540835 test: 0.4906721 best: 0.4904218 (814) total: 28.5s remaining: 2m 23s 828: learn: 0.2539269 test: 0.4906050 best: 0.4904218 (814) total: 28.6s remaining: 2m 23s 829: learn: 0.2536984 test: 0.4905372 best: 0.4904218 (814) total: 28.6s remaining: 2m 23s 830: learn: 0.2534765 test: 0.4904818 best: 0.4904218 (814) total: 28.6s remaining: 2m 23s 831: learn: 0.2532148 test: 0.4905637 best: 0.4904218 (814) total: 28.7s remaining: 2m 23s 832: learn: 0.2530528 test: 0.4906019 best: 0.4904218 (814) total: 28.7s remaining: 2m 23s 833: learn: 0.2525643 test: 0.4904959 best: 0.4904218 (814) total: 28.7s remaining: 2m 23s 834: learn: 0.2522514 test: 0.4905544 best: 0.4904218 (814) total: 28.8s remaining: 2m 23s 835: learn: 0.2520909 test: 0.4905376 best: 0.4904218 (814) total: 28.8s remaining: 2m 23s 836: learn: 0.2519475 test: 0.4904628 best: 0.4904218 (814) total: 28.8s remaining: 2m 23s 837: learn: 0.2517906 test: 0.4904159 best: 0.4904159 (837) total: 28.9s remaining: 2m 23s 838: learn: 0.2516793 test: 0.4903830 best: 0.4903830 (838) total: 28.9s remaining: 2m 23s 839: learn: 0.2514118 test: 0.4903353 best: 0.4903353 (839) total: 28.9s remaining: 2m 23s 840: learn: 0.2511826 test: 0.4903827 best: 0.4903353 (839) total: 29s remaining: 2m 23s 841: learn: 0.2510117 test: 0.4904246 best: 0.4903353 (839) total: 29s remaining: 2m 23s 842: learn: 0.2507904 test: 0.4903937 best: 0.4903353 (839) total: 29s remaining: 2m 23s 843: learn: 0.2503442 test: 0.4903040 best: 0.4903040 (843) total: 29.1s remaining: 2m 23s 844: learn: 0.2502183 test: 0.4902903 best: 0.4902903 (844) total: 29.1s remaining: 2m 23s 845: learn: 0.2499898 test: 0.4903197 best: 0.4902903 (844) total: 29.1s remaining: 2m 23s 846: learn: 0.2497256 test: 0.4904209 best: 0.4902903 (844) total: 29.2s remaining: 2m 23s 847: learn: 0.2494838 test: 0.4903641 best: 0.4902903 (844) total: 29.2s remaining: 2m 22s 848: learn: 0.2492119 test: 0.4902973 best: 0.4902903 (844) total: 29.2s remaining: 2m 22s 849: learn: 0.2488164 test: 0.4903470 best: 0.4902903 (844) total: 29.3s remaining: 2m 22s 850: learn: 0.2485028 test: 0.4903813 best: 0.4902903 (844) total: 29.3s remaining: 2m 22s 851: learn: 0.2484032 test: 0.4903878 best: 0.4902903 (844) total: 29.4s remaining: 2m 22s 852: learn: 0.2483033 test: 0.4903430 best: 0.4902903 (844) total: 29.4s remaining: 2m 22s 853: learn: 0.2481155 test: 0.4903185 best: 0.4902903 (844) total: 29.4s remaining: 2m 22s 854: learn: 0.2478481 test: 0.4903021 best: 0.4902903 (844) total: 29.4s remaining: 2m 22s 855: learn: 0.2475879 test: 0.4903285 best: 0.4902903 (844) total: 29.5s remaining: 2m 22s 856: learn: 0.2473931 test: 0.4903456 best: 0.4902903 (844) total: 29.5s remaining: 2m 22s 857: learn: 0.2473499 test: 0.4902933 best: 0.4902903 (844) total: 29.5s remaining: 2m 22s 858: learn: 0.2471707 test: 0.4903350 best: 0.4902903 (844) total: 29.6s remaining: 2m 22s 859: learn: 0.2470242 test: 0.4903605 best: 0.4902903 (844) total: 29.6s remaining: 2m 22s 860: learn: 0.2468909 test: 0.4904426 best: 0.4902903 (844) total: 29.6s remaining: 2m 22s 861: learn: 0.2467379 test: 0.4904976 best: 0.4902903 (844) total: 29.7s remaining: 2m 22s 862: learn: 0.2466588 test: 0.4904643 best: 0.4902903 (844) total: 29.7s remaining: 2m 22s 863: learn: 0.2464389 test: 0.4904755 best: 0.4902903 (844) total: 29.7s remaining: 2m 22s 864: learn: 0.2462606 test: 0.4904747 best: 0.4902903 (844) total: 29.8s remaining: 2m 22s 865: learn: 0.2461012 test: 0.4903900 best: 0.4902903 (844) total: 29.8s remaining: 2m 22s 866: learn: 0.2459558 test: 0.4903310 best: 0.4902903 (844) total: 29.8s remaining: 2m 22s 867: learn: 0.2458401 test: 0.4904002 best: 0.4902903 (844) total: 29.9s remaining: 2m 22s 868: learn: 0.2455174 test: 0.4903984 best: 0.4902903 (844) total: 29.9s remaining: 2m 22s 869: learn: 0.2452790 test: 0.4904356 best: 0.4902903 (844) total: 29.9s remaining: 2m 22s 870: learn: 0.2451846 test: 0.4903943 best: 0.4902903 (844) total: 30s remaining: 2m 22s 871: learn: 0.2449965 test: 0.4903553 best: 0.4902903 (844) total: 30s remaining: 2m 22s 872: learn: 0.2447329 test: 0.4903957 best: 0.4902903 (844) total: 30s remaining: 2m 22s 873: learn: 0.2445137 test: 0.4903537 best: 0.4902903 (844) total: 30.1s remaining: 2m 21s 874: learn: 0.2443751 test: 0.4904526 best: 0.4902903 (844) total: 30.1s remaining: 2m 21s 875: learn: 0.2441582 test: 0.4903631 best: 0.4902903 (844) total: 30.1s remaining: 2m 21s 876: learn: 0.2440462 test: 0.4903590 best: 0.4902903 (844) total: 30.2s remaining: 2m 21s 877: learn: 0.2439066 test: 0.4903021 best: 0.4902903 (844) total: 30.2s remaining: 2m 21s 878: learn: 0.2436997 test: 0.4902553 best: 0.4902553 (878) total: 30.2s remaining: 2m 21s 879: learn: 0.2433934 test: 0.4902929 best: 0.4902553 (878) total: 30.3s remaining: 2m 21s 880: learn: 0.2432255 test: 0.4903219 best: 0.4902553 (878) total: 30.3s remaining: 2m 21s 881: learn: 0.2430559 test: 0.4903027 best: 0.4902553 (878) total: 30.3s remaining: 2m 21s 882: learn: 0.2428633 test: 0.4902947 best: 0.4902553 (878) total: 30.4s remaining: 2m 21s 883: learn: 0.2427204 test: 0.4902484 best: 0.4902484 (883) total: 30.4s remaining: 2m 21s 884: learn: 0.2425730 test: 0.4902308 best: 0.4902308 (884) total: 30.4s remaining: 2m 21s 885: learn: 0.2423231 test: 0.4903108 best: 0.4902308 (884) total: 30.5s remaining: 2m 21s 886: learn: 0.2421924 test: 0.4903605 best: 0.4902308 (884) total: 30.5s remaining: 2m 21s 887: learn: 0.2420942 test: 0.4904071 best: 0.4902308 (884) total: 30.5s remaining: 2m 21s 888: learn: 0.2419063 test: 0.4904233 best: 0.4902308 (884) total: 30.6s remaining: 2m 21s 889: learn: 0.2417730 test: 0.4903910 best: 0.4902308 (884) total: 30.6s remaining: 2m 21s 890: learn: 0.2416058 test: 0.4903980 best: 0.4902308 (884) total: 30.6s remaining: 2m 21s 891: learn: 0.2413382 test: 0.4903867 best: 0.4902308 (884) total: 30.7s remaining: 2m 21s 892: learn: 0.2412460 test: 0.4903661 best: 0.4902308 (884) total: 30.7s remaining: 2m 21s 893: learn: 0.2410756 test: 0.4903352 best: 0.4902308 (884) total: 30.7s remaining: 2m 21s 894: learn: 0.2407236 test: 0.4903286 best: 0.4902308 (884) total: 30.8s remaining: 2m 21s 895: learn: 0.2404761 test: 0.4902635 best: 0.4902308 (884) total: 30.8s remaining: 2m 21s 896: learn: 0.2403832 test: 0.4902636 best: 0.4902308 (884) total: 30.8s remaining: 2m 21s 897: learn: 0.2402180 test: 0.4902838 best: 0.4902308 (884) total: 30.9s remaining: 2m 21s 898: learn: 0.2401291 test: 0.4902849 best: 0.4902308 (884) total: 30.9s remaining: 2m 20s 899: learn: 0.2400004 test: 0.4902894 best: 0.4902308 (884) total: 30.9s remaining: 2m 20s 900: learn: 0.2399023 test: 0.4902753 best: 0.4902308 (884) total: 31s remaining: 2m 20s 901: learn: 0.2398067 test: 0.4902988 best: 0.4902308 (884) total: 31s remaining: 2m 20s 902: learn: 0.2395436 test: 0.4903049 best: 0.4902308 (884) total: 31s remaining: 2m 20s 903: learn: 0.2393908 test: 0.4902819 best: 0.4902308 (884) total: 31.1s remaining: 2m 20s 904: learn: 0.2392937 test: 0.4903209 best: 0.4902308 (884) total: 31.1s remaining: 2m 20s 905: learn: 0.2392108 test: 0.4902813 best: 0.4902308 (884) total: 31.1s remaining: 2m 20s 906: learn: 0.2390973 test: 0.4902683 best: 0.4902308 (884) total: 31.2s remaining: 2m 20s 907: learn: 0.2387787 test: 0.4901894 best: 0.4901894 (907) total: 31.2s remaining: 2m 20s 908: learn: 0.2386234 test: 0.4901479 best: 0.4901479 (908) total: 31.3s remaining: 2m 20s 909: learn: 0.2385257 test: 0.4901809 best: 0.4901479 (908) total: 31.3s remaining: 2m 20s 910: learn: 0.2383190 test: 0.4901516 best: 0.4901479 (908) total: 31.3s remaining: 2m 20s 911: learn: 0.2382328 test: 0.4901412 best: 0.4901412 (911) total: 31.3s remaining: 2m 20s 912: learn: 0.2380038 test: 0.4901543 best: 0.4901412 (911) total: 31.4s remaining: 2m 20s 913: learn: 0.2378741 test: 0.4901700 best: 0.4901412 (911) total: 31.4s remaining: 2m 20s 914: learn: 0.2376506 test: 0.4902203 best: 0.4901412 (911) total: 31.4s remaining: 2m 20s 915: learn: 0.2375313 test: 0.4901867 best: 0.4901412 (911) total: 31.5s remaining: 2m 20s 916: learn: 0.2374190 test: 0.4902060 best: 0.4901412 (911) total: 31.5s remaining: 2m 20s 917: learn: 0.2372793 test: 0.4901369 best: 0.4901369 (917) total: 31.5s remaining: 2m 20s 918: learn: 0.2369644 test: 0.4900524 best: 0.4900524 (918) total: 31.6s remaining: 2m 20s 919: learn: 0.2368084 test: 0.4900668 best: 0.4900524 (918) total: 31.6s remaining: 2m 20s 920: learn: 0.2365776 test: 0.4900365 best: 0.4900365 (920) total: 31.6s remaining: 2m 20s 921: learn: 0.2362257 test: 0.4900381 best: 0.4900365 (920) total: 31.7s remaining: 2m 20s 922: learn: 0.2360464 test: 0.4899938 best: 0.4899938 (922) total: 31.7s remaining: 2m 20s 923: learn: 0.2359102 test: 0.4899880 best: 0.4899880 (923) total: 31.7s remaining: 2m 19s 924: learn: 0.2356933 test: 0.4899713 best: 0.4899713 (924) total: 31.8s remaining: 2m 19s 925: learn: 0.2355873 test: 0.4899651 best: 0.4899651 (925) total: 31.8s remaining: 2m 19s 926: learn: 0.2351232 test: 0.4898208 best: 0.4898208 (926) total: 31.8s remaining: 2m 19s 927: learn: 0.2349985 test: 0.4898551 best: 0.4898208 (926) total: 31.9s remaining: 2m 19s 928: learn: 0.2348398 test: 0.4898731 best: 0.4898208 (926) total: 31.9s remaining: 2m 19s 929: learn: 0.2346913 test: 0.4898281 best: 0.4898208 (926) total: 31.9s remaining: 2m 19s 930: learn: 0.2343585 test: 0.4898586 best: 0.4898208 (926) total: 32s remaining: 2m 19s 931: learn: 0.2342098 test: 0.4898280 best: 0.4898208 (926) total: 32s remaining: 2m 19s 932: learn: 0.2340783 test: 0.4898699 best: 0.4898208 (926) total: 32s remaining: 2m 19s 933: learn: 0.2338443 test: 0.4899332 best: 0.4898208 (926) total: 32.1s remaining: 2m 19s 934: learn: 0.2336190 test: 0.4899985 best: 0.4898208 (926) total: 32.1s remaining: 2m 19s 935: learn: 0.2332668 test: 0.4899647 best: 0.4898208 (926) total: 32.1s remaining: 2m 19s 936: learn: 0.2330866 test: 0.4899399 best: 0.4898208 (926) total: 32.2s remaining: 2m 19s 937: learn: 0.2329573 test: 0.4899939 best: 0.4898208 (926) total: 32.2s remaining: 2m 19s 938: learn: 0.2327415 test: 0.4899957 best: 0.4898208 (926) total: 32.2s remaining: 2m 19s 939: learn: 0.2326522 test: 0.4900524 best: 0.4898208 (926) total: 32.3s remaining: 2m 19s 940: learn: 0.2324305 test: 0.4901132 best: 0.4898208 (926) total: 32.3s remaining: 2m 19s 941: learn: 0.2322151 test: 0.4901082 best: 0.4898208 (926) total: 32.3s remaining: 2m 19s 942: learn: 0.2320531 test: 0.4901240 best: 0.4898208 (926) total: 32.4s remaining: 2m 19s 943: learn: 0.2319036 test: 0.4901553 best: 0.4898208 (926) total: 32.4s remaining: 2m 19s 944: learn: 0.2316571 test: 0.4901656 best: 0.4898208 (926) total: 32.4s remaining: 2m 19s 945: learn: 0.2315163 test: 0.4901774 best: 0.4898208 (926) total: 32.5s remaining: 2m 19s 946: learn: 0.2311820 test: 0.4901945 best: 0.4898208 (926) total: 32.5s remaining: 2m 19s 947: learn: 0.2311398 test: 0.4901880 best: 0.4898208 (926) total: 32.5s remaining: 2m 19s 948: learn: 0.2310423 test: 0.4901806 best: 0.4898208 (926) total: 32.6s remaining: 2m 18s 949: learn: 0.2309574 test: 0.4901415 best: 0.4898208 (926) total: 32.6s remaining: 2m 18s 950: learn: 0.2308172 test: 0.4901783 best: 0.4898208 (926) total: 32.6s remaining: 2m 18s 951: learn: 0.2306899 test: 0.4901867 best: 0.4898208 (926) total: 32.6s remaining: 2m 18s 952: learn: 0.2305959 test: 0.4901808 best: 0.4898208 (926) total: 32.7s remaining: 2m 18s 953: learn: 0.2303941 test: 0.4901921 best: 0.4898208 (926) total: 32.7s remaining: 2m 18s 954: learn: 0.2300891 test: 0.4902270 best: 0.4898208 (926) total: 32.7s remaining: 2m 18s 955: learn: 0.2299040 test: 0.4902128 best: 0.4898208 (926) total: 32.8s remaining: 2m 18s 956: learn: 0.2297484 test: 0.4901624 best: 0.4898208 (926) total: 32.8s remaining: 2m 18s 957: learn: 0.2296379 test: 0.4901617 best: 0.4898208 (926) total: 32.8s remaining: 2m 18s 958: learn: 0.2293743 test: 0.4902343 best: 0.4898208 (926) total: 32.9s remaining: 2m 18s 959: learn: 0.2292476 test: 0.4902139 best: 0.4898208 (926) total: 32.9s remaining: 2m 18s 960: learn: 0.2290634 test: 0.4901915 best: 0.4898208 (926) total: 32.9s remaining: 2m 18s 961: learn: 0.2288157 test: 0.4902235 best: 0.4898208 (926) total: 33s remaining: 2m 18s 962: learn: 0.2286365 test: 0.4901970 best: 0.4898208 (926) total: 33s remaining: 2m 18s 963: learn: 0.2284895 test: 0.4901552 best: 0.4898208 (926) total: 33s remaining: 2m 18s 964: learn: 0.2283988 test: 0.4901789 best: 0.4898208 (926) total: 33.1s remaining: 2m 18s 965: learn: 0.2283441 test: 0.4902193 best: 0.4898208 (926) total: 33.1s remaining: 2m 18s 966: learn: 0.2282120 test: 0.4902265 best: 0.4898208 (926) total: 33.1s remaining: 2m 18s 967: learn: 0.2281213 test: 0.4902152 best: 0.4898208 (926) total: 33.2s remaining: 2m 18s 968: learn: 0.2279820 test: 0.4902442 best: 0.4898208 (926) total: 33.2s remaining: 2m 18s 969: learn: 0.2276622 test: 0.4901578 best: 0.4898208 (926) total: 33.2s remaining: 2m 18s 970: learn: 0.2272495 test: 0.4901327 best: 0.4898208 (926) total: 33.3s remaining: 2m 18s 971: learn: 0.2270466 test: 0.4901347 best: 0.4898208 (926) total: 33.3s remaining: 2m 18s 972: learn: 0.2268531 test: 0.4901791 best: 0.4898208 (926) total: 33.3s remaining: 2m 17s 973: learn: 0.2267423 test: 0.4901638 best: 0.4898208 (926) total: 33.4s remaining: 2m 17s 974: learn: 0.2266236 test: 0.4901062 best: 0.4898208 (926) total: 33.4s remaining: 2m 17s 975: learn: 0.2264961 test: 0.4900127 best: 0.4898208 (926) total: 33.4s remaining: 2m 17s 976: learn: 0.2263738 test: 0.4900161 best: 0.4898208 (926) total: 33.5s remaining: 2m 17s 977: learn: 0.2261991 test: 0.4900244 best: 0.4898208 (926) total: 33.5s remaining: 2m 17s 978: learn: 0.2260806 test: 0.4899311 best: 0.4898208 (926) total: 33.5s remaining: 2m 17s 979: learn: 0.2259596 test: 0.4899195 best: 0.4898208 (926) total: 33.6s remaining: 2m 17s 980: learn: 0.2258028 test: 0.4899681 best: 0.4898208 (926) total: 33.6s remaining: 2m 17s 981: learn: 0.2256350 test: 0.4900593 best: 0.4898208 (926) total: 33.6s remaining: 2m 17s 982: learn: 0.2253846 test: 0.4900601 best: 0.4898208 (926) total: 33.7s remaining: 2m 17s 983: learn: 0.2250982 test: 0.4900272 best: 0.4898208 (926) total: 33.7s remaining: 2m 17s 984: learn: 0.2249954 test: 0.4899749 best: 0.4898208 (926) total: 33.8s remaining: 2m 17s 985: learn: 0.2248562 test: 0.4900102 best: 0.4898208 (926) total: 33.8s remaining: 2m 17s 986: learn: 0.2247468 test: 0.4900328 best: 0.4898208 (926) total: 33.8s remaining: 2m 17s 987: learn: 0.2245856 test: 0.4899943 best: 0.4898208 (926) total: 33.8s remaining: 2m 17s 988: learn: 0.2244420 test: 0.4900856 best: 0.4898208 (926) total: 33.9s remaining: 2m 17s 989: learn: 0.2243445 test: 0.4901268 best: 0.4898208 (926) total: 33.9s remaining: 2m 17s 990: learn: 0.2241971 test: 0.4901102 best: 0.4898208 (926) total: 33.9s remaining: 2m 17s 991: learn: 0.2241355 test: 0.4901088 best: 0.4898208 (926) total: 34s remaining: 2m 17s 992: learn: 0.2239205 test: 0.4901790 best: 0.4898208 (926) total: 34s remaining: 2m 17s 993: learn: 0.2238354 test: 0.4901237 best: 0.4898208 (926) total: 34s remaining: 2m 17s 994: learn: 0.2237709 test: 0.4900907 best: 0.4898208 (926) total: 34.1s remaining: 2m 17s 995: learn: 0.2236065 test: 0.4901436 best: 0.4898208 (926) total: 34.1s remaining: 2m 17s 996: learn: 0.2235406 test: 0.4901616 best: 0.4898208 (926) total: 34.1s remaining: 2m 17s 997: learn: 0.2234867 test: 0.4901609 best: 0.4898208 (926) total: 34.2s remaining: 2m 16s 998: learn: 0.2232558 test: 0.4901639 best: 0.4898208 (926) total: 34.2s remaining: 2m 16s 999: learn: 0.2230765 test: 0.4901762 best: 0.4898208 (926) total: 34.2s remaining: 2m 16s 1000: learn: 0.2229619 test: 0.4901972 best: 0.4898208 (926) total: 34.3s remaining: 2m 16s 1001: learn: 0.2227535 test: 0.4903337 best: 0.4898208 (926) total: 34.3s remaining: 2m 16s 1002: learn: 0.2226204 test: 0.4903401 best: 0.4898208 (926) total: 34.3s remaining: 2m 16s 1003: learn: 0.2224592 test: 0.4902412 best: 0.4898208 (926) total: 34.4s remaining: 2m 16s 1004: learn: 0.2223978 test: 0.4902086 best: 0.4898208 (926) total: 34.4s remaining: 2m 16s 1005: learn: 0.2221490 test: 0.4902527 best: 0.4898208 (926) total: 34.4s remaining: 2m 16s 1006: learn: 0.2219560 test: 0.4902325 best: 0.4898208 (926) total: 34.5s remaining: 2m 16s 1007: learn: 0.2218769 test: 0.4901976 best: 0.4898208 (926) total: 34.5s remaining: 2m 16s 1008: learn: 0.2217715 test: 0.4902020 best: 0.4898208 (926) total: 34.5s remaining: 2m 16s 1009: learn: 0.2213461 test: 0.4900626 best: 0.4898208 (926) total: 34.6s remaining: 2m 16s 1010: learn: 0.2212271 test: 0.4900751 best: 0.4898208 (926) total: 34.6s remaining: 2m 16s 1011: learn: 0.2211144 test: 0.4900394 best: 0.4898208 (926) total: 34.6s remaining: 2m 16s 1012: learn: 0.2208744 test: 0.4900670 best: 0.4898208 (926) total: 34.7s remaining: 2m 16s 1013: learn: 0.2207083 test: 0.4900741 best: 0.4898208 (926) total: 34.7s remaining: 2m 16s 1014: learn: 0.2204584 test: 0.4901122 best: 0.4898208 (926) total: 34.7s remaining: 2m 16s 1015: learn: 0.2203562 test: 0.4900169 best: 0.4898208 (926) total: 34.8s remaining: 2m 16s 1016: learn: 0.2201040 test: 0.4899556 best: 0.4898208 (926) total: 34.8s remaining: 2m 16s 1017: learn: 0.2199843 test: 0.4899874 best: 0.4898208 (926) total: 34.8s remaining: 2m 16s 1018: learn: 0.2197174 test: 0.4899926 best: 0.4898208 (926) total: 34.9s remaining: 2m 16s 1019: learn: 0.2194882 test: 0.4899911 best: 0.4898208 (926) total: 34.9s remaining: 2m 16s 1020: learn: 0.2192312 test: 0.4900346 best: 0.4898208 (926) total: 34.9s remaining: 2m 16s 1021: learn: 0.2190041 test: 0.4899437 best: 0.4898208 (926) total: 34.9s remaining: 2m 16s 1022: learn: 0.2188546 test: 0.4899737 best: 0.4898208 (926) total: 35s remaining: 2m 16s 1023: learn: 0.2187581 test: 0.4899831 best: 0.4898208 (926) total: 35s remaining: 2m 15s 1024: learn: 0.2186616 test: 0.4900473 best: 0.4898208 (926) total: 35s remaining: 2m 15s 1025: learn: 0.2185867 test: 0.4899903 best: 0.4898208 (926) total: 35.1s remaining: 2m 15s 1026: learn: 0.2184526 test: 0.4899548 best: 0.4898208 (926) total: 35.1s remaining: 2m 15s bestTest = 0.4898208409 bestIteration = 926 Shrink model to first 927 iterations. . &lt;catboost.core.CatBoostClassifier at 0x7f82b3f5e690&gt; . yhat_CBC = cbc.predict_proba(X_valid) logloss_CBC = log_loss(y_valid, yhat_CBC) print(&#39;Log loss using CatBoost Classifier:&#39;, logloss_CBC) . Log loss using CatBoost Classifier: 0.4887160745223992 . Submission . submission = pd.read_csv(&quot;sampleSubmission.csv&quot;) . test.drop(skewed_features, axis = 1, inplace = True) test.drop(&quot;id&quot;, axis = 1) . feat_1 feat_2 feat_3 feat_4 feat_7 feat_8 feat_9 feat_10 feat_11 feat_12 feat_13 feat_14 feat_15 feat_16 feat_17 feat_18 feat_19 feat_20 feat_21 feat_22 feat_23 feat_24 feat_25 feat_26 feat_27 feat_28 feat_29 feat_30 feat_31 feat_32 feat_33 feat_34 feat_35 feat_36 feat_37 feat_38 feat_39 feat_40 feat_41 feat_42 ... feat_50 feat_52 feat_53 feat_54 feat_55 feat_56 feat_57 feat_58 feat_59 feat_60 feat_61 feat_62 feat_63 feat_64 feat_65 feat_66 feat_67 feat_68 feat_69 feat_70 feat_71 feat_72 feat_73 feat_74 feat_75 feat_76 feat_78 feat_79 feat_80 feat_82 feat_83 feat_85 feat_86 feat_87 feat_88 feat_89 feat_90 feat_91 feat_92 feat_93 . 0 -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.023292 | -5.199338 | -5.199338 | -5.199338 | 0.888309 | 1.104747 | 0.252311 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.020888 | 0.945616 | 1.012474 | 1.159515 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.634851 | -5.199338 | -5.199338 | 0.890172 | -5.199338 | 0.864365 | -5.199338 | 0.303168 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.283836 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.239352 | 0.604448 | -5.199338 | 0.800496 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.241016 | 1.107059 | 3.089935 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | . 1 1.575613 | 1.792374 | 2.241016 | 2.511791 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.104747 | 0.644086 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.100140 | -5.199338 | 0.835558 | -5.199338 | 1.761948 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.063775 | -5.199338 | -5.199338 | -5.199338 | 2.241016 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | 2.611712 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.554270 | 3.090775 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.625274 | 1.837975 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.023292 | -5.199338 | -5.199338 | 1.562721 | -5.199338 | . 2 -5.199338 | 1.411293 | 2.085356 | 0.961429 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.401177 | 1.521525 | -5.199338 | -5.199338 | -5.199338 | 1.805023 | -5.199338 | 1.304121 | -5.199338 | -5.199338 | -5.199338 | 1.750222 | -5.199338 | -5.199338 | -5.199338 | 1.012474 | 1.159515 | 1.756055 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.979511 | -5.199338 | -5.199338 | 1.318946 | -5.199338 | -5.199338 | -5.199338 | 0.819656 | ... | -5.199338 | -5.199338 | -5.199338 | 1.625274 | 1.147315 | -5.199338 | 1.649237 | -5.199338 | -5.199338 | 0.890172 | 2.408550 | 0.437620 | 1.529558 | -5.199338 | -5.199338 | 0.817902 | -0.085414 | 1.280981 | 1.541796 | -5.199338 | -5.199338 | 1.225943 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.858747 | 1.837975 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.116369 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.756055 | . 3 -5.199338 | -5.199338 | -5.199338 | 0.961429 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.256745 | 0.943656 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.194628 | -5.199338 | -5.199338 | -5.199338 | 1.425011 | -5.199338 | -5.199338 | -5.199338 | 1.157061 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.864365 | -5.199338 | -5.199338 | 1.780005 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | 0.593940 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.085356 | -5.199338 | -5.199338 | 1.895259 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.926403 | -5.199338 | 1.918441 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.684464 | 0.487893 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | . 4 1.166919 | -5.199338 | -5.199338 | 0.961429 | 1.533612 | 1.349498 | -5.199338 | 2.023292 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.835558 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.511791 | -5.199338 | 0.630254 | -5.199338 | 0.926176 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.192071 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.107953 | -5.199338 | 1.858747 | 2.408550 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.266860 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.456904 | -5.199338 | -5.199338 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 144363 -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.352623 | -5.199338 | -5.199338 | 1.116369 | -5.199338 | -5.199338 | -5.199338 | 2.225823 | -5.199338 | -5.199338 | 1.027247 | -5.199338 | 0.645631 | -5.199338 | 0.303168 | 0.628724 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.634851 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.315958 | -5.199338 | -5.199338 | 0.819656 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.679303 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.072973 | -5.199338 | -5.199338 | 1.446104 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.800496 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.705570 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.959440 | 1.107059 | 0.689529 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | . 144364 -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.888309 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.644369 | 0.303168 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.634851 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.023003 | 1.497997 | 1.107059 | 2.386340 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | . 144365 -5.199338 | 1.411293 | -5.199338 | -5.199338 | 1.533612 | 0.869846 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.453280 | -5.199338 | 0.252311 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -0.115678 | 1.411293 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.634851 | -5.199338 | 0.979511 | -5.199338 | 2.131682 | 0.864365 | -5.199338 | 1.486537 | -5.199338 | 0.819656 | ... | -5.199338 | -5.199338 | -5.199338 | 0.286135 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.890172 | -5.199338 | 0.437620 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -0.085414 | 1.280981 | -5.199338 | 1.750222 | -5.199338 | -5.199338 | 1.159515 | 1.482760 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.023003 | 1.278137 | 1.107059 | 0.689529 | -5.199338 | -5.199338 | 1.606755 | -5.199338 | -5.199338 | . 144366 -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.327982 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.303168 | 1.887753 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.926176 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.186981 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | 0.286135 | 1.147315 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.604448 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.694923 | -5.199338 | -5.199338 | -5.199338 | 1.027247 | -5.199338 | . 144367 -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 0.346817 | 1.104747 | 0.252311 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.391202 | -5.199338 | -5.199338 | 0.835558 | 0.147316 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.315958 | -5.199338 | -5.199338 | -5.199338 | ... | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 1.283836 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | 2.085356 | 1.107059 | 1.831224 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | -5.199338 | . 144368 rows × 87 columns . pred = cbc.predict_proba(test.drop(&quot;id&quot;, axis = 1)) . res = pd.concat([pd.DataFrame(test.id.copy()), pd.DataFrame(pred)], axis = 1) . res.columns = submission.columns res . id Class_1 Class_2 Class_3 Class_4 Class_5 Class_6 Class_7 Class_8 Class_9 . 0 1 | 2.035351e-05 | 4.155254e-01 | 3.875593e-01 | 1.915983e-01 | 2.915440e-07 | 1.226470e-05 | 0.005280 | 4.166429e-06 | 4.079353e-07 | . 1 2 | 1.044036e-03 | 1.270427e-03 | 4.121220e-05 | 5.418426e-05 | 2.820285e-05 | 8.437974e-01 | 0.001460 | 1.520497e-01 | 2.549993e-04 | . 2 3 | 2.135121e-05 | 9.982301e-07 | 8.321858e-08 | 7.266236e-07 | 2.065122e-08 | 9.990895e-01 | 0.000011 | 8.304871e-04 | 4.548511e-05 | . 3 4 | 4.848869e-07 | 4.703883e-01 | 5.147936e-01 | 1.479793e-02 | 2.397874e-08 | 4.605196e-07 | 0.000002 | 9.831731e-07 | 1.593111e-05 | . 4 5 | 2.125766e-01 | 5.142562e-06 | 4.496004e-06 | 2.344188e-07 | 7.721577e-06 | 2.403795e-03 | 0.002119 | 9.924007e-02 | 6.836430e-01 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 144363 144364 | 1.150385e-01 | 7.801859e-03 | 2.000100e-03 | 8.358705e-03 | 9.410110e-06 | 6.613778e-01 | 0.057846 | 3.151485e-02 | 1.160532e-01 | . 144364 144365 | 6.608930e-06 | 2.086875e-01 | 5.610609e-01 | 2.138117e-01 | 5.130627e-07 | 7.407528e-06 | 0.016423 | 1.153003e-06 | 8.949993e-07 | . 144365 144366 | 1.019279e-05 | 8.252583e-01 | 1.191019e-01 | 5.058851e-02 | 7.644276e-07 | 5.194041e-05 | 0.004964 | 1.954108e-05 | 4.841306e-06 | . 144366 144367 | 4.536330e-06 | 4.586519e-01 | 2.200562e-02 | 5.193098e-01 | 1.357773e-06 | 5.442924e-07 | 0.000025 | 1.044206e-06 | 1.014232e-07 | . 144367 144368 | 1.441037e-06 | 4.370224e-01 | 5.449411e-01 | 1.317581e-02 | 1.802159e-07 | 2.440202e-05 | 0.004833 | 8.787453e-07 | 3.929546e-07 | . 144368 rows × 10 columns . res.to_csv(&quot;submission.csv&quot;, index = False) . !kaggle competitions submit -c otto-group-product-classification-challenge -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 27.4M/27.4M [00:01&lt;00:00, 15.4MB/s] Successfully submitted to Otto Group Product Classification Challenge .",
            "url": "https://raukrauk.github.io/ML-DL/ssuda/machine%20learning/2021/11/28/Otto_group.html",
            "relUrl": "/ssuda/machine%20learning/2021/11/28/Otto_group.html",
            "date": " • Nov 28, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "[Kaggle] PUBG Dataset",
            "content": "PUBG . In a PUBG game, up to 100 players start in each match (matchId). Players can be on teams (groupId) which get ranked at the end of the game (winPlacePerc) based on how many other teams are still alive when they are eliminated. In game, players can pick up different munitions, revive downed-but-not-out (knocked) teammates, drive vehicles, swim, run, shoot, and experience all of the consequences -- such as falling too far or running themselves over and eliminating themselves. . | You are provided with a large number of anonymized PUBG game stats, formatted so that each row contains one player&#39;s post-game stats. The data comes from matches of all types: solos, duos, squads, and custom; there is no guarantee of there being 100 players per match, nor at most 4 player per group. . | You must create a model which predicts players&#39; finishing placement based on their final stats, on a scale from 1 (first place) to 0 (last place). . | . Data fields . DBNOs - Number of enemy players knocked. | assists - Number of enemy players this player damaged that were killed by teammates. | boosts - Number of boost items used. | damageDealt - Total damage dealt. Note: Self inflicted damage is subtracted. | headshotKills - Number of enemy players killed with headshots. | heals - Number of healing items used. | Id - Player’s Id | killPlace - Ranking in match of number of enemy players killed. | killPoints - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.) If there is a value other than -1 in rankPoints, then any 0 in killPoints should be treated as a “None”. | killStreaks - Max number of enemy players killed in a short amount of time. | kills - Number of enemy players killed. | longestKill - Longest distance between player and player killed at time of death. This may be misleading, as downing a player and driving away may lead to a large longestKill stat. | matchDuration - Duration of match in seconds. | matchId - ID to identify match. There are no matches that are in both the training and testing set. | matchType - String identifying the game mode that the data comes from. The standard modes are “solo”, “duo”, “squad”, “solo-fpp”, “duo-fpp”, and “squad-fpp”; other modes are from events or custom matches. | rankPoints - Elo-like ranking of player. This ranking is inconsistent and is being deprecated in the API’s next version, so use with caution. Value of -1 takes place of “None”. | revives - Number of times this player revived teammates. | rideDistance - Total distance traveled in vehicles measured in meters. | roadKills - Number of kills while in a vehicle. | swimDistance - Total distance traveled by swimming measured in meters. | teamKills - Number of times this player killed a teammate. | vehicleDestroys - Number of vehicles destroyed. | walkDistance - Total distance traveled on foot measured in meters. | weaponsAcquired - Number of weapons picked up. | winPoints - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.) If there is a value other than -1 in rankPoints, then any 0 in winPoints should be treated as a “None”. | groupId - ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time. | numGroups - Number of groups we have data for in the match. | maxPlace - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements. | winPlacePerc - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match. | . from google.colab import drive drive.mount(&#39;/content/drive/&#39;) . Mounted at /content/drive/ . from google.colab import files files.upload() !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 /root/.kaggle/kaggle.json . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . !kaggle competitions download -c pubg-finish-placement-prediction . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sample_submission_V2.csv.zip to /content 53% 9.00M/17.0M [00:00&lt;00:00, 38.7MB/s] 100% 17.0M/17.0M [00:00&lt;00:00, 48.8MB/s] Downloading test_V2.csv.zip to /content 99% 99.0M/100M [00:01&lt;00:00, 77.8MB/s] 100% 100M/100M [00:01&lt;00:00, 88.4MB/s] Downloading train_V2.csv.zip to /content 96% 233M/244M [00:03&lt;00:00, 96.5MB/s] 100% 244M/244M [00:03&lt;00:00, 78.5MB/s] . !unzip sample_submission_V2.csv.zip !unzip train_V2.csv.zip !unzip test_V2.csv.zip . Archive: sample_submission_V2.csv.zip inflating: sample_submission_V2.csv Archive: train_V2.csv.zip inflating: train_V2.csv Archive: test_V2.csv.zip inflating: test_V2.csv . import numpy as np import pandas as pd import seaborn as sns import gc from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor from sklearn.metrics import r2_score, mean_squared_error from sklearn.linear_model import Ridge from sklearn.preprocessing import normalize . train=pd.read_csv(&quot;/content/train_V2.csv&quot;) test=pd.read_csv(&quot;/content/test_V2.csv&quot;) sample_submission=pd.read_csv(&quot;/content/sample_submission_V2.csv&quot;) train.head() . Id groupId matchId assists boosts damageDealt DBNOs headshotKills heals killPlace killPoints kills killStreaks longestKill matchDuration matchType maxPlace numGroups rankPoints revives rideDistance roadKills swimDistance teamKills vehicleDestroys walkDistance weaponsAcquired winPoints winPlacePerc . 0 7f96b2f878858a | 4d4b580de459be | a10357fd1a4a91 | 0 | 0 | 0.00 | 0 | 0 | 0 | 60 | 1241 | 0 | 0 | 0.00 | 1306 | squad-fpp | 28 | 26 | -1 | 0 | 0.0000 | 0 | 0.00 | 0 | 0 | 244.80 | 1 | 1466 | 0.4444 | . 1 eef90569b9d03c | 684d5656442f9e | aeb375fc57110c | 0 | 0 | 91.47 | 0 | 0 | 0 | 57 | 0 | 0 | 0 | 0.00 | 1777 | squad-fpp | 26 | 25 | 1484 | 0 | 0.0045 | 0 | 11.04 | 0 | 0 | 1434.00 | 5 | 0 | 0.6400 | . 2 1eaf90ac73de72 | 6a4a42c3245a74 | 110163d8bb94ae | 1 | 0 | 68.00 | 0 | 0 | 0 | 47 | 0 | 0 | 0 | 0.00 | 1318 | duo | 50 | 47 | 1491 | 0 | 0.0000 | 0 | 0.00 | 0 | 0 | 161.80 | 2 | 0 | 0.7755 | . 3 4616d365dd2853 | a930a9c79cd721 | f1f1f4ef412d7e | 0 | 0 | 32.90 | 0 | 0 | 0 | 75 | 0 | 0 | 0 | 0.00 | 1436 | squad-fpp | 31 | 30 | 1408 | 0 | 0.0000 | 0 | 0.00 | 0 | 0 | 202.70 | 3 | 0 | 0.1667 | . 4 315c96c26c9aac | de04010b3458dd | 6dc8ff871e21e6 | 0 | 0 | 100.00 | 0 | 0 | 0 | 45 | 0 | 1 | 1 | 58.53 | 1424 | solo-fpp | 97 | 95 | 1560 | 0 | 0.0000 | 0 | 0.00 | 0 | 0 | 49.75 | 2 | 0 | 0.1875 | . id, groupid, matchid, matchType&#51008; str . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 4446966 entries, 0 to 4446965 Data columns (total 29 columns): # Column Dtype -- 0 Id object 1 groupId object 2 matchId object 3 assists int64 4 boosts int64 5 damageDealt float64 6 DBNOs int64 7 headshotKills int64 8 heals int64 9 killPlace int64 10 killPoints int64 11 kills int64 12 killStreaks int64 13 longestKill float64 14 matchDuration int64 15 matchType object 16 maxPlace int64 17 numGroups int64 18 rankPoints int64 19 revives int64 20 rideDistance float64 21 roadKills int64 22 swimDistance float64 23 teamKills int64 24 vehicleDestroys int64 25 walkDistance float64 26 weaponsAcquired int64 27 winPoints int64 28 winPlacePerc float64 dtypes: float64(6), int64(19), object(4) memory usage: 983.9+ MB . &#49345;&#44288;&#44288;&#44228;&#47196; &#54869;&#51064; . import matplotlib.pylab as plt plt.figure(figsize = (18,10)) sns.heatmap(train.corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fc11353e350&gt; . import matplotlib.pyplot as plt plt.figure(figsize=(12,6)) sns.distplot(train[&#39;winPlacePerc&#39;].values, bins=100, kde=True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fde16fa6990&gt; . NA . winPlacePerc에서 결측값이 하나 존재한다. | . train.isna().sum() . Id 0 groupId 0 matchId 0 assists 0 boosts 0 damageDealt 0 DBNOs 0 headshotKills 0 heals 0 killPlace 0 killPoints 0 kills 0 killStreaks 0 longestKill 0 matchDuration 0 matchType 0 maxPlace 0 numGroups 0 rankPoints 0 revives 0 rideDistance 0 roadKills 0 swimDistance 0 teamKills 0 vehicleDestroys 0 walkDistance 0 weaponsAcquired 0 winPoints 0 winPlacePerc 1 dtype: int64 . Memory Reducing . # 데이터형을 변화시키면서 크기를 줄이는 방법인듯... def reduce_mem_usage(df): &quot;&quot;&quot; iterate through all the columns of a dataframe and modify the data type to reduce memory usage. &quot;&quot;&quot; start_mem = df.memory_usage().sum() / 1024**2 print(&#39;Memory usage of dataframe is {:.2f} MB&#39;.format(start_mem)) for col in df.columns: col_type = df[col].dtype if col_type != object: c_min = df[col].min() c_max = df[col].max() if str(col_type)[:3] == &#39;int&#39;: if c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max: df[col] = df[col].astype(np.int8) elif c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max: df[col] = df[col].astype(np.int16) elif c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max: df[col] = df[col].astype(np.int32) elif c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max: df[col] = df[col].astype(np.int64) else: if c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max: df[col] = df[col].astype(np.float16) elif c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max: df[col] = df[col].astype(np.float32) else: df[col] = df[col].astype(np.float64) end_mem = df.memory_usage().sum() / 1024**2 print(&#39;Memory usage after optimization is: {:.2f} MB&#39;.format(end_mem)) print(&#39;Decreased by {:.1f}%&#39;.format(100 * (start_mem - end_mem) / start_mem)) return df . dataset = pd.concat([train, test], sort=True) dataset = reduce_mem_usage(dataset) . Memory usage of dataframe is 1460.53 MB Memory usage after optimization is: 462.50 MB Decreased by 68.3% . # 좀비모드인가? 좀비들이 싹다 팀으로 취급되는 건가? plt.figure(figsize=(12,6)) plt.title(&#39;Number of Team Members&#39;) tmp = dataset.groupby([&#39;matchId&#39;,&#39;groupId&#39;])[&#39;Id&#39;].agg(&#39;count&#39;) sns.countplot(tmp) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fde0ac76310&gt; . plt.figure(figsize=(12,6)) plt.title(&#39;Number of Team Members&#39;) ax = sns.countplot(x=&#39;matchType&#39;, data=dataset) ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=&quot;right&quot;) plt.tight_layout() . def min_by_team(df): cols_to_drop = [&#39;Id&#39;, &#39;groupId&#39;, &#39;matchId&#39;, &#39;matchType&#39;, &#39;winPlacePerc&#39;] features = [col for col in df.columns if col not in cols_to_drop] agg = df.groupby([&#39;matchId&#39;,&#39;groupId&#39;])[features].min() return df.merge(agg, suffixes=[&#39;&#39;, &#39;_min&#39;], how=&#39;left&#39;, on=[&#39;matchId&#39;, &#39;groupId&#39;]) def max_by_team(df): cols_to_drop = [&#39;Id&#39;, &#39;groupId&#39;, &#39;matchId&#39;, &#39;matchType&#39;, &#39;winPlacePerc&#39;] features = [col for col in df.columns if col not in cols_to_drop] agg = df.groupby([&#39;matchId&#39;, &#39;groupId&#39;])[features].max() return df.merge(agg, suffixes=[&#39;&#39;, &#39;_max&#39;], how=&#39;left&#39;, on=[&#39;matchId&#39;, &#39;groupId&#39;]) def sum_by_team(df): cols_to_drop = [&#39;Id&#39;, &#39;groupId&#39;, &#39;matchId&#39;, &#39;matchType&#39;, &#39;winPlacePerc&#39;] features = [col for col in df.columns if col not in cols_to_drop] agg = df.groupby([&#39;matchId&#39;, &#39;groupId&#39;])[features].sum() return df.merge(agg, suffixes=[&#39;&#39;, &#39;_sum&#39;], how=&#39;left&#39;, on=[&#39;matchId&#39;, &#39;groupId&#39;]) def median_by_team(df): cols_to_drop = [&#39;Id&#39;, &#39;groupId&#39;, &#39;matchId&#39;, &#39;matchType&#39;, &#39;winPlacePerc&#39;] features = [col for col in df.columns if col not in cols_to_drop] agg = df.groupby([&#39;matchId&#39;, &#39;groupId&#39;])[features].median() return df.merge(agg, suffixes=[&#39;&#39;, &#39;_median&#39;], how=&#39;left&#39;, on=[&#39;matchId&#39;, &#39;groupId&#39;]) def mean_by_team(df): cols_to_drop = [&#39;Id&#39;, &#39;groupId&#39;, &#39;matchId&#39;, &#39;matchType&#39;, &#39;winPlacePerc&#39;] features = [col for col in df.columns if col not in cols_to_drop] agg = df.groupby([&#39;matchId&#39;, &#39;groupId&#39;])[features].mean() return df.merge(agg, suffixes=[&#39;&#39;, &#39;_mean&#39;], how=&#39;left&#39;, on=[&#39;matchId&#39;, &#39;groupId&#39;]) def rank_by_team(df): cols_to_drop = [&#39;Id&#39;, &#39;groupId&#39;, &#39;matchId&#39;, &#39;matchType&#39;, &#39;winPlacePerc&#39;] features = [col for col in df.columns if col not in cols_to_drop] agg = df.groupby([&#39;matchId&#39;, &#39;groupId&#39;])[features].mean() agg = agg.groupby(&#39;matchId&#39;)[features].rank(pct=True) return df.merge(agg, suffixes=[&#39;&#39;, &#39;_mean_rank&#39;], how=&#39;left&#39;, on=[&#39;matchId&#39;, &#39;groupId&#39;]) . dataset = pd.concat([train, test], sort=True) dataset = reduce_mem_usage(dataset) # dataset = mean_by_team(dataset) dataset = rank_by_team(dataset) gc.collect() dataset.head() . Memory usage of dataframe is 1460.53 MB Memory usage after optimization is: 462.50 MB Decreased by 68.3% . DBNOs Id assists boosts damageDealt groupId headshotKills heals killPlace killPoints killStreaks kills longestKill matchDuration matchId matchType maxPlace numGroups rankPoints revives rideDistance roadKills swimDistance teamKills vehicleDestroys walkDistance weaponsAcquired winPlacePerc winPoints DBNOs_mean_rank assists_mean_rank boosts_mean_rank damageDealt_mean_rank headshotKills_mean_rank heals_mean_rank killPlace_mean_rank killPoints_mean_rank killStreaks_mean_rank kills_mean_rank longestKill_mean_rank matchDuration_mean_rank maxPlace_mean_rank numGroups_mean_rank rankPoints_mean_rank revives_mean_rank rideDistance_mean_rank roadKills_mean_rank swimDistance_mean_rank teamKills_mean_rank vehicleDestroys_mean_rank walkDistance_mean_rank weaponsAcquired_mean_rank winPoints_mean_rank . 0 0 | 7f96b2f878858a | 0 | 0 | 0.00000 | 4d4b580de459be | 0 | 0 | 60 | 1241 | 0 | 0 | 0.00000 | 1306 | a10357fd1a4a91 | squad-fpp | 28 | 26 | -1 | 0 | 0.000000 | 0 | 0.000000 | 0 | 0 | 244.75 | 1 | 0.444336 | 1466 | 0.403846 | 0.250000 | 0.230769 | 0.500000 | 0.653846 | 0.192308 | 0.615385 | 0.538462 | 0.288462 | 0.634615 | 0.384615 | 0.519231 | 0.519231 | 0.519231 | 0.519231 | 0.326923 | 0.423077 | 0.519231 | 0.500000 | 0.442308 | 0.519231 | 0.384615 | 0.192308 | 0.192308 | . 1 0 | eef90569b9d03c | 0 | 0 | 91.50000 | 684d5656442f9e | 0 | 0 | 57 | 0 | 0 | 0 | 0.00000 | 1777 | aeb375fc57110c | squad-fpp | 26 | 25 | 1484 | 0 | 0.004501 | 0 | 11.039062 | 0 | 0 | 1434.00 | 5 | 0.640137 | 0 | 0.400000 | 0.820000 | 0.720000 | 0.680000 | 0.280000 | 0.880000 | 0.440000 | 0.520000 | 0.620000 | 0.620000 | 0.720000 | 0.520000 | 0.520000 | 0.520000 | 0.600000 | 0.360000 | 0.920000 | 0.520000 | 0.960000 | 0.500000 | 0.500000 | 0.560000 | 0.860000 | 0.520000 | . 2 0 | 1eaf90ac73de72 | 1 | 0 | 68.00000 | 6a4a42c3245a74 | 0 | 0 | 47 | 0 | 0 | 0 | 0.00000 | 1318 | 110163d8bb94ae | duo | 50 | 47 | 1491 | 0 | 0.000000 | 0 | 0.000000 | 0 | 0 | 161.75 | 2 | 0.775391 | 0 | 0.574468 | 0.840426 | 0.755319 | 0.595745 | 0.372340 | 0.670213 | 0.319149 | 0.510638 | 0.500000 | 0.617021 | 0.489362 | 0.510638 | 0.510638 | 0.510638 | 0.585106 | 0.446809 | 0.872340 | 0.510638 | 0.489362 | 0.510638 | 0.510638 | 0.553191 | 0.308511 | 0.510638 | . 3 0 | 4616d365dd2853 | 0 | 0 | 32.90625 | a930a9c79cd721 | 0 | 0 | 75 | 0 | 0 | 0 | 0.00000 | 1436 | f1f1f4ef412d7e | squad-fpp | 31 | 30 | 1408 | 0 | 0.000000 | 0 | 0.000000 | 0 | 0 | 202.75 | 3 | 0.166748 | 0 | 0.150000 | 0.316667 | 0.166667 | 0.100000 | 0.300000 | 0.133333 | 0.900000 | 0.516667 | 0.200000 | 0.200000 | 0.200000 | 0.516667 | 0.516667 | 0.516667 | 0.100000 | 0.283333 | 0.400000 | 0.516667 | 0.483333 | 0.466667 | 0.516667 | 0.166667 | 0.316667 | 0.516667 | . 4 0 | 315c96c26c9aac | 0 | 0 | 100.00000 | de04010b3458dd | 0 | 0 | 45 | 0 | 1 | 1 | 58.53125 | 1424 | 6dc8ff871e21e6 | solo-fpp | 97 | 95 | 1560 | 0 | 0.000000 | 0 | 0.000000 | 0 | 0 | 49.75 | 2 | 0.187500 | 0 | 0.505263 | 0.484211 | 0.284211 | 0.578947 | 0.389474 | 0.315789 | 0.463158 | 0.505263 | 0.763158 | 0.673684 | 0.852632 | 0.505263 | 0.505263 | 0.505263 | 0.557895 | 0.505263 | 0.447368 | 0.505263 | 0.463158 | 0.494737 | 0.505263 | 0.210526 | 0.284211 | 0.505263 | . Modeling . from sklearn.model_selection import KFold, StratifiedKFold, train_test_split from sklearn.metrics import mean_absolute_error . def oof_model_preds(df, model, num_folds, params): # Divide in training/validation and test data train_df = df[df[&#39;winPlacePerc&#39;].notnull()] test_df = df[df[&#39;winPlacePerc&#39;].isnull()] print(&quot;Starting LightGBM. Train shape: {}, test shape: {}&quot;.format(train_df.shape, test_df.shape)) del df gc.collect() # Create arrays and dataframes to store results oof_preds = np.zeros(train_df.shape[0]) sub_preds = np.zeros(test_df.shape[0]) feature_importance_df = pd.DataFrame() drop_features = [&#39;Id&#39;, &#39;groupId&#39;, &#39;matchId&#39;, &#39;matchType&#39;, &#39;winPlacePerc&#39;] feats = [f for f in train_df.columns if f not in drop_features] # Create model if num_folds == 1: train_x, valid_x, train_y, valid_y = train_test_split(train_df[feats], train_df[&#39;winPlacePerc&#39;], test_size=0.2, random_state=1001) model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= &#39;mae&#39;, verbose=params[&#39;verbose&#39;], early_stopping_rounds=params[&#39;early_stopping_rounds&#39;]) oof_preds = model.predict(train_df[feats]) sub_preds = model.predict(test_df[feats]) fold_importance_df = pd.DataFrame() fold_importance_df[&quot;feature&quot;] = feats fold_importance_df[&quot;importance&quot;] = model.feature_importances_ fold_importance_df[&quot;fold&quot;] = 1 feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) print(&#39;MAE : %.6f&#39; % (mean_absolute_error(train_df[&#39;winPlacePerc&#39;], oof_preds))) del train_x, train_y, valid_x, valid_y gc.collect() # Cross validation model elif num_folds &gt; 1: folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001) for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df[&#39;winPlacePerc&#39;])): train_x, train_y = train_df[feats].iloc[train_idx], train_df[&#39;winPlacePerc&#39;].iloc[train_idx] valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df[&#39;winPlacePerc&#39;].iloc[valid_idx] model.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], eval_metric= &#39;mae&#39;, verbose=params[&#39;verbose&#39;], early_stopping_rounds=params[&#39;early_stopping_rounds&#39;]) oof_preds[valid_idx] = model.predict(valid_x) sub_preds += model.predict(test_df[feats]) / folds.n_splits fold_importance_df = pd.DataFrame() fold_importance_df[&quot;feature&quot;] = feats fold_importance_df[&quot;importance&quot;] = model.feature_importances_ fold_importance_df[&quot;fold&quot;] = n_fold + 1 feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0) print(&#39;Fold %2d MAE : %.6f&#39; % (n_fold + 1, mean_absolute_error(valid_y, oof_preds[valid_idx]))) del train_x, train_y, valid_x, valid_y gc.collect() print(&#39;Full MAE score %.6f&#39; % mean_absolute_error(train_df[&#39;winPlacePerc&#39;], oof_preds)) return oof_preds, sub_preds, feature_importance_df . import lightgbm as lgb params = { &#39;num_leaves&#39;: 144, &#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 800, &#39;max_depth&#39;:12, &#39;max_bin&#39;:55, &#39;bagging_fraction&#39;:0.8, &#39;bagging_freq&#39;:5, &#39;feature_fraction&#39;:0.9, &#39;verbose&#39;:50, &#39;early_stopping_rounds&#39;:100 } # LightGBM parameters lgbm_reg = lgb.LGBMRegressor(num_leaves=params[&#39;num_leaves&#39;], learning_rate=params[&#39;learning_rate&#39;], n_estimators=params[&#39;n_estimators&#39;], max_depth=params[&#39;max_depth&#39;], max_bin = params[&#39;max_bin&#39;], bagging_fraction = params[&#39;bagging_fraction&#39;], bagging_freq = params[&#39;bagging_freq&#39;], feature_fraction = params[&#39;feature_fraction&#39;], ) lgb_oof_preds, lgb_sub_preds, lgb_feature_importance_df = oof_model_preds(dataset, lgbm_reg, num_folds=4, params=params) . Starting LightGBM. Train shape: (4446965, 53), test shape: (1934175, 53) . &#51088;&#44984; &#53076;&#47017;&#51060; &#53552;&#51648;&#45348; &#12619;&#12619;&#12619;&#12619;&#12619; . def display_importances(feature_importance_df_): cols = feature_importance_df_[[&quot;feature&quot;, &quot;importance&quot;]].groupby(&quot;feature&quot;).mean().sort_values(by=&quot;importance&quot;, ascending=False)[:40].index best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)] plt.figure(figsize=(8, 10)) sns.barplot(x=&quot;importance&quot;, y=&quot;feature&quot;, data=best_features.sort_values(by=&quot;importance&quot;, ascending=False)) plt.title(&#39;LightGBM Features (avg over folds)&#39;) plt.tight_layout() plt.savefig(&#39;lgbm_importances.png . display_importances(lgb_feature_importance_df) . . sub = pd.DataFrame() sub[&#39;Id&#39;] = test_df[&#39;Id&#39;] sub[&#39;winPlacePerc&#39;] = lgb_sub_preds sub[&#39;winPlacePerc&#39;][sub[&#39;winPlacePerc&#39;] &gt; 1] = 1 sub.to_csv(&#39;lgb_submission.csv&#39;,index=False) . Random Forest&#51060;&#50857; . 터짐 ㅡㅡ | . train = train[train[&#39;maxPlace&#39;] &gt; 1] . target = &#39;winPlacePerc&#39; features = list(train.columns) features.remove(&quot;Id&quot;) features.remove(&quot;matchId&quot;) features.remove(&quot;groupId&quot;) features.remove(&quot;matchType&quot;) y_train = np.array(train[target]) features.remove(target) x_train = train[features] x_test = test[features] print(x_test.shape,x_train.shape,y_train.shape) . (1934174, 24) (4446965, 24) (4446965,) . random_seed=1 x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.1, random_state=random_seed) . m3 = RandomForestRegressor(n_estimators=70, min_samples_leaf=3, max_features=0.5, n_jobs=-1) . %%time m3.fit(x_train, y_train) . print(&#39;mae train: &#39;, mean_absolute_error(m3.predict(x_train), y_train)) print(&#39;mae val: &#39;, mean_absolute_error(m3.predict(x_val), y_val)) . %%time pred = m3.predict(x_test) df_test[&#39;winPlacePerc&#39;] = pred submission = df_test[[&#39;Id&#39;, &#39;winPlacePerc&#39;]] submission.to_csv(&#39;submission_rf.csv&#39;, index=False) .",
            "url": "https://raukrauk.github.io/ML-DL/ssuda/kaggle/pubg/2021/11/21/PUBG.html",
            "relUrl": "/ssuda/kaggle/pubg/2021/11/21/PUBG.html",
            "date": " • Nov 21, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "[Dacon 연습] 신용카드 연체자 분류",
            "content": "&#45936;&#51060;&#53552; &#48320;&#49688; &#54869;&#51064; . index - 딱히 필요한가? . | gender: 성별 (M/F) . | car: 차량 소유 여부 (Y/N) . | reality: 부동산 소유 여부 (Y/N) . | child_num: 자녀 수 (0,1,2...19) . | income_total: 연간 소득 (변환되어 상대적인 지표로 생각하라고 문의사항에 있음.) . | income_type: 소득 분류 . [&#39;Commercial associate&#39;, &#39;Working&#39;, &#39;State servant&#39;, &#39;Pensioner&#39;, &#39;Student&#39;] [자영업/사업 소득, 일반소득, 국가 공무원, 연금수급, 학생] . | edu_type: 교육 수준 . [&#39;Higher education&#39; ,&#39;Secondary / secondary special&#39;, &#39;Incomplete higher&#39;, &#39;Lower secondary&#39;, &#39;Academic degree&#39;] [고등교육(대학재학/휴학), 고등학교 졸업, 고등학교 자퇴, 중학교 졸업, 학사 학위(대학졸업)] . | family_type: 결혼 여부 . [&#39;Married&#39;, &#39;Civil marriage&#39;, &#39;Separated&#39;, &#39;Single / not married&#39;, &#39;Widow&#39;] [기혼, 법률혼, 별거, 미혼, 과부] . | house_type: 생활 방식 . [&#39;Municipal apartment&#39;, &#39;House / apartment&#39;, &#39;With parents&#39;, &#39;Co-op apartment&#39;, &#39;Rented apartment&#39;, &#39;Office apartment&#39;] [시영/시립아파트, 주택/아파트, 부모와 함께 거주, 협동아파트, 임대아파트, 오피스아파트] . | DAYS_BIRTH: 출생일 . 데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전에 태어났음을 의미 . | DAYS_EMPLOYED: 업무 시작일 . 데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 하루 전부터 일을 시작함을 의미, 양수 값은 고용되지 않은 상태를 의미함 . | FLAG_MOBIL: 핸드폰 소유 여부 (0, 1) . | work_phone: 업무용 전화 소유 여부 (0, 1) . | phone: 가정용 전화 소유 여부 (0, 1) . | email: 이메일 소유 여부 (0, 1) . | occyp_type: 직업 유형 - 무직 NAN?이 존재한다. . [nan, &#39;Laborers&#39;, &#39;Managers&#39;, &#39;Sales staff&#39;, &#39;High skill tech staff&#39;, &#39;Core staff&#39;, &#39;Drivers&#39;, &#39;Medicine staff&#39;, &#39;Accountants&#39;, &#39;Realty agents&#39;, &#39;Security staff&#39;, &#39;Cleaning staff&#39;, &#39;Private service staff&#39;, &#39;Cooking staff&#39;, &#39;Secretaries&#39;, &#39;HR staff&#39;, &#39;IT staff&#39;, &#39;Low-skill Laborers&#39;, &#39;Waiters/barmen staff&#39;] [무직, 노무자, 관리자, 영업 직원, 고급 기술 직원, 핵심 직원, 운전 기사, 의료 직원, 회계사, 부동산 에이전트, 보안 직원, 청소 직원, 개인 서비스 직원, 요리 직원, 비서, 인사 담당자, IT 직원, 저숙련 노동자, 웨이터/바텐더 직원] . | family_size: 가족 규모 (1 ~ 20) . | begin_month: 신용카드 발급 월 . 데이터 수집 당시 (0)부터 역으로 셈, 즉, -1은 데이터 수집일 한 달 전에 신용카드를 발급함을 의미 . | Target: credit: 사용자의 신용카드 대금 연체를 기준으로 한 신용도 . =&gt; 낮을 수록 높은 신용의 신용카드 사용자를 의미함 . from google.colab import drive drive.mount(&#39;/content/drive/&#39;) . Mounted at /content/drive/ . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt . train = pd.read_csv(&quot;/content/drive/MyDrive/open/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/open/test.csv&quot;) sample_sub = pd.read_csv(&quot;/content/drive/MyDrive/open/sample_submission.csv&quot;) . train . index gender car reality child_num income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email occyp_type family_size begin_month credit . 0 0 | F | N | N | 0 | 202500.0 | Commercial associate | Higher education | Married | Municipal apartment | -13899 | -4709 | 1 | 0 | 0 | 0 | NaN | 2.0 | -6.0 | 1.0 | . 1 1 | F | N | Y | 1 | 247500.0 | Commercial associate | Secondary / secondary special | Civil marriage | House / apartment | -11380 | -1540 | 1 | 0 | 0 | 1 | Laborers | 3.0 | -5.0 | 1.0 | . 2 2 | M | Y | Y | 0 | 450000.0 | Working | Higher education | Married | House / apartment | -19087 | -4434 | 1 | 0 | 1 | 0 | Managers | 2.0 | -22.0 | 2.0 | . 3 3 | F | N | Y | 0 | 202500.0 | Commercial associate | Secondary / secondary special | Married | House / apartment | -15088 | -2092 | 1 | 0 | 1 | 0 | Sales staff | 2.0 | -37.0 | 0.0 | . 4 4 | F | Y | Y | 0 | 157500.0 | State servant | Higher education | Married | House / apartment | -15037 | -2105 | 1 | 0 | 0 | 0 | Managers | 2.0 | -26.0 | 2.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 26452 26452 | F | N | N | 2 | 225000.0 | State servant | Secondary / secondary special | Married | House / apartment | -12079 | -1984 | 1 | 0 | 0 | 0 | Core staff | 4.0 | -2.0 | 1.0 | . 26453 26453 | F | N | Y | 1 | 180000.0 | Working | Higher education | Separated | House / apartment | -15291 | -2475 | 1 | 0 | 0 | 0 | NaN | 2.0 | -47.0 | 2.0 | . 26454 26454 | F | Y | N | 0 | 292500.0 | Working | Secondary / secondary special | Civil marriage | With parents | -10082 | -2015 | 1 | 0 | 0 | 0 | Core staff | 2.0 | -25.0 | 2.0 | . 26455 26455 | M | N | Y | 0 | 171000.0 | Working | Incomplete higher | Single / not married | House / apartment | -10145 | -107 | 1 | 0 | 0 | 0 | Laborers | 1.0 | -59.0 | 2.0 | . 26456 26456 | F | N | N | 0 | 81000.0 | Working | Secondary / secondary special | Civil marriage | House / apartment | -19569 | -1013 | 1 | 0 | 0 | 0 | Security staff | 2.0 | -9.0 | 2.0 | . 26457 rows × 20 columns . train.describe() . index child_num income_total DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email family_size begin_month credit . count 26457.000000 | 26457.000000 | 2.645700e+04 | 26457.000000 | 26457.000000 | 26457.0 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | . mean 13228.000000 | 0.428658 | 1.873065e+05 | -15958.053899 | 59068.750728 | 1.0 | 0.224742 | 0.294251 | 0.091280 | 2.196848 | -26.123294 | 1.519560 | . std 7637.622372 | 0.747326 | 1.018784e+05 | 4201.589022 | 137475.427503 | 0.0 | 0.417420 | 0.455714 | 0.288013 | 0.916717 | 16.559550 | 0.702283 | . min 0.000000 | 0.000000 | 2.700000e+04 | -25152.000000 | -15713.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | -60.000000 | 0.000000 | . 25% 6614.000000 | 0.000000 | 1.215000e+05 | -19431.000000 | -3153.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -39.000000 | 1.000000 | . 50% 13228.000000 | 0.000000 | 1.575000e+05 | -15547.000000 | -1539.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -24.000000 | 2.000000 | . 75% 19842.000000 | 1.000000 | 2.250000e+05 | -12446.000000 | -407.000000 | 1.0 | 0.000000 | 1.000000 | 0.000000 | 3.000000 | -12.000000 | 2.000000 | . max 26456.000000 | 19.000000 | 1.575000e+06 | -7705.000000 | 365243.000000 | 1.0 | 1.000000 | 1.000000 | 1.000000 | 20.000000 | 0.000000 | 2.000000 | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 26457 entries, 0 to 26456 Data columns (total 20 columns): # Column Non-Null Count Dtype -- -- 0 index 26457 non-null int64 1 gender 26457 non-null object 2 car 26457 non-null object 3 reality 26457 non-null object 4 child_num 26457 non-null int64 5 income_total 26457 non-null float64 6 income_type 26457 non-null object 7 edu_type 26457 non-null object 8 family_type 26457 non-null object 9 house_type 26457 non-null object 10 DAYS_BIRTH 26457 non-null int64 11 DAYS_EMPLOYED 26457 non-null int64 12 FLAG_MOBIL 26457 non-null int64 13 work_phone 26457 non-null int64 14 phone 26457 non-null int64 15 email 26457 non-null int64 16 occyp_type 18286 non-null object 17 family_size 26457 non-null float64 18 begin_month 26457 non-null float64 19 credit 26457 non-null float64 dtypes: float64(4), int64(8), object(8) memory usage: 4.0+ MB . Train_set &#54869;&#51064;&#44284;&#51221; . family_size와 child_num 변수간에 높은 상관관계 -&gt; 당연함. | 자녀의 수 보다는 총 부양가족수가 좀 더 신용과 연관되어있다고 생각됨. -&gt; child_num을 제외한다. | . sns.set(rc = {&#39;figure.figsize&#39;:(12,8)}) sns.heatmap(train.corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69fa10ae10&gt; . income을 log 변환함 | . f, axes = plt.subplots(1, 2, figsize=(10, 5)) sns.distplot(train.income_total, kde = True, ax = axes[0]) sns.distplot(np.log1p(train.income_total), norm_hist=True, ax = axes[1], color = &quot;red&quot;) # income을 log 변환함 # 이게 정규성을 따르는 건가? . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69fa036110&gt; . &#47749;&#47785;&#54805;/&#49692;&#49436;&#54805; &#51088;&#47308;&#46308;&#51032; &#44060;&#49688;&#47484; &#54869;&#51064;&#54644; &#48376;&#45796;. . fig, axes = plt.subplots(1,2,figsize=(12, 7)) fig.autofmt_xdate(rotation=90) sns.countplot(train.income_type, ax = axes[0], order = train.income_type.value_counts().index) sns.countplot(train.edu_type, ax = axes[1], order = train.edu_type.value_counts().index) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69f9d3af90&gt; . p1 = pd.DataFrame(train.groupby([&#39;credit&#39;,&#39;income_type&#39;]).income_type.count().unstack()) p1 = p1.T.fillna(0) p2 = pd.DataFrame(train.groupby([&#39;credit&#39;,&#39;edu_type&#39;]).edu_type.count().unstack()) p2 = p2.T.fillna(0) p1.plot.barh(rot=0) p2.plot.barh(rot=0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb889006310&gt; . fig, axes = plt.subplots(1,2,figsize=(12, 7)) fig.autofmt_xdate(rotation=90) sns.countplot(train.family_type, ax = axes[0], order = train.family_type.value_counts().index) sns.countplot(train.house_type, ax = axes[1],order = train.house_type.value_counts().index) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69f9d03210&gt; . p1 = pd.DataFrame(train.groupby([&#39;credit&#39;,&#39;family_type&#39;]).family_type.count().unstack()) p1 = p1.T.fillna(0) p2 = pd.DataFrame(train.groupby([&#39;credit&#39;,&#39;house_type&#39;]).house_type.count().unstack()) p2 = p2.T.fillna(0) p1.plot.barh(rot=0) p2.plot.barh(rot=0) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb88926a0d0&gt; . train.occyp_type.fillna(&quot;not employed&quot;, inplace = True) . sns.countplot(train.occyp_type, order = train.occyp_type.value_counts().index) plt.xticks(rotation=90) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . (array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]), &lt;a list of 19 Text major ticklabel objects&gt;) . -[&#39;DAYS_BIRTH&#39;, &#39;DAYS_EMPLOYED&#39;, &#39;begin_month&#39;] 이 변수들은 음수로 되어 있음 -&gt; 의미가 반대가 되므로 양수로 변환을 한다. . train.describe() . index child_num income_total DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email family_size begin_month credit . count 26457.000000 | 26457.000000 | 2.645700e+04 | 26457.000000 | 26457.000000 | 26457.0 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | 26457.000000 | . mean 13228.000000 | 0.428658 | 1.873065e+05 | -15958.053899 | 59068.750728 | 1.0 | 0.224742 | 0.294251 | 0.091280 | 2.196848 | -26.123294 | 1.519560 | . std 7637.622372 | 0.747326 | 1.018784e+05 | 4201.589022 | 137475.427503 | 0.0 | 0.417420 | 0.455714 | 0.288013 | 0.916717 | 16.559550 | 0.702283 | . min 0.000000 | 0.000000 | 2.700000e+04 | -25152.000000 | -15713.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 1.000000 | -60.000000 | 0.000000 | . 25% 6614.000000 | 0.000000 | 1.215000e+05 | -19431.000000 | -3153.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -39.000000 | 1.000000 | . 50% 13228.000000 | 0.000000 | 1.575000e+05 | -15547.000000 | -1539.000000 | 1.0 | 0.000000 | 0.000000 | 0.000000 | 2.000000 | -24.000000 | 2.000000 | . 75% 19842.000000 | 1.000000 | 2.250000e+05 | -12446.000000 | -407.000000 | 1.0 | 0.000000 | 1.000000 | 0.000000 | 3.000000 | -12.000000 | 2.000000 | . max 26456.000000 | 19.000000 | 1.575000e+06 | -7705.000000 | 365243.000000 | 1.0 | 1.000000 | 1.000000 | 1.000000 | 20.000000 | 0.000000 | 2.000000 | . &#48516;&#54252;&#47484; &#54869;&#51064;&#54644; &#48376;&#45796;. . DAYS_EMPLOYED(고용된 날짜) 분포가 더럽군요. | . for k in [&#39;DAYS_BIRTH&#39;, &#39;DAYS_EMPLOYED&#39;, &#39;begin_month&#39;]: train[k] = -train[k] . fig, axes = plt.subplots(1,3,figsize=(15, 8)) sns.distplot(train.DAYS_BIRTH, ax = axes[0], norm_hist=True) sns.distplot(train.DAYS_EMPLOYED, ax = axes[1], norm_hist=True) sns.distplot(train.begin_month, ax = axes[2], norm_hist=True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69f7d03210&gt; . DAYS_EMPLOYED(&#44256;&#50857;&#46108; &#45216;&#51676;)&#50640;&#49436; &#51020;&#49688;&#51064; &#44050;&#51060; &#46020;&#52636;&#46120;. . 고용이 되지 않음 -&gt; 백수, 연금수급자라고 생각됨. | 음수이면 0 -&gt; 고용된 날짜가 없음과 동일하게 취급 | 튀어나온게 약 26000개 정도 됨 | . train[train.DAYS_EMPLOYED &lt; 0] = 0 . sns.distplot(train.DAYS_EMPLOYED, norm_hist=True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69f8384c10&gt; . &#52376;&#47532;&#44032; &#50504;&#46108;&#45796; &#51068;&#45800; &#48260;&#47548; . train.DAYS_EMPLOYED = np.log1p(train.DAYS_EMPLOYED) . sns.distplot(train.DAYS_EMPLOYED, norm_hist=True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69f7f67c50&gt; . begin_month&#51012; &#51068;&#51088;&#47196; &#52376;&#47532;&#54616;&#47732; &#45908; &#51060;&#49345;&#54644;&#51664; &#44109; &#45460;&#46176; . sns.distplot(train.begin_month * 30) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f69f764f3d0&gt; . &#51473;&#48373;&#46108; &#44050;&#51008; &#51316;&#51116;&#54616;&#51648; &#50506;&#51020; . train[train.duplicated()] . index gender car reality child_num income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED FLAG_MOBIL work_phone phone email occyp_type family_size begin_month credit . Target &#48320;&#49688;&#46308;&#51060; &#50612;&#46523;&#44172; &#48516;&#54252; &#46104;&#50612;&#51080;&#45716;&#51648; &#54869;&#51064;&#54620;&#45796;. . credit이 작을수록 신용이 좋음을 나타냄 | 데이터들이 불균형하게 나타나있음 -&gt; resampling? | . sns.set(rc = {&#39;figure.figsize&#39;:(8,5)}) sns.countplot(train.credit, order = train.credit.value_counts().index) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fde017848d0&gt; . &#44480;&#52270;&#51004;&#45768;&#44624; &#54620;&#48264;&#50640; &#52376;&#47532; idx = 0 ~ 26456&#50640;&#49436; train, test &#48516;&#47532; . train = pd.read_csv(&quot;/content/drive/MyDrive/open/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/open/test.csv&quot;) sample_sub = pd.read_csv(&quot;/content/drive/MyDrive/open/sample_submission.csv&quot;) . tmp = pd.concat([train, test]) tmp.reset_index(drop = True, inplace = True) . occyp_type&#50640;&#49436; &#44208;&#52769;&#44050;&#46308;&#51060; &#51316;&#51116;&#54620;&#45796;. . 여기서는 무직(not employed)으로 취급하여 채워준다. | . tmp.occyp_type.fillna(&quot;not employed&quot;, inplace = True) . &#49352;&#47196;&#50868; &#54028;&#49373;&#48320;&#49688; &#52628;&#44032; . ID = 생일 + 소득 + 직업 + 학력 + 이메일 + 가족 구성원 수 | . tmp[&quot;ID&quot;] = tmp.gender.apply(str) + tmp.DAYS_BIRTH.apply(str) + tmp.income_total.apply(str) + tmp.income_type.apply(str) + tmp.edu_type.apply(str) + tmp.email.apply(str) + tmp.family_size.apply(str) . tmp.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 36457 entries, 0 to 36456 Data columns (total 21 columns): # Column Non-Null Count Dtype -- -- 0 index 36457 non-null int64 1 gender 36457 non-null object 2 car 36457 non-null object 3 reality 36457 non-null object 4 child_num 36457 non-null int64 5 income_total 36457 non-null float64 6 income_type 36457 non-null object 7 edu_type 36457 non-null object 8 family_type 36457 non-null object 9 house_type 36457 non-null object 10 DAYS_BIRTH 36457 non-null int64 11 DAYS_EMPLOYED 36457 non-null int64 12 FLAG_MOBIL 36457 non-null int64 13 work_phone 36457 non-null int64 14 phone 36457 non-null int64 15 email 36457 non-null int64 16 occyp_type 36457 non-null object 17 family_size 36457 non-null float64 18 begin_month 36457 non-null float64 19 credit 26457 non-null float64 20 ID 36457 non-null object dtypes: float64(4), int64(8), object(9) memory usage: 5.8+ MB . &#52376;&#47532;&#44284;&#51221; . 1. . Index는 필요하진 않을것 같음 | child_num 제외 | FLAG_MOBIL은 다 1이어서 제거 | phone(집전화) 변수는 큰의미가 없을것 같음. | DAYS_EMPLOYED 임시로 제거 . | income을 로그변환 . | . tmp.drop(&#39;index&#39;, axis = 1, inplace = True) tmp.drop(&#39;child_num&#39;, axis = 1, inplace = True) tmp.drop(&#39;FLAG_MOBIL&#39;, axis = 1, inplace = True) tmp.drop(&#39;phone&#39;, axis = 1, inplace = True) #tmp.drop(&#39;DAYS_EMPLOYED&#39;, axis = 1, inplace = True) . tmp.income_total = np.log1p(tmp.income_total) . tmp . gender car reality income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED work_phone email occyp_type family_size begin_month credit ID . 0 F | N | N | 12.218500 | Commercial associate | Higher education | Married | Municipal apartment | -13899 | -4709 | 0 | 0 | not employed | 2.0 | -6.0 | 1.0 | F-13899202500.0Commercial associateHigher educ... | . 1 F | N | Y | 12.419170 | Commercial associate | Secondary / secondary special | Civil marriage | House / apartment | -11380 | -1540 | 0 | 1 | Laborers | 3.0 | -5.0 | 1.0 | F-11380247500.0Commercial associateSecondary /... | . 2 M | Y | Y | 13.017005 | Working | Higher education | Married | House / apartment | -19087 | -4434 | 0 | 0 | Managers | 2.0 | -22.0 | 2.0 | M-19087450000.0WorkingHigher education02.0 | . 3 F | N | Y | 12.218500 | Commercial associate | Secondary / secondary special | Married | House / apartment | -15088 | -2092 | 0 | 0 | Sales staff | 2.0 | -37.0 | 0.0 | F-15088202500.0Commercial associateSecondary /... | . 4 F | Y | Y | 11.967187 | State servant | Higher education | Married | House / apartment | -15037 | -2105 | 0 | 0 | Managers | 2.0 | -26.0 | 2.0 | F-15037157500.0State servantHigher education02.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 36452 F | Y | Y | 12.218500 | Working | Incomplete higher | Married | House / apartment | -18593 | -5434 | 1 | 0 | Accountants | 2.0 | -19.0 | NaN | F-18593202500.0WorkingIncomplete higher02.0 | . 36453 M | Y | Y | 12.218500 | Working | Secondary / secondary special | Civil marriage | House / apartment | -10886 | -1315 | 1 | 0 | Laborers | 2.0 | -34.0 | NaN | M-10886202500.0WorkingSecondary / secondary sp... | . 36454 F | N | Y | 12.586223 | Working | Secondary / secondary special | Married | House / apartment | -21016 | -14018 | 0 | 0 | Medicine staff | 2.0 | -55.0 | NaN | F-21016292500.0WorkingSecondary / secondary sp... | . 36455 F | Y | N | 12.100718 | Commercial associate | Secondary / secondary special | Married | House / apartment | -16541 | -1085 | 0 | 0 | not employed | 2.0 | -33.0 | NaN | F-16541180000.0Commercial associateSecondary /... | . 36456 F | N | Y | 12.506181 | Working | Higher education | Married | House / apartment | -9154 | -187 | 0 | 1 | Laborers | 2.0 | -11.0 | NaN | F-9154270000.0WorkingHigher education12.0 | . 36457 rows × 17 columns . Category형으로 변환해준다. | gender, car, reality, income_type, family_type, house_type, occyp_type, edu_type | . cate = [&#39;gender&#39;, &#39;car&#39;, &#39;reality&#39;, &#39;income_type&#39;, &#39;family_type&#39;, &#39;house_type&#39;, &#39;occyp_type&#39;, &#39;edu_type&#39;,&#39;ID&#39;] from sklearn.preprocessing import LabelEncoder le = LabelEncoder() for k in cate: tmp[k] = le.fit_transform(tmp[k]) tmp[k] = tmp[k].astype(&quot;category&quot;) . 3. DAYS_BIRTH DAYS_EMPLOYED begin_month . 이 변수들은 음수로 되어 있음 -&gt; 의미가 반대가 되므로 양수로 변환을 한다. | . for k in [&#39;DAYS_BIRTH&#39;, &#39;begin_month&#39;, &#39;DAYS_EMPLOYED&#39;]: tmp[k] = -tmp[k] . #tmp.DAYS_EMPLOYED[tmp.DAYS_EMPLOYED &lt; 0] = 0 #tmp.DAYS_EMPLOYED = np.log1p(tmp.DAYS_EMPLOYED) . 4. before_employed, income_mean &#48320;&#49688; &#52628;&#44032; . tmp[&quot;before_employed&quot;] = tmp[&quot;DAYS_BIRTH&quot;] - tmp[&quot;DAYS_EMPLOYED&quot;] tmp[&quot;before_employed_month&quot;] = np.floor(tmp[&quot;before_employed&quot;] / 30) - (np.floor(tmp[&quot;before_employed&quot;] / 30) / 12).astype(int)*12 tmp[&quot;before_employed_week&quot;] = np.floor(tmp[&quot;before_employed&quot;] / 7) - (np.floor(tmp[&quot;before_employed&quot;] / 7) / 4).astype(int)*4 tmp[&quot;income_mean&quot;] = tmp[&#39;income_total&#39;] / tmp[&#39;family_size&#39;] . train,test_set&#51004;&#47196; &#48516;&#47532; . test에서는 7명이 넘는 값은 존재하지 않으므로 굳이 학습할 필요없음.family_size가 7 초과인 데이터들을 지워본다. | . train.family_size.value_counts() . 2.0 14106 1.0 5109 3.0 4632 4.0 2260 5.0 291 6.0 44 7.0 9 15.0 3 9.0 2 20.0 1 Name: family_size, dtype: int64 . test.family_size.value_counts() . 2.0 5357 1.0 1878 3.0 1789 4.0 846 5.0 106 6.0 14 7.0 10 Name: family_size, dtype: int64 . train_set = tmp.iloc[0:26457,] test_set = tmp.iloc[26457:,] . train_set = train_set[train_set.family_size &lt;=7] . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(train_set.drop(&quot;credit&quot;, axis = 1), train_set[&quot;credit&quot;], test_size=0.2, random_state=60, stratify = train_set[&quot;credit&quot;]) . X_train . gender car reality income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED work_phone email occyp_type family_size begin_month ID before_employed before_employed_month before_employed_week income_mean . 198 0 | 0 | 1 | 12.323860 | 2 | 4 | 1 | 1 | 13077 | 5394 | 1 | 0 | 11 | 4.0 | 26.0 | 1326 | 7683 | 4.0 | 1.0 | 3.080965 | . 25137 1 | 0 | 1 | 11.813037 | 4 | 2 | 3 | 5 | 8231 | 731 | 0 | 0 | 2 | 1.0 | 17.0 | 9347 | 7500 | 10.0 | 3.0 | 11.813037 | . 739 1 | 1 | 1 | 12.218500 | 4 | 4 | 1 | 1 | 18613 | 3883 | 0 | 0 | 8 | 3.0 | 41.0 | 8597 | 14730 | 11.0 | 0.0 | 4.072833 | . 7236 0 | 0 | 0 | 11.630717 | 4 | 4 | 0 | 1 | 14308 | 655 | 0 | 0 | 18 | 3.0 | 54.0 | 1917 | 13653 | 11.0 | 2.0 | 3.876906 | . 11634 0 | 0 | 1 | 12.400821 | 1 | 4 | 1 | 1 | 9699 | -365243 | 0 | 1 | 18 | 2.0 | 27.0 | 6218 | 374942 | 6.0 | 3.0 | 6.200410 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 7834 1 | 1 | 1 | 11.967187 | 4 | 4 | 1 | 1 | 19242 | 546 | 0 | 0 | 8 | 2.0 | 4.0 | 8731 | 18696 | 11.0 | 2.0 | 5.983594 | . 23544 0 | 0 | 1 | 12.419170 | 0 | 4 | 1 | 1 | 12309 | 2788 | 0 | 0 | 8 | 2.0 | 27.0 | 978 | 9521 | 5.0 | 0.0 | 6.209585 | . 24568 0 | 0 | 0 | 11.407576 | 4 | 4 | 2 | 1 | 11682 | 678 | 0 | 0 | 14 | 1.0 | 22.0 | 697 | 11004 | 6.0 | 0.0 | 11.407576 | . 9283 0 | 0 | 1 | 11.707678 | 4 | 4 | 0 | 1 | 18513 | 8163 | 0 | 0 | 6 | 2.0 | 31.0 | 3717 | 10350 | 9.0 | 2.0 | 5.853839 | . 24831 1 | 1 | 0 | 12.323860 | 4 | 4 | 1 | 1 | 10785 | 1345 | 0 | 0 | 18 | 2.0 | 36.0 | 6559 | 9440 | 2.0 | 0.0 | 6.161930 | . 21160 rows × 20 columns . Imbalce&#47484; &#54644;&#44208;&#54616;&#45716; &#44592;&#48277;&#51008; &#51339;&#51648;&#50506;&#51008; &#49457;&#45733;&#51012; &#48372;&#50668;&#51456;&#45796; . 사용 X | . from imblearn.over_sampling import RandomOverSampler from imblearn.under_sampling import RandomUnderSampler from imblearn.over_sampling import SMOTE # train데이터를 넣어 복제함 X_resampled, y_resampled = RandomUnderSampler(random_state=60).fit_resample(X_train, y_train) . /usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24. warnings.warn(msg, category=FutureWarning) . X_resampled = pd.DataFrame(X_resampled) X_resampled.columns = X_train.columns X_resampled . gender car reality income_total income_type edu_type family_type house_type DAYS_BIRTH DAYS_EMPLOYED work_phone email occyp_type family_size begin_month . 0 1.0 | 1.0 | 0.0 | 12.323860 | 4.0 | 4.0 | 1.0 | 1.0 | 13502.0 | 7.721792 | 1.0 | 0.0 | 4.0 | 2.0 | 20.0 | . 1 1.0 | 0.0 | 1.0 | 12.100718 | 0.0 | 1.0 | 2.0 | 1.0 | 10282.0 | 4.691348 | 0.0 | 0.0 | 3.0 | 1.0 | -0.0 | . 2 0.0 | 0.0 | 0.0 | 11.630717 | 4.0 | 4.0 | 1.0 | 5.0 | 11499.0 | 8.295798 | 0.0 | 1.0 | 3.0 | 3.0 | 39.0 | . 3 1.0 | 1.0 | 0.0 | 12.702295 | 4.0 | 1.0 | 1.0 | 1.0 | 14897.0 | 7.884200 | 1.0 | 0.0 | 18.0 | 2.0 | 26.0 | . 4 0.0 | 0.0 | 0.0 | 11.967187 | 0.0 | 4.0 | 1.0 | 1.0 | 10425.0 | 7.336286 | 1.0 | 0.0 | 14.0 | 2.0 | 4.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 7729 1.0 | 0.0 | 1.0 | 11.813037 | 4.0 | 4.0 | 1.0 | 4.0 | 10414.0 | 7.916807 | 1.0 | 0.0 | 8.0 | 3.0 | 6.0 | . 7730 1.0 | 1.0 | 1.0 | 12.586223 | 4.0 | 4.0 | 1.0 | 1.0 | 16666.0 | 8.405144 | 0.0 | 0.0 | 8.0 | 3.0 | 12.0 | . 7731 0.0 | 0.0 | 0.0 | 12.196027 | 1.0 | 4.0 | 4.0 | 1.0 | 22002.0 | 0.000000 | 0.0 | 0.0 | 18.0 | 1.0 | 48.0 | . 7732 0.0 | 0.0 | 0.0 | 12.793862 | 0.0 | 4.0 | 1.0 | 1.0 | 18580.0 | 7.496097 | 0.0 | 0.0 | 10.0 | 2.0 | 24.0 | . 7733 0.0 | 0.0 | 1.0 | 11.630717 | 4.0 | 4.0 | 1.0 | 1.0 | 13741.0 | 8.560636 | 0.0 | 0.0 | 8.0 | 3.0 | 16.0 | . 7734 rows × 15 columns . sns.countplot(y_resampled) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4e3d4d1b10&gt; . for k in cate: X_resampled[k] = X_resampled[k].astype(int) X_resampled[k] = X_resampled[k].astype(&quot;category&quot;) . Modeling . pip install catboost . Collecting catboost Downloading catboost-1.0.3-cp37-none-manylinux1_x86_64.whl (76.3 MB) |████████████████████████████████| 76.3 MB 26 kB/s Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.11.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (1.3.3) Installing collected packages: catboost Successfully installed catboost-1.0.3 . from catboost import CatBoostClassifier cat = CatBoostClassifier(iterations = 500,learning_rate=0.1, cat_features = cate, random_state = 60, depth=7, task_type=&quot;GPU&quot;) cat.fit(X_train,y_train,eval_set=[(X_test, y_test)],verbose = True, early_stopping_rounds = 100, use_best_model=True) . 0: learn: 1.0424704 test: 1.0416503 best: 1.0416503 (0) total: 24.8ms remaining: 12.4s 1: learn: 0.9985628 test: 0.9972396 best: 0.9972396 (1) total: 54.9ms remaining: 13.7s 2: learn: 0.9640350 test: 0.9622696 best: 0.9622696 (2) total: 76.6ms remaining: 12.7s 3: learn: 0.9362448 test: 0.9338144 best: 0.9338144 (3) total: 99.4ms remaining: 12.3s 4: learn: 0.9139808 test: 0.9114770 best: 0.9114770 (4) total: 122ms remaining: 12s 5: learn: 0.8953069 test: 0.8929587 best: 0.8929587 (5) total: 145ms remaining: 12s 6: learn: 0.8798386 test: 0.8772798 best: 0.8772798 (6) total: 167ms remaining: 11.8s 7: learn: 0.8673182 test: 0.8648580 best: 0.8648580 (7) total: 190ms remaining: 11.7s 8: learn: 0.8566346 test: 0.8546163 best: 0.8546163 (8) total: 221ms remaining: 12s 9: learn: 0.8477110 test: 0.8455330 best: 0.8455330 (9) total: 246ms remaining: 12s 10: learn: 0.8404088 test: 0.8381990 best: 0.8381990 (10) total: 268ms remaining: 11.9s 11: learn: 0.8261194 test: 0.8205447 best: 0.8205447 (11) total: 299ms remaining: 12.1s 12: learn: 0.8144615 test: 0.8056420 best: 0.8056420 (12) total: 321ms remaining: 12s 13: learn: 0.8050253 test: 0.7941390 best: 0.7941390 (13) total: 344ms remaining: 11.9s 14: learn: 0.7966782 test: 0.7836701 best: 0.7836701 (14) total: 366ms remaining: 11.8s 15: learn: 0.7895632 test: 0.7750891 best: 0.7750891 (15) total: 390ms remaining: 11.8s 16: learn: 0.7836813 test: 0.7678032 best: 0.7678032 (16) total: 413ms remaining: 11.7s 17: learn: 0.7788151 test: 0.7614798 best: 0.7614798 (17) total: 439ms remaining: 11.7s 18: learn: 0.7748508 test: 0.7570873 best: 0.7570873 (18) total: 467ms remaining: 11.8s 19: learn: 0.7711222 test: 0.7525767 best: 0.7525767 (19) total: 502ms remaining: 12s 20: learn: 0.7680945 test: 0.7488612 best: 0.7488612 (20) total: 525ms remaining: 12s 21: learn: 0.7653580 test: 0.7452712 best: 0.7452712 (21) total: 548ms remaining: 11.9s 22: learn: 0.7631552 test: 0.7430977 best: 0.7430977 (22) total: 578ms remaining: 12s 23: learn: 0.7614519 test: 0.7412697 best: 0.7412697 (23) total: 602ms remaining: 11.9s 24: learn: 0.7594864 test: 0.7390298 best: 0.7390298 (24) total: 624ms remaining: 11.9s 25: learn: 0.7580255 test: 0.7374913 best: 0.7374913 (25) total: 648ms remaining: 11.8s 26: learn: 0.7565976 test: 0.7354912 best: 0.7354912 (26) total: 675ms remaining: 11.8s 27: learn: 0.7553615 test: 0.7343834 best: 0.7343834 (27) total: 704ms remaining: 11.9s 28: learn: 0.7540793 test: 0.7329034 best: 0.7329034 (28) total: 727ms remaining: 11.8s 29: learn: 0.7531573 test: 0.7316831 best: 0.7316831 (29) total: 749ms remaining: 11.7s 30: learn: 0.7521606 test: 0.7301760 best: 0.7301760 (30) total: 772ms remaining: 11.7s 31: learn: 0.7514437 test: 0.7295019 best: 0.7295019 (31) total: 795ms remaining: 11.6s 32: learn: 0.7505368 test: 0.7284557 best: 0.7284557 (32) total: 824ms remaining: 11.7s 33: learn: 0.7494882 test: 0.7276682 best: 0.7276682 (33) total: 851ms remaining: 11.7s 34: learn: 0.7487780 test: 0.7271724 best: 0.7271724 (34) total: 879ms remaining: 11.7s 35: learn: 0.7476550 test: 0.7264017 best: 0.7264017 (35) total: 904ms remaining: 11.7s 36: learn: 0.7466262 test: 0.7257254 best: 0.7257254 (36) total: 928ms remaining: 11.6s 37: learn: 0.7460277 test: 0.7251834 best: 0.7251834 (37) total: 952ms remaining: 11.6s 38: learn: 0.7456309 test: 0.7246826 best: 0.7246826 (38) total: 974ms remaining: 11.5s 39: learn: 0.7444237 test: 0.7238586 best: 0.7238586 (39) total: 998ms remaining: 11.5s 40: learn: 0.7436796 test: 0.7235806 best: 0.7235806 (40) total: 1.02s remaining: 11.4s 41: learn: 0.7431336 test: 0.7230441 best: 0.7230441 (41) total: 1.04s remaining: 11.4s 42: learn: 0.7422744 test: 0.7227054 best: 0.7227054 (42) total: 1.07s remaining: 11.4s 43: learn: 0.7414179 test: 0.7221712 best: 0.7221712 (43) total: 1.1s remaining: 11.4s 44: learn: 0.7408585 test: 0.7218124 best: 0.7218124 (44) total: 1.13s remaining: 11.4s 45: learn: 0.7402055 test: 0.7215398 best: 0.7215398 (45) total: 1.15s remaining: 11.3s 46: learn: 0.7392094 test: 0.7209650 best: 0.7209650 (46) total: 1.17s remaining: 11.3s 47: learn: 0.7382702 test: 0.7205900 best: 0.7205900 (47) total: 1.19s remaining: 11.2s 48: learn: 0.7376019 test: 0.7202852 best: 0.7202852 (48) total: 1.22s remaining: 11.2s 49: learn: 0.7371893 test: 0.7201105 best: 0.7201105 (49) total: 1.24s remaining: 11.1s 50: learn: 0.7363985 test: 0.7197004 best: 0.7197004 (50) total: 1.26s remaining: 11.1s 51: learn: 0.7360679 test: 0.7194593 best: 0.7194593 (51) total: 1.28s remaining: 11s 52: learn: 0.7355528 test: 0.7193439 best: 0.7193439 (52) total: 1.31s remaining: 11s 53: learn: 0.7348043 test: 0.7190404 best: 0.7190404 (53) total: 1.34s remaining: 11.1s 54: learn: 0.7345141 test: 0.7188157 best: 0.7188157 (54) total: 1.36s remaining: 11s 55: learn: 0.7339621 test: 0.7185572 best: 0.7185572 (55) total: 1.38s remaining: 11s 56: learn: 0.7334671 test: 0.7184689 best: 0.7184689 (56) total: 1.41s remaining: 10.9s 57: learn: 0.7331436 test: 0.7183552 best: 0.7183552 (57) total: 1.43s remaining: 10.9s 58: learn: 0.7326854 test: 0.7180941 best: 0.7180941 (58) total: 1.46s remaining: 10.9s 59: learn: 0.7323243 test: 0.7179025 best: 0.7179025 (59) total: 1.5s remaining: 11s 60: learn: 0.7318026 test: 0.7173878 best: 0.7173878 (60) total: 1.52s remaining: 11s 61: learn: 0.7315823 test: 0.7174029 best: 0.7173878 (60) total: 1.54s remaining: 10.9s 62: learn: 0.7310312 test: 0.7173049 best: 0.7173049 (62) total: 1.57s remaining: 10.9s 63: learn: 0.7302854 test: 0.7170226 best: 0.7170226 (63) total: 1.59s remaining: 10.8s 64: learn: 0.7298227 test: 0.7169637 best: 0.7169637 (64) total: 1.61s remaining: 10.8s 65: learn: 0.7293142 test: 0.7167018 best: 0.7167018 (65) total: 1.63s remaining: 10.7s 66: learn: 0.7290386 test: 0.7165754 best: 0.7165754 (66) total: 1.66s remaining: 10.7s 67: learn: 0.7287075 test: 0.7167147 best: 0.7165754 (66) total: 1.68s remaining: 10.6s 68: learn: 0.7285490 test: 0.7166757 best: 0.7165754 (66) total: 1.7s remaining: 10.6s 69: learn: 0.7282071 test: 0.7165957 best: 0.7165754 (66) total: 1.72s remaining: 10.6s 70: learn: 0.7276388 test: 0.7165658 best: 0.7165658 (70) total: 1.75s remaining: 10.6s 71: learn: 0.7268854 test: 0.7164184 best: 0.7164184 (71) total: 1.77s remaining: 10.5s 72: learn: 0.7263249 test: 0.7163349 best: 0.7163349 (72) total: 1.8s remaining: 10.5s 73: learn: 0.7258308 test: 0.7163357 best: 0.7163349 (72) total: 1.82s remaining: 10.5s 74: learn: 0.7252296 test: 0.7161216 best: 0.7161216 (74) total: 1.84s remaining: 10.5s 75: learn: 0.7248121 test: 0.7160872 best: 0.7160872 (75) total: 1.87s remaining: 10.4s 76: learn: 0.7245089 test: 0.7161066 best: 0.7160872 (75) total: 1.89s remaining: 10.4s 77: learn: 0.7243483 test: 0.7161187 best: 0.7160872 (75) total: 1.91s remaining: 10.3s 78: learn: 0.7241053 test: 0.7160313 best: 0.7160313 (78) total: 1.94s remaining: 10.3s 79: learn: 0.7235632 test: 0.7158389 best: 0.7158389 (79) total: 1.96s remaining: 10.3s 80: learn: 0.7230018 test: 0.7157378 best: 0.7157378 (80) total: 1.99s remaining: 10.3s 81: learn: 0.7224438 test: 0.7153752 best: 0.7153752 (81) total: 2.01s remaining: 10.2s 82: learn: 0.7217532 test: 0.7151827 best: 0.7151827 (82) total: 2.03s remaining: 10.2s 83: learn: 0.7212100 test: 0.7151302 best: 0.7151302 (83) total: 2.05s remaining: 10.2s 84: learn: 0.7208689 test: 0.7151601 best: 0.7151302 (83) total: 2.08s remaining: 10.1s 85: learn: 0.7203896 test: 0.7151232 best: 0.7151232 (85) total: 2.1s remaining: 10.1s 86: learn: 0.7202816 test: 0.7150467 best: 0.7150467 (86) total: 2.12s remaining: 10.1s 87: learn: 0.7197704 test: 0.7148641 best: 0.7148641 (87) total: 2.14s remaining: 10s 88: learn: 0.7195913 test: 0.7148654 best: 0.7148641 (87) total: 2.17s remaining: 10s 89: learn: 0.7191193 test: 0.7149324 best: 0.7148641 (87) total: 2.2s remaining: 10s 90: learn: 0.7189596 test: 0.7148824 best: 0.7148641 (87) total: 2.22s remaining: 9.97s 91: learn: 0.7187933 test: 0.7147526 best: 0.7147526 (91) total: 2.24s remaining: 9.93s 92: learn: 0.7183766 test: 0.7145849 best: 0.7145849 (92) total: 2.26s remaining: 9.89s 93: learn: 0.7181727 test: 0.7145561 best: 0.7145561 (93) total: 2.28s remaining: 9.87s 94: learn: 0.7178835 test: 0.7144581 best: 0.7144581 (94) total: 2.31s remaining: 9.83s 95: learn: 0.7173020 test: 0.7142744 best: 0.7142744 (95) total: 2.33s remaining: 9.8s 96: learn: 0.7171008 test: 0.7142821 best: 0.7142744 (95) total: 2.35s remaining: 9.76s 97: learn: 0.7165519 test: 0.7143001 best: 0.7142744 (95) total: 2.38s remaining: 9.75s 98: learn: 0.7161019 test: 0.7142045 best: 0.7142045 (98) total: 2.4s remaining: 9.74s 99: learn: 0.7158281 test: 0.7141173 best: 0.7141173 (99) total: 2.42s remaining: 9.7s 100: learn: 0.7157303 test: 0.7140912 best: 0.7140912 (100) total: 2.44s remaining: 9.66s 101: learn: 0.7155339 test: 0.7140700 best: 0.7140700 (101) total: 2.47s remaining: 9.62s 102: learn: 0.7151628 test: 0.7139580 best: 0.7139580 (102) total: 2.5s remaining: 9.64s 103: learn: 0.7146658 test: 0.7140031 best: 0.7139580 (102) total: 2.53s remaining: 9.62s 104: learn: 0.7143100 test: 0.7139206 best: 0.7139206 (104) total: 2.55s remaining: 9.6s 105: learn: 0.7139781 test: 0.7137894 best: 0.7137894 (105) total: 2.57s remaining: 9.57s 106: learn: 0.7133730 test: 0.7138093 best: 0.7137894 (105) total: 2.6s remaining: 9.54s 107: learn: 0.7129325 test: 0.7137523 best: 0.7137523 (107) total: 2.62s remaining: 9.51s 108: learn: 0.7126963 test: 0.7136564 best: 0.7136564 (108) total: 2.64s remaining: 9.47s 109: learn: 0.7123919 test: 0.7137201 best: 0.7136564 (108) total: 2.66s remaining: 9.44s 110: learn: 0.7118770 test: 0.7135967 best: 0.7135967 (110) total: 2.68s remaining: 9.41s 111: learn: 0.7114885 test: 0.7135480 best: 0.7135480 (111) total: 2.71s remaining: 9.37s 112: learn: 0.7112159 test: 0.7135605 best: 0.7135480 (111) total: 2.73s remaining: 9.34s 113: learn: 0.7107373 test: 0.7134553 best: 0.7134553 (113) total: 2.75s remaining: 9.31s 114: learn: 0.7102662 test: 0.7134715 best: 0.7134553 (113) total: 2.78s remaining: 9.32s 115: learn: 0.7098206 test: 0.7134493 best: 0.7134493 (115) total: 2.81s remaining: 9.3s 116: learn: 0.7090682 test: 0.7132093 best: 0.7132093 (116) total: 2.83s remaining: 9.27s 117: learn: 0.7085936 test: 0.7130425 best: 0.7130425 (117) total: 2.85s remaining: 9.24s 118: learn: 0.7081138 test: 0.7130276 best: 0.7130276 (118) total: 2.88s remaining: 9.21s 119: learn: 0.7075463 test: 0.7129281 best: 0.7129281 (119) total: 2.91s remaining: 9.21s 120: learn: 0.7068820 test: 0.7129074 best: 0.7129074 (120) total: 2.93s remaining: 9.18s 121: learn: 0.7065127 test: 0.7126598 best: 0.7126598 (121) total: 2.95s remaining: 9.14s 122: learn: 0.7059353 test: 0.7126716 best: 0.7126598 (121) total: 2.97s remaining: 9.12s 123: learn: 0.7054418 test: 0.7125289 best: 0.7125289 (123) total: 3s remaining: 9.1s 124: learn: 0.7052337 test: 0.7124573 best: 0.7124573 (124) total: 3.02s remaining: 9.07s 125: learn: 0.7050608 test: 0.7124662 best: 0.7124573 (124) total: 3.04s remaining: 9.04s 126: learn: 0.7044235 test: 0.7122318 best: 0.7122318 (126) total: 3.07s remaining: 9.01s 127: learn: 0.7040619 test: 0.7121661 best: 0.7121661 (127) total: 3.09s remaining: 8.98s 128: learn: 0.7036892 test: 0.7120443 best: 0.7120443 (128) total: 3.12s remaining: 8.98s 129: learn: 0.7032635 test: 0.7121137 best: 0.7120443 (128) total: 3.14s remaining: 8.95s 130: learn: 0.7028812 test: 0.7122085 best: 0.7120443 (128) total: 3.17s remaining: 8.92s 131: learn: 0.7022036 test: 0.7121506 best: 0.7120443 (128) total: 3.19s remaining: 8.89s 132: learn: 0.7021203 test: 0.7120704 best: 0.7120443 (128) total: 3.22s remaining: 8.87s 133: learn: 0.7017174 test: 0.7119328 best: 0.7119328 (133) total: 3.24s remaining: 8.85s 134: learn: 0.7014952 test: 0.7119426 best: 0.7119328 (133) total: 3.26s remaining: 8.81s 135: learn: 0.7007965 test: 0.7119745 best: 0.7119328 (133) total: 3.28s remaining: 8.79s 136: learn: 0.7007241 test: 0.7119651 best: 0.7119328 (133) total: 3.3s remaining: 8.75s 137: learn: 0.7003424 test: 0.7119715 best: 0.7119328 (133) total: 3.33s remaining: 8.72s 138: learn: 0.7000573 test: 0.7118967 best: 0.7118967 (138) total: 3.35s remaining: 8.69s 139: learn: 0.6996327 test: 0.7117585 best: 0.7117585 (139) total: 3.37s remaining: 8.68s 140: learn: 0.6992976 test: 0.7117055 best: 0.7117055 (140) total: 3.4s remaining: 8.65s 141: learn: 0.6984258 test: 0.7113627 best: 0.7113627 (141) total: 3.42s remaining: 8.64s 142: learn: 0.6981711 test: 0.7112387 best: 0.7112387 (142) total: 3.45s remaining: 8.61s 143: learn: 0.6977295 test: 0.7111559 best: 0.7111559 (143) total: 3.49s remaining: 8.62s 144: learn: 0.6974812 test: 0.7110782 best: 0.7110782 (144) total: 3.51s remaining: 8.59s 145: learn: 0.6967176 test: 0.7111323 best: 0.7110782 (144) total: 3.53s remaining: 8.57s 146: learn: 0.6964923 test: 0.7110352 best: 0.7110352 (146) total: 3.55s remaining: 8.53s 147: learn: 0.6962947 test: 0.7110172 best: 0.7110172 (147) total: 3.58s remaining: 8.5s 148: learn: 0.6959648 test: 0.7110612 best: 0.7110172 (147) total: 3.6s remaining: 8.47s 149: learn: 0.6956564 test: 0.7110951 best: 0.7110172 (147) total: 3.62s remaining: 8.45s 150: learn: 0.6950953 test: 0.7108924 best: 0.7108924 (150) total: 3.65s remaining: 8.43s 151: learn: 0.6944086 test: 0.7106826 best: 0.7106826 (151) total: 3.67s remaining: 8.4s 152: learn: 0.6942471 test: 0.7105938 best: 0.7105938 (152) total: 3.7s remaining: 8.39s 153: learn: 0.6938798 test: 0.7104909 best: 0.7104909 (153) total: 3.72s remaining: 8.36s 154: learn: 0.6936016 test: 0.7104873 best: 0.7104873 (154) total: 3.75s remaining: 8.34s 155: learn: 0.6932265 test: 0.7104965 best: 0.7104873 (154) total: 3.77s remaining: 8.31s 156: learn: 0.6931169 test: 0.7104529 best: 0.7104529 (156) total: 3.79s remaining: 8.28s 157: learn: 0.6928372 test: 0.7105149 best: 0.7104529 (156) total: 3.81s remaining: 8.25s 158: learn: 0.6927891 test: 0.7105068 best: 0.7104529 (156) total: 3.83s remaining: 8.22s 159: learn: 0.6926853 test: 0.7105124 best: 0.7104529 (156) total: 3.86s remaining: 8.2s 160: learn: 0.6921400 test: 0.7104189 best: 0.7104189 (160) total: 3.88s remaining: 8.17s 161: learn: 0.6917380 test: 0.7104296 best: 0.7104189 (160) total: 3.9s remaining: 8.15s 162: learn: 0.6913539 test: 0.7104267 best: 0.7104189 (160) total: 3.93s remaining: 8.12s 163: learn: 0.6910435 test: 0.7103996 best: 0.7103996 (163) total: 3.95s remaining: 8.09s 164: learn: 0.6903541 test: 0.7104207 best: 0.7103996 (163) total: 3.97s remaining: 8.06s 165: learn: 0.6900530 test: 0.7103306 best: 0.7103306 (165) total: 4s remaining: 8.04s 166: learn: 0.6896219 test: 0.7104294 best: 0.7103306 (165) total: 4.03s remaining: 8.03s 167: learn: 0.6889109 test: 0.7101890 best: 0.7101890 (167) total: 4.05s remaining: 8s 168: learn: 0.6884845 test: 0.7101906 best: 0.7101890 (167) total: 4.08s remaining: 8s 169: learn: 0.6879645 test: 0.7101221 best: 0.7101221 (169) total: 4.11s remaining: 7.98s 170: learn: 0.6875486 test: 0.7100080 best: 0.7100080 (170) total: 4.13s remaining: 7.95s 171: learn: 0.6872845 test: 0.7099429 best: 0.7099429 (171) total: 4.17s remaining: 7.96s 172: learn: 0.6866552 test: 0.7096246 best: 0.7096246 (172) total: 4.21s remaining: 7.96s 173: learn: 0.6863010 test: 0.7095203 best: 0.7095203 (173) total: 4.23s remaining: 7.93s 174: learn: 0.6859236 test: 0.7095215 best: 0.7095203 (173) total: 4.26s remaining: 7.91s 175: learn: 0.6854650 test: 0.7094370 best: 0.7094370 (175) total: 4.29s remaining: 7.9s 176: learn: 0.6851673 test: 0.7094192 best: 0.7094192 (176) total: 4.31s remaining: 7.87s 177: learn: 0.6849257 test: 0.7093487 best: 0.7093487 (177) total: 4.34s remaining: 7.85s 178: learn: 0.6845831 test: 0.7094840 best: 0.7093487 (177) total: 4.36s remaining: 7.82s 179: learn: 0.6843561 test: 0.7093844 best: 0.7093487 (177) total: 4.38s remaining: 7.79s 180: learn: 0.6840471 test: 0.7094027 best: 0.7093487 (177) total: 4.41s remaining: 7.77s 181: learn: 0.6839139 test: 0.7093746 best: 0.7093487 (177) total: 4.43s remaining: 7.74s 182: learn: 0.6837011 test: 0.7094866 best: 0.7093487 (177) total: 4.45s remaining: 7.71s 183: learn: 0.6831669 test: 0.7094508 best: 0.7093487 (177) total: 4.48s remaining: 7.7s 184: learn: 0.6827114 test: 0.7095362 best: 0.7093487 (177) total: 4.51s remaining: 7.68s 185: learn: 0.6825692 test: 0.7094913 best: 0.7093487 (177) total: 4.53s remaining: 7.65s 186: learn: 0.6821689 test: 0.7095153 best: 0.7093487 (177) total: 4.55s remaining: 7.62s 187: learn: 0.6819271 test: 0.7094335 best: 0.7093487 (177) total: 4.57s remaining: 7.59s 188: learn: 0.6814522 test: 0.7094165 best: 0.7093487 (177) total: 4.6s remaining: 7.57s 189: learn: 0.6811083 test: 0.7093808 best: 0.7093487 (177) total: 4.62s remaining: 7.54s 190: learn: 0.6808922 test: 0.7092905 best: 0.7092905 (190) total: 4.64s remaining: 7.51s 191: learn: 0.6804691 test: 0.7090831 best: 0.7090831 (191) total: 4.66s remaining: 7.48s 192: learn: 0.6799430 test: 0.7091876 best: 0.7090831 (191) total: 4.69s remaining: 7.46s 193: learn: 0.6795236 test: 0.7091492 best: 0.7090831 (191) total: 4.73s remaining: 7.46s 194: learn: 0.6793904 test: 0.7090823 best: 0.7090823 (194) total: 4.75s remaining: 7.43s 195: learn: 0.6787588 test: 0.7090434 best: 0.7090434 (195) total: 4.77s remaining: 7.4s 196: learn: 0.6784254 test: 0.7089969 best: 0.7089969 (196) total: 4.79s remaining: 7.37s 197: learn: 0.6781641 test: 0.7090110 best: 0.7089969 (196) total: 4.82s remaining: 7.35s 198: learn: 0.6775794 test: 0.7089385 best: 0.7089385 (198) total: 4.84s remaining: 7.33s 199: learn: 0.6772093 test: 0.7089667 best: 0.7089385 (198) total: 4.87s remaining: 7.3s 200: learn: 0.6766289 test: 0.7090209 best: 0.7089385 (198) total: 4.89s remaining: 7.27s 201: learn: 0.6762079 test: 0.7091395 best: 0.7089385 (198) total: 4.91s remaining: 7.25s 202: learn: 0.6756597 test: 0.7091201 best: 0.7089385 (198) total: 4.94s remaining: 7.23s 203: learn: 0.6752901 test: 0.7090861 best: 0.7089385 (198) total: 4.96s remaining: 7.2s 204: learn: 0.6745709 test: 0.7091302 best: 0.7089385 (198) total: 4.99s remaining: 7.19s 205: learn: 0.6743965 test: 0.7090500 best: 0.7089385 (198) total: 5.01s remaining: 7.16s 206: learn: 0.6739830 test: 0.7088888 best: 0.7088888 (206) total: 5.04s remaining: 7.13s 207: learn: 0.6737909 test: 0.7089111 best: 0.7088888 (206) total: 5.06s remaining: 7.1s 208: learn: 0.6734964 test: 0.7090367 best: 0.7088888 (206) total: 5.08s remaining: 7.08s 209: learn: 0.6734131 test: 0.7090248 best: 0.7088888 (206) total: 5.1s remaining: 7.05s 210: learn: 0.6729295 test: 0.7091104 best: 0.7088888 (206) total: 5.13s remaining: 7.02s 211: learn: 0.6725573 test: 0.7091286 best: 0.7088888 (206) total: 5.16s remaining: 7s 212: learn: 0.6722549 test: 0.7091964 best: 0.7088888 (206) total: 5.18s remaining: 6.98s 213: learn: 0.6719374 test: 0.7090908 best: 0.7088888 (206) total: 5.2s remaining: 6.95s 214: learn: 0.6713789 test: 0.7090988 best: 0.7088888 (206) total: 5.23s remaining: 6.93s 215: learn: 0.6712255 test: 0.7090379 best: 0.7088888 (206) total: 5.25s remaining: 6.91s 216: learn: 0.6709619 test: 0.7090870 best: 0.7088888 (206) total: 5.28s remaining: 6.88s 217: learn: 0.6703915 test: 0.7089476 best: 0.7088888 (206) total: 5.3s remaining: 6.86s 218: learn: 0.6702562 test: 0.7089754 best: 0.7088888 (206) total: 5.32s remaining: 6.83s 219: learn: 0.6697246 test: 0.7091830 best: 0.7088888 (206) total: 5.35s remaining: 6.81s 220: learn: 0.6694307 test: 0.7090993 best: 0.7088888 (206) total: 5.38s remaining: 6.79s 221: learn: 0.6692270 test: 0.7090790 best: 0.7088888 (206) total: 5.4s remaining: 6.76s 222: learn: 0.6690480 test: 0.7090048 best: 0.7088888 (206) total: 5.42s remaining: 6.74s 223: learn: 0.6687489 test: 0.7089288 best: 0.7088888 (206) total: 5.45s remaining: 6.71s 224: learn: 0.6680270 test: 0.7088241 best: 0.7088241 (224) total: 5.48s remaining: 6.7s 225: learn: 0.6677674 test: 0.7087220 best: 0.7087220 (225) total: 5.5s remaining: 6.67s 226: learn: 0.6673670 test: 0.7084730 best: 0.7084730 (226) total: 5.53s remaining: 6.65s 227: learn: 0.6670035 test: 0.7083986 best: 0.7083986 (227) total: 5.55s remaining: 6.62s 228: learn: 0.6666595 test: 0.7082972 best: 0.7082972 (228) total: 5.58s remaining: 6.6s 229: learn: 0.6664248 test: 0.7082103 best: 0.7082103 (229) total: 5.6s remaining: 6.58s 230: learn: 0.6659726 test: 0.7080973 best: 0.7080973 (230) total: 5.63s remaining: 6.55s 231: learn: 0.6652198 test: 0.7080541 best: 0.7080541 (231) total: 5.65s remaining: 6.53s 232: learn: 0.6647298 test: 0.7080160 best: 0.7080160 (232) total: 5.68s remaining: 6.51s 233: learn: 0.6643199 test: 0.7079953 best: 0.7079953 (233) total: 5.71s remaining: 6.49s 234: learn: 0.6641818 test: 0.7080876 best: 0.7079953 (233) total: 5.73s remaining: 6.46s 235: learn: 0.6637108 test: 0.7080693 best: 0.7079953 (233) total: 5.75s remaining: 6.44s 236: learn: 0.6634207 test: 0.7082190 best: 0.7079953 (233) total: 5.78s remaining: 6.41s 237: learn: 0.6631345 test: 0.7081945 best: 0.7079953 (233) total: 5.8s remaining: 6.39s 238: learn: 0.6628879 test: 0.7081890 best: 0.7079953 (233) total: 5.83s remaining: 6.36s 239: learn: 0.6625966 test: 0.7081827 best: 0.7079953 (233) total: 5.85s remaining: 6.33s 240: learn: 0.6624263 test: 0.7081933 best: 0.7079953 (233) total: 5.87s remaining: 6.31s 241: learn: 0.6621345 test: 0.7082026 best: 0.7079953 (233) total: 5.89s remaining: 6.28s 242: learn: 0.6617988 test: 0.7081245 best: 0.7079953 (233) total: 5.92s remaining: 6.26s 243: learn: 0.6612879 test: 0.7080313 best: 0.7079953 (233) total: 5.95s remaining: 6.24s 244: learn: 0.6611515 test: 0.7080568 best: 0.7079953 (233) total: 5.97s remaining: 6.21s 245: learn: 0.6608870 test: 0.7080227 best: 0.7079953 (233) total: 6s remaining: 6.19s 246: learn: 0.6605475 test: 0.7080612 best: 0.7079953 (233) total: 6.02s remaining: 6.17s 247: learn: 0.6603079 test: 0.7080918 best: 0.7079953 (233) total: 6.04s remaining: 6.14s 248: learn: 0.6599326 test: 0.7080418 best: 0.7079953 (233) total: 6.07s remaining: 6.12s 249: learn: 0.6595734 test: 0.7079953 best: 0.7079953 (249) total: 6.09s remaining: 6.09s 250: learn: 0.6592340 test: 0.7080421 best: 0.7079953 (249) total: 6.11s remaining: 6.07s 251: learn: 0.6588384 test: 0.7081220 best: 0.7079953 (249) total: 6.14s remaining: 6.04s 252: learn: 0.6584415 test: 0.7080683 best: 0.7079953 (249) total: 6.16s remaining: 6.01s 253: learn: 0.6578836 test: 0.7081132 best: 0.7079953 (249) total: 6.19s remaining: 5.99s 254: learn: 0.6576083 test: 0.7081561 best: 0.7079953 (249) total: 6.21s remaining: 5.97s 255: learn: 0.6573206 test: 0.7081823 best: 0.7079953 (249) total: 6.24s remaining: 5.95s 256: learn: 0.6571550 test: 0.7081556 best: 0.7079953 (249) total: 6.26s remaining: 5.92s 257: learn: 0.6566651 test: 0.7080360 best: 0.7079953 (249) total: 6.29s remaining: 5.9s 258: learn: 0.6561562 test: 0.7080779 best: 0.7079953 (249) total: 6.31s remaining: 5.87s 259: learn: 0.6560206 test: 0.7079587 best: 0.7079587 (259) total: 6.33s remaining: 5.85s 260: learn: 0.6558130 test: 0.7080396 best: 0.7079587 (259) total: 6.35s remaining: 5.82s 261: learn: 0.6554466 test: 0.7080464 best: 0.7079587 (259) total: 6.38s remaining: 5.79s 262: learn: 0.6552085 test: 0.7079007 best: 0.7079007 (262) total: 6.4s remaining: 5.77s 263: learn: 0.6547451 test: 0.7079342 best: 0.7079007 (262) total: 6.43s remaining: 5.75s 264: learn: 0.6543546 test: 0.7079593 best: 0.7079007 (262) total: 6.47s remaining: 5.73s 265: learn: 0.6541375 test: 0.7079838 best: 0.7079007 (262) total: 6.49s remaining: 5.71s 266: learn: 0.6538625 test: 0.7079920 best: 0.7079007 (262) total: 6.51s remaining: 5.68s 267: learn: 0.6535562 test: 0.7079257 best: 0.7079007 (262) total: 6.54s remaining: 5.66s 268: learn: 0.6530598 test: 0.7075134 best: 0.7075134 (268) total: 6.56s remaining: 5.64s 269: learn: 0.6528058 test: 0.7075483 best: 0.7075134 (268) total: 6.58s remaining: 5.61s 270: learn: 0.6525020 test: 0.7074909 best: 0.7074909 (270) total: 6.61s remaining: 5.58s 271: learn: 0.6522742 test: 0.7075653 best: 0.7074909 (270) total: 6.64s remaining: 5.56s 272: learn: 0.6519663 test: 0.7076348 best: 0.7074909 (270) total: 6.66s remaining: 5.54s 273: learn: 0.6518774 test: 0.7075847 best: 0.7074909 (270) total: 6.68s remaining: 5.51s 274: learn: 0.6516656 test: 0.7076186 best: 0.7074909 (270) total: 6.71s remaining: 5.49s 275: learn: 0.6513428 test: 0.7076264 best: 0.7074909 (270) total: 6.73s remaining: 5.46s 276: learn: 0.6510765 test: 0.7077186 best: 0.7074909 (270) total: 6.75s remaining: 5.43s 277: learn: 0.6506747 test: 0.7077351 best: 0.7074909 (270) total: 6.77s remaining: 5.41s 278: learn: 0.6504342 test: 0.7077090 best: 0.7074909 (270) total: 6.8s remaining: 5.39s 279: learn: 0.6501970 test: 0.7076354 best: 0.7074909 (270) total: 6.82s remaining: 5.36s 280: learn: 0.6499357 test: 0.7076621 best: 0.7074909 (270) total: 6.85s remaining: 5.34s 281: learn: 0.6496932 test: 0.7076078 best: 0.7074909 (270) total: 6.87s remaining: 5.31s 282: learn: 0.6495311 test: 0.7076327 best: 0.7074909 (270) total: 6.89s remaining: 5.29s 283: learn: 0.6491179 test: 0.7075846 best: 0.7074909 (270) total: 6.92s remaining: 5.26s 284: learn: 0.6489443 test: 0.7075941 best: 0.7074909 (270) total: 6.94s remaining: 5.23s 285: learn: 0.6486605 test: 0.7075915 best: 0.7074909 (270) total: 6.96s remaining: 5.21s 286: learn: 0.6484253 test: 0.7075755 best: 0.7074909 (270) total: 6.98s remaining: 5.18s 287: learn: 0.6480964 test: 0.7075429 best: 0.7074909 (270) total: 7s remaining: 5.16s 288: learn: 0.6477210 test: 0.7076099 best: 0.7074909 (270) total: 7.03s remaining: 5.13s 289: learn: 0.6474308 test: 0.7074819 best: 0.7074819 (289) total: 7.06s remaining: 5.11s 290: learn: 0.6470279 test: 0.7074631 best: 0.7074631 (290) total: 7.09s remaining: 5.09s 291: learn: 0.6467254 test: 0.7074189 best: 0.7074189 (291) total: 7.11s remaining: 5.06s 292: learn: 0.6465868 test: 0.7074205 best: 0.7074189 (291) total: 7.13s remaining: 5.04s 293: learn: 0.6463256 test: 0.7073550 best: 0.7073550 (293) total: 7.15s remaining: 5.01s 294: learn: 0.6459398 test: 0.7072975 best: 0.7072975 (294) total: 7.18s remaining: 4.99s 295: learn: 0.6457585 test: 0.7072999 best: 0.7072975 (294) total: 7.2s remaining: 4.96s 296: learn: 0.6456203 test: 0.7073357 best: 0.7072975 (294) total: 7.22s remaining: 4.94s 297: learn: 0.6454301 test: 0.7071984 best: 0.7071984 (297) total: 7.24s remaining: 4.91s 298: learn: 0.6451717 test: 0.7072609 best: 0.7071984 (297) total: 7.27s remaining: 4.89s 299: learn: 0.6448577 test: 0.7072367 best: 0.7071984 (297) total: 7.31s remaining: 4.87s 300: learn: 0.6444805 test: 0.7071052 best: 0.7071052 (300) total: 7.33s remaining: 4.84s 301: learn: 0.6441828 test: 0.7070115 best: 0.7070115 (301) total: 7.35s remaining: 4.82s 302: learn: 0.6437638 test: 0.7068758 best: 0.7068758 (302) total: 7.38s remaining: 4.8s 303: learn: 0.6436383 test: 0.7068368 best: 0.7068368 (303) total: 7.4s remaining: 4.77s 304: learn: 0.6433728 test: 0.7066679 best: 0.7066679 (304) total: 7.42s remaining: 4.74s 305: learn: 0.6429769 test: 0.7065821 best: 0.7065821 (305) total: 7.44s remaining: 4.72s 306: learn: 0.6427659 test: 0.7065616 best: 0.7065616 (306) total: 7.48s remaining: 4.7s 307: learn: 0.6424441 test: 0.7065472 best: 0.7065472 (307) total: 7.5s remaining: 4.68s 308: learn: 0.6421577 test: 0.7064843 best: 0.7064843 (308) total: 7.53s remaining: 4.65s 309: learn: 0.6419783 test: 0.7064692 best: 0.7064692 (309) total: 7.55s remaining: 4.63s 310: learn: 0.6418209 test: 0.7064658 best: 0.7064658 (310) total: 7.57s remaining: 4.6s 311: learn: 0.6414613 test: 0.7064035 best: 0.7064035 (311) total: 7.6s remaining: 4.58s 312: learn: 0.6410949 test: 0.7063840 best: 0.7063840 (312) total: 7.62s remaining: 4.55s 313: learn: 0.6408543 test: 0.7064428 best: 0.7063840 (312) total: 7.64s remaining: 4.53s 314: learn: 0.6405632 test: 0.7064260 best: 0.7063840 (312) total: 7.67s remaining: 4.5s 315: learn: 0.6400103 test: 0.7063519 best: 0.7063519 (315) total: 7.71s remaining: 4.49s 316: learn: 0.6398539 test: 0.7063674 best: 0.7063519 (315) total: 7.73s remaining: 4.46s 317: learn: 0.6397875 test: 0.7063641 best: 0.7063519 (315) total: 7.75s remaining: 4.43s 318: learn: 0.6394935 test: 0.7063717 best: 0.7063519 (315) total: 7.77s remaining: 4.41s 319: learn: 0.6391981 test: 0.7063440 best: 0.7063440 (319) total: 7.79s remaining: 4.38s 320: learn: 0.6389409 test: 0.7062483 best: 0.7062483 (320) total: 7.82s remaining: 4.36s 321: learn: 0.6386806 test: 0.7062965 best: 0.7062483 (320) total: 7.84s remaining: 4.33s 322: learn: 0.6385438 test: 0.7063500 best: 0.7062483 (320) total: 7.87s remaining: 4.31s 323: learn: 0.6381363 test: 0.7063075 best: 0.7062483 (320) total: 7.89s remaining: 4.29s 324: learn: 0.6378952 test: 0.7062948 best: 0.7062483 (320) total: 7.92s remaining: 4.26s 325: learn: 0.6374662 test: 0.7062700 best: 0.7062483 (320) total: 7.94s remaining: 4.24s 326: learn: 0.6373376 test: 0.7063117 best: 0.7062483 (320) total: 7.96s remaining: 4.21s 327: learn: 0.6370923 test: 0.7063142 best: 0.7062483 (320) total: 7.99s remaining: 4.19s 328: learn: 0.6367907 test: 0.7062881 best: 0.7062483 (320) total: 8.01s remaining: 4.16s 329: learn: 0.6366342 test: 0.7062616 best: 0.7062483 (320) total: 8.03s remaining: 4.14s 330: learn: 0.6363896 test: 0.7063089 best: 0.7062483 (320) total: 8.05s remaining: 4.11s 331: learn: 0.6360070 test: 0.7063633 best: 0.7062483 (320) total: 8.08s remaining: 4.09s 332: learn: 0.6354231 test: 0.7062477 best: 0.7062477 (332) total: 8.11s remaining: 4.07s 333: learn: 0.6350572 test: 0.7062508 best: 0.7062477 (332) total: 8.14s remaining: 4.05s 334: learn: 0.6348114 test: 0.7061937 best: 0.7061937 (334) total: 8.17s remaining: 4.02s 335: learn: 0.6346349 test: 0.7061822 best: 0.7061822 (335) total: 8.19s remaining: 4s 336: learn: 0.6344346 test: 0.7062468 best: 0.7061822 (335) total: 8.22s remaining: 3.97s 337: learn: 0.6342861 test: 0.7061915 best: 0.7061822 (335) total: 8.24s remaining: 3.95s 338: learn: 0.6339075 test: 0.7060933 best: 0.7060933 (338) total: 8.26s remaining: 3.92s 339: learn: 0.6335819 test: 0.7060870 best: 0.7060870 (339) total: 8.29s remaining: 3.9s 340: learn: 0.6334097 test: 0.7060852 best: 0.7060852 (340) total: 8.31s remaining: 3.87s 341: learn: 0.6330053 test: 0.7061730 best: 0.7060852 (340) total: 8.33s remaining: 3.85s 342: learn: 0.6325399 test: 0.7063352 best: 0.7060852 (340) total: 8.37s remaining: 3.83s 343: learn: 0.6323127 test: 0.7063123 best: 0.7060852 (340) total: 8.39s remaining: 3.81s 344: learn: 0.6320390 test: 0.7063216 best: 0.7060852 (340) total: 8.42s remaining: 3.78s 345: learn: 0.6319118 test: 0.7062633 best: 0.7060852 (340) total: 8.44s remaining: 3.75s 346: learn: 0.6314882 test: 0.7061826 best: 0.7060852 (340) total: 8.47s remaining: 3.74s 347: learn: 0.6312655 test: 0.7062087 best: 0.7060852 (340) total: 8.5s remaining: 3.71s 348: learn: 0.6309959 test: 0.7061912 best: 0.7060852 (340) total: 8.52s remaining: 3.69s 349: learn: 0.6306406 test: 0.7061010 best: 0.7060852 (340) total: 8.54s remaining: 3.66s 350: learn: 0.6301978 test: 0.7059182 best: 0.7059182 (350) total: 8.57s remaining: 3.64s 351: learn: 0.6299100 test: 0.7059334 best: 0.7059182 (350) total: 8.6s remaining: 3.62s 352: learn: 0.6296881 test: 0.7059875 best: 0.7059182 (350) total: 8.62s remaining: 3.59s 353: learn: 0.6291458 test: 0.7060029 best: 0.7059182 (350) total: 8.65s remaining: 3.57s 354: learn: 0.6288842 test: 0.7058645 best: 0.7058645 (354) total: 8.68s remaining: 3.54s 355: learn: 0.6286983 test: 0.7058923 best: 0.7058645 (354) total: 8.7s remaining: 3.52s 356: learn: 0.6283938 test: 0.7058405 best: 0.7058405 (356) total: 8.72s remaining: 3.49s 357: learn: 0.6282258 test: 0.7058255 best: 0.7058255 (357) total: 8.74s remaining: 3.47s 358: learn: 0.6279884 test: 0.7057343 best: 0.7057343 (358) total: 8.77s remaining: 3.44s 359: learn: 0.6274878 test: 0.7057310 best: 0.7057310 (359) total: 8.8s remaining: 3.42s 360: learn: 0.6272736 test: 0.7057488 best: 0.7057310 (359) total: 8.82s remaining: 3.4s 361: learn: 0.6271710 test: 0.7057942 best: 0.7057310 (359) total: 8.85s remaining: 3.37s 362: learn: 0.6269532 test: 0.7057371 best: 0.7057310 (359) total: 8.87s remaining: 3.35s 363: learn: 0.6265171 test: 0.7056358 best: 0.7056358 (363) total: 8.9s remaining: 3.32s 364: learn: 0.6261830 test: 0.7056185 best: 0.7056185 (364) total: 8.92s remaining: 3.3s 365: learn: 0.6260464 test: 0.7056647 best: 0.7056185 (364) total: 8.94s remaining: 3.27s 366: learn: 0.6257448 test: 0.7056857 best: 0.7056185 (364) total: 8.96s remaining: 3.25s 367: learn: 0.6253826 test: 0.7057360 best: 0.7056185 (364) total: 8.99s remaining: 3.22s 368: learn: 0.6249944 test: 0.7057823 best: 0.7056185 (364) total: 9.02s remaining: 3.2s 369: learn: 0.6247013 test: 0.7057803 best: 0.7056185 (364) total: 9.04s remaining: 3.18s 370: learn: 0.6244577 test: 0.7058164 best: 0.7056185 (364) total: 9.06s remaining: 3.15s 371: learn: 0.6242588 test: 0.7059012 best: 0.7056185 (364) total: 9.09s remaining: 3.13s 372: learn: 0.6240280 test: 0.7058573 best: 0.7056185 (364) total: 9.12s remaining: 3.1s 373: learn: 0.6239156 test: 0.7058649 best: 0.7056185 (364) total: 9.14s remaining: 3.08s 374: learn: 0.6236132 test: 0.7057620 best: 0.7056185 (364) total: 9.16s remaining: 3.05s 375: learn: 0.6233704 test: 0.7058303 best: 0.7056185 (364) total: 9.19s remaining: 3.03s 376: learn: 0.6227542 test: 0.7057753 best: 0.7056185 (364) total: 9.21s remaining: 3s 377: learn: 0.6225331 test: 0.7056882 best: 0.7056185 (364) total: 9.24s remaining: 2.98s 378: learn: 0.6224177 test: 0.7056147 best: 0.7056147 (378) total: 9.26s remaining: 2.96s 379: learn: 0.6221208 test: 0.7056718 best: 0.7056147 (378) total: 9.28s remaining: 2.93s 380: learn: 0.6218517 test: 0.7056431 best: 0.7056147 (378) total: 9.3s remaining: 2.9s 381: learn: 0.6215304 test: 0.7057221 best: 0.7056147 (378) total: 9.33s remaining: 2.88s 382: learn: 0.6210812 test: 0.7058422 best: 0.7056147 (378) total: 9.37s remaining: 2.86s 383: learn: 0.6206526 test: 0.7058379 best: 0.7056147 (378) total: 9.39s remaining: 2.84s 384: learn: 0.6204333 test: 0.7056702 best: 0.7056147 (378) total: 9.41s remaining: 2.81s 385: learn: 0.6198980 test: 0.7057985 best: 0.7056147 (378) total: 9.44s remaining: 2.79s 386: learn: 0.6194263 test: 0.7057638 best: 0.7056147 (378) total: 9.48s remaining: 2.77s 387: learn: 0.6192267 test: 0.7057773 best: 0.7056147 (378) total: 9.5s remaining: 2.74s 388: learn: 0.6191334 test: 0.7058018 best: 0.7056147 (378) total: 9.52s remaining: 2.72s 389: learn: 0.6188849 test: 0.7058296 best: 0.7056147 (378) total: 9.54s remaining: 2.69s 390: learn: 0.6186054 test: 0.7059039 best: 0.7056147 (378) total: 9.56s remaining: 2.67s 391: learn: 0.6183973 test: 0.7059594 best: 0.7056147 (378) total: 9.59s remaining: 2.64s 392: learn: 0.6181843 test: 0.7059482 best: 0.7056147 (378) total: 9.61s remaining: 2.62s 393: learn: 0.6179609 test: 0.7060814 best: 0.7056147 (378) total: 9.64s remaining: 2.59s 394: learn: 0.6177916 test: 0.7061098 best: 0.7056147 (378) total: 9.67s remaining: 2.57s 395: learn: 0.6176534 test: 0.7060767 best: 0.7056147 (378) total: 9.7s remaining: 2.55s 396: learn: 0.6172277 test: 0.7060579 best: 0.7056147 (378) total: 9.72s remaining: 2.52s 397: learn: 0.6168470 test: 0.7060710 best: 0.7056147 (378) total: 9.75s remaining: 2.5s 398: learn: 0.6165985 test: 0.7061115 best: 0.7056147 (378) total: 9.77s remaining: 2.47s 399: learn: 0.6163760 test: 0.7060246 best: 0.7056147 (378) total: 9.79s remaining: 2.45s 400: learn: 0.6161244 test: 0.7059459 best: 0.7056147 (378) total: 9.82s remaining: 2.42s 401: learn: 0.6158687 test: 0.7060312 best: 0.7056147 (378) total: 9.84s remaining: 2.4s 402: learn: 0.6154179 test: 0.7060124 best: 0.7056147 (378) total: 9.86s remaining: 2.37s 403: learn: 0.6149598 test: 0.7059849 best: 0.7056147 (378) total: 9.89s remaining: 2.35s 404: learn: 0.6145130 test: 0.7060295 best: 0.7056147 (378) total: 9.92s remaining: 2.33s 405: learn: 0.6140032 test: 0.7061936 best: 0.7056147 (378) total: 9.95s remaining: 2.3s 406: learn: 0.6136346 test: 0.7063739 best: 0.7056147 (378) total: 9.97s remaining: 2.28s 407: learn: 0.6134888 test: 0.7063470 best: 0.7056147 (378) total: 9.99s remaining: 2.25s 408: learn: 0.6132430 test: 0.7062589 best: 0.7056147 (378) total: 10s remaining: 2.23s 409: learn: 0.6129072 test: 0.7062420 best: 0.7056147 (378) total: 10s remaining: 2.2s 410: learn: 0.6125468 test: 0.7061405 best: 0.7056147 (378) total: 10.1s remaining: 2.18s 411: learn: 0.6121761 test: 0.7061014 best: 0.7056147 (378) total: 10.1s remaining: 2.15s 412: learn: 0.6119625 test: 0.7060454 best: 0.7056147 (378) total: 10.1s remaining: 2.13s 413: learn: 0.6116103 test: 0.7060701 best: 0.7056147 (378) total: 10.1s remaining: 2.1s 414: learn: 0.6114289 test: 0.7059806 best: 0.7056147 (378) total: 10.2s remaining: 2.08s 415: learn: 0.6109749 test: 0.7061135 best: 0.7056147 (378) total: 10.2s remaining: 2.06s 416: learn: 0.6105400 test: 0.7060436 best: 0.7056147 (378) total: 10.2s remaining: 2.03s 417: learn: 0.6102335 test: 0.7060922 best: 0.7056147 (378) total: 10.2s remaining: 2.01s 418: learn: 0.6099296 test: 0.7060587 best: 0.7056147 (378) total: 10.3s remaining: 1.98s 419: learn: 0.6097321 test: 0.7060871 best: 0.7056147 (378) total: 10.3s remaining: 1.96s 420: learn: 0.6093774 test: 0.7059711 best: 0.7056147 (378) total: 10.3s remaining: 1.93s 421: learn: 0.6091249 test: 0.7057965 best: 0.7056147 (378) total: 10.3s remaining: 1.91s 422: learn: 0.6089436 test: 0.7058383 best: 0.7056147 (378) total: 10.4s remaining: 1.89s 423: learn: 0.6086912 test: 0.7058503 best: 0.7056147 (378) total: 10.4s remaining: 1.86s 424: learn: 0.6082869 test: 0.7058276 best: 0.7056147 (378) total: 10.4s remaining: 1.84s 425: learn: 0.6080287 test: 0.7058633 best: 0.7056147 (378) total: 10.4s remaining: 1.81s 426: learn: 0.6077508 test: 0.7058851 best: 0.7056147 (378) total: 10.5s remaining: 1.79s 427: learn: 0.6075041 test: 0.7059080 best: 0.7056147 (378) total: 10.5s remaining: 1.77s 428: learn: 0.6070514 test: 0.7058303 best: 0.7056147 (378) total: 10.5s remaining: 1.74s 429: learn: 0.6067788 test: 0.7056585 best: 0.7056147 (378) total: 10.6s remaining: 1.72s 430: learn: 0.6065300 test: 0.7056298 best: 0.7056147 (378) total: 10.6s remaining: 1.69s 431: learn: 0.6061207 test: 0.7055673 best: 0.7055673 (431) total: 10.6s remaining: 1.67s 432: learn: 0.6056642 test: 0.7054166 best: 0.7054166 (432) total: 10.6s remaining: 1.64s 433: learn: 0.6053692 test: 0.7053234 best: 0.7053234 (433) total: 10.7s remaining: 1.62s 434: learn: 0.6048434 test: 0.7054240 best: 0.7053234 (433) total: 10.7s remaining: 1.59s 435: learn: 0.6044418 test: 0.7053934 best: 0.7053234 (433) total: 10.7s remaining: 1.57s 436: learn: 0.6040344 test: 0.7055579 best: 0.7053234 (433) total: 10.7s remaining: 1.55s 437: learn: 0.6036074 test: 0.7054638 best: 0.7053234 (433) total: 10.8s remaining: 1.52s 438: learn: 0.6031700 test: 0.7055721 best: 0.7053234 (433) total: 10.8s remaining: 1.5s 439: learn: 0.6027640 test: 0.7055563 best: 0.7053234 (433) total: 10.8s remaining: 1.47s 440: learn: 0.6026359 test: 0.7055260 best: 0.7053234 (433) total: 10.8s remaining: 1.45s 441: learn: 0.6023635 test: 0.7054749 best: 0.7053234 (433) total: 10.8s remaining: 1.42s 442: learn: 0.6021887 test: 0.7054906 best: 0.7053234 (433) total: 10.9s remaining: 1.4s 443: learn: 0.6018685 test: 0.7055020 best: 0.7053234 (433) total: 10.9s remaining: 1.37s 444: learn: 0.6017056 test: 0.7054741 best: 0.7053234 (433) total: 10.9s remaining: 1.35s 445: learn: 0.6014097 test: 0.7054144 best: 0.7053234 (433) total: 11s remaining: 1.32s 446: learn: 0.6011548 test: 0.7054094 best: 0.7053234 (433) total: 11s remaining: 1.3s 447: learn: 0.6008759 test: 0.7053403 best: 0.7053234 (433) total: 11s remaining: 1.28s 448: learn: 0.6007650 test: 0.7053551 best: 0.7053234 (433) total: 11s remaining: 1.25s 449: learn: 0.6006035 test: 0.7052670 best: 0.7052670 (449) total: 11s remaining: 1.23s 450: learn: 0.6002089 test: 0.7051834 best: 0.7051834 (450) total: 11.1s remaining: 1.2s 451: learn: 0.5999161 test: 0.7051227 best: 0.7051227 (451) total: 11.1s remaining: 1.18s 452: learn: 0.5995862 test: 0.7051589 best: 0.7051227 (451) total: 11.1s remaining: 1.15s 453: learn: 0.5993674 test: 0.7052534 best: 0.7051227 (451) total: 11.1s remaining: 1.13s 454: learn: 0.5989666 test: 0.7051834 best: 0.7051227 (451) total: 11.2s remaining: 1.1s 455: learn: 0.5985148 test: 0.7052050 best: 0.7051227 (451) total: 11.2s remaining: 1.08s 456: learn: 0.5982024 test: 0.7051242 best: 0.7051227 (451) total: 11.2s remaining: 1.06s 457: learn: 0.5976328 test: 0.7050274 best: 0.7050274 (457) total: 11.3s remaining: 1.03s 458: learn: 0.5973813 test: 0.7050124 best: 0.7050124 (458) total: 11.3s remaining: 1.01s 459: learn: 0.5969461 test: 0.7050177 best: 0.7050124 (458) total: 11.3s remaining: 983ms 460: learn: 0.5965664 test: 0.7049720 best: 0.7049720 (460) total: 11.3s remaining: 958ms 461: learn: 0.5961556 test: 0.7050179 best: 0.7049720 (460) total: 11.3s remaining: 934ms 462: learn: 0.5957623 test: 0.7051324 best: 0.7049720 (460) total: 11.4s remaining: 909ms 463: learn: 0.5954348 test: 0.7049950 best: 0.7049720 (460) total: 11.4s remaining: 885ms 464: learn: 0.5950400 test: 0.7048782 best: 0.7048782 (464) total: 11.4s remaining: 860ms 465: learn: 0.5948960 test: 0.7047926 best: 0.7047926 (465) total: 11.5s remaining: 837ms 466: learn: 0.5946021 test: 0.7047490 best: 0.7047490 (466) total: 11.5s remaining: 812ms 467: learn: 0.5944629 test: 0.7047168 best: 0.7047168 (467) total: 11.5s remaining: 787ms 468: learn: 0.5943130 test: 0.7047349 best: 0.7047168 (467) total: 11.5s remaining: 762ms 469: learn: 0.5940189 test: 0.7048488 best: 0.7047168 (467) total: 11.6s remaining: 738ms 470: learn: 0.5935653 test: 0.7047961 best: 0.7047168 (467) total: 11.6s remaining: 714ms 471: learn: 0.5934466 test: 0.7047866 best: 0.7047168 (467) total: 11.6s remaining: 689ms 472: learn: 0.5932188 test: 0.7047762 best: 0.7047168 (467) total: 11.6s remaining: 664ms 473: learn: 0.5931258 test: 0.7048383 best: 0.7047168 (467) total: 11.7s remaining: 640ms 474: learn: 0.5927629 test: 0.7047544 best: 0.7047168 (467) total: 11.7s remaining: 615ms 475: learn: 0.5926842 test: 0.7047811 best: 0.7047168 (467) total: 11.7s remaining: 591ms 476: learn: 0.5923645 test: 0.7047897 best: 0.7047168 (467) total: 11.8s remaining: 567ms 477: learn: 0.5921254 test: 0.7046454 best: 0.7046454 (477) total: 11.8s remaining: 542ms 478: learn: 0.5918759 test: 0.7046550 best: 0.7046454 (477) total: 11.8s remaining: 517ms 479: learn: 0.5914705 test: 0.7045539 best: 0.7045539 (479) total: 11.8s remaining: 493ms 480: learn: 0.5912532 test: 0.7045425 best: 0.7045425 (480) total: 11.9s remaining: 468ms 481: learn: 0.5908412 test: 0.7043259 best: 0.7043259 (481) total: 11.9s remaining: 443ms 482: learn: 0.5904397 test: 0.7042378 best: 0.7042378 (482) total: 11.9s remaining: 419ms 483: learn: 0.5900386 test: 0.7041655 best: 0.7041655 (483) total: 11.9s remaining: 394ms 484: learn: 0.5898262 test: 0.7042243 best: 0.7041655 (483) total: 11.9s remaining: 370ms 485: learn: 0.5893277 test: 0.7043380 best: 0.7041655 (483) total: 12s remaining: 345ms 486: learn: 0.5889968 test: 0.7043583 best: 0.7041655 (483) total: 12s remaining: 320ms 487: learn: 0.5887373 test: 0.7044576 best: 0.7041655 (483) total: 12s remaining: 296ms 488: learn: 0.5885525 test: 0.7044628 best: 0.7041655 (483) total: 12.1s remaining: 271ms 489: learn: 0.5883401 test: 0.7044718 best: 0.7041655 (483) total: 12.1s remaining: 246ms 490: learn: 0.5880385 test: 0.7046277 best: 0.7041655 (483) total: 12.1s remaining: 222ms 491: learn: 0.5878262 test: 0.7045125 best: 0.7041655 (483) total: 12.1s remaining: 197ms 492: learn: 0.5875707 test: 0.7046046 best: 0.7041655 (483) total: 12.1s remaining: 172ms 493: learn: 0.5873305 test: 0.7047382 best: 0.7041655 (483) total: 12.2s remaining: 148ms 494: learn: 0.5871575 test: 0.7047429 best: 0.7041655 (483) total: 12.2s remaining: 123ms 495: learn: 0.5868453 test: 0.7046409 best: 0.7041655 (483) total: 12.2s remaining: 98.6ms 496: learn: 0.5866779 test: 0.7047187 best: 0.7041655 (483) total: 12.3s remaining: 74ms 497: learn: 0.5861024 test: 0.7047495 best: 0.7041655 (483) total: 12.3s remaining: 49.3ms 498: learn: 0.5858932 test: 0.7047261 best: 0.7041655 (483) total: 12.3s remaining: 24.7ms 499: learn: 0.5856087 test: 0.7046194 best: 0.7041655 (483) total: 12.3s remaining: 0us bestTest = 0.7041655162 bestIteration = 483 Shrink model to first 484 iterations. . &lt;catboost.core.CatBoostClassifier at 0x7fb8895d6190&gt; . y_pred = cat.predict_proba(X_test) . y_pred . array([[0.05583019, 0.07185238, 0.87231743], [0.02672221, 0.07745499, 0.8958228 ], [0.08710456, 0.12360685, 0.78928859], ..., [0.03677252, 0.04416106, 0.91906642], [0.09366507, 0.18346747, 0.72286746], [0.09915601, 0.08362091, 0.81722308]]) . from sklearn.metrics import log_loss log_loss(y_test, y_pred) . 0.7041654956166968 . fig, axes = plt.subplots(1,2,figsize=(12, 6)) sns.countplot(y_test,ax = axes[0]) sns.countplot(cat.predict(X_test).flatten(), ax = axes[1]) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb875617cd0&gt; . K-Fold . 소수의 값들을 잘 예측하지 못한다. -&gt; 어떻게 해결? . Test &#49483;&#50640; &#51201;&#50857;&#54620;&#45796;. . y_pred_df = pd.DataFrame(cat.predict_proba(test_set.drop(&quot;credit&quot;,axis = 1))) . sample_sub.iloc[:,1] = y_pred_df.iloc[:,0] sample_sub.iloc[:,2] = y_pred_df.iloc[:,1] sample_sub.iloc[:,3] = y_pred_df.iloc[:,2] . sample_sub.to_csv(&#39;submission.csv&#39;, index = False) . sample_sub . index 0 1 2 . 0 26457 | 0.048226 | 0.163969 | 0.787805 | . 1 26458 | 0.205943 | 0.289282 | 0.504774 | . 2 26459 | 0.041657 | 0.051783 | 0.906560 | . 3 26460 | 0.088498 | 0.115628 | 0.795873 | . 4 26461 | 0.185538 | 0.233234 | 0.581228 | . ... ... | ... | ... | ... | . 9995 36452 | 0.097636 | 0.173143 | 0.729221 | . 9996 36453 | 0.219814 | 0.261698 | 0.518488 | . 9997 36454 | 0.039801 | 0.083876 | 0.876323 | . 9998 36455 | 0.178216 | 0.311593 | 0.510191 | . 9999 36456 | 0.170260 | 0.413283 | 0.416457 | . 10000 rows × 4 columns .",
            "url": "https://raukrauk.github.io/ML-DL/ssuda/machine%20learning/dacon/2021/11/14/%EC%8B%A0%EC%9A%A9%EC%98%88%EC%B8%A1.html",
            "relUrl": "/ssuda/machine%20learning/dacon/2021/11/14/%EC%8B%A0%EC%9A%A9%EC%98%88%EC%B8%A1.html",
            "date": " • Nov 14, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "[Dacon] 따릉이 대여량 예측",
            "content": "Rental Bicycle . date_time : 일별 날짜 . | wind_direction: 풍향 (degree) -&gt; 4방위? 8방위? -&gt; category형으로 변환 . | sky_condition : 하늘 상태 (하단 설명 참조) -&gt; category형으로 변환 . | precipitation_form : 강수 형태 (하단 설명 참조) . | wind_speed : 풍속 (m/s) . | humidity : 습도 (%) . | low_temp : 최저 기온 ( `C) . | high_temp : 최고 기온 ( `C) - 온도는 묶어서 처리? . | Precipitation_Probability : 강수 확률 (%) . | . number_of_rentals : 따릉이 대여량(TARGET) . &#45936;&#51060;&#53552; &#49345;&#49464; &#49444;&#47749; . 기상 데이터는 하루에 총 8번 3시간 간격으로 발표되는 기상단기예보(SHRT) 데이터를 1일 평균으로 변환한 데이터입니다. | sky_condition (하늘 상태) 코드 : 맑음(1), 구름많음(3), 흐림(4) | precipitation_form (강수 형태) 코드 : 없음(0), 비(1), 진눈깨비(2), 눈(3), 소나기(4) 위 데이터는 4월~6월의 기상 데이터만을 추출한 것이기 때문에 없음(0), 비(1)만 등장했습니다. | 따라서 precipitation_form 이 0.5인 경우: &quot;하루의 절반은 비가 올 것으로 예측하고, 나머지 절반은 맑을 것으로 예측했다&quot;는 의미로 해석해주시기 바랍니다. | . from google.colab import drive drive.mount(&#39;/content/drive/&#39;) . Mounted at /content/drive/ . import pandas as pd import numpy as np import seaborn as sns import matplotlib.pyplot as plt import datetime . train = pd.read_csv(&quot;/content/drive/MyDrive/dataset/train.csv&quot;) test = pd.read_csv(&quot;/content/drive/MyDrive/dataset/test.csv&quot;) sample_sub = pd.read_csv(&quot;/content/drive/MyDrive/dataset/sample_submission.csv&quot;) . &#51068;&#45800; &#44208;&#52769;&#44050;&#51008; &#50630;&#51020; . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 273 entries, 0 to 272 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 date_time 273 non-null object 1 wind_direction 273 non-null float64 2 sky_condition 273 non-null float64 3 precipitation_form 273 non-null float64 4 wind_speed 273 non-null float64 5 humidity 273 non-null float64 6 low_temp 273 non-null float64 7 high_temp 273 non-null float64 8 Precipitation_Probability 273 non-null float64 9 number_of_rentals 273 non-null int64 dtypes: float64(8), int64(1), object(1) memory usage: 21.5+ KB . train.describe() . wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability number_of_rentals . count 273.000000 | 273.000000 | 273.000000 | 273.000000 | 273.000000 | 273.000000 | 273.000000 | 273.000000 | 273.000000 | . mean 202.750967 | 2.288256 | 0.100963 | 2.480963 | 56.745491 | 13.795249 | 23.384733 | 16.878103 | 59574.978022 | . std 56.659232 | 0.961775 | 0.203193 | 0.884397 | 12.351268 | 5.107711 | 5.204605 | 16.643772 | 27659.575774 | . min 57.047000 | 1.000000 | 0.000000 | 0.753000 | 24.831000 | 1.938000 | 9.895000 | 0.000000 | 1037.000000 | . 25% 171.541000 | 1.405000 | 0.000000 | 1.820000 | 47.196000 | 9.938000 | 19.842000 | 4.054000 | 36761.000000 | . 50% 209.774000 | 2.167000 | 0.000000 | 2.411000 | 55.845000 | 14.375000 | 24.158000 | 12.162000 | 63032.000000 | . 75% 238.412000 | 3.000000 | 0.088000 | 2.924000 | 66.419000 | 18.000000 | 27.526000 | 22.973000 | 81515.000000 | . max 321.622000 | 4.000000 | 1.000000 | 5.607000 | 88.885000 | 22.312000 | 33.421000 | 82.162000 | 110377.000000 | . sns.set(rc = {&#39;figure.figsize&#39;:(8,5)}) sns.pairplot(train) . &lt;seaborn.axisgrid.PairGrid at 0x7faae078e2d0&gt; . &#49345;&#44288;&#44288;&#44228; heatmap . low_temp와 high_temp가 0.92 | 강수확률과 날씨상태가 0.91 | 강수확률과 강수형태가 0.91 | . 높은 상관관계를 보이는 feature들이 존재한다.-&gt; 확인 . sns.set(rc = {&#39;figure.figsize&#39;:(8,5)}) sns.heatmap(train.drop(&quot;date_time&quot;, axis = 1).corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fab02441610&gt; . train, test&#47484; &#54633;&#52432;&#49436; &#48320;&#49688;&#46308;&#51012; &#51312;&#51221;&#54620;&#45796;. . tmp = pd.concat([train,test]) . tmp[&#39;date_time&#39;] = tmp.date_time.apply(lambda x: datetime.datetime.strptime(x, &#39;%Y-%m-%d&#39;)) . tmp.reset_index(drop = True, inplace = True) . tmp.describe() . wind_direction sky_condition precipitation_form wind_speed humidity low_temp high_temp Precipitation_Probability number_of_rentals . count 364.000000 | 364.000000 | 364.000000 | 364.000000 | 364.000000 | 364.000000 | 364.000000 | 364.000000 | 273.000000 | . mean 198.634725 | 2.319503 | 0.121179 | 2.383970 | 58.386926 | 13.891959 | 23.356146 | 17.328552 | 59574.978022 | . std 56.845031 | 0.976251 | 0.230958 | 0.868715 | 12.396575 | 4.980942 | 4.969229 | 16.315020 | 27659.575774 | . min 44.123000 | 1.000000 | 0.000000 | 0.753000 | 24.831000 | 1.938000 | 9.895000 | 0.000000 | 1037.000000 | . 25% 168.287250 | 1.405000 | 0.000000 | 1.743500 | 48.809000 | 10.172250 | 19.895000 | 4.357750 | 36761.000000 | . 50% 205.729500 | 2.196000 | 0.000000 | 2.310500 | 58.784000 | 14.500000 | 24.000000 | 13.209500 | 63032.000000 | . 75% 234.637250 | 3.143750 | 0.142000 | 2.800000 | 68.446000 | 18.000000 | 27.526000 | 25.608500 | 81515.000000 | . max 321.622000 | 4.000000 | 1.514000 | 5.607000 | 88.885000 | 22.312000 | 33.421000 | 82.162000 | 110377.000000 | . sns.heatmap(tmp.corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faaee587d10&gt; . EDA . 1. &#54413;&#54693;&#51012; 8&#48169;&#50948;&#47196; &#44396;&#48516;&#54644;&#48372;&#51088; . 일반적으로 생각해 봐도 수요와 큰 관계가 있지는 않다. | (배도 아니고 동풍이 부니깐 자전거 타기 좋구만 이런식으로 생각하는 사람은 없을듯) | 풍속이 오히려 더 나을 것 같다. | . . cardinal_directions = np.zeros(len(tmp)) cardinal_directions[tmp.query(&#39;wind_direction &lt;= 45&#39;).index] = 1 # 북 ~ 북동 cardinal_directions[tmp.query(&#39;wind_direction &gt; 45 and wind_direction &lt;= 90&#39;).index] = 2 # 북동 ~ 동 cardinal_directions[tmp.query(&#39;wind_direction &gt; 90 and wind_direction &lt;= 135&#39;).index] = 3 # 동 ~ 남동 cardinal_directions[tmp.query(&#39;wind_direction &gt; 135 and wind_direction &lt;= 180&#39;).index] = 4 # 남동 ~ 남 cardinal_directions[tmp.query(&#39;wind_direction &gt; 180 and wind_direction &lt;= 225&#39;).index] = 5 # 남 ~ 남서 cardinal_directions[tmp.query(&#39;wind_direction &gt; 225 and wind_direction &lt;= 270&#39;).index] = 6 # 남서 ~ 서 cardinal_directions[tmp.query(&#39;wind_direction &gt; 270 and wind_direction &lt;= 315&#39;).index] = 7 # 서 ~ 북서 cardinal_directions[tmp.query(&#39;wind_direction &gt; 315 and wind_direction &lt;= 360&#39;).index] = 8 # 북서 ~ 북 . tmp[&#39;cardinal_directions&#39;] = cardinal_directions.astype(int) tmp[&#39;cardinal_directions&#39;] = tmp[&#39;cardinal_directions&#39;].astype(&#39;category&#39;) # 카테고리형으로 변환한다. . wind_direction &#50676;&#51012; &#49325;&#51228;&#54620;&#45796;. . tmp.drop(&quot;wind_direction&quot;,axis = 1, inplace = True) . tmp.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 364 entries, 0 to 363 Data columns (total 10 columns): # Column Non-Null Count Dtype -- -- 0 date_time 364 non-null datetime64[ns] 1 sky_condition 364 non-null float64 2 precipitation_form 364 non-null float64 3 wind_speed 364 non-null float64 4 humidity 364 non-null float64 5 low_temp 364 non-null float64 6 high_temp 364 non-null float64 7 Precipitation_Probability 364 non-null float64 8 number_of_rentals 273 non-null float64 9 cardinal_directions 364 non-null category dtypes: category(1), datetime64[ns](1), float64(8) memory usage: 26.4 KB . 2. low_temp&#50752; high_temp&#45716; &#46168;&#51032; &#54217;&#44512;&#51004;&#47196; &#51480;&#48260;&#47540;&#44620;? - &#45824;&#44053; &#54616;&#47336;&#51032; &#54217;&#44512;&#44592;&#50728;? . tmp[&quot;mean_temp&quot;] = (tmp[&quot;low_temp&quot;] * 0.4 + tmp[&quot;high_temp&quot;] * 0.6) # 가중치를 최고기온에다가 더 준다. . tmp.drop([&quot;low_temp&quot;,&#39;high_temp&#39;],axis = 1, inplace = True) . 3. Skycondition - &#44109; &#45460;&#46176; . 하늘상태 코드는 설명에서는 이산형으로 설명되어 있지만 연속형으로 나옴 | 어떤 의미인지 정확하게 파악 X 반올림해서 이산형으로 바꿈 | . tmp.sky_condition.describe() . count 364.000000 mean 2.319503 std 0.976251 min 1.000000 25% 1.405000 50% 2.196000 75% 3.143750 max 4.000000 Name: sky_condition, dtype: float64 . tmp.sky_condition = np.round(tmp.sky_condition) tmp.sky_condition = tmp.sky_condition.astype(int) . &#44536;&#47111;&#44172; &#51060;&#49345;&#54644; &#48372;&#51060;&#51652; &#50506;&#51020;... . sns.countplot(tmp.sky_condition) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7feca55f31d0&gt; . sns.barplot(np.unique(tmp.sky_condition), tmp.groupby(tmp.sky_condition)[&quot;number_of_rentals&quot;].sum()) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7feca5590110&gt; . tmp[&#39;sky_condition&#39;] = tmp[&#39;sky_condition&#39;].astype(&#39;category&#39;) # 카테고리형으로 변환한다. . 4. date_time&#51012; &#44396;&#48516;&#54616;&#50668; &#49352;&#47196;&#50868; category &#50676;&#47196; &#47564;&#46304;&#45796;. . 연 / 월 / 일 / 주중,말(T/F) | . tmp[&#39;month&#39;] = tmp[&#39;date_time&#39;].apply(lambda x: x.month) tmp[&#39;year&#39;] = tmp[&#39;date_time&#39;].apply(lambda x: x.year) tmp[&#39;date&#39;] = tmp[&#39;date_time&#39;].apply(lambda x: x.day) tmp[&#39;weekday&#39;] = tmp[&#39;date_time&#39;].apply(lambda x: x.weekday() &gt;= 5) . tmp.drop(&quot;date_time&quot;, axis = 1, inplace = True) . tmp.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 364 entries, 0 to 363 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 sky_condition 364 non-null float64 1 precipitation_form 364 non-null float64 2 wind_speed 364 non-null float64 3 humidity 364 non-null float64 4 Precipitation_Probability 364 non-null float64 5 number_of_rentals 273 non-null float64 6 cardinal_directions 364 non-null category 7 mean_temp 364 non-null float64 8 month 364 non-null int64 9 year 364 non-null int64 10 date 364 non-null int64 11 weekday 364 non-null bool dtypes: bool(1), category(1), float64(7), int64(3) memory usage: 29.6 KB . sns.barplot(np.unique(tmp.weekday), tmp.groupby(tmp.weekday)[&quot;number_of_rentals&quot;].mean()) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faadff74710&gt; . sns.barplot(np.unique(tmp.year), tmp.groupby(tmp.year)[&quot;number_of_rentals&quot;].mean()) # 증가하는 추세이다.-&gt; 연도 영향이 있을것으로 예상됨 . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faadfe92b50&gt; . 5. precipitation_form&#47484; 0 &#44284; 1&#47196; &#44396;&#48516;&#54644; &#48376;&#45796;. - &#48324;&#47196; &#54952;&#44284;&#44032; &#50630;&#45716; &#44163; &#44057;&#51020; . 주최측에서 제공한 설명이 뭔지 정확하게 와닿지 않는다. | 0이면 하룻동안 비가올 확률이 전혀 없음 | 1이면 비가올 가능성이 있음 . | precipitation_probability를 이용? . | . sns.histplot(tmp.precipitation_form) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faaee3fc5d0&gt; . tmp.precipitation_form[tmp.precipitation_form &gt; 0] = 1 . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy &#34;&#34;&#34;Entry point for launching an IPython kernel. . tmp.precipitation_form = tmp.precipitation_form.astype(int) . sns.barplot(np.unique(tmp.precipitation_form), tmp.groupby(tmp.precipitation_form)[&quot;number_of_rentals&quot;].mean()) . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faaee33e8d0&gt; . sns.distplot(tmp.Precipitation_Probability, kde = True, hist_kws=dict(edgecolor=&quot;black&quot;)) # 로그변환을 해볼까? - 우측으로 꼬리가 긴 분포를 따른다. . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faadae1c290&gt; . . . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faadad2f350&gt; . 6. &#48520;&#53132;&#51648;&#49688;&#47484; &#52628;&#44032;&#54644; &#48372;&#51088; (discomfort index, DI) . DI = 9/5Ta-0.55(1-RH)(9/5Ta-26)+32 | Ta : 건구온도 (℃) - 평균온도를 이용함 | RH : 상대습도 (소수단위) | . tmp[&#39;discomfort_idx&#39;] = 9/5 * tmp[&#39;mean_temp&#39;] - 0.55 * (1 - tmp[&#39;humidity&#39;] / 100) * (9/5 * tmp[&#39;mean_temp&#39;] - 26) + 32 . sns.distplot(tmp[&#39;discomfort_idx&#39;], kde = True, hist_kws=dict(edgecolor=&quot;black&quot;)) # 정규분포에서 약간 우측으로 간것 같음. . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faaee2b17d0&gt; . &#44592;&#53440; &#52376;&#47532;&#49324;&#54637; . tmp.drop(&quot;month&quot;, axis = 1, inplace = True) # Month와 상관관계가 높은변수가 두개 있어서 버림. . tmp.drop(&quot;mean_temp&quot;, axis = 1, inplace = True) # 온도도 불쾌지수와 상관관계가 높아서 버림 . tmp.drop(&quot;Precipitation_Probability&quot;, axis = 1, inplace = True) . tmp.weekday = tmp.weekday.astype(int) . sns.heatmap(tmp.corr(), annot = True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faae0bef4d0&gt; . &#54984;&#47144;&#45936;&#51060;&#53552;&#50752; &#53580;&#49828;&#53944;&#45936;&#51060;&#53552; &#48516;&#47532; . tmp . sky_condition precipitation_form wind_speed humidity number_of_rentals cardinal_directions year date weekday discomfort_idx . 0 4.000 | 0 | 0.473218 | 0.783230 | 22994.0 | 5 | 2018 | 1 | 1 | 62.961100 | . 1 2.950 | 0 | 0.520190 | 0.702563 | 28139.0 | 5 | 2018 | 2 | 0 | 61.123335 | . 2 2.911 | 0 | 0.399052 | 0.781341 | 26817.0 | 5 | 2018 | 3 | 0 | 56.246960 | . 3 3.692 | 1 | 0.491347 | 0.734037 | 26034.0 | 4 | 2018 | 4 | 0 | 52.372929 | . 4 4.000 | 1 | 0.501236 | 0.764246 | 2833.0 | 3 | 2018 | 5 | 0 | 49.000863 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 359 3.980 | 1 | 0.032053 | 0.805915 | NaN | 4 | 2021 | 26 | 1 | 74.770214 | . 360 2.777 | 1 | 0.089036 | 0.729458 | NaN | 4 | 2021 | 27 | 1 | 74.980730 | . 361 3.338 | 1 | 0.191300 | 0.731234 | NaN | 3 | 2021 | 28 | 0 | 74.914695 | . 362 3.270 | 1 | 0.134826 | 0.733584 | NaN | 4 | 2021 | 29 | 0 | 75.501415 | . 363 3.270 | 1 | 0.061053 | 0.815925 | NaN | 5 | 2021 | 30 | 0 | 77.092730 | . 364 rows × 10 columns . tmp.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 364 entries, 0 to 363 Data columns (total 11 columns): # Column Non-Null Count Dtype -- -- 0 sky_condition 364 non-null float64 1 precipitation_form 364 non-null int64 2 wind_speed 364 non-null float64 3 humidity 364 non-null float64 4 Precipitation_Probability 364 non-null float64 5 number_of_rentals 273 non-null float64 6 cardinal_directions 364 non-null category 7 year 364 non-null int64 8 date 364 non-null int64 9 weekday 364 non-null int64 10 discomfort_idx 364 non-null float64 dtypes: category(1), float64(6), int64(4) memory usage: 29.3 KB . train_set = tmp.iloc[:273,:] test_set = tmp.iloc[273:,:] . s_feature = [&#39;wind_speed&#39; ,&#39;humidity&#39;] from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() for x in s_feature: train_set[x] = scaler.fit_transform(train_set[x].to_numpy().reshape(-1,1)) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy import sys . for x in s_feature: test_set[x] = scaler.fit_transform(test_set[x].to_numpy().reshape(-1,1)) . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy . valid : 204 ~ 213 / 235 ~ 244 / 265 ~ 274 . val_idx = [] val_idx.extend(np.arange(204,208)) val_idx.extend(np.arange(240,244)) val_idx.extend(np.arange(269,273)) X_train = train_set.drop(&quot;number_of_rentals&quot;, axis = 1) #X_train = X_train.drop(val_idx) y_train = train_set[&quot;number_of_rentals&quot;] #y_train = y_train.drop(val_idx) X_valid = train_set.drop(&quot;number_of_rentals&quot;, axis = 1).iloc[val_idx] y_valid = train_set[&quot;number_of_rentals&quot;].iloc[val_idx] X_test = test_set.drop(&quot;number_of_rentals&quot;, axis = 1) . y_valid . 204 64765.0 205 72883.0 206 78419.0 207 82600.0 240 103999.0 241 105172.0 242 93547.0 243 99930.0 269 107001.0 270 98568.0 271 70053.0 272 38086.0 Name: number_of_rentals, dtype: float64 . Modeling . randomforest | lightgbm regressor | . import numpy as np def NMAE(true, pred): score = np.mean(np.abs(true-pred) / true) return score . from sklearn.ensemble import RandomForestRegressor rf = RandomForestRegressor(n_estimators = 200, max_depth = 5, max_features = 0.8, n_jobs = -1, random_state = 42) rf.fit(X_train,y_train) . RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion=&#39;mse&#39;, max_depth=5, max_features=0.8, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=-1, oob_score=False, random_state=42, verbose=0, warm_start=False) . y_pred = rf.predict(X_valid) . y_pred . array([66353.55370043, 73848.71981842, 75066.33162792, 75919.62959063, 97935.15947797, 97187.68617053, 94154.17995879, 89853.36555028, 93593.36988453, 99239.11358574, 89951.79435054, 44466.49418879]) . NMAE(y_valid, y_pred) . 0.0822210930520343 . y_pred = rf.predict(X_test) . sample_sub[&#39;number_of_rentals&#39;] = y_pred sample_sub . date_time number_of_rentals . 0 2021-04-01 | 63811.993090 | . 1 2021-04-02 | 45112.683520 | . 2 2021-04-03 | 46337.923919 | . 3 2021-04-04 | 37971.209619 | . 4 2021-04-05 | 76147.067287 | . ... ... | ... | . 86 2021-06-26 | 80972.736513 | . 87 2021-06-27 | 91119.588146 | . 88 2021-06-28 | 87231.442977 | . 89 2021-06-29 | 85405.723561 | . 90 2021-06-30 | 87193.553894 | . 91 rows × 2 columns . sample_sub.to_csv(&#39;submission.csv&#39;, index=False, encoding=&#39;cp949&#39;) . from lightgbm import LGBMRegressor lgbm_r = LGBMRegressor(n_estimators = 300, learning_rate=0.03, random_state = 42) lgbm_r.fit(X_train,y_train, eval_set=[(X_valid, y_valid)], eval_metric = &#39;rmse&#39;,early_stopping_rounds = 50) y_pred2 = lgbm_r.predict(X_valid) . [1] valid_0&#39;s rmse: 31118.2 valid_0&#39;s l2: 9.68341e+08 Training until validation scores don&#39;t improve for 50 rounds. [2] valid_0&#39;s rmse: 30295.3 valid_0&#39;s l2: 9.17806e+08 [3] valid_0&#39;s rmse: 29543.7 valid_0&#39;s l2: 8.72831e+08 [4] valid_0&#39;s rmse: 28890.4 valid_0&#39;s l2: 8.34654e+08 [5] valid_0&#39;s rmse: 28198.4 valid_0&#39;s l2: 7.95147e+08 [6] valid_0&#39;s rmse: 27581.4 valid_0&#39;s l2: 7.60735e+08 [7] valid_0&#39;s rmse: 26829.9 valid_0&#39;s l2: 7.19846e+08 [8] valid_0&#39;s rmse: 26276.5 valid_0&#39;s l2: 6.90455e+08 [9] valid_0&#39;s rmse: 25706.4 valid_0&#39;s l2: 6.60818e+08 [10] valid_0&#39;s rmse: 25023.5 valid_0&#39;s l2: 6.26177e+08 [11] valid_0&#39;s rmse: 24490 valid_0&#39;s l2: 5.99761e+08 [12] valid_0&#39;s rmse: 23849.9 valid_0&#39;s l2: 5.68817e+08 [13] valid_0&#39;s rmse: 23233.2 valid_0&#39;s l2: 5.3978e+08 [14] valid_0&#39;s rmse: 22749 valid_0&#39;s l2: 5.17517e+08 [15] valid_0&#39;s rmse: 22305.9 valid_0&#39;s l2: 4.97555e+08 [16] valid_0&#39;s rmse: 21853.7 valid_0&#39;s l2: 4.77586e+08 [17] valid_0&#39;s rmse: 21356.6 valid_0&#39;s l2: 4.56103e+08 [18] valid_0&#39;s rmse: 20958.3 valid_0&#39;s l2: 4.3925e+08 [19] valid_0&#39;s rmse: 20549.6 valid_0&#39;s l2: 4.22287e+08 [20] valid_0&#39;s rmse: 20059.5 valid_0&#39;s l2: 4.02383e+08 [21] valid_0&#39;s rmse: 19594.6 valid_0&#39;s l2: 3.83949e+08 [22] valid_0&#39;s rmse: 19139.1 valid_0&#39;s l2: 3.66304e+08 [23] valid_0&#39;s rmse: 18701.6 valid_0&#39;s l2: 3.4975e+08 [24] valid_0&#39;s rmse: 18378.3 valid_0&#39;s l2: 3.37761e+08 [25] valid_0&#39;s rmse: 18069.6 valid_0&#39;s l2: 3.26509e+08 [26] valid_0&#39;s rmse: 17675.8 valid_0&#39;s l2: 3.12434e+08 [27] valid_0&#39;s rmse: 17381.4 valid_0&#39;s l2: 3.02112e+08 [28] valid_0&#39;s rmse: 17040.5 valid_0&#39;s l2: 2.90377e+08 [29] valid_0&#39;s rmse: 16663.9 valid_0&#39;s l2: 2.77687e+08 [30] valid_0&#39;s rmse: 16377.7 valid_0&#39;s l2: 2.68228e+08 [31] valid_0&#39;s rmse: 16032.1 valid_0&#39;s l2: 2.57029e+08 [32] valid_0&#39;s rmse: 15738.9 valid_0&#39;s l2: 2.47712e+08 [33] valid_0&#39;s rmse: 15420.4 valid_0&#39;s l2: 2.37788e+08 [34] valid_0&#39;s rmse: 15109.4 valid_0&#39;s l2: 2.28294e+08 [35] valid_0&#39;s rmse: 14867 valid_0&#39;s l2: 2.21027e+08 [36] valid_0&#39;s rmse: 14581.9 valid_0&#39;s l2: 2.12632e+08 [37] valid_0&#39;s rmse: 14300.1 valid_0&#39;s l2: 2.04493e+08 [38] valid_0&#39;s rmse: 14075 valid_0&#39;s l2: 1.98106e+08 [39] valid_0&#39;s rmse: 13812.6 valid_0&#39;s l2: 1.90787e+08 [40] valid_0&#39;s rmse: 13610 valid_0&#39;s l2: 1.85231e+08 [41] valid_0&#39;s rmse: 13365.4 valid_0&#39;s l2: 1.78634e+08 [42] valid_0&#39;s rmse: 13170.6 valid_0&#39;s l2: 1.73466e+08 [43] valid_0&#39;s rmse: 12934.1 valid_0&#39;s l2: 1.6729e+08 [44] valid_0&#39;s rmse: 12722.1 valid_0&#39;s l2: 1.61853e+08 [45] valid_0&#39;s rmse: 12555.7 valid_0&#39;s l2: 1.57645e+08 [46] valid_0&#39;s rmse: 12371.5 valid_0&#39;s l2: 1.53053e+08 [47] valid_0&#39;s rmse: 12213.5 valid_0&#39;s l2: 1.4917e+08 [48] valid_0&#39;s rmse: 12095.5 valid_0&#39;s l2: 1.46302e+08 [49] valid_0&#39;s rmse: 11984.7 valid_0&#39;s l2: 1.43633e+08 [50] valid_0&#39;s rmse: 11880.6 valid_0&#39;s l2: 1.4115e+08 [51] valid_0&#39;s rmse: 11783.1 valid_0&#39;s l2: 1.38841e+08 [52] valid_0&#39;s rmse: 11691.7 valid_0&#39;s l2: 1.36695e+08 [53] valid_0&#39;s rmse: 11606.1 valid_0&#39;s l2: 1.34702e+08 [54] valid_0&#39;s rmse: 11526.2 valid_0&#39;s l2: 1.32853e+08 [55] valid_0&#39;s rmse: 11451.5 valid_0&#39;s l2: 1.31136e+08 [56] valid_0&#39;s rmse: 11362.7 valid_0&#39;s l2: 1.29111e+08 [57] valid_0&#39;s rmse: 11300.9 valid_0&#39;s l2: 1.2771e+08 [58] valid_0&#39;s rmse: 11169.2 valid_0&#39;s l2: 1.24751e+08 [59] valid_0&#39;s rmse: 11066.8 valid_0&#39;s l2: 1.22473e+08 [60] valid_0&#39;s rmse: 10946.8 valid_0&#39;s l2: 1.19833e+08 [61] valid_0&#39;s rmse: 10856.1 valid_0&#39;s l2: 1.17856e+08 [62] valid_0&#39;s rmse: 10788.4 valid_0&#39;s l2: 1.1639e+08 [63] valid_0&#39;s rmse: 10707.2 valid_0&#39;s l2: 1.14645e+08 [64] valid_0&#39;s rmse: 10633.3 valid_0&#39;s l2: 1.13068e+08 [65] valid_0&#39;s rmse: 10562.7 valid_0&#39;s l2: 1.11571e+08 [66] valid_0&#39;s rmse: 10498.7 valid_0&#39;s l2: 1.10223e+08 [67] valid_0&#39;s rmse: 10449.3 valid_0&#39;s l2: 1.09188e+08 [68] valid_0&#39;s rmse: 10390.9 valid_0&#39;s l2: 1.07971e+08 [69] valid_0&#39;s rmse: 10338.5 valid_0&#39;s l2: 1.06884e+08 [70] valid_0&#39;s rmse: 10259 valid_0&#39;s l2: 1.05247e+08 [71] valid_0&#39;s rmse: 10212.1 valid_0&#39;s l2: 1.04287e+08 [72] valid_0&#39;s rmse: 10168.8 valid_0&#39;s l2: 1.03404e+08 [73] valid_0&#39;s rmse: 10158 valid_0&#39;s l2: 1.03186e+08 [74] valid_0&#39;s rmse: 10120.3 valid_0&#39;s l2: 1.0242e+08 [75] valid_0&#39;s rmse: 10087.1 valid_0&#39;s l2: 1.01749e+08 [76] valid_0&#39;s rmse: 10082 valid_0&#39;s l2: 1.01648e+08 [77] valid_0&#39;s rmse: 10052 valid_0&#39;s l2: 1.01042e+08 [78] valid_0&#39;s rmse: 9994.61 valid_0&#39;s l2: 9.98923e+07 [79] valid_0&#39;s rmse: 9993.65 valid_0&#39;s l2: 9.9873e+07 [80] valid_0&#39;s rmse: 9940.82 valid_0&#39;s l2: 9.88199e+07 [81] valid_0&#39;s rmse: 9890.68 valid_0&#39;s l2: 9.78255e+07 [82] valid_0&#39;s rmse: 9870.01 valid_0&#39;s l2: 9.74171e+07 [83] valid_0&#39;s rmse: 9823.83 valid_0&#39;s l2: 9.65077e+07 [84] valid_0&#39;s rmse: 9827.32 valid_0&#39;s l2: 9.65763e+07 [85] valid_0&#39;s rmse: 9784.63 valid_0&#39;s l2: 9.5739e+07 [86] valid_0&#39;s rmse: 9768.05 valid_0&#39;s l2: 9.54148e+07 [87] valid_0&#39;s rmse: 9726.93 valid_0&#39;s l2: 9.46132e+07 [88] valid_0&#39;s rmse: 9733.09 valid_0&#39;s l2: 9.4733e+07 [89] valid_0&#39;s rmse: 9719.53 valid_0&#39;s l2: 9.44692e+07 [90] valid_0&#39;s rmse: 9723.42 valid_0&#39;s l2: 9.45449e+07 [91] valid_0&#39;s rmse: 9658.54 valid_0&#39;s l2: 9.32874e+07 [92] valid_0&#39;s rmse: 9664.22 valid_0&#39;s l2: 9.33971e+07 [93] valid_0&#39;s rmse: 9633.95 valid_0&#39;s l2: 9.2813e+07 [94] valid_0&#39;s rmse: 9577.04 valid_0&#39;s l2: 9.17197e+07 [95] valid_0&#39;s rmse: 9552.07 valid_0&#39;s l2: 9.12421e+07 [96] valid_0&#39;s rmse: 9560.77 valid_0&#39;s l2: 9.14084e+07 [97] valid_0&#39;s rmse: 9521.39 valid_0&#39;s l2: 9.06569e+07 [98] valid_0&#39;s rmse: 9483.93 valid_0&#39;s l2: 8.9945e+07 [99] valid_0&#39;s rmse: 9496.62 valid_0&#39;s l2: 9.01859e+07 [100] valid_0&#39;s rmse: 9461.61 valid_0&#39;s l2: 8.9522e+07 [101] valid_0&#39;s rmse: 9442.46 valid_0&#39;s l2: 8.91601e+07 [102] valid_0&#39;s rmse: 9455.55 valid_0&#39;s l2: 8.94075e+07 [103] valid_0&#39;s rmse: 9450.15 valid_0&#39;s l2: 8.93053e+07 [104] valid_0&#39;s rmse: 9412.42 valid_0&#39;s l2: 8.85936e+07 [105] valid_0&#39;s rmse: 9392.25 valid_0&#39;s l2: 8.82143e+07 [106] valid_0&#39;s rmse: 9405.78 valid_0&#39;s l2: 8.84687e+07 [107] valid_0&#39;s rmse: 9380 valid_0&#39;s l2: 8.79844e+07 [108] valid_0&#39;s rmse: 9368.19 valid_0&#39;s l2: 8.7763e+07 [109] valid_0&#39;s rmse: 9381.93 valid_0&#39;s l2: 8.80205e+07 [110] valid_0&#39;s rmse: 9361.87 valid_0&#39;s l2: 8.76445e+07 [111] valid_0&#39;s rmse: 9343.46 valid_0&#39;s l2: 8.73002e+07 [112] valid_0&#39;s rmse: 9357.38 valid_0&#39;s l2: 8.75605e+07 [113] valid_0&#39;s rmse: 9350.97 valid_0&#39;s l2: 8.74407e+07 [114] valid_0&#39;s rmse: 9345.93 valid_0&#39;s l2: 8.73464e+07 [115] valid_0&#39;s rmse: 9346.52 valid_0&#39;s l2: 8.73575e+07 [116] valid_0&#39;s rmse: 9342.03 valid_0&#39;s l2: 8.72735e+07 [117] valid_0&#39;s rmse: 9343.4 valid_0&#39;s l2: 8.72991e+07 [118] valid_0&#39;s rmse: 9338.34 valid_0&#39;s l2: 8.72046e+07 [119] valid_0&#39;s rmse: 9308.88 valid_0&#39;s l2: 8.66553e+07 [120] valid_0&#39;s rmse: 9305.1 valid_0&#39;s l2: 8.65849e+07 [121] valid_0&#39;s rmse: 9293.61 valid_0&#39;s l2: 8.63713e+07 [122] valid_0&#39;s rmse: 9265.31 valid_0&#39;s l2: 8.58459e+07 [123] valid_0&#39;s rmse: 9255.69 valid_0&#39;s l2: 8.56678e+07 [124] valid_0&#39;s rmse: 9228.45 valid_0&#39;s l2: 8.51644e+07 [125] valid_0&#39;s rmse: 9221.99 valid_0&#39;s l2: 8.5045e+07 [126] valid_0&#39;s rmse: 9209.52 valid_0&#39;s l2: 8.48152e+07 [127] valid_0&#39;s rmse: 9183.14 valid_0&#39;s l2: 8.43301e+07 [128] valid_0&#39;s rmse: 9097.28 valid_0&#39;s l2: 8.27604e+07 [129] valid_0&#39;s rmse: 9110.94 valid_0&#39;s l2: 8.30093e+07 [130] valid_0&#39;s rmse: 9029.08 valid_0&#39;s l2: 8.15242e+07 [131] valid_0&#39;s rmse: 9003.29 valid_0&#39;s l2: 8.10592e+07 [132] valid_0&#39;s rmse: 8995.15 valid_0&#39;s l2: 8.09127e+07 [133] valid_0&#39;s rmse: 8921.15 valid_0&#39;s l2: 7.95869e+07 [134] valid_0&#39;s rmse: 8934.11 valid_0&#39;s l2: 7.98183e+07 [135] valid_0&#39;s rmse: 8909.27 valid_0&#39;s l2: 7.93751e+07 [136] valid_0&#39;s rmse: 8901.41 valid_0&#39;s l2: 7.9235e+07 [137] valid_0&#39;s rmse: 8889.82 valid_0&#39;s l2: 7.90289e+07 [138] valid_0&#39;s rmse: 8884.77 valid_0&#39;s l2: 7.89392e+07 [139] valid_0&#39;s rmse: 8874.51 valid_0&#39;s l2: 7.8757e+07 [140] valid_0&#39;s rmse: 8799.59 valid_0&#39;s l2: 7.74328e+07 [141] valid_0&#39;s rmse: 8789.66 valid_0&#39;s l2: 7.72582e+07 [142] valid_0&#39;s rmse: 8787.39 valid_0&#39;s l2: 7.72182e+07 [143] valid_0&#39;s rmse: 8778.79 valid_0&#39;s l2: 7.70671e+07 [144] valid_0&#39;s rmse: 8768.96 valid_0&#39;s l2: 7.68947e+07 [145] valid_0&#39;s rmse: 8791.62 valid_0&#39;s l2: 7.72926e+07 [146] valid_0&#39;s rmse: 8783.46 valid_0&#39;s l2: 7.71492e+07 [147] valid_0&#39;s rmse: 8775.29 valid_0&#39;s l2: 7.70056e+07 [148] valid_0&#39;s rmse: 8744.59 valid_0&#39;s l2: 7.64678e+07 [149] valid_0&#39;s rmse: 8746.18 valid_0&#39;s l2: 7.64956e+07 [150] valid_0&#39;s rmse: 8693.96 valid_0&#39;s l2: 7.55849e+07 [151] valid_0&#39;s rmse: 8713.22 valid_0&#39;s l2: 7.59202e+07 [152] valid_0&#39;s rmse: 8663.27 valid_0&#39;s l2: 7.50523e+07 [153] valid_0&#39;s rmse: 8651.5 valid_0&#39;s l2: 7.48484e+07 [154] valid_0&#39;s rmse: 8650.44 valid_0&#39;s l2: 7.48302e+07 [155] valid_0&#39;s rmse: 8591.28 valid_0&#39;s l2: 7.38101e+07 [156] valid_0&#39;s rmse: 8544.01 valid_0&#39;s l2: 7.3e+07 [157] valid_0&#39;s rmse: 8557.88 valid_0&#39;s l2: 7.32373e+07 [158] valid_0&#39;s rmse: 8516.03 valid_0&#39;s l2: 7.25227e+07 [159] valid_0&#39;s rmse: 8490.58 valid_0&#39;s l2: 7.209e+07 [160] valid_0&#39;s rmse: 8447.64 valid_0&#39;s l2: 7.13627e+07 [161] valid_0&#39;s rmse: 8448.26 valid_0&#39;s l2: 7.13731e+07 [162] valid_0&#39;s rmse: 8417.33 valid_0&#39;s l2: 7.08515e+07 [163] valid_0&#39;s rmse: 8405.95 valid_0&#39;s l2: 7.06601e+07 [164] valid_0&#39;s rmse: 8385.57 valid_0&#39;s l2: 7.03178e+07 [165] valid_0&#39;s rmse: 8345.18 valid_0&#39;s l2: 6.9642e+07 [166] valid_0&#39;s rmse: 8315.01 valid_0&#39;s l2: 6.91394e+07 [167] valid_0&#39;s rmse: 8295 valid_0&#39;s l2: 6.8807e+07 [168] valid_0&#39;s rmse: 8266.3 valid_0&#39;s l2: 6.83317e+07 [169] valid_0&#39;s rmse: 8255.63 valid_0&#39;s l2: 6.81554e+07 [170] valid_0&#39;s rmse: 8237.63 valid_0&#39;s l2: 6.78586e+07 [171] valid_0&#39;s rmse: 8198.27 valid_0&#39;s l2: 6.72116e+07 [172] valid_0&#39;s rmse: 8179.46 valid_0&#39;s l2: 6.69036e+07 [173] valid_0&#39;s rmse: 8152.25 valid_0&#39;s l2: 6.64591e+07 [174] valid_0&#39;s rmse: 8142.12 valid_0&#39;s l2: 6.6294e+07 [175] valid_0&#39;s rmse: 8145.57 valid_0&#39;s l2: 6.63504e+07 [176] valid_0&#39;s rmse: 8109.99 valid_0&#39;s l2: 6.57719e+07 [177] valid_0&#39;s rmse: 8100.33 valid_0&#39;s l2: 6.56154e+07 [178] valid_0&#39;s rmse: 8084.54 valid_0&#39;s l2: 6.53597e+07 [179] valid_0&#39;s rmse: 8056.29 valid_0&#39;s l2: 6.49039e+07 [180] valid_0&#39;s rmse: 8038.88 valid_0&#39;s l2: 6.46236e+07 [181] valid_0&#39;s rmse: 8014.61 valid_0&#39;s l2: 6.4234e+07 [182] valid_0&#39;s rmse: 8005.59 valid_0&#39;s l2: 6.40894e+07 [183] valid_0&#39;s rmse: 7987.29 valid_0&#39;s l2: 6.37969e+07 [184] valid_0&#39;s rmse: 7944.02 valid_0&#39;s l2: 6.31074e+07 [185] valid_0&#39;s rmse: 7935.84 valid_0&#39;s l2: 6.29775e+07 [186] valid_0&#39;s rmse: 7910.41 valid_0&#39;s l2: 6.25747e+07 [187] valid_0&#39;s rmse: 7878.97 valid_0&#39;s l2: 6.20782e+07 [188] valid_0&#39;s rmse: 7899.03 valid_0&#39;s l2: 6.23947e+07 [189] valid_0&#39;s rmse: 7881.18 valid_0&#39;s l2: 6.2113e+07 [190] valid_0&#39;s rmse: 7856.46 valid_0&#39;s l2: 6.1724e+07 [191] valid_0&#39;s rmse: 7847.88 valid_0&#39;s l2: 6.15892e+07 [192] valid_0&#39;s rmse: 7837.49 valid_0&#39;s l2: 6.14262e+07 [193] valid_0&#39;s rmse: 7803.29 valid_0&#39;s l2: 6.08913e+07 [194] valid_0&#39;s rmse: 7779.09 valid_0&#39;s l2: 6.05143e+07 [195] valid_0&#39;s rmse: 7770.88 valid_0&#39;s l2: 6.03865e+07 [196] valid_0&#39;s rmse: 7792.76 valid_0&#39;s l2: 6.07272e+07 [197] valid_0&#39;s rmse: 7763.04 valid_0&#39;s l2: 6.02649e+07 [198] valid_0&#39;s rmse: 7779.42 valid_0&#39;s l2: 6.05193e+07 [199] valid_0&#39;s rmse: 7760.42 valid_0&#39;s l2: 6.02242e+07 [200] valid_0&#39;s rmse: 7742.11 valid_0&#39;s l2: 5.99403e+07 [201] valid_0&#39;s rmse: 7719.11 valid_0&#39;s l2: 5.95846e+07 [202] valid_0&#39;s rmse: 7711.05 valid_0&#39;s l2: 5.94603e+07 [203] valid_0&#39;s rmse: 7699.74 valid_0&#39;s l2: 5.9286e+07 [204] valid_0&#39;s rmse: 7667.23 valid_0&#39;s l2: 5.87865e+07 [205] valid_0&#39;s rmse: 7636.79 valid_0&#39;s l2: 5.83206e+07 [206] valid_0&#39;s rmse: 7633.3 valid_0&#39;s l2: 5.82673e+07 [207] valid_0&#39;s rmse: 7608.18 valid_0&#39;s l2: 5.78844e+07 [208] valid_0&#39;s rmse: 7579.35 valid_0&#39;s l2: 5.74466e+07 [209] valid_0&#39;s rmse: 7595.07 valid_0&#39;s l2: 5.7685e+07 [210] valid_0&#39;s rmse: 7569.56 valid_0&#39;s l2: 5.72982e+07 [211] valid_0&#39;s rmse: 7547.53 valid_0&#39;s l2: 5.69652e+07 [212] valid_0&#39;s rmse: 7509.49 valid_0&#39;s l2: 5.63924e+07 [213] valid_0&#39;s rmse: 7488.67 valid_0&#39;s l2: 5.60801e+07 [214] valid_0&#39;s rmse: 7474.12 valid_0&#39;s l2: 5.58625e+07 [215] valid_0&#39;s rmse: 7440.25 valid_0&#39;s l2: 5.53573e+07 [216] valid_0&#39;s rmse: 7418.11 valid_0&#39;s l2: 5.50283e+07 [217] valid_0&#39;s rmse: 7394.83 valid_0&#39;s l2: 5.46835e+07 [218] valid_0&#39;s rmse: 7359.92 valid_0&#39;s l2: 5.41684e+07 [219] valid_0&#39;s rmse: 7340.8 valid_0&#39;s l2: 5.38873e+07 [220] valid_0&#39;s rmse: 7322.52 valid_0&#39;s l2: 5.36193e+07 [221] valid_0&#39;s rmse: 7289.11 valid_0&#39;s l2: 5.31311e+07 [222] valid_0&#39;s rmse: 7273.77 valid_0&#39;s l2: 5.29077e+07 [223] valid_0&#39;s rmse: 7258.39 valid_0&#39;s l2: 5.26843e+07 [224] valid_0&#39;s rmse: 7248.92 valid_0&#39;s l2: 5.25468e+07 [225] valid_0&#39;s rmse: 7231.67 valid_0&#39;s l2: 5.22971e+07 [226] valid_0&#39;s rmse: 7199.45 valid_0&#39;s l2: 5.1832e+07 [227] valid_0&#39;s rmse: 7172.19 valid_0&#39;s l2: 5.14403e+07 [228] valid_0&#39;s rmse: 7155.9 valid_0&#39;s l2: 5.1207e+07 [229] valid_0&#39;s rmse: 7139 valid_0&#39;s l2: 5.09653e+07 [230] valid_0&#39;s rmse: 7112.75 valid_0&#39;s l2: 5.05912e+07 [231] valid_0&#39;s rmse: 7095.16 valid_0&#39;s l2: 5.03413e+07 [232] valid_0&#39;s rmse: 7078.43 valid_0&#39;s l2: 5.01042e+07 [233] valid_0&#39;s rmse: 7047.37 valid_0&#39;s l2: 4.96654e+07 [234] valid_0&#39;s rmse: 7040.23 valid_0&#39;s l2: 4.95649e+07 [235] valid_0&#39;s rmse: 7019.9 valid_0&#39;s l2: 4.9279e+07 [236] valid_0&#39;s rmse: 7013.68 valid_0&#39;s l2: 4.91917e+07 [237] valid_0&#39;s rmse: 7001.78 valid_0&#39;s l2: 4.90249e+07 [238] valid_0&#39;s rmse: 6984.18 valid_0&#39;s l2: 4.87788e+07 [239] valid_0&#39;s rmse: 6983.05 valid_0&#39;s l2: 4.8763e+07 [240] valid_0&#39;s rmse: 6986.71 valid_0&#39;s l2: 4.88141e+07 [241] valid_0&#39;s rmse: 6957.17 valid_0&#39;s l2: 4.84022e+07 [242] valid_0&#39;s rmse: 6939.87 valid_0&#39;s l2: 4.81618e+07 [243] valid_0&#39;s rmse: 6920.48 valid_0&#39;s l2: 4.78931e+07 [244] valid_0&#39;s rmse: 6914.68 valid_0&#39;s l2: 4.78128e+07 [245] valid_0&#39;s rmse: 6903.65 valid_0&#39;s l2: 4.76604e+07 [246] valid_0&#39;s rmse: 6902.98 valid_0&#39;s l2: 4.76511e+07 [247] valid_0&#39;s rmse: 6879.52 valid_0&#39;s l2: 4.73278e+07 [248] valid_0&#39;s rmse: 6863.68 valid_0&#39;s l2: 4.71101e+07 [249] valid_0&#39;s rmse: 6857.59 valid_0&#39;s l2: 4.70265e+07 [250] valid_0&#39;s rmse: 6847.42 valid_0&#39;s l2: 4.68871e+07 [251] valid_0&#39;s rmse: 6847.71 valid_0&#39;s l2: 4.68912e+07 [252] valid_0&#39;s rmse: 6848.25 valid_0&#39;s l2: 4.68985e+07 [253] valid_0&#39;s rmse: 6846.08 valid_0&#39;s l2: 4.68688e+07 [254] valid_0&#39;s rmse: 6846.74 valid_0&#39;s l2: 4.68779e+07 [255] valid_0&#39;s rmse: 6836.78 valid_0&#39;s l2: 4.67415e+07 [256] valid_0&#39;s rmse: 6837.5 valid_0&#39;s l2: 4.67514e+07 [257] valid_0&#39;s rmse: 6839.44 valid_0&#39;s l2: 4.67779e+07 [258] valid_0&#39;s rmse: 6835.54 valid_0&#39;s l2: 4.67246e+07 [259] valid_0&#39;s rmse: 6837.66 valid_0&#39;s l2: 4.67536e+07 [260] valid_0&#39;s rmse: 6825.48 valid_0&#39;s l2: 4.65872e+07 [261] valid_0&#39;s rmse: 6816.14 valid_0&#39;s l2: 4.64597e+07 [262] valid_0&#39;s rmse: 6809.52 valid_0&#39;s l2: 4.63695e+07 [263] valid_0&#39;s rmse: 6807.09 valid_0&#39;s l2: 4.63364e+07 [264] valid_0&#39;s rmse: 6798.12 valid_0&#39;s l2: 4.62144e+07 [265] valid_0&#39;s rmse: 6807.78 valid_0&#39;s l2: 4.63458e+07 [266] valid_0&#39;s rmse: 6795.63 valid_0&#39;s l2: 4.61805e+07 [267] valid_0&#39;s rmse: 6792.09 valid_0&#39;s l2: 4.61325e+07 [268] valid_0&#39;s rmse: 6785.86 valid_0&#39;s l2: 4.60479e+07 [269] valid_0&#39;s rmse: 6783.24 valid_0&#39;s l2: 4.60123e+07 [270] valid_0&#39;s rmse: 6774.7 valid_0&#39;s l2: 4.58966e+07 [271] valid_0&#39;s rmse: 6783.17 valid_0&#39;s l2: 4.60115e+07 [272] valid_0&#39;s rmse: 6757.24 valid_0&#39;s l2: 4.56603e+07 [273] valid_0&#39;s rmse: 6749.51 valid_0&#39;s l2: 4.55559e+07 [274] valid_0&#39;s rmse: 6721.42 valid_0&#39;s l2: 4.51775e+07 [275] valid_0&#39;s rmse: 6722.24 valid_0&#39;s l2: 4.51885e+07 [276] valid_0&#39;s rmse: 6725.4 valid_0&#39;s l2: 4.52311e+07 [277] valid_0&#39;s rmse: 6734.47 valid_0&#39;s l2: 4.53531e+07 [278] valid_0&#39;s rmse: 6726.6 valid_0&#39;s l2: 4.52471e+07 [279] valid_0&#39;s rmse: 6708.23 valid_0&#39;s l2: 4.50003e+07 [280] valid_0&#39;s rmse: 6717.11 valid_0&#39;s l2: 4.51196e+07 [281] valid_0&#39;s rmse: 6700.13 valid_0&#39;s l2: 4.48917e+07 [282] valid_0&#39;s rmse: 6687.85 valid_0&#39;s l2: 4.47273e+07 [283] valid_0&#39;s rmse: 6672.72 valid_0&#39;s l2: 4.45251e+07 [284] valid_0&#39;s rmse: 6671.05 valid_0&#39;s l2: 4.4503e+07 [285] valid_0&#39;s rmse: 6646 valid_0&#39;s l2: 4.41694e+07 [286] valid_0&#39;s rmse: 6626.19 valid_0&#39;s l2: 4.39064e+07 [287] valid_0&#39;s rmse: 6608.59 valid_0&#39;s l2: 4.36735e+07 [288] valid_0&#39;s rmse: 6616.6 valid_0&#39;s l2: 4.37794e+07 [289] valid_0&#39;s rmse: 6601.26 valid_0&#39;s l2: 4.35766e+07 [290] valid_0&#39;s rmse: 6576.56 valid_0&#39;s l2: 4.32512e+07 [291] valid_0&#39;s rmse: 6545.16 valid_0&#39;s l2: 4.28392e+07 [292] valid_0&#39;s rmse: 6530.28 valid_0&#39;s l2: 4.26446e+07 [293] valid_0&#39;s rmse: 6529.37 valid_0&#39;s l2: 4.26327e+07 [294] valid_0&#39;s rmse: 6514.72 valid_0&#39;s l2: 4.24416e+07 [295] valid_0&#39;s rmse: 6500.85 valid_0&#39;s l2: 4.2261e+07 [296] valid_0&#39;s rmse: 6498.8 valid_0&#39;s l2: 4.22345e+07 [297] valid_0&#39;s rmse: 6478.4 valid_0&#39;s l2: 4.19696e+07 [298] valid_0&#39;s rmse: 6462.87 valid_0&#39;s l2: 4.17686e+07 [299] valid_0&#39;s rmse: 6456.54 valid_0&#39;s l2: 4.16869e+07 [300] valid_0&#39;s rmse: 6462.58 valid_0&#39;s l2: 4.1765e+07 Did not meet early stopping. Best iteration is: [299] valid_0&#39;s rmse: 6456.54 valid_0&#39;s l2: 4.16869e+07 . NMAE(y_valid, y_pred2) . 0.08110524597547468 . y_pred2 = lgbm_r.predict(X_test) . y_pred2 . array([ 70923.39958255, 51533.25785371, 63963.15154037, 54810.96753197, 84326.90239366, 80491.00831213, 69206.90740948, 74179.47218397, 84021.69323662, 83873.85467653, 86116.50473254, 42272.89237486, 55635.53932227, 66888.70641843, 72433.72400908, 48435.71793999, 53722.55890703, 79302.53972151, 82555.77546427, 89022.67627234, 96161.61227013, 83694.35676425, 35419.64608583, 78870.73950357, 83648.32460774, 87686.26928496, 70728.81707171, 78098.28799485, 70524.42612666, 52459.41657819, 43851.06545657, 85676.54066391, 85750.47281738, 46440.50734995, 78942.21873052, 87776.08218116, 67691.54241193, 81737.62225983, 83484.24678798, 76046.11690444, 97403.00013878, 101658.95478514, 94394.55827646, 95789.53451566, 85827.12497728, 70746.6608407 , 82931.6163995 , 100719.84347118, 94583.81870734, 73338.02044061, 48409.13957579, 97543.18613226, 100578.96901174, 92558.68997736, 80167.96338371, 73013.25798437, 63345.88168153, 62788.46338065, 90242.95087339, 83903.04844172, 66894.4857991 , 99543.98608684, 101225.57200743, 81060.1584282 , 89019.01279755, 93874.47349485, 99544.92561857, 111997.81812308, 92835.96253596, 107030.34465041, 92639.19717083, 64934.1259377 , 104153.20311766, 104367.26241244, 106406.02666556, 69563.44938679, 94784.81742151, 102159.71129668, 90993.52600616, 104375.87013754, 95094.44493272, 103500.59814837, 89741.96205798, 87177.46944052, 94477.78811791, 104788.86724321, 74536.84126441, 98139.40796479, 92215.89169709, 93339.32524144, 96427.61199259]) . from lightgbm import plot_importance fig, ax = plt.subplots(figsize=(15, 15)) plot_importance(lgbm_r, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faae07fd690&gt; . pip install catboost . Collecting catboost Downloading catboost-1.0.3-cp37-none-manylinux1_x86_64.whl (76.3 MB) |████████████████████████████████| 76.3 MB 22 kB/s Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.10.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.2) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (1.3.3) Installing collected packages: catboost Successfully installed catboost-1.0.3 . from catboost import CatBoostRegressor cat = CatBoostRegressor(iterations = 350,loss_function = &quot;RMSE&quot;, learning_rate=0.05, cat_features = [&#39;cardinal_directions&#39;], random_state = 42) cat.fit(X_train,y_train,eval_set=[(X_valid, y_valid)],verbose = True, early_stopping_rounds = 100) y_pred3 = cat.predict(X_valid) . 0: learn: 26581.4814666 test: 32144.8749947 best: 32144.8749947 (0) total: 1.46ms remaining: 509ms 1: learn: 25835.5322350 test: 31321.0220471 best: 31321.0220471 (1) total: 3.95ms remaining: 688ms 2: learn: 25064.4835532 test: 30390.1443384 best: 30390.1443384 (2) total: 5.65ms remaining: 654ms 3: learn: 24282.8223642 test: 29368.5166292 best: 29368.5166292 (3) total: 8.07ms remaining: 698ms 4: learn: 23547.2988114 test: 28566.6581612 best: 28566.6581612 (4) total: 10.5ms remaining: 723ms 5: learn: 23019.0470899 test: 28174.9069066 best: 28174.9069066 (5) total: 11.6ms remaining: 668ms 6: learn: 22328.0250075 test: 27120.0132612 best: 27120.0132612 (6) total: 14.2ms remaining: 697ms 7: learn: 21725.9731007 test: 26450.0160942 best: 26450.0160942 (7) total: 16.9ms remaining: 724ms 8: learn: 21168.6497402 test: 25843.0615970 best: 25843.0615970 (8) total: 19.6ms remaining: 742ms 9: learn: 20568.0437199 test: 24960.6478618 best: 24960.6478618 (9) total: 22.5ms remaining: 765ms 10: learn: 20075.7431958 test: 24485.6268201 best: 24485.6268201 (10) total: 25.2ms remaining: 778ms 11: learn: 19583.3108889 test: 23897.3069460 best: 23897.3069460 (11) total: 27.9ms remaining: 785ms 12: learn: 19169.6295131 test: 23571.9263324 best: 23571.9263324 (12) total: 30.4ms remaining: 789ms 13: learn: 18753.4446985 test: 23139.6412043 best: 23139.6412043 (13) total: 33ms remaining: 792ms 14: learn: 18504.6164295 test: 22975.6019162 best: 22975.6019162 (14) total: 33.9ms remaining: 758ms 15: learn: 18124.1273451 test: 22602.9702926 best: 22602.9702926 (15) total: 36.4ms remaining: 761ms 16: learn: 17678.4897073 test: 22030.6814709 best: 22030.6814709 (16) total: 38.9ms remaining: 763ms 17: learn: 17278.7095000 test: 21541.7251656 best: 21541.7251656 (17) total: 40.4ms remaining: 745ms 18: learn: 16840.9070431 test: 21014.4464336 best: 21014.4464336 (18) total: 43.6ms remaining: 760ms 19: learn: 16437.9822438 test: 20454.3575367 best: 20454.3575367 (19) total: 46.6ms remaining: 770ms 20: learn: 16133.8854693 test: 20073.1886521 best: 20073.1886521 (20) total: 49.6ms remaining: 777ms 21: learn: 15812.3948138 test: 19729.3917084 best: 19729.3917084 (21) total: 52.4ms remaining: 781ms 22: learn: 15425.5540072 test: 19209.5401269 best: 19209.5401269 (22) total: 55.4ms remaining: 787ms 23: learn: 15096.0416217 test: 18921.5353897 best: 18921.5353897 (23) total: 58.7ms remaining: 797ms 24: learn: 14777.4981266 test: 18557.5408970 best: 18557.5408970 (24) total: 61.4ms remaining: 799ms 25: learn: 14479.8833061 test: 18176.5236329 best: 18176.5236329 (25) total: 64.3ms remaining: 801ms 26: learn: 14188.1073871 test: 17787.1989361 best: 17787.1989361 (26) total: 66.9ms remaining: 801ms 27: learn: 13861.5485161 test: 17501.3477328 best: 17501.3477328 (27) total: 69.8ms remaining: 802ms 28: learn: 13665.6431697 test: 17374.1785334 best: 17374.1785334 (28) total: 72.4ms remaining: 802ms 29: learn: 13403.7668075 test: 16980.7950871 best: 16980.7950871 (29) total: 75.3ms remaining: 803ms 30: learn: 13180.4856498 test: 16722.8242083 best: 16722.8242083 (30) total: 78.4ms remaining: 807ms 31: learn: 12933.1887801 test: 16323.5423356 best: 16323.5423356 (31) total: 81.2ms remaining: 807ms 32: learn: 12744.3809362 test: 16097.9827167 best: 16097.9827167 (32) total: 84.2ms remaining: 808ms 33: learn: 12540.6118445 test: 15750.3203502 best: 15750.3203502 (33) total: 86.7ms remaining: 806ms 34: learn: 12304.0655338 test: 15463.9915674 best: 15463.9915674 (34) total: 89.4ms remaining: 805ms 35: learn: 12104.3820972 test: 15272.9856263 best: 15272.9856263 (35) total: 92ms remaining: 803ms 36: learn: 11970.4166863 test: 15135.9040356 best: 15135.9040356 (36) total: 94ms remaining: 796ms 37: learn: 11754.9302983 test: 14861.5964636 best: 14861.5964636 (37) total: 96.8ms remaining: 794ms 38: learn: 11550.4370891 test: 14702.8830236 best: 14702.8830236 (38) total: 99.5ms remaining: 793ms 39: learn: 11364.2744419 test: 14665.6302133 best: 14665.6302133 (39) total: 102ms remaining: 793ms 40: learn: 11251.0010427 test: 14549.1992892 best: 14549.1992892 (40) total: 105ms remaining: 793ms 41: learn: 11122.1294311 test: 14453.6945586 best: 14453.6945586 (41) total: 108ms remaining: 792ms 42: learn: 11008.9720871 test: 14280.6182095 best: 14280.6182095 (42) total: 109ms remaining: 780ms 43: learn: 10861.5375894 test: 14097.2265186 best: 14097.2265186 (43) total: 112ms remaining: 780ms 44: learn: 10725.0553378 test: 14041.8413356 best: 14041.8413356 (44) total: 115ms remaining: 780ms 45: learn: 10563.0090957 test: 13789.4901589 best: 13789.4901589 (45) total: 118ms remaining: 779ms 46: learn: 10422.2006936 test: 13615.1102215 best: 13615.1102215 (46) total: 121ms remaining: 778ms 47: learn: 10295.0686338 test: 13551.9143187 best: 13551.9143187 (47) total: 123ms remaining: 775ms 48: learn: 10186.5858598 test: 13397.2893632 best: 13397.2893632 (48) total: 126ms remaining: 775ms 49: learn: 10065.2708675 test: 13287.4329555 best: 13287.4329555 (49) total: 129ms remaining: 773ms 50: learn: 9930.4737790 test: 13188.3123198 best: 13188.3123198 (50) total: 132ms remaining: 771ms 51: learn: 9826.7204780 test: 13085.8075417 best: 13085.8075417 (51) total: 133ms remaining: 763ms 52: learn: 9719.1477739 test: 13091.1911882 best: 13085.8075417 (51) total: 136ms remaining: 761ms 53: learn: 9621.5082955 test: 13052.2252602 best: 13052.2252602 (53) total: 140ms remaining: 765ms 54: learn: 9512.4843661 test: 12930.1285870 best: 12930.1285870 (54) total: 142ms remaining: 764ms 55: learn: 9409.5026212 test: 12823.1191207 best: 12823.1191207 (55) total: 145ms remaining: 762ms 56: learn: 9323.7192885 test: 12706.8046273 best: 12706.8046273 (56) total: 148ms remaining: 760ms 57: learn: 9230.6685581 test: 12614.6835987 best: 12614.6835987 (57) total: 150ms remaining: 757ms 58: learn: 9134.6425256 test: 12495.6289586 best: 12495.6289586 (58) total: 153ms remaining: 756ms 59: learn: 9061.2804514 test: 12461.6082135 best: 12461.6082135 (59) total: 156ms remaining: 756ms 60: learn: 9006.3657225 test: 12412.3504248 best: 12412.3504248 (60) total: 159ms remaining: 752ms 61: learn: 8902.2541304 test: 12283.9184233 best: 12283.9184233 (61) total: 161ms remaining: 750ms 62: learn: 8843.0448046 test: 12240.2818611 best: 12240.2818611 (62) total: 164ms remaining: 748ms 63: learn: 8754.8966597 test: 12143.9562771 best: 12143.9562771 (63) total: 167ms remaining: 745ms 64: learn: 8704.7082983 test: 12107.9595380 best: 12107.9595380 (64) total: 169ms remaining: 743ms 65: learn: 8648.5641510 test: 12084.1900150 best: 12084.1900150 (65) total: 172ms remaining: 741ms 66: learn: 8591.1205812 test: 12012.5990653 best: 12012.5990653 (66) total: 175ms remaining: 739ms 67: learn: 8529.3273931 test: 11992.6603824 best: 11992.6603824 (67) total: 178ms remaining: 737ms 68: learn: 8466.0835819 test: 11880.4031416 best: 11880.4031416 (68) total: 183ms remaining: 744ms 69: learn: 8387.2226141 test: 11799.0296671 best: 11799.0296671 (69) total: 189ms remaining: 756ms 70: learn: 8370.4294629 test: 11798.2721947 best: 11798.2721947 (70) total: 191ms remaining: 749ms 71: learn: 8324.8170832 test: 11753.9161604 best: 11753.9161604 (71) total: 192ms remaining: 741ms 72: learn: 8241.4346519 test: 11699.1312062 best: 11699.1312062 (72) total: 194ms remaining: 736ms 73: learn: 8163.9728207 test: 11736.7864728 best: 11699.1312062 (72) total: 196ms remaining: 731ms 74: learn: 8101.3155894 test: 11777.2261432 best: 11699.1312062 (72) total: 198ms remaining: 725ms 75: learn: 8058.2411687 test: 11747.2289343 best: 11699.1312062 (72) total: 200ms remaining: 720ms 76: learn: 8007.3702742 test: 11682.8842076 best: 11682.8842076 (76) total: 201ms remaining: 714ms 77: learn: 7962.7255212 test: 11659.4361851 best: 11659.4361851 (77) total: 203ms remaining: 709ms 78: learn: 7900.5488584 test: 11635.5977567 best: 11635.5977567 (78) total: 205ms remaining: 703ms 79: learn: 7851.1814194 test: 11628.4911021 best: 11628.4911021 (79) total: 207ms remaining: 698ms 80: learn: 7775.5468882 test: 11605.3344505 best: 11605.3344505 (80) total: 209ms remaining: 693ms 81: learn: 7720.5827791 test: 11580.3076865 best: 11580.3076865 (81) total: 211ms remaining: 688ms 82: learn: 7663.5808044 test: 11600.8343869 best: 11580.3076865 (81) total: 212ms remaining: 683ms 83: learn: 7634.8080787 test: 11621.2077787 best: 11580.3076865 (81) total: 214ms remaining: 678ms 84: learn: 7581.3029316 test: 11604.2432500 best: 11580.3076865 (81) total: 216ms remaining: 673ms 85: learn: 7546.3684090 test: 11570.6045513 best: 11570.6045513 (85) total: 218ms remaining: 668ms 86: learn: 7506.4973959 test: 11568.8636000 best: 11568.8636000 (86) total: 220ms remaining: 664ms 87: learn: 7442.4263081 test: 11546.5922166 best: 11546.5922166 (87) total: 222ms remaining: 660ms 88: learn: 7406.3845803 test: 11546.3961832 best: 11546.3961832 (88) total: 223ms remaining: 655ms 89: learn: 7356.5034970 test: 11536.2035228 best: 11536.2035228 (89) total: 225ms remaining: 651ms 90: learn: 7319.6938024 test: 11527.3681162 best: 11527.3681162 (90) total: 227ms remaining: 646ms 91: learn: 7280.3168049 test: 11483.6198333 best: 11483.6198333 (91) total: 229ms remaining: 643ms 92: learn: 7244.1769344 test: 11489.5524250 best: 11483.6198333 (91) total: 231ms remaining: 639ms 93: learn: 7188.3184753 test: 11412.0531803 best: 11412.0531803 (93) total: 233ms remaining: 635ms 94: learn: 7141.2218470 test: 11413.3629520 best: 11412.0531803 (93) total: 235ms remaining: 631ms 95: learn: 7097.0581554 test: 11430.7578978 best: 11412.0531803 (93) total: 237ms remaining: 627ms 96: learn: 7049.9972714 test: 11401.5907935 best: 11401.5907935 (96) total: 239ms remaining: 623ms 97: learn: 7014.4032537 test: 11396.7110102 best: 11396.7110102 (97) total: 241ms remaining: 619ms 98: learn: 6998.7755687 test: 11398.2435187 best: 11396.7110102 (97) total: 242ms remaining: 615ms 99: learn: 6989.8723680 test: 11387.4341563 best: 11387.4341563 (99) total: 243ms remaining: 609ms 100: learn: 6957.8308725 test: 11359.1553703 best: 11359.1553703 (100) total: 245ms remaining: 605ms 101: learn: 6934.1058454 test: 11352.6667476 best: 11352.6667476 (101) total: 247ms remaining: 601ms 102: learn: 6918.8111220 test: 11312.2777073 best: 11312.2777073 (102) total: 248ms remaining: 595ms 103: learn: 6895.2456983 test: 11390.9723476 best: 11312.2777073 (102) total: 250ms remaining: 591ms 104: learn: 6834.8470273 test: 11350.8487147 best: 11312.2777073 (102) total: 252ms remaining: 587ms 105: learn: 6804.5897793 test: 11332.3924412 best: 11312.2777073 (102) total: 253ms remaining: 584ms 106: learn: 6774.5381856 test: 11327.5586756 best: 11312.2777073 (102) total: 255ms remaining: 580ms 107: learn: 6741.1132273 test: 11321.9580650 best: 11312.2777073 (102) total: 257ms remaining: 576ms 108: learn: 6694.1414738 test: 11328.6056749 best: 11312.2777073 (102) total: 259ms remaining: 573ms 109: learn: 6660.2070029 test: 11282.9771206 best: 11282.9771206 (109) total: 261ms remaining: 569ms 110: learn: 6636.7093660 test: 11329.2687035 best: 11282.9771206 (109) total: 263ms remaining: 565ms 111: learn: 6607.7284198 test: 11320.4918055 best: 11282.9771206 (109) total: 264ms remaining: 562ms 112: learn: 6574.4406153 test: 11295.2992383 best: 11282.9771206 (109) total: 266ms remaining: 558ms 113: learn: 6537.9031260 test: 11324.4574461 best: 11282.9771206 (109) total: 268ms remaining: 554ms 114: learn: 6499.7768850 test: 11361.1115287 best: 11282.9771206 (109) total: 269ms remaining: 550ms 115: learn: 6466.2650615 test: 11306.7265162 best: 11282.9771206 (109) total: 271ms remaining: 547ms 116: learn: 6432.7653796 test: 11315.0243493 best: 11282.9771206 (109) total: 273ms remaining: 544ms 117: learn: 6410.6750657 test: 11313.7479749 best: 11282.9771206 (109) total: 275ms remaining: 541ms 118: learn: 6383.2753749 test: 11266.9340323 best: 11266.9340323 (118) total: 277ms remaining: 537ms 119: learn: 6353.6012403 test: 11300.2669829 best: 11266.9340323 (118) total: 279ms remaining: 534ms 120: learn: 6315.5877285 test: 11346.1462750 best: 11266.9340323 (118) total: 281ms remaining: 531ms 121: learn: 6298.2041325 test: 11318.5959279 best: 11266.9340323 (118) total: 283ms remaining: 528ms 122: learn: 6283.3332478 test: 11316.3144899 best: 11266.9340323 (118) total: 284ms remaining: 525ms 123: learn: 6253.1866090 test: 11295.8491228 best: 11266.9340323 (118) total: 286ms remaining: 522ms 124: learn: 6224.3497293 test: 11285.7564767 best: 11266.9340323 (118) total: 288ms remaining: 518ms 125: learn: 6186.6573493 test: 11268.9826736 best: 11266.9340323 (118) total: 290ms remaining: 515ms 126: learn: 6154.4951798 test: 11294.9059216 best: 11266.9340323 (118) total: 292ms remaining: 512ms 127: learn: 6131.2781914 test: 11330.0167942 best: 11266.9340323 (118) total: 293ms remaining: 509ms 128: learn: 6094.4760680 test: 11386.7899839 best: 11266.9340323 (118) total: 295ms remaining: 506ms 129: learn: 6062.9058416 test: 11414.3168947 best: 11266.9340323 (118) total: 297ms remaining: 503ms 130: learn: 6031.3293455 test: 11434.6165781 best: 11266.9340323 (118) total: 299ms remaining: 500ms 131: learn: 6008.4408167 test: 11413.9039799 best: 11266.9340323 (118) total: 301ms remaining: 497ms 132: learn: 5977.4948988 test: 11443.5215691 best: 11266.9340323 (118) total: 303ms remaining: 494ms 133: learn: 5949.5023810 test: 11438.0243860 best: 11266.9340323 (118) total: 305ms remaining: 491ms 134: learn: 5939.1745884 test: 11460.4680958 best: 11266.9340323 (118) total: 306ms remaining: 488ms 135: learn: 5915.3609409 test: 11457.1000604 best: 11266.9340323 (118) total: 308ms remaining: 485ms 136: learn: 5877.1125671 test: 11423.1351017 best: 11266.9340323 (118) total: 310ms remaining: 482ms 137: learn: 5869.4345983 test: 11422.3286312 best: 11266.9340323 (118) total: 312ms remaining: 479ms 138: learn: 5849.3637379 test: 11368.3545245 best: 11266.9340323 (118) total: 314ms remaining: 476ms 139: learn: 5824.5272618 test: 11408.2217693 best: 11266.9340323 (118) total: 315ms remaining: 473ms 140: learn: 5797.3284359 test: 11385.5158989 best: 11266.9340323 (118) total: 317ms remaining: 470ms 141: learn: 5780.6715943 test: 11388.7396723 best: 11266.9340323 (118) total: 319ms remaining: 467ms 142: learn: 5750.8238570 test: 11382.6864957 best: 11266.9340323 (118) total: 321ms remaining: 465ms 143: learn: 5713.4536044 test: 11369.1202402 best: 11266.9340323 (118) total: 323ms remaining: 462ms 144: learn: 5697.5783074 test: 11347.6136803 best: 11266.9340323 (118) total: 325ms remaining: 459ms 145: learn: 5676.8665933 test: 11311.5624442 best: 11266.9340323 (118) total: 327ms remaining: 456ms 146: learn: 5671.0361625 test: 11317.5707069 best: 11266.9340323 (118) total: 328ms remaining: 453ms 147: learn: 5655.4411838 test: 11306.7146829 best: 11266.9340323 (118) total: 330ms remaining: 451ms 148: learn: 5643.0191597 test: 11301.2292258 best: 11266.9340323 (118) total: 332ms remaining: 448ms 149: learn: 5627.3644673 test: 11272.0399610 best: 11266.9340323 (118) total: 334ms remaining: 445ms 150: learn: 5609.3728722 test: 11244.1052952 best: 11244.1052952 (150) total: 336ms remaining: 443ms 151: learn: 5571.1503088 test: 11217.3845770 best: 11217.3845770 (151) total: 338ms remaining: 440ms 152: learn: 5555.2862299 test: 11221.9433007 best: 11217.3845770 (151) total: 340ms remaining: 438ms 153: learn: 5539.9997676 test: 11246.0253583 best: 11217.3845770 (151) total: 343ms remaining: 437ms 154: learn: 5514.9416274 test: 11220.3498111 best: 11217.3845770 (151) total: 345ms remaining: 434ms 155: learn: 5486.3904444 test: 11248.9563568 best: 11217.3845770 (151) total: 347ms remaining: 431ms 156: learn: 5459.6582728 test: 11253.0840394 best: 11217.3845770 (151) total: 349ms remaining: 429ms 157: learn: 5453.0365582 test: 11261.2209210 best: 11217.3845770 (151) total: 351ms remaining: 426ms 158: learn: 5431.6248093 test: 11292.0268146 best: 11217.3845770 (151) total: 353ms remaining: 424ms 159: learn: 5411.1650386 test: 11279.2833878 best: 11217.3845770 (151) total: 355ms remaining: 421ms 160: learn: 5405.2679492 test: 11282.4190366 best: 11217.3845770 (151) total: 356ms remaining: 418ms 161: learn: 5398.5188363 test: 11305.1895532 best: 11217.3845770 (151) total: 358ms remaining: 416ms 162: learn: 5386.8632157 test: 11307.5631627 best: 11217.3845770 (151) total: 360ms remaining: 413ms 163: learn: 5380.9484325 test: 11309.8313397 best: 11217.3845770 (151) total: 362ms remaining: 411ms 164: learn: 5374.9944077 test: 11314.7756285 best: 11217.3845770 (151) total: 364ms remaining: 408ms 165: learn: 5369.2684357 test: 11317.7166111 best: 11217.3845770 (151) total: 370ms remaining: 410ms 166: learn: 5348.1885757 test: 11306.7604358 best: 11217.3845770 (151) total: 374ms remaining: 410ms 167: learn: 5342.9557177 test: 11330.8807662 best: 11217.3845770 (151) total: 379ms remaining: 410ms 168: learn: 5332.7038583 test: 11351.9978262 best: 11217.3845770 (151) total: 384ms remaining: 412ms 169: learn: 5324.6191887 test: 11381.2839728 best: 11217.3845770 (151) total: 387ms remaining: 409ms 170: learn: 5301.7532160 test: 11376.8468749 best: 11217.3845770 (151) total: 388ms remaining: 406ms 171: learn: 5280.6227728 test: 11371.9060191 best: 11217.3845770 (151) total: 390ms remaining: 404ms 172: learn: 5249.5978709 test: 11355.7758113 best: 11217.3845770 (151) total: 392ms remaining: 401ms 173: learn: 5243.6922362 test: 11370.5042031 best: 11217.3845770 (151) total: 393ms remaining: 398ms 174: learn: 5238.3698585 test: 11371.9027493 best: 11217.3845770 (151) total: 396ms remaining: 396ms 175: learn: 5218.8812528 test: 11337.5976714 best: 11217.3845770 (151) total: 398ms remaining: 393ms 176: learn: 5212.6763045 test: 11340.3210754 best: 11217.3845770 (151) total: 400ms remaining: 391ms 177: learn: 5192.4113611 test: 11321.1915939 best: 11217.3845770 (151) total: 402ms remaining: 388ms 178: learn: 5165.1957478 test: 11345.6384125 best: 11217.3845770 (151) total: 403ms remaining: 385ms 179: learn: 5152.0138803 test: 11319.9523046 best: 11217.3845770 (151) total: 405ms remaining: 383ms 180: learn: 5148.3169478 test: 11340.9384127 best: 11217.3845770 (151) total: 407ms remaining: 380ms 181: learn: 5135.1282273 test: 11329.3411890 best: 11217.3845770 (151) total: 409ms remaining: 378ms 182: learn: 5126.9817522 test: 11308.0619158 best: 11217.3845770 (151) total: 411ms remaining: 375ms 183: learn: 5122.4391371 test: 11306.1004758 best: 11217.3845770 (151) total: 413ms remaining: 372ms 184: learn: 5111.1571313 test: 11299.6277768 best: 11217.3845770 (151) total: 415ms remaining: 370ms 185: learn: 5106.6119932 test: 11304.2056576 best: 11217.3845770 (151) total: 416ms remaining: 367ms 186: learn: 5101.8798146 test: 11304.9437197 best: 11217.3845770 (151) total: 418ms remaining: 365ms 187: learn: 5086.1946923 test: 11277.1126330 best: 11217.3845770 (151) total: 420ms remaining: 362ms 188: learn: 5080.7685585 test: 11279.0309546 best: 11217.3845770 (151) total: 422ms remaining: 360ms 189: learn: 5075.2646230 test: 11307.0600965 best: 11217.3845770 (151) total: 424ms remaining: 357ms 190: learn: 5058.4400927 test: 11280.9791033 best: 11217.3845770 (151) total: 426ms remaining: 355ms 191: learn: 5026.4792163 test: 11247.7744407 best: 11217.3845770 (151) total: 428ms remaining: 352ms 192: learn: 5024.3905979 test: 11252.0319557 best: 11217.3845770 (151) total: 430ms remaining: 350ms 193: learn: 5018.8277580 test: 11279.0368336 best: 11217.3845770 (151) total: 432ms remaining: 347ms 194: learn: 5009.0826905 test: 11265.3622353 best: 11217.3845770 (151) total: 433ms remaining: 345ms 195: learn: 4998.4163412 test: 11258.4905326 best: 11217.3845770 (151) total: 435ms remaining: 342ms 196: learn: 4988.4809745 test: 11268.3903502 best: 11217.3845770 (151) total: 437ms remaining: 340ms 197: learn: 4981.9604207 test: 11261.5335676 best: 11217.3845770 (151) total: 439ms remaining: 337ms 198: learn: 4958.1366062 test: 11259.0273112 best: 11217.3845770 (151) total: 441ms remaining: 335ms 199: learn: 4935.1739343 test: 11259.3673067 best: 11217.3845770 (151) total: 443ms remaining: 332ms 200: learn: 4912.2448367 test: 11263.9199405 best: 11217.3845770 (151) total: 445ms remaining: 330ms 201: learn: 4910.7488082 test: 11266.9851191 best: 11217.3845770 (151) total: 447ms remaining: 327ms 202: learn: 4907.4930782 test: 11276.8484399 best: 11217.3845770 (151) total: 449ms remaining: 325ms 203: learn: 4881.1773014 test: 11262.5153063 best: 11217.3845770 (151) total: 451ms remaining: 322ms 204: learn: 4850.4400595 test: 11250.6168310 best: 11217.3845770 (151) total: 453ms remaining: 320ms 205: learn: 4844.4621574 test: 11273.9660398 best: 11217.3845770 (151) total: 454ms remaining: 318ms 206: learn: 4836.4814754 test: 11283.4186092 best: 11217.3845770 (151) total: 456ms remaining: 315ms 207: learn: 4832.5050845 test: 11298.6013674 best: 11217.3845770 (151) total: 458ms remaining: 313ms 208: learn: 4829.4215371 test: 11322.0488051 best: 11217.3845770 (151) total: 460ms remaining: 311ms 209: learn: 4825.8194739 test: 11327.1665296 best: 11217.3845770 (151) total: 462ms remaining: 308ms 210: learn: 4802.5248737 test: 11320.8811560 best: 11217.3845770 (151) total: 464ms remaining: 306ms 211: learn: 4790.6311967 test: 11305.4137115 best: 11217.3845770 (151) total: 466ms remaining: 303ms 212: learn: 4787.5141135 test: 11306.0555777 best: 11217.3845770 (151) total: 468ms remaining: 301ms 213: learn: 4784.2502874 test: 11301.2551408 best: 11217.3845770 (151) total: 470ms remaining: 299ms 214: learn: 4773.9583344 test: 11304.9283455 best: 11217.3845770 (151) total: 472ms remaining: 296ms 215: learn: 4763.5916856 test: 11316.1945513 best: 11217.3845770 (151) total: 474ms remaining: 294ms 216: learn: 4752.1572299 test: 11317.8718730 best: 11217.3845770 (151) total: 476ms remaining: 292ms 217: learn: 4749.0748619 test: 11320.5258287 best: 11217.3845770 (151) total: 478ms remaining: 289ms 218: learn: 4733.7436633 test: 11338.8559356 best: 11217.3845770 (151) total: 479ms remaining: 287ms 219: learn: 4730.1573490 test: 11335.5745204 best: 11217.3845770 (151) total: 481ms remaining: 284ms 220: learn: 4726.8369859 test: 11332.2083216 best: 11217.3845770 (151) total: 483ms remaining: 282ms 221: learn: 4701.9867835 test: 11316.9923234 best: 11217.3845770 (151) total: 485ms remaining: 280ms 222: learn: 4698.8793257 test: 11319.4713981 best: 11217.3845770 (151) total: 487ms remaining: 277ms 223: learn: 4673.8174221 test: 11315.8403057 best: 11217.3845770 (151) total: 489ms remaining: 275ms 224: learn: 4657.5539004 test: 11304.3286985 best: 11217.3845770 (151) total: 491ms remaining: 273ms 225: learn: 4654.7466955 test: 11315.5115282 best: 11217.3845770 (151) total: 493ms remaining: 270ms 226: learn: 4631.6805106 test: 11296.2384678 best: 11217.3845770 (151) total: 495ms remaining: 268ms 227: learn: 4614.7281809 test: 11289.4770537 best: 11217.3845770 (151) total: 497ms remaining: 266ms 228: learn: 4586.5260978 test: 11241.1460505 best: 11217.3845770 (151) total: 499ms remaining: 264ms 229: learn: 4584.9325962 test: 11244.8050475 best: 11217.3845770 (151) total: 503ms remaining: 262ms 230: learn: 4557.8820441 test: 11243.3910978 best: 11217.3845770 (151) total: 505ms remaining: 260ms 231: learn: 4542.0074111 test: 11237.3145889 best: 11217.3845770 (151) total: 507ms remaining: 258ms 232: learn: 4515.5471825 test: 11216.7664856 best: 11216.7664856 (232) total: 509ms remaining: 255ms 233: learn: 4512.6322706 test: 11216.3156896 best: 11216.3156896 (233) total: 510ms remaining: 253ms 234: learn: 4509.7562342 test: 11213.2880054 best: 11213.2880054 (234) total: 512ms remaining: 251ms 235: learn: 4491.7667746 test: 11193.5710432 best: 11193.5710432 (235) total: 514ms remaining: 248ms 236: learn: 4484.3687721 test: 11195.1095283 best: 11193.5710432 (235) total: 516ms remaining: 246ms 237: learn: 4481.5577602 test: 11192.1579664 best: 11192.1579664 (237) total: 518ms remaining: 244ms 238: learn: 4469.4620421 test: 11198.3841506 best: 11192.1579664 (237) total: 520ms remaining: 241ms 239: learn: 4451.0717792 test: 11192.9916614 best: 11192.1579664 (237) total: 522ms remaining: 239ms 240: learn: 4449.3722323 test: 11206.8573286 best: 11192.1579664 (237) total: 524ms remaining: 237ms 241: learn: 4441.2330049 test: 11179.7486944 best: 11179.7486944 (241) total: 525ms remaining: 235ms 242: learn: 4437.1796274 test: 11174.1047117 best: 11174.1047117 (242) total: 527ms remaining: 232ms 243: learn: 4419.7583292 test: 11169.9416981 best: 11169.9416981 (243) total: 529ms remaining: 230ms 244: learn: 4391.7903344 test: 11179.9738033 best: 11169.9416981 (243) total: 531ms remaining: 228ms 245: learn: 4388.4466909 test: 11177.1314470 best: 11169.9416981 (243) total: 533ms remaining: 225ms 246: learn: 4377.8549189 test: 11179.2111268 best: 11169.9416981 (243) total: 535ms remaining: 223ms 247: learn: 4351.7921983 test: 11160.8634441 best: 11160.8634441 (247) total: 537ms remaining: 221ms 248: learn: 4336.5431509 test: 11150.9767147 best: 11150.9767147 (248) total: 539ms remaining: 219ms 249: learn: 4333.6988882 test: 11150.5536735 best: 11150.5536735 (249) total: 541ms remaining: 216ms 250: learn: 4331.0502963 test: 11145.6717176 best: 11145.6717176 (250) total: 543ms remaining: 214ms 251: learn: 4317.0644927 test: 11153.6143317 best: 11145.6717176 (250) total: 545ms remaining: 212ms 252: learn: 4302.3809874 test: 11152.6692665 best: 11145.6717176 (250) total: 547ms remaining: 210ms 253: learn: 4293.9413046 test: 11156.2096158 best: 11145.6717176 (250) total: 550ms remaining: 208ms 254: learn: 4274.0307903 test: 11147.6722416 best: 11145.6717176 (250) total: 554ms remaining: 206ms 255: learn: 4264.7714270 test: 11149.4718909 best: 11145.6717176 (250) total: 557ms remaining: 205ms 256: learn: 4243.0647936 test: 11117.7158886 best: 11117.7158886 (256) total: 559ms remaining: 202ms 257: learn: 4239.8997264 test: 11124.0696208 best: 11117.7158886 (256) total: 561ms remaining: 200ms 258: learn: 4234.3710565 test: 11120.5835860 best: 11117.7158886 (256) total: 562ms remaining: 198ms 259: learn: 4232.6601417 test: 11132.8773509 best: 11117.7158886 (256) total: 564ms remaining: 195ms 260: learn: 4231.8468802 test: 11133.1267837 best: 11117.7158886 (256) total: 566ms remaining: 193ms 261: learn: 4215.9008069 test: 11159.3746263 best: 11117.7158886 (256) total: 567ms remaining: 191ms 262: learn: 4213.3508811 test: 11156.4498941 best: 11117.7158886 (256) total: 569ms remaining: 188ms 263: learn: 4210.9710346 test: 11156.4284133 best: 11117.7158886 (256) total: 570ms remaining: 186ms 264: learn: 4209.7069265 test: 11159.5898158 best: 11117.7158886 (256) total: 572ms remaining: 184ms 265: learn: 4207.3221734 test: 11157.7019257 best: 11117.7158886 (256) total: 574ms remaining: 181ms 266: learn: 4206.1017516 test: 11169.6108494 best: 11117.7158886 (256) total: 576ms remaining: 179ms 267: learn: 4203.7169391 test: 11175.1660806 best: 11117.7158886 (256) total: 578ms remaining: 177ms 268: learn: 4195.5874755 test: 11184.0247289 best: 11117.7158886 (256) total: 580ms remaining: 175ms 269: learn: 4184.8335015 test: 11172.7308362 best: 11117.7158886 (256) total: 582ms remaining: 172ms 270: learn: 4177.8734077 test: 11188.4057674 best: 11117.7158886 (256) total: 584ms remaining: 170ms 271: learn: 4157.4959472 test: 11179.9129799 best: 11117.7158886 (256) total: 586ms remaining: 168ms 272: learn: 4156.9540212 test: 11180.3702512 best: 11117.7158886 (256) total: 588ms remaining: 166ms 273: learn: 4143.7896961 test: 11184.8773503 best: 11117.7158886 (256) total: 590ms remaining: 164ms 274: learn: 4132.4053303 test: 11151.5323451 best: 11117.7158886 (256) total: 592ms remaining: 161ms 275: learn: 4114.4820143 test: 11142.7700991 best: 11117.7158886 (256) total: 594ms remaining: 159ms 276: learn: 4109.7489917 test: 11142.4095117 best: 11117.7158886 (256) total: 596ms remaining: 157ms 277: learn: 4086.0810560 test: 11147.6898690 best: 11117.7158886 (256) total: 598ms remaining: 155ms 278: learn: 4076.7625709 test: 11128.6953597 best: 11117.7158886 (256) total: 600ms remaining: 153ms 279: learn: 4056.4945293 test: 11136.9416201 best: 11117.7158886 (256) total: 602ms remaining: 150ms 280: learn: 4041.0888929 test: 11144.7219024 best: 11117.7158886 (256) total: 604ms remaining: 148ms 281: learn: 4038.7209104 test: 11144.1126310 best: 11117.7158886 (256) total: 606ms remaining: 146ms 282: learn: 4020.5249892 test: 11123.4256169 best: 11117.7158886 (256) total: 608ms remaining: 144ms 283: learn: 4018.2835065 test: 11119.2106257 best: 11117.7158886 (256) total: 610ms remaining: 142ms 284: learn: 3996.4719522 test: 11128.6651536 best: 11117.7158886 (256) total: 612ms remaining: 140ms 285: learn: 3989.6374669 test: 11125.5853418 best: 11117.7158886 (256) total: 614ms remaining: 137ms 286: learn: 3987.5170214 test: 11125.4521475 best: 11117.7158886 (256) total: 616ms remaining: 135ms 287: learn: 3985.5155312 test: 11123.7047925 best: 11117.7158886 (256) total: 618ms remaining: 133ms 288: learn: 3975.8310134 test: 11119.0639091 best: 11117.7158886 (256) total: 620ms remaining: 131ms 289: learn: 3960.6134141 test: 11118.9738590 best: 11117.7158886 (256) total: 622ms remaining: 129ms 290: learn: 3958.5483294 test: 11119.5704132 best: 11117.7158886 (256) total: 624ms remaining: 126ms 291: learn: 3927.8401098 test: 11115.5685186 best: 11115.5685186 (291) total: 625ms remaining: 124ms 292: learn: 3927.1762569 test: 11115.8576280 best: 11115.5685186 (291) total: 627ms remaining: 122ms 293: learn: 3920.2206482 test: 11129.8208267 best: 11115.5685186 (291) total: 629ms remaining: 120ms 294: learn: 3913.1164129 test: 11141.1618450 best: 11115.5685186 (291) total: 631ms remaining: 118ms 295: learn: 3907.8871760 test: 11142.2310718 best: 11115.5685186 (291) total: 633ms remaining: 115ms 296: learn: 3884.6281492 test: 11138.2615552 best: 11115.5685186 (291) total: 635ms remaining: 113ms 297: learn: 3872.6331595 test: 11151.0927533 best: 11115.5685186 (291) total: 637ms remaining: 111ms 298: learn: 3869.9600066 test: 11151.6520410 best: 11115.5685186 (291) total: 639ms remaining: 109ms 299: learn: 3867.8766768 test: 11153.2629576 best: 11115.5685186 (291) total: 641ms remaining: 107ms 300: learn: 3852.3414416 test: 11160.4416668 best: 11115.5685186 (291) total: 643ms remaining: 105ms 301: learn: 3850.5380922 test: 11158.5270778 best: 11115.5685186 (291) total: 644ms remaining: 102ms 302: learn: 3848.7326305 test: 11156.5901234 best: 11115.5685186 (291) total: 646ms remaining: 100ms 303: learn: 3841.6326178 test: 11155.7792283 best: 11115.5685186 (291) total: 648ms remaining: 98.1ms 304: learn: 3839.7737620 test: 11162.7086305 best: 11115.5685186 (291) total: 650ms remaining: 95.9ms 305: learn: 3819.8619772 test: 11161.9707476 best: 11115.5685186 (291) total: 652ms remaining: 93.7ms 306: learn: 3814.2478011 test: 11159.8794393 best: 11115.5685186 (291) total: 654ms remaining: 91.5ms 307: learn: 3798.1269860 test: 11145.6633336 best: 11115.5685186 (291) total: 655ms remaining: 89.4ms 308: learn: 3789.1580846 test: 11140.5376528 best: 11115.5685186 (291) total: 657ms remaining: 87.2ms 309: learn: 3788.1572165 test: 11149.9750723 best: 11115.5685186 (291) total: 659ms remaining: 85ms 310: learn: 3780.9809807 test: 11156.7199684 best: 11115.5685186 (291) total: 661ms remaining: 82.9ms 311: learn: 3779.3967882 test: 11154.4925098 best: 11115.5685186 (291) total: 663ms remaining: 80.7ms 312: learn: 3778.8434356 test: 11156.9381451 best: 11115.5685186 (291) total: 665ms remaining: 78.6ms 313: learn: 3754.1904985 test: 11166.5927045 best: 11115.5685186 (291) total: 667ms remaining: 76.4ms 314: learn: 3746.9058142 test: 11167.1505381 best: 11115.5685186 (291) total: 668ms remaining: 74.3ms 315: learn: 3742.7888348 test: 11164.6618158 best: 11115.5685186 (291) total: 670ms remaining: 72.1ms 316: learn: 3741.5850666 test: 11165.6765402 best: 11115.5685186 (291) total: 672ms remaining: 70ms 317: learn: 3739.1867965 test: 11165.0672179 best: 11115.5685186 (291) total: 674ms remaining: 67.8ms 318: learn: 3716.2087760 test: 11151.1741438 best: 11115.5685186 (291) total: 676ms remaining: 65.7ms 319: learn: 3707.7713686 test: 11152.4671251 best: 11115.5685186 (291) total: 678ms remaining: 63.5ms 320: learn: 3707.1968615 test: 11156.5138372 best: 11115.5685186 (291) total: 680ms remaining: 61.4ms 321: learn: 3695.4278384 test: 11155.0402510 best: 11115.5685186 (291) total: 682ms remaining: 59.3ms 322: learn: 3693.8822321 test: 11152.9118578 best: 11115.5685186 (291) total: 683ms remaining: 57.1ms 323: learn: 3676.2222030 test: 11153.8511311 best: 11115.5685186 (291) total: 685ms remaining: 55ms 324: learn: 3674.8648463 test: 11155.1028185 best: 11115.5685186 (291) total: 687ms remaining: 52.9ms 325: learn: 3670.4011825 test: 11174.8592945 best: 11115.5685186 (291) total: 689ms remaining: 50.7ms 326: learn: 3654.1157274 test: 11172.1060753 best: 11115.5685186 (291) total: 691ms remaining: 48.6ms 327: learn: 3652.5345779 test: 11168.1338515 best: 11115.5685186 (291) total: 693ms remaining: 46.5ms 328: learn: 3644.5853526 test: 11148.7974124 best: 11115.5685186 (291) total: 695ms remaining: 44.3ms 329: learn: 3636.9456832 test: 11155.7153123 best: 11115.5685186 (291) total: 696ms remaining: 42.2ms 330: learn: 3635.4029672 test: 11153.8305403 best: 11115.5685186 (291) total: 698ms remaining: 40.1ms 331: learn: 3627.0917021 test: 11148.7550708 best: 11115.5685186 (291) total: 702ms remaining: 38ms 332: learn: 3613.6988346 test: 11162.7547588 best: 11115.5685186 (291) total: 704ms remaining: 35.9ms 333: learn: 3592.3631789 test: 11152.4853014 best: 11115.5685186 (291) total: 710ms remaining: 34ms 334: learn: 3590.9761954 test: 11150.7431798 best: 11115.5685186 (291) total: 712ms remaining: 31.9ms 335: learn: 3573.5575740 test: 11141.1513988 best: 11115.5685186 (291) total: 713ms remaining: 29.7ms 336: learn: 3552.5682555 test: 11125.3076532 best: 11115.5685186 (291) total: 715ms remaining: 27.6ms 337: learn: 3546.3790349 test: 11125.3205584 best: 11115.5685186 (291) total: 717ms remaining: 25.5ms 338: learn: 3524.8618962 test: 11114.1704724 best: 11114.1704724 (338) total: 719ms remaining: 23.3ms 339: learn: 3514.2508009 test: 11111.6971743 best: 11111.6971743 (339) total: 721ms remaining: 21.2ms 340: learn: 3500.2117681 test: 11100.3674694 best: 11100.3674694 (340) total: 723ms remaining: 19.1ms 341: learn: 3491.9864233 test: 11114.8569037 best: 11100.3674694 (340) total: 725ms remaining: 17ms 342: learn: 3479.7447327 test: 11127.7210010 best: 11100.3674694 (340) total: 727ms remaining: 14.8ms 343: learn: 3462.0890939 test: 11113.4417548 best: 11100.3674694 (340) total: 728ms remaining: 12.7ms 344: learn: 3460.9342183 test: 11114.8940474 best: 11100.3674694 (340) total: 730ms remaining: 10.6ms 345: learn: 3452.9434403 test: 11112.1108160 best: 11100.3674694 (340) total: 732ms remaining: 8.46ms 346: learn: 3451.5283245 test: 11110.0147764 best: 11100.3674694 (340) total: 734ms remaining: 6.35ms 347: learn: 3439.1063605 test: 11123.8831229 best: 11100.3674694 (340) total: 740ms remaining: 4.25ms 348: learn: 3437.5201086 test: 11128.9481767 best: 11100.3674694 (340) total: 744ms remaining: 2.13ms 349: learn: 3430.4820146 test: 11128.4193042 best: 11100.3674694 (340) total: 746ms remaining: 0us bestTest = 11100.36747 bestIteration = 340 Shrink model to first 341 iterations. . NMAE(y_valid, y_pred3) . 0.10494460420195155 . y_pred3 = cat.predict(X_test) . sample_sub[&#39;number_of_rentals&#39;] = y_pred3 sample_sub . date_time number_of_rentals . 0 2021-04-01 | 73466.458943 | . 1 2021-04-02 | 61450.624526 | . 2 2021-04-03 | 56936.172445 | . 3 2021-04-04 | 51232.051269 | . 4 2021-04-05 | 75850.945544 | . ... ... | ... | . 86 2021-06-26 | 71683.058010 | . 87 2021-06-27 | 95431.866968 | . 88 2021-06-28 | 91140.601128 | . 89 2021-06-29 | 87974.427154 | . 90 2021-06-30 | 78840.377887 | . 91 rows × 2 columns . sample_sub.to_csv(&#39;submission.csv&#39;, index=False, encoding=&#39;cp949&#39;) . f_pred = (y_pred + y_pred2 + y_pred3) / 3 . NMAE(y_valid, f_pred) . 0.13628047278111174 . sample_sub[&#39;number_of_rentals&#39;] = f_pred sample_sub . date_time number_of_rentals . 0 2021-04-01 | 70857.280015 | . 1 2021-04-02 | 53827.393733 | . 2 2021-04-03 | 54091.414599 | . 3 2021-04-04 | 46248.006629 | . 4 2021-04-05 | 75992.571841 | . ... ... | ... | . 86 2021-06-26 | 78346.102180 | . 87 2021-06-27 | 91920.414965 | . 88 2021-06-28 | 91907.462463 | . 89 2021-06-29 | 90440.518272 | . 90 2021-06-30 | 88038.135333 | . 91 rows × 2 columns . sns.distplot(y_train, kde = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faadf22c150&gt; . sns.distplot(y_pred2, kde = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faae0987e50&gt; . sns.distplot(y_pred, kde = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faae09056d0&gt; . sns.distplot(y_pred3, kde = True) . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7faae0883490&gt; .",
            "url": "https://raukrauk.github.io/ML-DL/ssuda/machinelearning/dacon/2021/11/06/I_want_to_ride_my_bicycle.html",
            "relUrl": "/ssuda/machinelearning/dacon/2021/11/06/I_want_to_ride_my_bicycle.html",
            "date": " • Nov 6, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "[Kaggle] Predict Future Sales",
            "content": "File descriptions . sales_train.csv - the training set. Daily historical data from January 2013 to October 2015. [2013년 1월 ~ 2015년 10월 일간데이터 훈련용] . | test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.[2015년 11월 가게,상품의 판매량을 예측] . | sample_submission.csv - a sample submission file in the correct format. [제출예제] - (shop_id, item_id) 별로 월간 판매량을 예측해야한다. | . items.csv - supplemental information about the items/products. [상품에 대한 추가정보] | item_categories.csv - supplemental information about the items categories. [상품 카테고리에 대한 추가정보] | shops.csv- supplemental information about the shops. [가게에 대한 추가정보] | . Data fields . ID - an Id that represents a (Shop, Item) tuple within the test set | shop_id - unique identifier of a shop | item_id - unique identifier of a product | item_category_id - unique identifier of item category | item_cnt_day - number of products sold. You are predicting a monthly amount of this measure | item_price - current price of an item | date - date in format dd/mm/yyyy | date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33 | item_name - name of item | shop_name - name of shop | item_category_name - name of item category | This dataset is permitted to be used for any purpose, including commercial use. | . from google.colab import files files.upload() !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 /root/.kaggle/kaggle.json . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . !kaggle competitions download -c competitive-data-science-predict-future-sales . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading sales_train.csv.zip to /content 38% 5.00M/13.3M [00:00&lt;00:00, 29.9MB/s] 100% 13.3M/13.3M [00:00&lt;00:00, 52.6MB/s] Downloading items.csv.zip to /content 0% 0.00/368k [00:00&lt;?, ?B/s] 100% 368k/368k [00:00&lt;00:00, 106MB/s] Downloading item_categories.csv to /content 0% 0.00/3.49k [00:00&lt;?, ?B/s] 100% 3.49k/3.49k [00:00&lt;00:00, 2.87MB/s] Downloading sample_submission.csv.zip to /content 0% 0.00/468k [00:00&lt;?, ?B/s] 100% 468k/468k [00:00&lt;00:00, 144MB/s] Downloading shops.csv to /content 0% 0.00/2.91k [00:00&lt;?, ?B/s] 100% 2.91k/2.91k [00:00&lt;00:00, 7.73MB/s] Downloading test.csv.zip to /content 0% 0.00/1.02M [00:00&lt;?, ?B/s] 100% 1.02M/1.02M [00:00&lt;00:00, 67.9MB/s] . !unzip items.csv.zip !unzip sales_train.csv.zip !unzip sample_submission.csv.zip !unzip test.csv.zip . Archive: items.csv.zip inflating: items.csv Archive: sales_train.csv.zip inflating: sales_train.csv Archive: sample_submission.csv.zip inflating: sample_submission.csv Archive: test.csv.zip inflating: test.csv . import numpy as np import pandas as pd import matplotlib.pyplot as plt from datetime import datetime import warnings warnings.filterwarnings(&quot;ignore&quot;) . sales_train = pd.read_csv(&#39;sales_train.csv&#39;) sales_test = pd.read_csv(&#39;test.csv&#39;) . items = pd.read_csv(&#39;items.csv&#39;) item_categories = pd.read_csv(&#39;item_categories.csv&#39;) shops = pd.read_csv(&#39;shops.csv&#39;) . &#45216;&#51676; &#54805;&#49885; &#48320;&#44221; . sales_train[&quot;date&quot;] = sales_train[&quot;date&quot;].apply(lambda x: datetime.strptime(x, &#39;%d.%m.%Y&#39;)) . sales_train.sort_values(&quot;date&quot;, inplace = True) sales_train.reset_index(drop=True, inplace = True) sales_train . date date_block_num shop_id item_id item_price item_cnt_day . 0 2013-01-01 | 0 | 18 | 5823 | 2500.0 | 1.0 | . 1 2013-01-01 | 0 | 27 | 5573 | 849.0 | 1.0 | . 2 2013-01-01 | 0 | 7 | 1006 | 399.0 | 1.0 | . 3 2013-01-01 | 0 | 19 | 17707 | 899.0 | 1.0 | . 4 2013-01-01 | 0 | 14 | 19548 | 149.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | . 2935844 2015-10-31 | 33 | 41 | 21386 | 169.0 | 1.0 | . 2935845 2015-10-31 | 33 | 21 | 988 | 199.0 | 1.0 | . 2935846 2015-10-31 | 33 | 41 | 21377 | 169.0 | 1.0 | . 2935847 2015-10-31 | 33 | 22 | 10207 | 1199.0 | 1.0 | . 2935848 2015-10-31 | 33 | 24 | 3042 | 3199.0 | 1.0 | . 2935849 rows × 6 columns . &#44208;&#52769;&#44050;&#51008; &#51316;&#51116;&#54616;&#51648; &#50506;&#51020; . sales_train.isna().sum() . date 0 date_block_num 0 shop_id 0 item_id 0 item_price 0 item_cnt_day 0 dtype: int64 . sales_train.describe() . date_block_num shop_id item_id item_price item_cnt_day . count 2.935849e+06 | 2.935849e+06 | 2.935849e+06 | 2.935849e+06 | 2.935849e+06 | . mean 1.456991e+01 | 3.300173e+01 | 1.019723e+04 | 8.908532e+02 | 1.242641e+00 | . std 9.422988e+00 | 1.622697e+01 | 6.324297e+03 | 1.729800e+03 | 2.618834e+00 | . min 0.000000e+00 | 0.000000e+00 | 0.000000e+00 | -1.000000e+00 | -2.200000e+01 | . 25% 7.000000e+00 | 2.200000e+01 | 4.476000e+03 | 2.490000e+02 | 1.000000e+00 | . 50% 1.400000e+01 | 3.100000e+01 | 9.343000e+03 | 3.990000e+02 | 1.000000e+00 | . 75% 2.300000e+01 | 4.700000e+01 | 1.568400e+04 | 9.990000e+02 | 1.000000e+00 | . max 3.300000e+01 | 5.900000e+01 | 2.216900e+04 | 3.079800e+05 | 2.169000e+03 | . &#51473;&#48373;&#46108; &#45936;&#51060;&#53552;&#44032; &#51080;&#45716;&#51648; &#54869;&#51064;&#54620;&#45796;! - &#51228;&#44144;&#54620;&#45796;. . 6개의 중복된 데이터가 존재한다. | . duplicate = sales_train[sales_train.duplicated()] duplicate . date date_block_num shop_id item_id item_price item_cnt_day . 23142 2013-01-05 | 0 | 54 | 20130 | 149.0 | 1.0 | . 1441022 2014-02-23 | 13 | 50 | 3423 | 999.0 | 1.0 | . 1525377 2014-03-23 | 14 | 21 | 3423 | 999.0 | 1.0 | . 1628996 2014-05-01 | 16 | 50 | 3423 | 999.0 | 1.0 | . 1816729 2014-07-12 | 18 | 25 | 3423 | 999.0 | 1.0 | . 2321620 2014-12-31 | 23 | 42 | 21619 | 499.0 | 1.0 | . sales_train.drop(duplicate.index, inplace = True) ## 제거한다. . &#51060;&#49345;&#52824;&#47196; &#54032;&#45800;&#46104;&#45716; &#44163;&#51012; &#51228;&#44144;&#54620;&#45796;. - &#51228;&#44144;&#50504;&#54632;! . Kaggle의 평가기준을 확인해 보면 최종 결과값으로 (0, 20) 사이의 값므로 변환하여 제출 하라고 함. why? - 튀는 값들이 있는데 RMSE가 튀는 값들에 의해 커질 우려가 있어서 임의로 정한 것 같음 | 훈련시 부터 target 값들을 np.clip(target, 0, 20)으로 둔다면 이상치의 영향을 크게 받지 않을 것이라고 판단하여 | . import seaborn as sns sns.boxplot(sales_train.item_price) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9e8450ef90&gt; . import seaborn as sns sns.boxplot(sales_train.item_cnt_day) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f9e73f25610&gt; . def outlier(df=None, column=None, weight=1.5): quantile_25 = np.percentile(df[column].values, 25) quantile_75 = np.percentile(df[column].values, 75) IQR = quantile_75 - quantile_25 IQR_weight = IQR*weight lowest = quantile_25 - IQR_weight highest = quantile_75 + IQR_weight outlier_idx = df[column][ (df[column] &lt; lowest) | (df[column] &gt; highest) ].index return outlier_idx #out_index1 = outlier(df=sales_train, column = &quot;item_cnt_day&quot;,weight = 1.5) #out_index2 = outlier(df=sales_train, column = &quot;item_price&quot;,weight = 1.5) #out_idx = set(out_index1).union(set(out_index2)) # 이상치를 제거한 데이터 #n1 = sales_train.drop(out_idx, axis=0) . #out_index2 = outlier(df=sales_train, column = &quot;item_price&quot;,weight = 3) #out_idx = set(out_index1).union(set(out_index2)) . #print(&quot;item_price 이상치의 수 :&quot;,len(out_index2)) #print(&quot;최종적으로 제거되는 이상치의 수 :&quot;, len(out_idx)) # 이상치로 판정되는 것들 . item_cnt_day 이상치의 수: 306477 item_price 이상치의 수 : 78853 최종적으로 제거되는 이상치의 수 : 374314 . . item_price&#50752; item_cnt_day&#50640; &#51020;&#49688;&#44050;&#51060; &#51316;&#51116;&#54620;&#45796;. . len(sales_train[sales_train[&quot;item_price&quot;] &lt;= 0]) # 단 1개 존재 . 1 . sales_train[sales_train[&quot;item_price&quot;] &lt;= 0] . date date_block_num shop_id item_id item_price item_cnt_day . 482092 2013-05-15 | 4 | 32 | 2973 | -1.0 | 1.0 | . sales_train.drop([482092], inplace = True) . len(sales_train[sales_train[&quot;item_price&quot;] &lt;= 0]) . 0 . &#54036;&#47536;&#44060;&#49688;&#44032; &#51020;&#49688;? &#47956; &#49548;&#47536;&#44032;? - &#44060;&#49688;&#47484; &#52628;&#51221;&#54616;&#45716; &#44163;&#51060;&#48064;&#47196; &#44536;&#45824;&#47196; &#45347;&#45716;&#44163;&#51060; &#51339;&#51648; &#50506;&#51012;&#44620;? . 해당 품목이 반품, 환불등으로 가게로 다시 돌아옴? | 가게에서 품목을 주문한 것이 더 들어옴? | 0으로 둔 모델 1 | 그냥 그대로 둔 모델 2 | . len(sales_train[sales_train[&quot;item_cnt_day&quot;] &lt; 0]) # 7356개 존재 . 7356 . test&#50640;&#45716; &#51316;&#51116;&#54616;&#44256;, train&#50640; &#51316;&#51116;&#54616;&#51648; &#50506;&#45716; shop&#51012; &#54869;&#51064;. - &#50630;&#51020; . set(sales_test.shop_id) - set(sales_train.shop_id) . set() . test&#50640;&#45716; &#51316;&#51116;&#54616;&#44256;, train&#50640; &#51316;&#51116;&#54616;&#51648; &#50506;&#45716; Item_id&#47484; &#54869;&#51064;. . 전혀 훈련되지 않았던 값이므로 예측을 못하는 경우 발생 - 이것땜에 모델에서 예측을 잘못하는 경우 있음 | 해당 ID들을 처리해줄 필요가 있어보임. How? - 판매가 이루어지지 않았던 품목이므로 판매량을 0으로 주던지... | . len(set(sales_test.item_id).difference(set(sales_train.item_id))) . 363 . Grouping - &#9733; . test셋에만 있는 item_id를 train에 넣어줄 예정 | . grouped = sales_train.groupby([&quot;date_block_num&quot;,&quot;shop_id&quot;,&quot;item_id&quot;]) grouped = grouped.agg({&#39;item_cnt_day&#39;:&#39;sum&#39;}).reset_index() . display(grouped) . date_block_num shop_id item_id item_cnt_day . 0 0 | 0 | 32 | 6.0 | . 1 0 | 0 | 33 | 3.0 | . 2 0 | 0 | 35 | 1.0 | . 3 0 | 0 | 43 | 1.0 | . 4 0 | 0 | 51 | 2.0 | . ... ... | ... | ... | ... | . 1609119 33 | 59 | 22087 | 6.0 | . 1609120 33 | 59 | 22088 | 2.0 | . 1609121 33 | 59 | 22091 | 1.0 | . 1609122 33 | 59 | 22100 | 1.0 | . 1609123 33 | 59 | 22102 | 1.0 | . 1609124 rows × 4 columns . sales_test &#45936;&#51060;&#53552;&#47484; grouped&#50752; &#50976;&#49324;&#54616;&#44172; &#47564;&#46304;&#45796;. . test셋에만 있는 item_id를 train에 넣어준다. | . sales_test = pd.read_csv(&#39;/content/drive/MyDrive/test.csv&#39;) . sales_test.columns = [&quot;date_block_num&quot;, &quot;shop_id&quot;, &quot;item_id&quot;] sales_test[&quot;date_block_num&quot;] = 34 sales_test.head() . date_block_num shop_id item_id . 0 34 | 5 | 5037 | . 1 34 | 5 | 5320 | . 2 34 | 5 | 5233 | . 3 34 | 5 | 5232 | . 4 34 | 5 | 5268 | . test&#49483;&#50640;&#47564; &#51080;&#45716; item_id&#47484; train&#50640; &#45347;&#45716; &#48169;&#48277; &#49892;&#54744; - &#49892;&#54665;&#54624; &#54596;&#50836; X . date_block_num이 1인 데이터들에 대해 shop_id와 item_id를 추출한다. (block_num에 따라 변동될것임) | sales_test set에 대해 shop_id와 item_id를 추출 (고정) | shop_id와 item_id 기준으로 outer조인을 한다. | 중복된 값이 없는지 확인한다. | [데이터가 원래 것에 비해 많이 커진다.] . y1 = grouped[grouped[&quot;date_block_num&quot;] == 1] y1 = y1[[&quot;shop_id&quot;, &quot;item_id&quot;]] y1 . shop_id item_id . 63224 0 | 30 | . 63225 0 | 31 | . 63226 0 | 32 | . 63227 0 | 33 | . 63228 0 | 35 | . ... ... | ... | . 123154 59 | 22020 | . 123155 59 | 22024 | . 123156 59 | 22087 | . 123157 59 | 22130 | . 123158 59 | 22134 | . 59935 rows × 2 columns . y2 = sales_test[[&quot;shop_id&quot;, &quot;item_id&quot;]] y2 . shop_id item_id . 0 5 | 5037 | . 1 5 | 5320 | . 2 5 | 5233 | . 3 5 | 5232 | . 4 5 | 5268 | . ... ... | ... | . 214195 45 | 18454 | . 214196 45 | 16188 | . 214197 45 | 15757 | . 214198 45 | 19648 | . 214199 45 | 969 | . 214200 rows × 2 columns . yy = pd.merge(y1, y2, on = [&quot;shop_id&quot;, &quot;item_id&quot;], how = &quot;outer&quot;) yy.drop_duplicates([&quot;shop_id&quot;, &quot;item_id&quot;]).sort_values(by = [&quot;shop_id&quot;, &quot;item_id&quot;]) . shop_id item_id . 0 0 | 30 | . 1 0 | 31 | . 2 0 | 32 | . 3 0 | 33 | . 4 0 | 35 | . ... ... | ... | . 203149 59 | 22162 | . 202405 59 | 22163 | . 202824 59 | 22164 | . 205811 59 | 22166 | . 202009 59 | 22167 | . 265912 rows × 2 columns . &#49352;&#47196;&#50868; &#45936;&#51060;&#53552; &#54532;&#47112;&#51076;&#51012; &#47564;&#46304;&#45796;. - test&#49483;&#51032; item_id&#44032; &#47784;&#46160; &#54252;&#54632;&#46104;&#46020;&#47197;! . training이 160만개 정도 | 모든 조합을 고려한다면 33 60 22170 = 4천만개 행이 만들어짐...(너무 많음) | 각 shop_id, block_num을 기반으로 train item_id에 test item_id가 없는 것만 넣을수 있을까? - 11489568, 천만개 정도...... | 더 나은 방법이 있을까? (60개 전체에 대해 했는데 test에 존재하는 shop들만 포함?) | . &#50948;&#50640;&#49436; &#49892;&#54744;&#54620; &#44163; for&#47928;&#51004;&#47196; &#46028;&#47548; . ddf = [] . st = sales_test[[&quot;shop_id&quot;, &quot;item_id&quot;]] for i in range(0, 34): tmp = grouped[grouped[&quot;date_block_num&quot;] == i] tmp = tmp[[&quot;shop_id&quot;, &quot;item_id&quot;]] yy = pd.merge(tmp, st, on = [&quot;shop_id&quot;, &quot;item_id&quot;], how = &quot;outer&quot;) tmp2 = np.array(yy.drop_duplicates([&quot;shop_id&quot;, &quot;item_id&quot;]).sort_values(by = [&quot;shop_id&quot;, &quot;item_id&quot;])) ddf.append(np.append(tmp2,np.repeat(i, len(tmp2)).reshape(len(tmp2), 1), axis = 1)) . ddf[3] . array([[ 2, 30, 3], [ 2, 31, 3], [ 2, 32, 3], ..., [ 59, 22164, 3], [ 59, 22166, 3], [ 59, 22167, 3]]) . &#52509; 8291765&#44060;&#47196; &#45936;&#51060;&#53552;&#44032; &#51201;&#51648;&#45716; &#50506;&#45796;... . tot = 0 for i in range(0, 34): tot += len(ddf[i]) tot . 8291765 . n_save = pd.DataFrame() . for i in range(0, 34): n_tt = pd.DataFrame(ddf[i], columns = [&quot;shop_id&quot;, &quot;item_id&quot;, &quot;date_block_num&quot;]) n_save = pd.concat([n_save, n_tt]) . n_save = n_save.reset_index() n_save.drop(&quot;index&quot;, axis = 1, inplace = True) n_save = n_save[[&quot;date_block_num&quot;, &quot;shop_id&quot;, &quot;item_id&quot;]] . n_save . date_block_num shop_id item_id . 0 0 | 0 | 32 | . 1 0 | 0 | 33 | . 2 0 | 0 | 35 | . 3 0 | 0 | 43 | . 4 0 | 0 | 51 | . ... ... | ... | ... | . 8291760 33 | 59 | 22162 | . 8291761 33 | 59 | 22163 | . 8291762 33 | 59 | 22164 | . 8291763 33 | 59 | 22166 | . 8291764 33 | 59 | 22167 | . 8291765 rows × 3 columns . test_data&#50752; &#54633;&#52840; . f_data = pd.concat([n_save,sales_test]) f_data = f_data.reset_index() f_data.drop(&quot;index&quot;, axis = 1, inplace = True) f_data . date_block_num shop_id item_id . 0 0 | 0 | 32 | . 1 0 | 0 | 33 | . 2 0 | 0 | 35 | . 3 0 | 0 | 43 | . 4 0 | 0 | 51 | . ... ... | ... | ... | . 8505960 34 | 45 | 18454 | . 8505961 34 | 45 | 16188 | . 8505962 34 | 45 | 15757 | . 8505963 34 | 45 | 19648 | . 8505964 34 | 45 | 969 | . 8505965 rows × 3 columns . f_data = pd.merge(f_data, grouped, on = [&quot;date_block_num&quot;, &quot;shop_id&quot;, &quot;item_id&quot;], how = &quot;left&quot;) . f_data . date_block_num shop_id item_id item_cnt_day . 0 0 | 0 | 32 | 6.0 | . 1 0 | 0 | 33 | 3.0 | . 2 0 | 0 | 35 | 1.0 | . 3 0 | 0 | 43 | 1.0 | . 4 0 | 0 | 51 | 2.0 | . ... ... | ... | ... | ... | . 8505960 34 | 45 | 18454 | NaN | . 8505961 34 | 45 | 16188 | NaN | . 8505962 34 | 45 | 15757 | NaN | . 8505963 34 | 45 | 19648 | NaN | . 8505964 34 | 45 | 969 | NaN | . 8505965 rows × 4 columns . &#45208;&#50728; Null&#44050;&#46308;&#51008; &#44032;&#44201;&#50640; &#45824;&#54620; &#51221;&#48372;&#44032; &#50630;&#51020;. . f_data.isna().sum() . date_block_num 0 shop_id 0 item_id 0 item_cnt_day 6896841 dtype: int64 . Null&#51012; 0&#51004;&#47196; &#52292;&#50892;&#51468; . f_data = f_data.fillna(0) . &#52572;&#51333;&#54805;&#53468; . f_data . date_block_num shop_id item_id item_cnt_day . 0 0 | 0 | 32 | 6.0 | . 1 0 | 0 | 33 | 3.0 | . 2 0 | 0 | 35 | 1.0 | . 3 0 | 0 | 43 | 1.0 | . 4 0 | 0 | 51 | 2.0 | . ... ... | ... | ... | ... | . 8505960 34 | 45 | 18454 | 0.0 | . 8505961 34 | 45 | 16188 | 0.0 | . 8505962 34 | 45 | 15757 | 0.0 | . 8505963 34 | 45 | 19648 | 0.0 | . 8505964 34 | 45 | 969 | 0.0 | . 8505965 rows × 4 columns . &#51200;&#51109;&#54620;&#45796;! . . f_data = pd.read_csv(&#39;/content/drive/MyDrive/f_data.csv&#39;) f_data.head() . date_block_num shop_id item_id item_cnt_day . 0 0 | 0 | 32 | 6.0 | . 1 0 | 0 | 33 | 3.0 | . 2 0 | 0 | 35 | 1.0 | . 3 0 | 0 | 43 | 1.0 | . 4 0 | 0 | 51 | 2.0 | . item_category_id&#47484; join&#54632; - f_data&#51060;&#50857;&#54644; &#48388; . items . item_name item_id item_category_id . 0 ! ВО ВЛАСТИ НАВАЖДЕНИЯ (ПЛАСТ.) D | 0 | 40 | . 1 !ABBYY FineReader 12 Professional Edition Full... | 1 | 76 | . 2 ***В ЛУЧАХ СЛАВЫ (UNV) D | 2 | 40 | . 3 ***ГОЛУБАЯ ВОЛНА (Univ) D | 3 | 40 | . 4 ***КОРОБКА (СТЕКЛО) D | 4 | 40 | . ... ... | ... | ... | . 22165 Ядерный титбит 2 [PC, Цифровая версия] | 22165 | 31 | . 22166 Язык запросов 1С:Предприятия [Цифровая версия] | 22166 | 54 | . 22167 Язык запросов 1С:Предприятия 8 (+CD). Хрустале... | 22167 | 49 | . 22168 Яйцо для Little Inu | 22168 | 62 | . 22169 Яйцо дракона (Игра престолов) | 22169 | 69 | . 22170 rows × 3 columns . grouped_data = f_data.merge(items, on = &quot;item_id&quot;) grouped_data.drop([&quot;item_name&quot;], axis = 1, inplace = True) . grouped_data . date_block_num shop_id item_id item_cnt_day item_category_id . 0 0 | 0 | 32 | 6.0 | 40 | . 1 0 | 1 | 32 | 7.0 | 40 | . 2 0 | 2 | 32 | 0.0 | 40 | . 3 0 | 3 | 32 | 3.0 | 40 | . 4 0 | 4 | 32 | 2.0 | 40 | . ... ... | ... | ... | ... | ... | . 8505960 33 | 55 | 7126 | 1.0 | 31 | . 8505961 33 | 55 | 7716 | 1.0 | 31 | . 8505962 33 | 55 | 13092 | 1.0 | 36 | . 8505963 33 | 55 | 16797 | 1.0 | 78 | . 8505964 33 | 55 | 18060 | 1.0 | 44 | . 8505965 rows × 5 columns . grouped_data&#51012; &#44592;&#48152;&#51004;&#47196; &#54616;&#50668; &#51216;&#51216; &#45908; &#49332;&#51012; &#48537;&#50668;&#44032;&#45716; &#48169;&#54693;&#51004;&#47196; &#51204;&#52376;&#47532; &#49884;&#46020; . EDA &#44284;&#51221; . shops&#51032; &#46020;&#49884;&#47484; &#48977;&#50500;&#48372;&#51088; . shops.head() . shop_name shop_id . 0 !Якутск Орджоникидзе, 56 фран | 0 | . 1 !Якутск ТЦ &quot;Центральный&quot; фран | 1 | . 2 Адыгея ТЦ &quot;Мега&quot; | 2 | . 3 Балашиха ТРК &quot;Октябрь-Киномир&quot; | 3 | . 4 Волжский ТЦ &quot;Волга Молл&quot; | 4 | . &#47084;&#49884;&#50500;&#50612; -&gt; &#50689;&#50612; &#48264;&#50669; - selenium&#51012; &#51060;&#50857;! . !pip install selenium !apt-get update !apt install chromium-chromedriver . Requirement already satisfied: selenium in /usr/local/lib/python3.7/dist-packages (3.141.0) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from selenium) (1.24.3) Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease Hit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 InRelease Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 Release Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease Fetched 163 kB in 2s (73.9 kB/s) Reading package lists... Done Reading package lists... Done Building dependency tree Reading state information... Done chromium-chromedriver is already the newest version (92.0.4515.159-0ubuntu0.18.04.1). The following package was automatically installed and is no longer required: libnvidia-common-460 Use &#39;apt autoremove&#39; to remove it. 0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded. . from selenium import webdriver from selenium.webdriver.common.keys import Keys import time . chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(&#39;--headless&#39;) chrome_options.add_argument(&#39;--no-sandbox&#39;) chrome_options.add_argument(&#39;--disable-dev-shm-usage&#39;) driver = webdriver.Chrome(&#39;chromedriver&#39;, chrome_options=chrome_options) . url = &quot;https://translate.google.co.kr/?hl=ko&amp;sl=auto&amp;tl=ko&amp;op=translate&quot; driver.get(url) driver.find_element_by_css_selector(&quot;#i13 &gt; span.VfPpkd-YVzG2b&quot;).click() # 번역어를 영어로 클릭한다. . crawled_txt = np.array([]) for x in shops.shop_name: trans = driver.find_element_by_css_selector(&#39;#yDmH0d &gt; c-wiz &gt; div &gt; div.WFnNle &gt; c-wiz &gt; div.OlSOob &gt; c-wiz &gt; div.ccvoYb &gt; div.AxqVh &gt; div.OPPzxe &gt; c-wiz.rm1UF.UnxENd &gt; span &gt; span &gt; div &gt; textarea&#39;) trans.send_keys(Keys.CONTROL, &#39;a&#39;) trans.send_keys(Keys.BACKSPACE) trans.send_keys(x) time.sleep(2.5) res = driver.find_element_by_css_selector(&quot;#yDmH0d &gt; c-wiz &gt; div &gt; div.WFnNle &gt; c-wiz &gt; div.OlSOob &gt; c-wiz &gt; div.ccvoYb &gt; div.AxqVh &gt; div.OPPzxe &gt; c-wiz.P6w8m.BDJ8fb &gt; div.dePhmb &gt; div &gt; div.J0lOec &gt; span.VIiyi &gt; span &gt; span&quot;).text crawled_txt = np.append(crawled_txt, np.array([res])) . shops.shop_name = crawled_txt # 번역된 것으로 바꿔준다. . &#48320;&#54872;&#46108; &#44163;&#51012; &#54869;&#51064;&#54624; &#49688; &#51080;&#51020;. . shops.head() . shop_name shop_id . 0 ! Yakutsk Ordzhonikidze, 56 fran | 0 | . 1 ! Yakutsk shopping center &quot;Central&quot; Fran | 1 | . 2 Adygea shopping center &quot;Mega&quot; | 2 | . 3 Balashiha TRK &quot;October-Kinomir&quot; | 3 | . 4 Volzhsky shopping center &quot;Volga Mall&quot; | 4 | . #shops.to_csv(&#39;/content/drive/MyDrive/shops_new.csv&#39;, index=False) . shop_city&#47484; &#47564;&#46304;&#45796;. . shops_new를 불러와서 사용한다! | . shops = pd.read_csv(&quot;/content/drive/MyDrive/shops_new.csv&quot;) . shops.head() . shop_name shop_id . 0 ! Yakutsk Ordzhonikidze, 56 fran | 0 | . 1 ! Yakutsk shopping center &quot;Central&quot; Fran | 1 | . 2 Adygea shopping center &quot;Mega&quot; | 2 | . 3 Balashiha TRK &quot;October-Kinomir&quot; | 3 | . 4 Volzhsky shopping center &quot;Volga Mall&quot; | 4 | . import re shop_city = shops.shop_name.apply(lambda x: x.split(&quot; &quot;)[0]) . shop_city[0] = shop_city[1] = &quot;Yakutsk&quot; shop_city = shop_city.apply(lambda x: x.capitalize()) np.sort(shop_city.unique()) . array([&#39;1c-online&#39;, &#39;Adygea&#39;, &#39;Balashiha&#39;, &#39;Chekhov&#39;, &#39;Exit&#39;, &#39;Himki&#39;, &#39;Kaluga&#39;, &#39;Kazan&#39;, &#39;Kolo&#39;, &#39;Krasnoyarsk&#39;, &#39;Kursk&#39;, &#39;Moscow&#39;, &#39;Mytishchi&#39;, &#39;N.novgorod&#39;, &#39;Novosibirsk&#39;, &#39;Omsk&#39;, &#39;Online&#39;, &#39;Rostnone&#39;, &#39;Rostov&#39;, &#39;Rostovnadon&#39;, &#39;Samara&#39;, &#39;Sergiev&#39;, &#39;Spb&#39;, &#39;Surgut&#39;, &#39;Tomsk&#39;, &#39;Tyumen&#39;, &#39;Ufa&#39;, &#39;Vologda&#39;, &#39;Volzhsky&#39;, &#39;Voronezh&#39;, &#39;Voronež&#39;, &#39;Yakutsk&#39;, &#39;Yaroslavl&#39;, &#39;Zhukovsky&#39;], dtype=object) . shop_city &#46972;&#48296;&#51064;&#53076;&#46377;? . from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder le = LabelEncoder() oh = OneHotEncoder() shops[&quot;shop_city&quot;] = le.fit_transform(shop_city) le.fit_transform(shop_city) . array([31, 31, 1, 2, 28, 27, 29, 29, 30, 4, 33, 33, 16, 7, 7, 6, 8, 9, 9, 10, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 12, 13, 13, 14, 14, 15, 17, 18, 19, 22, 22, 20, 20, 21, 23, 24, 25, 25, 25, 26, 26, 5, 0, 3, 31, 31, 32]) . grouped_data = grouped_data.merge(shops, on = &quot;shop_id&quot;) grouped_data.drop([&quot;shop_name&quot;], axis = 1, inplace = True) . grouped_data . date_block_num shop_id item_id item_cnt_day item_category_id shop_city . 0 0 | 0 | 32 | 6.0 | 40 | 31 | . 1 1 | 0 | 32 | 10.0 | 40 | 31 | . 2 0 | 0 | 33 | 3.0 | 37 | 31 | . 3 1 | 0 | 33 | 3.0 | 37 | 31 | . 4 0 | 0 | 35 | 1.0 | 40 | 31 | . ... ... | ... | ... | ... | ... | ... | . 8505960 33 | 20 | 21003 | 1.0 | 61 | 11 | . 8505961 33 | 20 | 21008 | 5.0 | 61 | 11 | . 8505962 33 | 20 | 21012 | 1.0 | 61 | 11 | . 8505963 33 | 20 | 21047 | 1.0 | 61 | 11 | . 8505964 33 | 20 | 21101 | 2.0 | 61 | 11 | . 8505965 rows × 6 columns . Item_categories&#51060;&#50857; &#51328;&#45908; Rough&#54616;&#44172; &#48516;&#47448;&#54644;&#48376;&#45796;. . item_categories . item_category_name item_category_id . 0 PC - Гарнитуры/Наушники | 0 | . 1 Аксессуары - PS2 | 1 | . 2 Аксессуары - PS3 | 2 | . 3 Аксессуары - PS4 | 3 | . 4 Аксессуары - PSP | 4 | . ... ... | ... | . 79 Служебные | 79 | . 80 Служебные - Билеты | 80 | . 81 Чистые носители (шпиль) | 81 | . 82 Чистые носители (штучные) | 82 | . 83 Элементы питания | 83 | . 84 rows × 2 columns . &#48264;&#50669;&#54616;&#44592; . url = &quot;https://translate.google.co.kr/?hl=ko&amp;sl=auto&amp;tl=ko&amp;op=translate&quot; driver.get(url) driver.find_element_by_css_selector(&quot;#i13 &gt; span.VfPpkd-YVzG2b&quot;).click() # 번역어를 영어로 클릭한다. . crawled_txt2 = np.array([]) for x in item_categories.item_category_name: trans = driver.find_element_by_css_selector(&#39;#yDmH0d &gt; c-wiz &gt; div &gt; div.WFnNle &gt; c-wiz &gt; div.OlSOob &gt; c-wiz &gt; div.ccvoYb &gt; div.AxqVh &gt; div.OPPzxe &gt; c-wiz.rm1UF.UnxENd &gt; span &gt; span &gt; div &gt; textarea&#39;) trans.send_keys(Keys.CONTROL, &#39;a&#39;) trans.send_keys(Keys.BACKSPACE) trans.send_keys(x) time.sleep(2.5) res2 = driver.find_element_by_css_selector(&quot;#yDmH0d &gt; c-wiz &gt; div &gt; div.WFnNle &gt; c-wiz &gt; div.OlSOob &gt; c-wiz &gt; div.ccvoYb &gt; div.AxqVh &gt; div.OPPzxe &gt; c-wiz.P6w8m.BDJ8fb &gt; div.dePhmb &gt; div &gt; div.J0lOec &gt; span.VIiyi &gt; span &gt; span&quot;).text crawled_txt2 = np.append(crawled_txt2, np.array([res2])) . item_categories.item_category_name = crawled_txt2 . #item_categories.to_csv(&#39;/content/drive/MyDrive/item_categories_new.csv&#39;, index=False) . &#39; - &#39;&#47484; &#44592;&#51456;&#51004;&#47196; &#48516;&#54624; &#54980; &#50526;&#50640;&#44163; &#46244;&#50640; &#44163;&#51004;&#47196; &#49352;&#47196;&#50868; &#50676;&#51012; &#47564;&#46304;&#45796;. . item_categories = pd.read_csv(&quot;/content/drive/MyDrive/item_categories_new.csv&quot;) . item_categories.head() . item_category_name item_category_id . 0 PC - Headset / Headphones | 0 | . 1 Accessories - PS2. | 1 | . 2 Accessories - PS3. | 2 | . 3 Accessories - PS4. | 3 | . 4 Accessories - PSP. | 4 | . import re sub_category_1 = item_categories.item_category_name.apply(lambda x: x.split(&quot;-&quot;)[0]) sub_category_1 = sub_category_1.apply(lambda x: x.capitalize().strip()) sub_category_2 = item_categories.item_category_name.apply(lambda x: x.split(&quot;-&quot;)).apply(lambda x: x[1].strip() if len(x) &gt; 1 else x[0].strip()) #sub_category_2 = sub_category_2.apply(lambda x: x.capitalize().strip()) . item_categories[&quot;sub_category_1&quot;] = sub_category_1 item_categories[&quot;sub_category_2&quot;] = sub_category_2 . item_categories . item_category_name item_category_id sub_category_1 sub_category_2 . 0 PC - Headset / Headphones | 0 | Pc | Headset / Headphones | . 1 Accessories - PS2. | 1 | Accessories | PS2. | . 2 Accessories - PS3. | 2 | Accessories | PS3. | . 3 Accessories - PS4. | 3 | Accessories | PS4. | . 4 Accessories - PSP. | 4 | Accessories | PSP. | . ... ... | ... | ... | ... | . 79 Service | 79 | Service | Service | . 80 Service - Tickets | 80 | Service | Tickets | . 81 Clean media (spire) | 81 | Clean media (spire) | Clean media (spire) | . 82 Clean media (piece) | 82 | Clean media (piece) | Clean media (piece) | . 83 Power elements | 83 | Power elements | Power elements | . 84 rows × 4 columns . from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder le = LabelEncoder() oh = OneHotEncoder() item_categories[&quot;sub_category_1&quot;] = le.fit_transform(item_categories[&quot;sub_category_1&quot;]) item_categories[&quot;sub_category_2&quot;] = le.fit_transform(item_categories[&quot;sub_category_2&quot;]) . grouped_data= grouped_data.merge(item_categories, on = &quot;item_category_id&quot;) grouped_data.drop([&quot;item_category_name&quot;], axis = 1, inplace = True) . grouped_data . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 . 0 0 | 0 | 32 | 6.0 | 40 | 31 | 3 | 18 | . 1 1 | 0 | 32 | 10.0 | 40 | 31 | 3 | 18 | . 2 0 | 0 | 35 | 1.0 | 40 | 31 | 3 | 18 | . 3 1 | 0 | 35 | 14.0 | 40 | 31 | 3 | 18 | . 4 0 | 0 | 43 | 1.0 | 40 | 31 | 3 | 18 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 8505960 6 | 31 | 3761 | 1.0 | 18 | 11 | 7 | 33 | . 8505961 27 | 35 | 6662 | 1.0 | 18 | 13 | 7 | 33 | . 8505962 0 | 50 | 3315 | 1.0 | 18 | 25 | 7 | 33 | . 8505963 3 | 26 | 6669 | 1.0 | 10 | 11 | 8 | 32 | . 8505964 18 | 38 | 17703 | 1.0 | 51 | 15 | 2 | 14 | . 8505965 rows × 8 columns . &#50672;&#46020;&#50752; &#50900;&#51012; &#52628;&#44032; . grouped_data[&quot;month&quot;] = grouped_data.date_block_num % 12 + 1 . grouped_data[&quot;year&quot;] = 0 . grouped_data.year[grouped_data.date_block_num // 12 == 0] = 2013 grouped_data.year[grouped_data.date_block_num // 12 == 1] = 2014 grouped_data.year[grouped_data.date_block_num // 12 == 2] = 2015 . grouped_data . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 month year . 0 0 | 0 | 32 | 6.0 | 40 | 31 | 3 | 18 | 1 | 2013 | . 1 1 | 0 | 32 | 10.0 | 40 | 31 | 3 | 18 | 2 | 2013 | . 2 0 | 0 | 35 | 1.0 | 40 | 31 | 3 | 18 | 1 | 2013 | . 3 1 | 0 | 35 | 14.0 | 40 | 31 | 3 | 18 | 2 | 2013 | . 4 0 | 0 | 43 | 1.0 | 40 | 31 | 3 | 18 | 1 | 2013 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8505960 6 | 31 | 3761 | 1.0 | 18 | 11 | 7 | 33 | 7 | 2013 | . 8505961 27 | 35 | 6662 | 1.0 | 18 | 13 | 7 | 33 | 4 | 2015 | . 8505962 0 | 50 | 3315 | 1.0 | 18 | 25 | 7 | 33 | 1 | 2013 | . 8505963 3 | 26 | 6669 | 1.0 | 10 | 11 | 8 | 32 | 4 | 2013 | . 8505964 18 | 38 | 17703 | 1.0 | 51 | 15 | 2 | 14 | 7 | 2014 | . 8505965 rows × 10 columns . sales_train . date date_block_num shop_id item_id item_price item_cnt_day . 0 01.01.2013 | 0 | 28 | 11613 | 1499.0 | 1.0 | . 1 01.01.2013 | 0 | 51 | 10283 | 667.0 | 1.0 | . 2 01.01.2013 | 0 | 19 | 2828 | 499.0 | 1.0 | . 3 01.01.2013 | 0 | 51 | 10390 | 150.0 | 1.0 | . 4 01.01.2013 | 0 | 51 | 10591 | 110.0 | 1.0 | . ... ... | ... | ... | ... | ... | ... | . 2935844 31.12.2014 | 23 | 10 | 15784 | 195.0 | 1.0 | . 2935845 31.12.2014 | 23 | 31 | 18624 | 499.0 | 1.0 | . 2935846 31.12.2014 | 23 | 31 | 18622 | 169.0 | 1.0 | . 2935847 31.12.2014 | 23 | 59 | 9412 | 299.0 | 1.0 | . 2935848 31.12.2014 | 23 | 34 | 15336 | 529.0 | 1.0 | . 2935842 rows × 6 columns . #grouped_data.to_csv(&#39;/content/drive/MyDrive/grouped_data.csv&#39;, index=False) . START . grouped_data = pd.read_csv(&#39;/content/drive/MyDrive/grouped_data.csv&#39;) grouped_data.head() . f_data = pd.read_csv(&#39;/content/drive/MyDrive/f_data.csv&#39;) f_data.head() . date_block_num shop_id item_id item_cnt_day . 0 0 | 0 | 32 | 6.0 | . 1 0 | 0 | 33 | 3.0 | . 2 0 | 0 | 35 | 1.0 | . 3 0 | 0 | 43 | 1.0 | . 4 0 | 0 | 51 | 2.0 | . &#51648;&#45212;&#45804; &#50500;&#51060;&#53596; &#54032;&#47588;&#49688;&#47049; &#52628;&#44032; - item_cnt_lag1 . dd = grouped_data[grouped_data[&quot;date_block_num&quot;] != 0] # date_block_num = 0을 제외한 데이터 . dd . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 month year . 1 1 | 0 | 32 | 10.0 | 40 | 31 | 3 | 18 | 2 | 2013 | . 3 1 | 0 | 35 | 14.0 | 40 | 31 | 3 | 18 | 2 | 2013 | . 10 1 | 0 | 98 | 5.0 | 40 | 31 | 3 | 18 | 2 | 2013 | . 12 1 | 0 | 947 | 2.0 | 40 | 31 | 3 | 18 | 2 | 2013 | . 16 1 | 0 | 2462 | 2.0 | 40 | 31 | 3 | 18 | 2 | 2013 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8505959 7 | 30 | 6723 | 1.0 | 18 | 11 | 7 | 33 | 8 | 2013 | . 8505960 6 | 31 | 3761 | 1.0 | 18 | 11 | 7 | 33 | 7 | 2013 | . 8505961 27 | 35 | 6662 | 1.0 | 18 | 13 | 7 | 33 | 4 | 2015 | . 8505963 3 | 26 | 6669 | 1.0 | 10 | 11 | 8 | 32 | 4 | 2013 | . 8505964 18 | 38 | 17703 | 1.0 | 51 | 15 | 2 | 14 | 7 | 2014 | . 8236380 rows × 10 columns . f_data &#50896;&#48376;&#50640; &#50689;&#54693;&#51012; &#51452;&#51648; &#50506;&#45716; &#48320;&#49688; &#49373;&#49457; . kk = f_data kk . date_block_num shop_id item_id item_cnt_day . 0 0 | 0 | 32 | 6.0 | . 1 0 | 0 | 33 | 3.0 | . 2 0 | 0 | 35 | 1.0 | . 3 0 | 0 | 43 | 1.0 | . 4 0 | 0 | 51 | 2.0 | . ... ... | ... | ... | ... | . 8505960 34 | 45 | 18454 | 0.0 | . 8505961 34 | 45 | 16188 | 0.0 | . 8505962 34 | 45 | 15757 | 0.0 | . 8505963 34 | 45 | 19648 | 0.0 | . 8505964 34 | 45 | 969 | 0.0 | . 8505965 rows × 4 columns . kk[&quot;date_block_num&quot;] += 1 kk.columns = [&#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;, &#39;item_cnt_lag1&#39;] . kk # 생성완료 . date_block_num shop_id item_id item_cnt_lag1 . 0 1 | 0 | 32 | 6.0 | . 1 1 | 0 | 33 | 3.0 | . 2 1 | 0 | 35 | 1.0 | . 3 1 | 0 | 43 | 1.0 | . 4 1 | 0 | 51 | 2.0 | . ... ... | ... | ... | ... | . 8505960 35 | 45 | 18454 | 0.0 | . 8505961 35 | 45 | 16188 | 0.0 | . 8505962 35 | 45 | 15757 | 0.0 | . 8505963 35 | 45 | 19648 | 0.0 | . 8505964 35 | 45 | 969 | 0.0 | . 8505965 rows × 4 columns . 1&#45804;&#51204; &#54217;&#44512;&#44032;&#47484; &#45347;&#50612;&#48372;&#44592; . sales_train = pd.read_csv(&#39;/content/drive/MyDrive/sales_train.csv&#39;) . t_tmp = sales_train.groupby([&quot;date_block_num&quot;,&quot;shop_id&quot;,&quot;item_id&quot;]) t_tmp = t_tmp.agg({&quot;item_price&quot; : &quot;mean&quot;, &#39;item_cnt_day&#39;:&#39;sum&#39;}).reset_index() t_tmp.drop(&quot;item_cnt_day&quot;,axis = 1 , inplace = True) t_tmp[&quot;date_block_num&quot;] += 1 t_tmp.columns = [&#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;, &#39;item_price_lag1&#39;] t_tmp . date_block_num shop_id item_id item_price_lag1 . 0 1 | 0 | 32 | 221.0 | . 1 1 | 0 | 33 | 347.0 | . 2 1 | 0 | 35 | 247.0 | . 3 1 | 0 | 43 | 221.0 | . 4 1 | 0 | 51 | 128.5 | . ... ... | ... | ... | ... | . 1609119 34 | 59 | 22087 | 119.0 | . 1609120 34 | 59 | 22088 | 119.0 | . 1609121 34 | 59 | 22091 | 179.0 | . 1609122 34 | 59 | 22100 | 629.0 | . 1609123 34 | 59 | 22102 | 1250.0 | . 1609124 rows × 4 columns . kk = pd.merge(kk, t_tmp, on = [&quot;date_block_num&quot;,&quot;shop_id&quot;, &quot;item_id&quot;],how = &quot;left&quot;) kk . date_block_num shop_id item_id item_cnt_lag1 item_price_lag1 . 0 1 | 0 | 32 | 6.0 | 221.0 | . 1 1 | 0 | 33 | 3.0 | 347.0 | . 2 1 | 0 | 35 | 1.0 | 247.0 | . 3 1 | 0 | 43 | 1.0 | 221.0 | . 4 1 | 0 | 51 | 2.0 | 128.5 | . ... ... | ... | ... | ... | ... | . 8505960 35 | 45 | 18454 | 0.0 | NaN | . 8505961 35 | 45 | 16188 | 0.0 | NaN | . 8505962 35 | 45 | 15757 | 0.0 | NaN | . 8505963 35 | 45 | 19648 | 0.0 | NaN | . 8505964 35 | 45 | 969 | 0.0 | NaN | . 8505965 rows × 5 columns . date_block_num = 0&#51012; &#51228;&#50808;&#54620; &#45936;&#51060;&#53552;&#50640; item_cnt_lag1, item_price_lag1&#51012; &#48537;&#50668;&#51456;&#45796;. . n_dd = pd.merge(dd, kk, on = [&quot;date_block_num&quot;,&quot;shop_id&quot;, &quot;item_id&quot;],how = &quot;left&quot;) . n_dd . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 month year item_cnt_lag1 item_price_lag1 . 0 1 | 0 | 32 | 10.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 6.0 | 221.0 | . 1 1 | 0 | 35 | 14.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 1.0 | 247.0 | . 2 1 | 0 | 98 | 5.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 25.0 | 268.0 | . 3 1 | 0 | 947 | 2.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 2.0 | 37.0 | . 4 1 | 0 | 2462 | 2.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 1.0 | 58.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8236375 7 | 30 | 6723 | 1.0 | 18 | 11 | 7 | 33 | 8 | 2013 | NaN | NaN | . 8236376 6 | 31 | 3761 | 1.0 | 18 | 11 | 7 | 33 | 7 | 2013 | NaN | NaN | . 8236377 27 | 35 | 6662 | 1.0 | 18 | 13 | 7 | 33 | 4 | 2015 | NaN | NaN | . 8236378 3 | 26 | 6669 | 1.0 | 10 | 11 | 8 | 32 | 4 | 2013 | NaN | NaN | . 8236379 18 | 38 | 17703 | 1.0 | 51 | 15 | 2 | 14 | 7 | 2014 | NaN | NaN | . 8236380 rows × 12 columns . n_dd.isna().sum() . date_block_num 0 shop_id 0 item_id 0 item_cnt_day 0 item_category_id 0 shop_city 0 sub_category_1 0 sub_category_2 0 month 0 year 0 item_cnt_lag1 572922 item_price_lag1 7255563 dtype: int64 . n_dd = n_dd.fillna(0) n_dd . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 month year item_cnt_lag1 item_price_lag1 . 0 1 | 0 | 32 | 10.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 6.0 | 221.0 | . 1 1 | 0 | 35 | 14.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 1.0 | 247.0 | . 2 1 | 0 | 98 | 5.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 25.0 | 268.0 | . 3 1 | 0 | 947 | 2.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 2.0 | 37.0 | . 4 1 | 0 | 2462 | 2.0 | 40 | 31 | 3 | 18 | 2 | 2013 | 1.0 | 58.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8236375 7 | 30 | 6723 | 1.0 | 18 | 11 | 7 | 33 | 8 | 2013 | 0.0 | 0.0 | . 8236376 6 | 31 | 3761 | 1.0 | 18 | 11 | 7 | 33 | 7 | 2013 | 0.0 | 0.0 | . 8236377 27 | 35 | 6662 | 1.0 | 18 | 13 | 7 | 33 | 4 | 2015 | 0.0 | 0.0 | . 8236378 3 | 26 | 6669 | 1.0 | 10 | 11 | 8 | 32 | 4 | 2013 | 0.0 | 0.0 | . 8236379 18 | 38 | 17703 | 1.0 | 51 | 15 | 2 | 14 | 7 | 2014 | 0.0 | 0.0 | . 8236380 rows × 12 columns . lag2 . 2개월전 자료 추가 | . kk[&quot;date_block_num&quot;] += 1 kk.columns = [&#39;date_block_num&#39;, &#39;shop_id&#39;, &#39;item_id&#39;, &#39;item_cnt_lag2&#39;, &#39;item_price_lag2&#39;] . kk . date_block_num shop_id item_id item_cnt_lag2 item_price_lag2 . 0 2 | 0 | 32 | 6.0 | 221.0 | . 1 2 | 0 | 33 | 3.0 | 347.0 | . 2 2 | 0 | 35 | 1.0 | 247.0 | . 3 2 | 0 | 43 | 1.0 | 221.0 | . 4 2 | 0 | 51 | 2.0 | 128.5 | . ... ... | ... | ... | ... | ... | . 8505960 36 | 45 | 18454 | 0.0 | NaN | . 8505961 36 | 45 | 16188 | 0.0 | NaN | . 8505962 36 | 45 | 15757 | 0.0 | NaN | . 8505963 36 | 45 | 19648 | 0.0 | NaN | . 8505964 36 | 45 | 969 | 0.0 | NaN | . 8505965 rows × 5 columns . n_dd&#50640; &#46160;&#45804;&#51204; &#51088;&#47308;&#47484; &#54633;&#52824;&#44592; . n_dd = pd.merge(n_dd, kk, on = [&quot;date_block_num&quot;,&quot;shop_id&quot;, &quot;item_id&quot;],how = &quot;left&quot;) . n_dd = n_dd[n_dd.date_block_num &gt; 1] n_dd . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 month year item_cnt_lag1 item_price_lag1 item_cnt_lag2 item_price_lag2 . 1356 2 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 3 | 2013 | 0.0 | 0.0 | 0.0 | NaN | . 1357 3 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 4 | 2013 | 0.0 | 0.0 | 0.0 | NaN | . 1358 4 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 5 | 2013 | 0.0 | 0.0 | 0.0 | NaN | . 1359 5 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 6 | 2013 | 0.0 | 0.0 | 0.0 | NaN | . 1360 6 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 7 | 2013 | 0.0 | 0.0 | 0.0 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8236375 7 | 30 | 6723 | 1.0 | 18 | 11 | 7 | 33 | 8 | 2013 | 0.0 | 0.0 | NaN | NaN | . 8236376 6 | 31 | 3761 | 1.0 | 18 | 11 | 7 | 33 | 7 | 2013 | 0.0 | 0.0 | NaN | NaN | . 8236377 27 | 35 | 6662 | 1.0 | 18 | 13 | 7 | 33 | 4 | 2015 | 0.0 | 0.0 | NaN | NaN | . 8236378 3 | 26 | 6669 | 1.0 | 10 | 11 | 8 | 32 | 4 | 2013 | 0.0 | 0.0 | NaN | NaN | . 8236379 18 | 38 | 17703 | 1.0 | 51 | 15 | 2 | 14 | 7 | 2014 | 0.0 | 0.0 | NaN | NaN | . 7970468 rows × 14 columns . n_dd.isna().sum() . date_block_num 0 shop_id 0 item_id 0 item_cnt_day 0 item_category_id 0 shop_city 0 sub_category_1 0 sub_category_2 0 month 0 year 0 item_cnt_lag1 0 item_price_lag1 0 item_cnt_lag2 577256 item_price_lag2 7074377 dtype: int64 . n_dd = n_dd.fillna(0) n_dd . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 month year item_cnt_lag1 item_price_lag1 item_cnt_lag2 item_price_lag2 . 1356 2 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 3 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 1357 3 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 4 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 1358 4 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 5 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 1359 5 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 6 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 1360 6 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 7 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 8236375 7 | 30 | 6723 | 1.0 | 18 | 11 | 7 | 33 | 8 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 8236376 6 | 31 | 3761 | 1.0 | 18 | 11 | 7 | 33 | 7 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 8236377 27 | 35 | 6662 | 1.0 | 18 | 13 | 7 | 33 | 4 | 2015 | 0.0 | 0.0 | 0.0 | 0.0 | . 8236378 3 | 26 | 6669 | 1.0 | 10 | 11 | 8 | 32 | 4 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 8236379 18 | 38 | 17703 | 1.0 | 51 | 15 | 2 | 14 | 7 | 2014 | 0.0 | 0.0 | 0.0 | 0.0 | . 7970468 rows × 14 columns . &#47564;&#46304;&#45936;&#51060;&#53552;&#49483; &#51200;&#51109;&#9733;&#9733;&#9733;&#9733;&#9733;&#9733;&#9733;&#9733;&#9733;&#9733;&#9733;&#9733; . . Modeling . 2013-01 ~ 2015-09 (date_block_num[0 ~ 32])&#51004;&#47196; &#47784;&#45944;&#47553;&#54616;&#50668; 2015-11&#50900; &#45824;&#49345;&#51004;&#47196; &#50696;&#52769; &#54980; &#54217;&#44032;&#54616;&#45716; &#48169;&#54693; . lag1_cnt: 지난달의 개수를 추가 (13년도 1월 이전의 값은 없으므로 13/01데이터는 일단 제외한다.) | 실제 train -&gt; 2013/02 ~ 2015/09 [1 ~ 32] | validation set -&gt; 2015/10 [33] | test set -&gt; 2015/11 [34] | . &#48155;&#51008; &#45936;&#51060;&#53552; &#49483;&#51004;&#47196; &#54644;&#48372;&#44592;! . n_dd를 이용한다! | . n_dd = pd.read_csv(&#39;/content/drive/MyDrive/n_dataset.csv&#39;) n_dd.head() . date_block_num shop_id item_id item_cnt_day item_category_id shop_city sub_category_1 sub_category_2 month year item_cnt_lag1 item_price_lag1 item_cnt_lag2 item_price_lag2 . 0 2 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 3 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 3 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 4 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 4 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 5 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 5 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 6 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 6 | 2 | 32 | 0.0 | 40 | 1 | 3 | 18 | 7 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . X = n_dd.drop([&quot;item_cnt_day&quot;], axis = 1) y = np.clip(n_dd.item_cnt_day, 0, 20) ## 중요!! . X_train = X[X[&quot;date_block_num&quot;] &lt;= 32] y_train = y[X_train.index] . X_valid = X[X[&quot;date_block_num&quot;] == 33] y_valid = y[X_valid.index] . X_test = X[X[&quot;date_block_num&quot;] == 34] y_test = y[X_test.index] . X_train . date_block_num shop_id item_id item_category_id shop_city sub_category_1 sub_category_2 month year item_cnt_lag1 item_price_lag1 item_cnt_lag2 item_price_lag2 . 0 2 | 2 | 32 | 40 | 1 | 3 | 18 | 3 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 1 3 | 2 | 32 | 40 | 1 | 3 | 18 | 4 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 2 4 | 2 | 32 | 40 | 1 | 3 | 18 | 5 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 3 5 | 2 | 32 | 40 | 1 | 3 | 18 | 6 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 4 6 | 2 | 32 | 40 | 1 | 3 | 18 | 7 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 7970463 7 | 30 | 6723 | 18 | 11 | 7 | 33 | 8 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 7970464 6 | 31 | 3761 | 18 | 11 | 7 | 33 | 7 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 7970465 27 | 35 | 6662 | 18 | 13 | 7 | 33 | 4 | 2015 | 0.0 | 0.0 | 0.0 | 0.0 | . 7970466 3 | 26 | 6669 | 10 | 11 | 8 | 32 | 4 | 2013 | 0.0 | 0.0 | 0.0 | 0.0 | . 7970467 18 | 38 | 17703 | 51 | 15 | 2 | 14 | 7 | 2014 | 0.0 | 0.0 | 0.0 | 0.0 | . 7539217 rows × 13 columns . Light GBM Regressor / Decision Tree Reressor / XGBoost Regressor &#46321; . https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMRegressor.html - 참고 | lightgbm.LGBMRegressor(boosting_type=&#39;gbdt&#39;, num_leaves=31, max_depth=- 1, learning_rate=0.1, n_estimators=100, subsample_for_bin=200000, objective=None, class_weight=None, min_split_gain=0.0, min_child_weight=0.001, min_child_samples=20, subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0, reg_lambda=0.0, random_state=None, n_jobs=- 1, silent=True, importance_type=&#39;split&#39;, **kwargs) | . from lightgbm import LGBMRegressor lgbm_r = LGBMRegressor(n_estimators = 300, max_depth = 8, learning_rate = 0.08, early_stopping_round = 30, colsample_bytree = 0.7, subsample=0.7, objective = &#39;rmse&#39;, metric = &#39;rmse&#39;, random_state = 42) lgbm_r.fit(X_train,y_train, eval_set=[(X_valid, y_valid)], eval_metric=&#39;rmse&#39;,verbose = True) y_pred = lgbm_r.predict(X_test) . [1] valid_0&#39;s rmse: 1.16422 Training until validation scores don&#39;t improve for 30 rounds. [2] valid_0&#39;s rmse: 1.14583 [3] valid_0&#39;s rmse: 1.13172 [4] valid_0&#39;s rmse: 1.12099 [5] valid_0&#39;s rmse: 1.11064 [6] valid_0&#39;s rmse: 1.09492 [7] valid_0&#39;s rmse: 1.08194 [8] valid_0&#39;s rmse: 1.07088 [9] valid_0&#39;s rmse: 1.06512 [10] valid_0&#39;s rmse: 1.05692 [11] valid_0&#39;s rmse: 1.05268 [12] valid_0&#39;s rmse: 1.04456 [13] valid_0&#39;s rmse: 1.04131 [14] valid_0&#39;s rmse: 1.03592 [15] valid_0&#39;s rmse: 1.02725 [16] valid_0&#39;s rmse: 1.02557 [17] valid_0&#39;s rmse: 1.02391 [18] valid_0&#39;s rmse: 1.01766 [19] valid_0&#39;s rmse: 1.01616 [20] valid_0&#39;s rmse: 1.01345 [21] valid_0&#39;s rmse: 1.01129 [22] valid_0&#39;s rmse: 1.00972 [23] valid_0&#39;s rmse: 1.00843 [24] valid_0&#39;s rmse: 1.00721 [25] valid_0&#39;s rmse: 1.00274 [26] valid_0&#39;s rmse: 1.00214 [27] valid_0&#39;s rmse: 0.998601 [28] valid_0&#39;s rmse: 0.998015 [29] valid_0&#39;s rmse: 0.997344 [30] valid_0&#39;s rmse: 0.996938 [31] valid_0&#39;s rmse: 0.996858 [32] valid_0&#39;s rmse: 0.996568 [33] valid_0&#39;s rmse: 0.9964 [34] valid_0&#39;s rmse: 0.995937 [35] valid_0&#39;s rmse: 0.995982 [36] valid_0&#39;s rmse: 0.995557 [37] valid_0&#39;s rmse: 0.995449 [38] valid_0&#39;s rmse: 0.995328 [39] valid_0&#39;s rmse: 0.995398 [40] valid_0&#39;s rmse: 0.993207 [41] valid_0&#39;s rmse: 0.992983 [42] valid_0&#39;s rmse: 0.990759 [43] valid_0&#39;s rmse: 0.990652 [44] valid_0&#39;s rmse: 0.990622 [45] valid_0&#39;s rmse: 0.990358 [46] valid_0&#39;s rmse: 0.990163 [47] valid_0&#39;s rmse: 0.989937 [48] valid_0&#39;s rmse: 0.990003 [49] valid_0&#39;s rmse: 0.989934 [50] valid_0&#39;s rmse: 0.989812 [51] valid_0&#39;s rmse: 0.989849 [52] valid_0&#39;s rmse: 0.989695 [53] valid_0&#39;s rmse: 0.989671 [54] valid_0&#39;s rmse: 0.988571 [55] valid_0&#39;s rmse: 0.988475 [56] valid_0&#39;s rmse: 0.986661 [57] valid_0&#39;s rmse: 0.986444 [58] valid_0&#39;s rmse: 0.986416 [59] valid_0&#39;s rmse: 0.986319 [60] valid_0&#39;s rmse: 0.986173 [61] valid_0&#39;s rmse: 0.986049 [62] valid_0&#39;s rmse: 0.985919 [63] valid_0&#39;s rmse: 0.985917 [64] valid_0&#39;s rmse: 0.985921 [65] valid_0&#39;s rmse: 0.98574 [66] valid_0&#39;s rmse: 0.985636 [67] valid_0&#39;s rmse: 0.985663 [68] valid_0&#39;s rmse: 0.985808 [69] valid_0&#39;s rmse: 0.985944 [70] valid_0&#39;s rmse: 0.985073 [71] valid_0&#39;s rmse: 0.984922 [72] valid_0&#39;s rmse: 0.984896 [73] valid_0&#39;s rmse: 0.984788 [74] valid_0&#39;s rmse: 0.984835 [75] valid_0&#39;s rmse: 0.984766 [76] valid_0&#39;s rmse: 0.984649 [77] valid_0&#39;s rmse: 0.984505 [78] valid_0&#39;s rmse: 0.983704 [79] valid_0&#39;s rmse: 0.983616 [80] valid_0&#39;s rmse: 0.982612 [81] valid_0&#39;s rmse: 0.982628 [82] valid_0&#39;s rmse: 0.982601 [83] valid_0&#39;s rmse: 0.98256 [84] valid_0&#39;s rmse: 0.982565 [85] valid_0&#39;s rmse: 0.982562 [86] valid_0&#39;s rmse: 0.982524 [87] valid_0&#39;s rmse: 0.982587 [88] valid_0&#39;s rmse: 0.982486 [89] valid_0&#39;s rmse: 0.982336 [90] valid_0&#39;s rmse: 0.982306 [91] valid_0&#39;s rmse: 0.982285 [92] valid_0&#39;s rmse: 0.982299 [93] valid_0&#39;s rmse: 0.981427 [94] valid_0&#39;s rmse: 0.981305 [95] valid_0&#39;s rmse: 0.98126 [96] valid_0&#39;s rmse: 0.981227 [97] valid_0&#39;s rmse: 0.981262 [98] valid_0&#39;s rmse: 0.98121 [99] valid_0&#39;s rmse: 0.981239 [100] valid_0&#39;s rmse: 0.980426 [101] valid_0&#39;s rmse: 0.980406 [102] valid_0&#39;s rmse: 0.980152 [103] valid_0&#39;s rmse: 0.980215 [104] valid_0&#39;s rmse: 0.980197 [105] valid_0&#39;s rmse: 0.980301 [106] valid_0&#39;s rmse: 0.979724 [107] valid_0&#39;s rmse: 0.97966 [108] valid_0&#39;s rmse: 0.979647 [109] valid_0&#39;s rmse: 0.979513 [110] valid_0&#39;s rmse: 0.979511 [111] valid_0&#39;s rmse: 0.979497 [112] valid_0&#39;s rmse: 0.979379 [113] valid_0&#39;s rmse: 0.979323 [114] valid_0&#39;s rmse: 0.97927 [115] valid_0&#39;s rmse: 0.97926 [116] valid_0&#39;s rmse: 0.979135 [117] valid_0&#39;s rmse: 0.979041 [118] valid_0&#39;s rmse: 0.978964 [119] valid_0&#39;s rmse: 0.978927 [120] valid_0&#39;s rmse: 0.978927 [121] valid_0&#39;s rmse: 0.97303 [122] valid_0&#39;s rmse: 0.972952 [123] valid_0&#39;s rmse: 0.972742 [124] valid_0&#39;s rmse: 0.972679 [125] valid_0&#39;s rmse: 0.972688 [126] valid_0&#39;s rmse: 0.972685 [127] valid_0&#39;s rmse: 0.972727 [128] valid_0&#39;s rmse: 0.972628 [129] valid_0&#39;s rmse: 0.972489 [130] valid_0&#39;s rmse: 0.97245 [131] valid_0&#39;s rmse: 0.97248 [132] valid_0&#39;s rmse: 0.972474 [133] valid_0&#39;s rmse: 0.972305 [134] valid_0&#39;s rmse: 0.972133 [135] valid_0&#39;s rmse: 0.972064 [136] valid_0&#39;s rmse: 0.971914 [137] valid_0&#39;s rmse: 0.971864 [138] valid_0&#39;s rmse: 0.971816 [139] valid_0&#39;s rmse: 0.971674 [140] valid_0&#39;s rmse: 0.971726 [141] valid_0&#39;s rmse: 0.971343 [142] valid_0&#39;s rmse: 0.971195 [143] valid_0&#39;s rmse: 0.971137 [144] valid_0&#39;s rmse: 0.971138 [145] valid_0&#39;s rmse: 0.971056 [146] valid_0&#39;s rmse: 0.970965 [147] valid_0&#39;s rmse: 0.97087 [148] valid_0&#39;s rmse: 0.970863 [149] valid_0&#39;s rmse: 0.970858 [150] valid_0&#39;s rmse: 0.970796 [151] valid_0&#39;s rmse: 0.970916 [152] valid_0&#39;s rmse: 0.97091 [153] valid_0&#39;s rmse: 0.970831 [154] valid_0&#39;s rmse: 0.970806 [155] valid_0&#39;s rmse: 0.970774 [156] valid_0&#39;s rmse: 0.970757 [157] valid_0&#39;s rmse: 0.970795 [158] valid_0&#39;s rmse: 0.970809 [159] valid_0&#39;s rmse: 0.97071 [160] valid_0&#39;s rmse: 0.970715 [161] valid_0&#39;s rmse: 0.970723 [162] valid_0&#39;s rmse: 0.970618 [163] valid_0&#39;s rmse: 0.970642 [164] valid_0&#39;s rmse: 0.970548 [165] valid_0&#39;s rmse: 0.970537 [166] valid_0&#39;s rmse: 0.970457 [167] valid_0&#39;s rmse: 0.970012 [168] valid_0&#39;s rmse: 0.970028 [169] valid_0&#39;s rmse: 0.970029 [170] valid_0&#39;s rmse: 0.969979 [171] valid_0&#39;s rmse: 0.969949 [172] valid_0&#39;s rmse: 0.970074 [173] valid_0&#39;s rmse: 0.969944 [174] valid_0&#39;s rmse: 0.969908 [175] valid_0&#39;s rmse: 0.969915 [176] valid_0&#39;s rmse: 0.969899 [177] valid_0&#39;s rmse: 0.969766 [178] valid_0&#39;s rmse: 0.969682 [179] valid_0&#39;s rmse: 0.969695 [180] valid_0&#39;s rmse: 0.969628 [181] valid_0&#39;s rmse: 0.969611 [182] valid_0&#39;s rmse: 0.969579 [183] valid_0&#39;s rmse: 0.969586 [184] valid_0&#39;s rmse: 0.969527 [185] valid_0&#39;s rmse: 0.969397 [186] valid_0&#39;s rmse: 0.969393 [187] valid_0&#39;s rmse: 0.969419 [188] valid_0&#39;s rmse: 0.969736 [189] valid_0&#39;s rmse: 0.969668 [190] valid_0&#39;s rmse: 0.969669 [191] valid_0&#39;s rmse: 0.969658 [192] valid_0&#39;s rmse: 0.969572 [193] valid_0&#39;s rmse: 0.969602 [194] valid_0&#39;s rmse: 0.969611 [195] valid_0&#39;s rmse: 0.969366 [196] valid_0&#39;s rmse: 0.969206 [197] valid_0&#39;s rmse: 0.969226 [198] valid_0&#39;s rmse: 0.969202 [199] valid_0&#39;s rmse: 0.969189 [200] valid_0&#39;s rmse: 0.969208 [201] valid_0&#39;s rmse: 0.969203 [202] valid_0&#39;s rmse: 0.969199 [203] valid_0&#39;s rmse: 0.969232 [204] valid_0&#39;s rmse: 0.969216 [205] valid_0&#39;s rmse: 0.969329 [206] valid_0&#39;s rmse: 0.969328 [207] valid_0&#39;s rmse: 0.969446 [208] valid_0&#39;s rmse: 0.969404 [209] valid_0&#39;s rmse: 0.969415 [210] valid_0&#39;s rmse: 0.969414 [211] valid_0&#39;s rmse: 0.969329 [212] valid_0&#39;s rmse: 0.969311 [213] valid_0&#39;s rmse: 0.969328 [214] valid_0&#39;s rmse: 0.96928 [215] valid_0&#39;s rmse: 0.96925 [216] valid_0&#39;s rmse: 0.969227 [217] valid_0&#39;s rmse: 0.96921 [218] valid_0&#39;s rmse: 0.968772 [219] valid_0&#39;s rmse: 0.96879 [220] valid_0&#39;s rmse: 0.96872 [221] valid_0&#39;s rmse: 0.968755 [222] valid_0&#39;s rmse: 0.968799 [223] valid_0&#39;s rmse: 0.968804 [224] valid_0&#39;s rmse: 0.968743 [225] valid_0&#39;s rmse: 0.96873 [226] valid_0&#39;s rmse: 0.968563 [227] valid_0&#39;s rmse: 0.96858 [228] valid_0&#39;s rmse: 0.968567 [229] valid_0&#39;s rmse: 0.968554 [230] valid_0&#39;s rmse: 0.968564 [231] valid_0&#39;s rmse: 0.968474 [232] valid_0&#39;s rmse: 0.96847 [233] valid_0&#39;s rmse: 0.968475 [234] valid_0&#39;s rmse: 0.96842 [235] valid_0&#39;s rmse: 0.9684 [236] valid_0&#39;s rmse: 0.968449 [237] valid_0&#39;s rmse: 0.968784 [238] valid_0&#39;s rmse: 0.968721 [239] valid_0&#39;s rmse: 0.968627 [240] valid_0&#39;s rmse: 0.96866 [241] valid_0&#39;s rmse: 0.968624 [242] valid_0&#39;s rmse: 0.968632 [243] valid_0&#39;s rmse: 0.968526 [244] valid_0&#39;s rmse: 0.968443 [245] valid_0&#39;s rmse: 0.968473 [246] valid_0&#39;s rmse: 0.968461 [247] valid_0&#39;s rmse: 0.968368 [248] valid_0&#39;s rmse: 0.968355 [249] valid_0&#39;s rmse: 0.968253 [250] valid_0&#39;s rmse: 0.968232 [251] valid_0&#39;s rmse: 0.968157 [252] valid_0&#39;s rmse: 0.968176 [253] valid_0&#39;s rmse: 0.968174 [254] valid_0&#39;s rmse: 0.968168 [255] valid_0&#39;s rmse: 0.968095 [256] valid_0&#39;s rmse: 0.968154 [257] valid_0&#39;s rmse: 0.968198 [258] valid_0&#39;s rmse: 0.968155 [259] valid_0&#39;s rmse: 0.968246 [260] valid_0&#39;s rmse: 0.96828 [261] valid_0&#39;s rmse: 0.968345 [262] valid_0&#39;s rmse: 0.968347 [263] valid_0&#39;s rmse: 0.968327 [264] valid_0&#39;s rmse: 0.968335 [265] valid_0&#39;s rmse: 0.967927 [266] valid_0&#39;s rmse: 0.967943 [267] valid_0&#39;s rmse: 0.967726 [268] valid_0&#39;s rmse: 0.967713 [269] valid_0&#39;s rmse: 0.967745 [270] valid_0&#39;s rmse: 0.967736 [271] valid_0&#39;s rmse: 0.967945 [272] valid_0&#39;s rmse: 0.967898 [273] valid_0&#39;s rmse: 0.967903 [274] valid_0&#39;s rmse: 0.967852 [275] valid_0&#39;s rmse: 0.967864 [276] valid_0&#39;s rmse: 0.967818 [277] valid_0&#39;s rmse: 0.967799 [278] valid_0&#39;s rmse: 0.968237 [279] valid_0&#39;s rmse: 0.968227 [280] valid_0&#39;s rmse: 0.968163 [281] valid_0&#39;s rmse: 0.968149 [282] valid_0&#39;s rmse: 0.968149 [283] valid_0&#39;s rmse: 0.968064 [284] valid_0&#39;s rmse: 0.968101 [285] valid_0&#39;s rmse: 0.968057 [286] valid_0&#39;s rmse: 0.968073 [287] valid_0&#39;s rmse: 0.968053 [288] valid_0&#39;s rmse: 0.967984 [289] valid_0&#39;s rmse: 0.968012 [290] valid_0&#39;s rmse: 0.968006 [291] valid_0&#39;s rmse: 0.968022 [292] valid_0&#39;s rmse: 0.968026 [293] valid_0&#39;s rmse: 0.968079 [294] valid_0&#39;s rmse: 0.968079 [295] valid_0&#39;s rmse: 0.968752 [296] valid_0&#39;s rmse: 0.968766 [297] valid_0&#39;s rmse: 0.968845 [298] valid_0&#39;s rmse: 0.968824 Early stopping, best iteration is: [268] valid_0&#39;s rmse: 0.967713 . from sklearn.metrics import mean_squared_error RMSE = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;RMSE :&quot;, RMSE) . RMSE : 0.7574486861695228 . RMSE &#48320;&#46041;&#44284;&#51221; . import lightgbm as lgb lgb.plot_metric(lgbm_r) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdd61940350&gt; . &#44368;&#52264;&#44160;&#51613;&#51012; &#49884;&#46020;&#54644; &#48376;&#45796; - Grouped K-Fold &#47484; &#51060;&#50857;&#54620;&#45796;. . from sklearn.model_selection import KFold kf = KFold(n_splits=3) . rmse_set = [] for train_idx, test_idx in kf.split(X, y, groups = X.date_block_num): train_X, train_y = X.loc[train_idx], y.loc[train_idx] test_X, test_y = X.loc[test_idx], y.loc[test_idx] lgbm_r.fit(train_X,train_y, eval_set=[(X_valid, y_valid)], eval_metric=&#39;rmse&#39;,verbose = True) pred_y = lgbm_r.predict(test_X) rmse_set.append(np.sqrt(mean_squared_error(test_y, pred_y))) . [1] valid_0&#39;s rmse: 1.16323 Training until validation scores don&#39;t improve for 30 rounds. [2] valid_0&#39;s rmse: 1.1455 [3] valid_0&#39;s rmse: 1.132 [4] valid_0&#39;s rmse: 1.12192 [5] valid_0&#39;s rmse: 1.11288 [6] valid_0&#39;s rmse: 1.09702 [7] valid_0&#39;s rmse: 1.08345 [8] valid_0&#39;s rmse: 1.0721 [9] valid_0&#39;s rmse: 1.06543 [10] valid_0&#39;s rmse: 1.05706 [11] valid_0&#39;s rmse: 1.05353 [12] valid_0&#39;s rmse: 1.04227 [13] valid_0&#39;s rmse: 1.03871 [14] valid_0&#39;s rmse: 1.0292 [15] valid_0&#39;s rmse: 1.02503 [16] valid_0&#39;s rmse: 1.02278 [17] valid_0&#39;s rmse: 1.02059 [18] valid_0&#39;s rmse: 1.01856 [19] valid_0&#39;s rmse: 1.0173 [20] valid_0&#39;s rmse: 1.01481 [21] valid_0&#39;s rmse: 1.01262 [22] valid_0&#39;s rmse: 1.01078 [23] valid_0&#39;s rmse: 1.00586 [24] valid_0&#39;s rmse: 1.00432 [25] valid_0&#39;s rmse: 1.00015 [26] valid_0&#39;s rmse: 0.999448 [27] valid_0&#39;s rmse: 0.998947 [28] valid_0&#39;s rmse: 0.998095 [29] valid_0&#39;s rmse: 0.997429 [30] valid_0&#39;s rmse: 0.996839 [31] valid_0&#39;s rmse: 0.996275 [32] valid_0&#39;s rmse: 0.996095 [33] valid_0&#39;s rmse: 0.995863 [34] valid_0&#39;s rmse: 0.995469 [35] valid_0&#39;s rmse: 0.995101 [36] valid_0&#39;s rmse: 0.994694 [37] valid_0&#39;s rmse: 0.994512 [38] valid_0&#39;s rmse: 0.994277 [39] valid_0&#39;s rmse: 0.994103 [40] valid_0&#39;s rmse: 0.991403 [41] valid_0&#39;s rmse: 0.991109 [42] valid_0&#39;s rmse: 0.988924 [43] valid_0&#39;s rmse: 0.988797 [44] valid_0&#39;s rmse: 0.988778 [45] valid_0&#39;s rmse: 0.988716 [46] valid_0&#39;s rmse: 0.989267 [47] valid_0&#39;s rmse: 0.989161 [48] valid_0&#39;s rmse: 0.98894 [49] valid_0&#39;s rmse: 0.98881 [50] valid_0&#39;s rmse: 0.988822 [51] valid_0&#39;s rmse: 0.988505 [52] valid_0&#39;s rmse: 0.988337 [53] valid_0&#39;s rmse: 0.986476 [54] valid_0&#39;s rmse: 0.986199 [55] valid_0&#39;s rmse: 0.986203 [56] valid_0&#39;s rmse: 0.986165 [57] valid_0&#39;s rmse: 0.986004 [58] valid_0&#39;s rmse: 0.98577 [59] valid_0&#39;s rmse: 0.985605 [60] valid_0&#39;s rmse: 0.985523 [61] valid_0&#39;s rmse: 0.985354 [62] valid_0&#39;s rmse: 0.985328 [63] valid_0&#39;s rmse: 0.983487 [64] valid_0&#39;s rmse: 0.98327 [65] valid_0&#39;s rmse: 0.978555 [66] valid_0&#39;s rmse: 0.978548 [67] valid_0&#39;s rmse: 0.978402 [68] valid_0&#39;s rmse: 0.978256 [69] valid_0&#39;s rmse: 0.978251 [70] valid_0&#39;s rmse: 0.977199 [71] valid_0&#39;s rmse: 0.977021 [72] valid_0&#39;s rmse: 0.977021 [73] valid_0&#39;s rmse: 0.976935 [74] valid_0&#39;s rmse: 0.976759 [75] valid_0&#39;s rmse: 0.976737 [76] valid_0&#39;s rmse: 0.976685 [77] valid_0&#39;s rmse: 0.976639 [78] valid_0&#39;s rmse: 0.975125 [79] valid_0&#39;s rmse: 0.975285 [80] valid_0&#39;s rmse: 0.975168 [81] valid_0&#39;s rmse: 0.97461 [82] valid_0&#39;s rmse: 0.974619 [83] valid_0&#39;s rmse: 0.974952 [84] valid_0&#39;s rmse: 0.973569 [85] valid_0&#39;s rmse: 0.969388 [86] valid_0&#39;s rmse: 0.969332 [87] valid_0&#39;s rmse: 0.96931 [88] valid_0&#39;s rmse: 0.969222 [89] valid_0&#39;s rmse: 0.968908 [90] valid_0&#39;s rmse: 0.968887 [91] valid_0&#39;s rmse: 0.968811 [92] valid_0&#39;s rmse: 0.968495 [93] valid_0&#39;s rmse: 0.968354 [94] valid_0&#39;s rmse: 0.968242 [95] valid_0&#39;s rmse: 0.968239 [96] valid_0&#39;s rmse: 0.967221 [97] valid_0&#39;s rmse: 0.967083 [98] valid_0&#39;s rmse: 0.96671 [99] valid_0&#39;s rmse: 0.966895 [100] valid_0&#39;s rmse: 0.9667 [101] valid_0&#39;s rmse: 0.96556 [102] valid_0&#39;s rmse: 0.96555 [103] valid_0&#39;s rmse: 0.965426 [104] valid_0&#39;s rmse: 0.965305 [105] valid_0&#39;s rmse: 0.965294 [106] valid_0&#39;s rmse: 0.965122 [107] valid_0&#39;s rmse: 0.965326 [108] valid_0&#39;s rmse: 0.965346 [109] valid_0&#39;s rmse: 0.965284 [110] valid_0&#39;s rmse: 0.965281 [111] valid_0&#39;s rmse: 0.965282 [112] valid_0&#39;s rmse: 0.964961 [113] valid_0&#39;s rmse: 0.964931 [114] valid_0&#39;s rmse: 0.964854 [115] valid_0&#39;s rmse: 0.964734 [116] valid_0&#39;s rmse: 0.964739 [117] valid_0&#39;s rmse: 0.963802 [118] valid_0&#39;s rmse: 0.963718 [119] valid_0&#39;s rmse: 0.963721 [120] valid_0&#39;s rmse: 0.963358 [121] valid_0&#39;s rmse: 0.963311 [122] valid_0&#39;s rmse: 0.963216 [123] valid_0&#39;s rmse: 0.963047 [124] valid_0&#39;s rmse: 0.962936 [125] valid_0&#39;s rmse: 0.962371 [126] valid_0&#39;s rmse: 0.962404 [127] valid_0&#39;s rmse: 0.962301 [128] valid_0&#39;s rmse: 0.96226 [129] valid_0&#39;s rmse: 0.962244 [130] valid_0&#39;s rmse: 0.962026 [131] valid_0&#39;s rmse: 0.957809 [132] valid_0&#39;s rmse: 0.957598 [133] valid_0&#39;s rmse: 0.957481 [134] valid_0&#39;s rmse: 0.957695 [135] valid_0&#39;s rmse: 0.95767 [136] valid_0&#39;s rmse: 0.957641 [137] valid_0&#39;s rmse: 0.957645 [138] valid_0&#39;s rmse: 0.957634 [139] valid_0&#39;s rmse: 0.9575 [140] valid_0&#39;s rmse: 0.957461 [141] valid_0&#39;s rmse: 0.95727 [142] valid_0&#39;s rmse: 0.957265 [143] valid_0&#39;s rmse: 0.957257 [144] valid_0&#39;s rmse: 0.954153 [145] valid_0&#39;s rmse: 0.954081 [146] valid_0&#39;s rmse: 0.953974 [147] valid_0&#39;s rmse: 0.953858 [148] valid_0&#39;s rmse: 0.953789 [149] valid_0&#39;s rmse: 0.95378 [150] valid_0&#39;s rmse: 0.953753 [151] valid_0&#39;s rmse: 0.953663 [152] valid_0&#39;s rmse: 0.953651 [153] valid_0&#39;s rmse: 0.955319 [154] valid_0&#39;s rmse: 0.955316 [155] valid_0&#39;s rmse: 0.955308 [156] valid_0&#39;s rmse: 0.954494 [157] valid_0&#39;s rmse: 0.954458 [158] valid_0&#39;s rmse: 0.954463 [159] valid_0&#39;s rmse: 0.954303 [160] valid_0&#39;s rmse: 0.954421 [161] valid_0&#39;s rmse: 0.954408 [162] valid_0&#39;s rmse: 0.95359 [163] valid_0&#39;s rmse: 0.953588 [164] valid_0&#39;s rmse: 0.953587 [165] valid_0&#39;s rmse: 0.953539 [166] valid_0&#39;s rmse: 0.953486 [167] valid_0&#39;s rmse: 0.953421 [168] valid_0&#39;s rmse: 0.953195 [169] valid_0&#39;s rmse: 0.952814 [170] valid_0&#39;s rmse: 0.952675 [171] valid_0&#39;s rmse: 0.952644 [172] valid_0&#39;s rmse: 0.952648 [173] valid_0&#39;s rmse: 0.95259 [174] valid_0&#39;s rmse: 0.952847 [175] valid_0&#39;s rmse: 0.952309 [176] valid_0&#39;s rmse: 0.952298 [177] valid_0&#39;s rmse: 0.952297 [178] valid_0&#39;s rmse: 0.952233 [179] valid_0&#39;s rmse: 0.952253 [180] valid_0&#39;s rmse: 0.952185 [181] valid_0&#39;s rmse: 0.952179 [182] valid_0&#39;s rmse: 0.952168 [183] valid_0&#39;s rmse: 0.952092 [184] valid_0&#39;s rmse: 0.952093 [185] valid_0&#39;s rmse: 0.952093 [186] valid_0&#39;s rmse: 0.952013 [187] valid_0&#39;s rmse: 0.950802 [188] valid_0&#39;s rmse: 0.950644 [189] valid_0&#39;s rmse: 0.950575 [190] valid_0&#39;s rmse: 0.950581 [191] valid_0&#39;s rmse: 0.950512 [192] valid_0&#39;s rmse: 0.950379 [193] valid_0&#39;s rmse: 0.950365 [194] valid_0&#39;s rmse: 0.950039 [195] valid_0&#39;s rmse: 0.949964 [196] valid_0&#39;s rmse: 0.949669 [197] valid_0&#39;s rmse: 0.949633 [198] valid_0&#39;s rmse: 0.949531 [199] valid_0&#39;s rmse: 0.949498 [200] valid_0&#39;s rmse: 0.949415 [201] valid_0&#39;s rmse: 0.94929 [202] valid_0&#39;s rmse: 0.949146 [203] valid_0&#39;s rmse: 0.948783 [204] valid_0&#39;s rmse: 0.948724 [205] valid_0&#39;s rmse: 0.948692 [206] valid_0&#39;s rmse: 0.948666 [207] valid_0&#39;s rmse: 0.948669 [208] valid_0&#39;s rmse: 0.948607 [209] valid_0&#39;s rmse: 0.94846 [210] valid_0&#39;s rmse: 0.94836 [211] valid_0&#39;s rmse: 0.948257 [212] valid_0&#39;s rmse: 0.948792 [213] valid_0&#39;s rmse: 0.948794 [214] valid_0&#39;s rmse: 0.948742 [215] valid_0&#39;s rmse: 0.948698 [216] valid_0&#39;s rmse: 0.948641 [217] valid_0&#39;s rmse: 0.948536 [218] valid_0&#39;s rmse: 0.948528 [219] valid_0&#39;s rmse: 0.948524 [220] valid_0&#39;s rmse: 0.948446 [221] valid_0&#39;s rmse: 0.948434 [222] valid_0&#39;s rmse: 0.947047 [223] valid_0&#39;s rmse: 0.94689 [224] valid_0&#39;s rmse: 0.946839 [225] valid_0&#39;s rmse: 0.946688 [226] valid_0&#39;s rmse: 0.946639 [227] valid_0&#39;s rmse: 0.946552 [228] valid_0&#39;s rmse: 0.946544 [229] valid_0&#39;s rmse: 0.947116 [230] valid_0&#39;s rmse: 0.947075 [231] valid_0&#39;s rmse: 0.945956 [232] valid_0&#39;s rmse: 0.945899 [233] valid_0&#39;s rmse: 0.945938 [234] valid_0&#39;s rmse: 0.945899 [235] valid_0&#39;s rmse: 0.945871 [236] valid_0&#39;s rmse: 0.945681 [237] valid_0&#39;s rmse: 0.945658 [238] valid_0&#39;s rmse: 0.945656 [239] valid_0&#39;s rmse: 0.945652 [240] valid_0&#39;s rmse: 0.945274 [241] valid_0&#39;s rmse: 0.945143 [242] valid_0&#39;s rmse: 0.945118 [243] valid_0&#39;s rmse: 0.945093 [244] valid_0&#39;s rmse: 0.945056 [245] valid_0&#39;s rmse: 0.945016 [246] valid_0&#39;s rmse: 0.944984 [247] valid_0&#39;s rmse: 0.94496 [248] valid_0&#39;s rmse: 0.944959 [249] valid_0&#39;s rmse: 0.944934 [250] valid_0&#39;s rmse: 0.944939 [251] valid_0&#39;s rmse: 0.944827 [252] valid_0&#39;s rmse: 0.94475 [253] valid_0&#39;s rmse: 0.944728 [254] valid_0&#39;s rmse: 0.944308 [255] valid_0&#39;s rmse: 0.944107 [256] valid_0&#39;s rmse: 0.94412 [257] valid_0&#39;s rmse: 0.944037 [258] valid_0&#39;s rmse: 0.944035 [259] valid_0&#39;s rmse: 0.945114 [260] valid_0&#39;s rmse: 0.944389 [261] valid_0&#39;s rmse: 0.944337 [262] valid_0&#39;s rmse: 0.944337 [263] valid_0&#39;s rmse: 0.94426 [264] valid_0&#39;s rmse: 0.944259 [265] valid_0&#39;s rmse: 0.944165 [266] valid_0&#39;s rmse: 0.943862 [267] valid_0&#39;s rmse: 0.943857 [268] valid_0&#39;s rmse: 0.943863 [269] valid_0&#39;s rmse: 0.943629 [270] valid_0&#39;s rmse: 0.943626 [271] valid_0&#39;s rmse: 0.943617 [272] valid_0&#39;s rmse: 0.943485 [273] valid_0&#39;s rmse: 0.943442 [274] valid_0&#39;s rmse: 0.945009 [275] valid_0&#39;s rmse: 0.945017 [276] valid_0&#39;s rmse: 0.944991 [277] valid_0&#39;s rmse: 0.944954 [278] valid_0&#39;s rmse: 0.944869 [279] valid_0&#39;s rmse: 0.944866 [280] valid_0&#39;s rmse: 0.942448 [281] valid_0&#39;s rmse: 0.942419 [282] valid_0&#39;s rmse: 0.942347 [283] valid_0&#39;s rmse: 0.942331 [284] valid_0&#39;s rmse: 0.942338 [285] valid_0&#39;s rmse: 0.942344 [286] valid_0&#39;s rmse: 0.942308 [287] valid_0&#39;s rmse: 0.9423 [288] valid_0&#39;s rmse: 0.942191 [289] valid_0&#39;s rmse: 0.942173 [290] valid_0&#39;s rmse: 0.942141 [291] valid_0&#39;s rmse: 0.942035 [292] valid_0&#39;s rmse: 0.941981 [293] valid_0&#39;s rmse: 0.941721 [294] valid_0&#39;s rmse: 0.941662 [295] valid_0&#39;s rmse: 0.941571 [296] valid_0&#39;s rmse: 0.941712 [297] valid_0&#39;s rmse: 0.94169 [298] valid_0&#39;s rmse: 0.941639 [299] valid_0&#39;s rmse: 0.941618 [300] valid_0&#39;s rmse: 0.941575 Did not meet early stopping. Best iteration is: [295] valid_0&#39;s rmse: 0.941571 [1] valid_0&#39;s rmse: 1.16204 Training until validation scores don&#39;t improve for 30 rounds. [2] valid_0&#39;s rmse: 1.14442 [3] valid_0&#39;s rmse: 1.13031 [4] valid_0&#39;s rmse: 1.11909 [5] valid_0&#39;s rmse: 1.10915 [6] valid_0&#39;s rmse: 1.09321 [7] valid_0&#39;s rmse: 1.08007 [8] valid_0&#39;s rmse: 1.06902 [9] valid_0&#39;s rmse: 1.06355 [10] valid_0&#39;s rmse: 1.05505 [11] valid_0&#39;s rmse: 1.05199 [12] valid_0&#39;s rmse: 1.04086 [13] valid_0&#39;s rmse: 1.03806 [14] valid_0&#39;s rmse: 1.03306 [15] valid_0&#39;s rmse: 1.0284 [16] valid_0&#39;s rmse: 1.02595 [17] valid_0&#39;s rmse: 1.02482 [18] valid_0&#39;s rmse: 1.01832 [19] valid_0&#39;s rmse: 1.01675 [20] valid_0&#39;s rmse: 1.01473 [21] valid_0&#39;s rmse: 1.01293 [22] valid_0&#39;s rmse: 1.01136 [23] valid_0&#39;s rmse: 1.00564 [24] valid_0&#39;s rmse: 1.00459 [25] valid_0&#39;s rmse: 1.00301 [26] valid_0&#39;s rmse: 1.00235 [27] valid_0&#39;s rmse: 1.00188 [28] valid_0&#39;s rmse: 1.00132 [29] valid_0&#39;s rmse: 1.00085 [30] valid_0&#39;s rmse: 1.00042 [31] valid_0&#39;s rmse: 0.999908 [32] valid_0&#39;s rmse: 0.999591 [33] valid_0&#39;s rmse: 0.999468 [34] valid_0&#39;s rmse: 0.999457 [35] valid_0&#39;s rmse: 0.999084 [36] valid_0&#39;s rmse: 0.998655 [37] valid_0&#39;s rmse: 0.998581 [38] valid_0&#39;s rmse: 0.99829 [39] valid_0&#39;s rmse: 0.99828 [40] valid_0&#39;s rmse: 0.998171 [41] valid_0&#39;s rmse: 0.997881 [42] valid_0&#39;s rmse: 0.997671 [43] valid_0&#39;s rmse: 0.997438 [44] valid_0&#39;s rmse: 0.997479 [45] valid_0&#39;s rmse: 0.997269 [46] valid_0&#39;s rmse: 0.997016 [47] valid_0&#39;s rmse: 0.99398 [48] valid_0&#39;s rmse: 0.99379 [49] valid_0&#39;s rmse: 0.993675 [50] valid_0&#39;s rmse: 0.993561 [51] valid_0&#39;s rmse: 0.993361 [52] valid_0&#39;s rmse: 0.993072 [53] valid_0&#39;s rmse: 0.992743 [54] valid_0&#39;s rmse: 0.992456 [55] valid_0&#39;s rmse: 0.992372 [56] valid_0&#39;s rmse: 0.98985 [57] valid_0&#39;s rmse: 0.989646 [58] valid_0&#39;s rmse: 0.989605 [59] valid_0&#39;s rmse: 0.989477 [60] valid_0&#39;s rmse: 0.989466 [61] valid_0&#39;s rmse: 0.987333 [62] valid_0&#39;s rmse: 0.987193 [63] valid_0&#39;s rmse: 0.989151 [64] valid_0&#39;s rmse: 0.989136 [65] valid_0&#39;s rmse: 0.988913 [66] valid_0&#39;s rmse: 0.9889 [67] valid_0&#39;s rmse: 0.986371 [68] valid_0&#39;s rmse: 0.986204 [69] valid_0&#39;s rmse: 0.986068 [70] valid_0&#39;s rmse: 0.983685 [71] valid_0&#39;s rmse: 0.98348 [72] valid_0&#39;s rmse: 0.983433 [73] valid_0&#39;s rmse: 0.983173 [74] valid_0&#39;s rmse: 0.983071 [75] valid_0&#39;s rmse: 0.98246 [76] valid_0&#39;s rmse: 0.982281 [77] valid_0&#39;s rmse: 0.982128 [78] valid_0&#39;s rmse: 0.982102 [79] valid_0&#39;s rmse: 0.982029 [80] valid_0&#39;s rmse: 0.981868 [81] valid_0&#39;s rmse: 0.981816 [82] valid_0&#39;s rmse: 0.981723 [83] valid_0&#39;s rmse: 0.981461 [84] valid_0&#39;s rmse: 0.981316 [85] valid_0&#39;s rmse: 0.981302 [86] valid_0&#39;s rmse: 0.981188 [87] valid_0&#39;s rmse: 0.981099 [88] valid_0&#39;s rmse: 0.980498 [89] valid_0&#39;s rmse: 0.980291 [90] valid_0&#39;s rmse: 0.980273 [91] valid_0&#39;s rmse: 0.980079 [92] valid_0&#39;s rmse: 0.980085 [93] valid_0&#39;s rmse: 0.980063 [94] valid_0&#39;s rmse: 0.979911 [95] valid_0&#39;s rmse: 0.979926 [96] valid_0&#39;s rmse: 0.979802 [97] valid_0&#39;s rmse: 0.979653 [98] valid_0&#39;s rmse: 0.979585 [99] valid_0&#39;s rmse: 0.979179 [100] valid_0&#39;s rmse: 0.977366 [101] valid_0&#39;s rmse: 0.977287 [102] valid_0&#39;s rmse: 0.977131 [103] valid_0&#39;s rmse: 0.977122 [104] valid_0&#39;s rmse: 0.977243 [105] valid_0&#39;s rmse: 0.977242 [106] valid_0&#39;s rmse: 0.975971 [107] valid_0&#39;s rmse: 0.975863 [108] valid_0&#39;s rmse: 0.975686 [109] valid_0&#39;s rmse: 0.975328 [110] valid_0&#39;s rmse: 0.97498 [111] valid_0&#39;s rmse: 0.974944 [112] valid_0&#39;s rmse: 0.974792 [113] valid_0&#39;s rmse: 0.974826 [114] valid_0&#39;s rmse: 0.973698 [115] valid_0&#39;s rmse: 0.973645 [116] valid_0&#39;s rmse: 0.973628 [117] valid_0&#39;s rmse: 0.973582 [118] valid_0&#39;s rmse: 0.973392 [119] valid_0&#39;s rmse: 0.973371 [120] valid_0&#39;s rmse: 0.973333 [121] valid_0&#39;s rmse: 0.973201 [122] valid_0&#39;s rmse: 0.973125 [123] valid_0&#39;s rmse: 0.973065 [124] valid_0&#39;s rmse: 0.972991 [125] valid_0&#39;s rmse: 0.97235 [126] valid_0&#39;s rmse: 0.972214 [127] valid_0&#39;s rmse: 0.972167 [128] valid_0&#39;s rmse: 0.972167 [129] valid_0&#39;s rmse: 0.971955 [130] valid_0&#39;s rmse: 0.97193 [131] valid_0&#39;s rmse: 0.971791 [132] valid_0&#39;s rmse: 0.971728 [133] valid_0&#39;s rmse: 0.971723 [134] valid_0&#39;s rmse: 0.970995 [135] valid_0&#39;s rmse: 0.97091 [136] valid_0&#39;s rmse: 0.970797 [137] valid_0&#39;s rmse: 0.97058 [138] valid_0&#39;s rmse: 0.969707 [139] valid_0&#39;s rmse: 0.96962 [140] valid_0&#39;s rmse: 0.969411 [141] valid_0&#39;s rmse: 0.969399 [142] valid_0&#39;s rmse: 0.969285 [143] valid_0&#39;s rmse: 0.969191 [144] valid_0&#39;s rmse: 0.969081 [145] valid_0&#39;s rmse: 0.968943 [146] valid_0&#39;s rmse: 0.968721 [147] valid_0&#39;s rmse: 0.968723 [148] valid_0&#39;s rmse: 0.968674 [149] valid_0&#39;s rmse: 0.96851 [150] valid_0&#39;s rmse: 0.968367 [151] valid_0&#39;s rmse: 0.968303 [152] valid_0&#39;s rmse: 0.968298 [153] valid_0&#39;s rmse: 0.968155 [154] valid_0&#39;s rmse: 0.968075 [155] valid_0&#39;s rmse: 0.968075 [156] valid_0&#39;s rmse: 0.968033 [157] valid_0&#39;s rmse: 0.968043 [158] valid_0&#39;s rmse: 0.96342 [159] valid_0&#39;s rmse: 0.963389 [160] valid_0&#39;s rmse: 0.963326 [161] valid_0&#39;s rmse: 0.963245 [162] valid_0&#39;s rmse: 0.963269 [163] valid_0&#39;s rmse: 0.963233 [164] valid_0&#39;s rmse: 0.963172 [165] valid_0&#39;s rmse: 0.962991 [166] valid_0&#39;s rmse: 0.962955 [167] valid_0&#39;s rmse: 0.96292 [168] valid_0&#39;s rmse: 0.962611 [169] valid_0&#39;s rmse: 0.962541 [170] valid_0&#39;s rmse: 0.962448 [171] valid_0&#39;s rmse: 0.962362 [172] valid_0&#39;s rmse: 0.962283 [173] valid_0&#39;s rmse: 0.962215 [174] valid_0&#39;s rmse: 0.962103 [175] valid_0&#39;s rmse: 0.962002 [176] valid_0&#39;s rmse: 0.961976 [177] valid_0&#39;s rmse: 0.961941 [178] valid_0&#39;s rmse: 0.961885 [179] valid_0&#39;s rmse: 0.96172 [180] valid_0&#39;s rmse: 0.961667 [181] valid_0&#39;s rmse: 0.961628 [182] valid_0&#39;s rmse: 0.961627 [183] valid_0&#39;s rmse: 0.961452 [184] valid_0&#39;s rmse: 0.956233 [185] valid_0&#39;s rmse: 0.956183 [186] valid_0&#39;s rmse: 0.95615 [187] valid_0&#39;s rmse: 0.956116 [188] valid_0&#39;s rmse: 0.956081 [189] valid_0&#39;s rmse: 0.955824 [190] valid_0&#39;s rmse: 0.955788 [191] valid_0&#39;s rmse: 0.955785 [192] valid_0&#39;s rmse: 0.955847 [193] valid_0&#39;s rmse: 0.955847 [194] valid_0&#39;s rmse: 0.956342 [195] valid_0&#39;s rmse: 0.956242 [196] valid_0&#39;s rmse: 0.95625 [197] valid_0&#39;s rmse: 0.956245 [198] valid_0&#39;s rmse: 0.95624 [199] valid_0&#39;s rmse: 0.956227 [200] valid_0&#39;s rmse: 0.956181 [201] valid_0&#39;s rmse: 0.956141 [202] valid_0&#39;s rmse: 0.956112 [203] valid_0&#39;s rmse: 0.956033 [204] valid_0&#39;s rmse: 0.956146 [205] valid_0&#39;s rmse: 0.956113 [206] valid_0&#39;s rmse: 0.956072 [207] valid_0&#39;s rmse: 0.955986 [208] valid_0&#39;s rmse: 0.955942 [209] valid_0&#39;s rmse: 0.955881 [210] valid_0&#39;s rmse: 0.95545 [211] valid_0&#39;s rmse: 0.955452 [212] valid_0&#39;s rmse: 0.955322 [213] valid_0&#39;s rmse: 0.950916 [214] valid_0&#39;s rmse: 0.950914 [215] valid_0&#39;s rmse: 0.950902 [216] valid_0&#39;s rmse: 0.953595 [217] valid_0&#39;s rmse: 0.953482 [218] valid_0&#39;s rmse: 0.953482 [219] valid_0&#39;s rmse: 0.953419 [220] valid_0&#39;s rmse: 0.95338 [221] valid_0&#39;s rmse: 0.953323 [222] valid_0&#39;s rmse: 0.953318 [223] valid_0&#39;s rmse: 0.953262 [224] valid_0&#39;s rmse: 0.953202 [225] valid_0&#39;s rmse: 0.95312 [226] valid_0&#39;s rmse: 0.952993 [227] valid_0&#39;s rmse: 0.952986 [228] valid_0&#39;s rmse: 0.952993 [229] valid_0&#39;s rmse: 0.952982 [230] valid_0&#39;s rmse: 0.952803 [231] valid_0&#39;s rmse: 0.952774 [232] valid_0&#39;s rmse: 0.952731 [233] valid_0&#39;s rmse: 0.952746 [234] valid_0&#39;s rmse: 0.952746 [235] valid_0&#39;s rmse: 0.952698 [236] valid_0&#39;s rmse: 0.952543 [237] valid_0&#39;s rmse: 0.952507 [238] valid_0&#39;s rmse: 0.9525 [239] valid_0&#39;s rmse: 0.952438 [240] valid_0&#39;s rmse: 0.952055 [241] valid_0&#39;s rmse: 0.948272 [242] valid_0&#39;s rmse: 0.948178 [243] valid_0&#39;s rmse: 0.948127 [244] valid_0&#39;s rmse: 0.948039 [245] valid_0&#39;s rmse: 0.948033 [246] valid_0&#39;s rmse: 0.94804 [247] valid_0&#39;s rmse: 0.948032 [248] valid_0&#39;s rmse: 0.948034 [249] valid_0&#39;s rmse: 0.948027 [250] valid_0&#39;s rmse: 0.947854 [251] valid_0&#39;s rmse: 0.94785 [252] valid_0&#39;s rmse: 0.947842 [253] valid_0&#39;s rmse: 0.947836 [254] valid_0&#39;s rmse: 0.947774 [255] valid_0&#39;s rmse: 0.947772 [256] valid_0&#39;s rmse: 0.947662 [257] valid_0&#39;s rmse: 0.947638 [258] valid_0&#39;s rmse: 0.94763 [259] valid_0&#39;s rmse: 0.94759 [260] valid_0&#39;s rmse: 0.94754 [261] valid_0&#39;s rmse: 0.94472 [262] valid_0&#39;s rmse: 0.94468 [263] valid_0&#39;s rmse: 0.944116 [264] valid_0&#39;s rmse: 0.944133 [265] valid_0&#39;s rmse: 0.944028 [266] valid_0&#39;s rmse: 0.943992 [267] valid_0&#39;s rmse: 0.943987 [268] valid_0&#39;s rmse: 0.943982 [269] valid_0&#39;s rmse: 0.944026 [270] valid_0&#39;s rmse: 0.944009 [271] valid_0&#39;s rmse: 0.943972 [272] valid_0&#39;s rmse: 0.94397 [273] valid_0&#39;s rmse: 0.943937 [274] valid_0&#39;s rmse: 0.943899 [275] valid_0&#39;s rmse: 0.943901 [276] valid_0&#39;s rmse: 0.943896 [277] valid_0&#39;s rmse: 0.943963 [278] valid_0&#39;s rmse: 0.943916 [279] valid_0&#39;s rmse: 0.943832 [280] valid_0&#39;s rmse: 0.943805 [281] valid_0&#39;s rmse: 0.943804 [282] valid_0&#39;s rmse: 0.943828 [283] valid_0&#39;s rmse: 0.943817 [284] valid_0&#39;s rmse: 0.943818 [285] valid_0&#39;s rmse: 0.943802 [286] valid_0&#39;s rmse: 0.943792 [287] valid_0&#39;s rmse: 0.943771 [288] valid_0&#39;s rmse: 0.943718 [289] valid_0&#39;s rmse: 0.943598 [290] valid_0&#39;s rmse: 0.943509 [291] valid_0&#39;s rmse: 0.944206 [292] valid_0&#39;s rmse: 0.94147 [293] valid_0&#39;s rmse: 0.94142 [294] valid_0&#39;s rmse: 0.941423 [295] valid_0&#39;s rmse: 0.941404 [296] valid_0&#39;s rmse: 0.941387 [297] valid_0&#39;s rmse: 0.941059 [298] valid_0&#39;s rmse: 0.941135 [299] valid_0&#39;s rmse: 0.940911 [300] valid_0&#39;s rmse: 0.940889 Did not meet early stopping. Best iteration is: [300] valid_0&#39;s rmse: 0.940889 [1] valid_0&#39;s rmse: 1.17561 Training until validation scores don&#39;t improve for 30 rounds. [2] valid_0&#39;s rmse: 1.16047 [3] valid_0&#39;s rmse: 1.14765 [4] valid_0&#39;s rmse: 1.14215 [5] valid_0&#39;s rmse: 1.13325 [6] valid_0&#39;s rmse: 1.11841 [7] valid_0&#39;s rmse: 1.10618 [8] valid_0&#39;s rmse: 1.09386 [9] valid_0&#39;s rmse: 1.08876 [10] valid_0&#39;s rmse: 1.08055 [11] valid_0&#39;s rmse: 1.07824 [12] valid_0&#39;s rmse: 1.07037 [13] valid_0&#39;s rmse: 1.06257 [14] valid_0&#39;s rmse: 1.05694 [15] valid_0&#39;s rmse: 1.05161 [16] valid_0&#39;s rmse: 1.05064 [17] valid_0&#39;s rmse: 1.04859 [18] valid_0&#39;s rmse: 1.0467 [19] valid_0&#39;s rmse: 1.04478 [20] valid_0&#39;s rmse: 1.0413 [21] valid_0&#39;s rmse: 1.03845 [22] valid_0&#39;s rmse: 1.03566 [23] valid_0&#39;s rmse: 1.03333 [24] valid_0&#39;s rmse: 1.03112 [25] valid_0&#39;s rmse: 1.02977 [26] valid_0&#39;s rmse: 1.02817 [27] valid_0&#39;s rmse: 1.02293 [28] valid_0&#39;s rmse: 1.01824 [29] valid_0&#39;s rmse: 1.01721 [30] valid_0&#39;s rmse: 1.01665 [31] valid_0&#39;s rmse: 1.01628 [32] valid_0&#39;s rmse: 1.016 [33] valid_0&#39;s rmse: 1.01562 [34] valid_0&#39;s rmse: 1.01524 [35] valid_0&#39;s rmse: 1.01471 [36] valid_0&#39;s rmse: 1.01114 [37] valid_0&#39;s rmse: 1.01068 [38] valid_0&#39;s rmse: 1.01005 [39] valid_0&#39;s rmse: 1.0071 [40] valid_0&#39;s rmse: 1.00688 [41] valid_0&#39;s rmse: 1.00679 [42] valid_0&#39;s rmse: 1.00657 [43] valid_0&#39;s rmse: 1.00647 [44] valid_0&#39;s rmse: 1.00663 [45] valid_0&#39;s rmse: 1.00647 [46] valid_0&#39;s rmse: 1.00633 [47] valid_0&#39;s rmse: 1.00593 [48] valid_0&#39;s rmse: 1.00588 [49] valid_0&#39;s rmse: 1.00531 [50] valid_0&#39;s rmse: 1.00503 [51] valid_0&#39;s rmse: 1.00498 [52] valid_0&#39;s rmse: 1.00307 [53] valid_0&#39;s rmse: 1.00289 [54] valid_0&#39;s rmse: 1.00267 [55] valid_0&#39;s rmse: 1.00284 [56] valid_0&#39;s rmse: 1.00278 [57] valid_0&#39;s rmse: 1.00086 [58] valid_0&#39;s rmse: 1.00089 [59] valid_0&#39;s rmse: 1.00078 [60] valid_0&#39;s rmse: 1.00062 [61] valid_0&#39;s rmse: 1.0008 [62] valid_0&#39;s rmse: 1.00061 [63] valid_0&#39;s rmse: 1.00039 [64] valid_0&#39;s rmse: 1.00028 [65] valid_0&#39;s rmse: 1.00026 [66] valid_0&#39;s rmse: 1.00023 [67] valid_0&#39;s rmse: 1.00023 [68] valid_0&#39;s rmse: 1.00021 [69] valid_0&#39;s rmse: 1.00018 [70] valid_0&#39;s rmse: 0.998701 [71] valid_0&#39;s rmse: 0.998703 [72] valid_0&#39;s rmse: 0.998687 [73] valid_0&#39;s rmse: 0.998419 [74] valid_0&#39;s rmse: 0.997082 [75] valid_0&#39;s rmse: 0.997105 [76] valid_0&#39;s rmse: 0.997004 [77] valid_0&#39;s rmse: 0.997 [78] valid_0&#39;s rmse: 0.996656 [79] valid_0&#39;s rmse: 0.996748 [80] valid_0&#39;s rmse: 0.995722 [81] valid_0&#39;s rmse: 0.995571 [82] valid_0&#39;s rmse: 0.995523 [83] valid_0&#39;s rmse: 0.995592 [84] valid_0&#39;s rmse: 0.995593 [85] valid_0&#39;s rmse: 0.995528 [86] valid_0&#39;s rmse: 0.995433 [87] valid_0&#39;s rmse: 0.995433 [88] valid_0&#39;s rmse: 0.995488 [89] valid_0&#39;s rmse: 0.995891 [90] valid_0&#39;s rmse: 0.995646 [91] valid_0&#39;s rmse: 0.995341 [92] valid_0&#39;s rmse: 0.995344 [93] valid_0&#39;s rmse: 0.995402 [94] valid_0&#39;s rmse: 0.995316 [95] valid_0&#39;s rmse: 0.995301 [96] valid_0&#39;s rmse: 0.995262 [97] valid_0&#39;s rmse: 0.995536 [98] valid_0&#39;s rmse: 0.995473 [99] valid_0&#39;s rmse: 0.995552 [100] valid_0&#39;s rmse: 0.99501 [101] valid_0&#39;s rmse: 0.995061 [102] valid_0&#39;s rmse: 0.995038 [103] valid_0&#39;s rmse: 0.994895 [104] valid_0&#39;s rmse: 0.994941 [105] valid_0&#39;s rmse: 0.994854 [106] valid_0&#39;s rmse: 0.994734 [107] valid_0&#39;s rmse: 0.994556 [108] valid_0&#39;s rmse: 0.994489 [109] valid_0&#39;s rmse: 0.994537 [110] valid_0&#39;s rmse: 0.994541 [111] valid_0&#39;s rmse: 0.994466 [112] valid_0&#39;s rmse: 0.99429 [113] valid_0&#39;s rmse: 0.994155 [114] valid_0&#39;s rmse: 0.99415 [115] valid_0&#39;s rmse: 0.993521 [116] valid_0&#39;s rmse: 0.993509 [117] valid_0&#39;s rmse: 0.993464 [118] valid_0&#39;s rmse: 0.993421 [119] valid_0&#39;s rmse: 0.993444 [120] valid_0&#39;s rmse: 0.99343 [121] valid_0&#39;s rmse: 0.993428 [122] valid_0&#39;s rmse: 0.993446 [123] valid_0&#39;s rmse: 0.99335 [124] valid_0&#39;s rmse: 0.993256 [125] valid_0&#39;s rmse: 0.993243 [126] valid_0&#39;s rmse: 0.993132 [127] valid_0&#39;s rmse: 0.992996 [128] valid_0&#39;s rmse: 0.992805 [129] valid_0&#39;s rmse: 0.993075 [130] valid_0&#39;s rmse: 0.993021 [131] valid_0&#39;s rmse: 0.992961 [132] valid_0&#39;s rmse: 0.993032 [133] valid_0&#39;s rmse: 0.992966 [134] valid_0&#39;s rmse: 0.993218 [135] valid_0&#39;s rmse: 0.993302 [136] valid_0&#39;s rmse: 0.993282 [137] valid_0&#39;s rmse: 0.993258 [138] valid_0&#39;s rmse: 0.990586 [139] valid_0&#39;s rmse: 0.990669 [140] valid_0&#39;s rmse: 0.990537 [141] valid_0&#39;s rmse: 0.990035 [142] valid_0&#39;s rmse: 0.990034 [143] valid_0&#39;s rmse: 0.98983 [144] valid_0&#39;s rmse: 0.989875 [145] valid_0&#39;s rmse: 0.989901 [146] valid_0&#39;s rmse: 0.989734 [147] valid_0&#39;s rmse: 0.989528 [148] valid_0&#39;s rmse: 0.987529 [149] valid_0&#39;s rmse: 0.98701 [150] valid_0&#39;s rmse: 0.987332 [151] valid_0&#39;s rmse: 0.987253 [152] valid_0&#39;s rmse: 0.98723 [153] valid_0&#39;s rmse: 0.987422 [154] valid_0&#39;s rmse: 0.987343 [155] valid_0&#39;s rmse: 0.987209 [156] valid_0&#39;s rmse: 0.987109 [157] valid_0&#39;s rmse: 0.987141 [158] valid_0&#39;s rmse: 0.987075 [159] valid_0&#39;s rmse: 0.987117 [160] valid_0&#39;s rmse: 0.987103 [161] valid_0&#39;s rmse: 0.987031 [162] valid_0&#39;s rmse: 0.98697 [163] valid_0&#39;s rmse: 0.986992 [164] valid_0&#39;s rmse: 0.987038 [165] valid_0&#39;s rmse: 0.987027 [166] valid_0&#39;s rmse: 0.987033 [167] valid_0&#39;s rmse: 0.986956 [168] valid_0&#39;s rmse: 0.986956 [169] valid_0&#39;s rmse: 0.986646 [170] valid_0&#39;s rmse: 0.984052 [171] valid_0&#39;s rmse: 0.98401 [172] valid_0&#39;s rmse: 0.983936 [173] valid_0&#39;s rmse: 0.984382 [174] valid_0&#39;s rmse: 0.984298 [175] valid_0&#39;s rmse: 0.9839 [176] valid_0&#39;s rmse: 0.983864 [177] valid_0&#39;s rmse: 0.983857 [178] valid_0&#39;s rmse: 0.983865 [179] valid_0&#39;s rmse: 0.983867 [180] valid_0&#39;s rmse: 0.983867 [181] valid_0&#39;s rmse: 0.984234 [182] valid_0&#39;s rmse: 0.984229 [183] valid_0&#39;s rmse: 0.984269 [184] valid_0&#39;s rmse: 0.984338 [185] valid_0&#39;s rmse: 0.984381 [186] valid_0&#39;s rmse: 0.984394 [187] valid_0&#39;s rmse: 0.984349 [188] valid_0&#39;s rmse: 0.984415 [189] valid_0&#39;s rmse: 0.984414 [190] valid_0&#39;s rmse: 0.984661 [191] valid_0&#39;s rmse: 0.984658 [192] valid_0&#39;s rmse: 0.984626 [193] valid_0&#39;s rmse: 0.984722 [194] valid_0&#39;s rmse: 0.984614 [195] valid_0&#39;s rmse: 0.984931 [196] valid_0&#39;s rmse: 0.984658 [197] valid_0&#39;s rmse: 0.98466 [198] valid_0&#39;s rmse: 0.984639 [199] valid_0&#39;s rmse: 0.984637 [200] valid_0&#39;s rmse: 0.983478 [201] valid_0&#39;s rmse: 0.983445 [202] valid_0&#39;s rmse: 0.983624 [203] valid_0&#39;s rmse: 0.983556 [204] valid_0&#39;s rmse: 0.983502 [205] valid_0&#39;s rmse: 0.983434 [206] valid_0&#39;s rmse: 0.983637 [207] valid_0&#39;s rmse: 0.98356 [208] valid_0&#39;s rmse: 0.983476 [209] valid_0&#39;s rmse: 0.983545 [210] valid_0&#39;s rmse: 0.983503 [211] valid_0&#39;s rmse: 0.98342 [212] valid_0&#39;s rmse: 0.983243 [213] valid_0&#39;s rmse: 0.983287 [214] valid_0&#39;s rmse: 0.983276 [215] valid_0&#39;s rmse: 0.983297 [216] valid_0&#39;s rmse: 0.983248 [217] valid_0&#39;s rmse: 0.983215 [218] valid_0&#39;s rmse: 0.982881 [219] valid_0&#39;s rmse: 0.982885 [220] valid_0&#39;s rmse: 0.983053 [221] valid_0&#39;s rmse: 0.982064 [222] valid_0&#39;s rmse: 0.982063 [223] valid_0&#39;s rmse: 0.982067 [224] valid_0&#39;s rmse: 0.982039 [225] valid_0&#39;s rmse: 0.982055 [226] valid_0&#39;s rmse: 0.982129 [227] valid_0&#39;s rmse: 0.982223 [228] valid_0&#39;s rmse: 0.982212 [229] valid_0&#39;s rmse: 0.982127 [230] valid_0&#39;s rmse: 0.982127 [231] valid_0&#39;s rmse: 0.982166 [232] valid_0&#39;s rmse: 0.98216 [233] valid_0&#39;s rmse: 0.982138 [234] valid_0&#39;s rmse: 0.981898 [235] valid_0&#39;s rmse: 0.981942 [236] valid_0&#39;s rmse: 0.981968 [237] valid_0&#39;s rmse: 0.981902 [238] valid_0&#39;s rmse: 0.98191 [239] valid_0&#39;s rmse: 0.981866 [240] valid_0&#39;s rmse: 0.982087 [241] valid_0&#39;s rmse: 0.982063 [242] valid_0&#39;s rmse: 0.981761 [243] valid_0&#39;s rmse: 0.981776 [244] valid_0&#39;s rmse: 0.981802 [245] valid_0&#39;s rmse: 0.981775 [246] valid_0&#39;s rmse: 0.981861 [247] valid_0&#39;s rmse: 0.981853 [248] valid_0&#39;s rmse: 0.981811 [249] valid_0&#39;s rmse: 0.981797 [250] valid_0&#39;s rmse: 0.981793 [251] valid_0&#39;s rmse: 0.981803 [252] valid_0&#39;s rmse: 0.981613 [253] valid_0&#39;s rmse: 0.981567 [254] valid_0&#39;s rmse: 0.981563 [255] valid_0&#39;s rmse: 0.981543 [256] valid_0&#39;s rmse: 0.981573 [257] valid_0&#39;s rmse: 0.981712 [258] valid_0&#39;s rmse: 0.981697 [259] valid_0&#39;s rmse: 0.981683 [260] valid_0&#39;s rmse: 0.981889 [261] valid_0&#39;s rmse: 0.981902 [262] valid_0&#39;s rmse: 0.981801 [263] valid_0&#39;s rmse: 0.981844 [264] valid_0&#39;s rmse: 0.981886 [265] valid_0&#39;s rmse: 0.981876 [266] valid_0&#39;s rmse: 0.982183 [267] valid_0&#39;s rmse: 0.982187 [268] valid_0&#39;s rmse: 0.982117 [269] valid_0&#39;s rmse: 0.981761 [270] valid_0&#39;s rmse: 0.981747 [271] valid_0&#39;s rmse: 0.981739 [272] valid_0&#39;s rmse: 0.981746 [273] valid_0&#39;s rmse: 0.981744 [274] valid_0&#39;s rmse: 0.981769 [275] valid_0&#39;s rmse: 0.981688 [276] valid_0&#39;s rmse: 0.981687 [277] valid_0&#39;s rmse: 0.981709 [278] valid_0&#39;s rmse: 0.981698 [279] valid_0&#39;s rmse: 0.981667 [280] valid_0&#39;s rmse: 0.981929 [281] valid_0&#39;s rmse: 0.98195 [282] valid_0&#39;s rmse: 0.981969 [283] valid_0&#39;s rmse: 0.982002 [284] valid_0&#39;s rmse: 0.982001 [285] valid_0&#39;s rmse: 0.981984 Early stopping, best iteration is: [255] valid_0&#39;s rmse: 0.981543 . print(&quot;평균 RMSE :&quot;,np.mean(rmse_set)) . 평균 RMSE : 1.057129789658154 . rmse_set . [1.1512972119311689, 1.112827784561918, 0.9072643724813747] . feature importance . from lightgbm import plot_importance fig, ax = plt.subplots(figsize=(15, 15)) plot_importance(lgbm_r, ax=ax) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdd618ac610&gt; . Catboost &#48169;&#48277;&#46020; &#51201;&#50857;&#54644; &#48372;&#51088;! . pip install catboost . Collecting catboost Downloading catboost-0.26.1-cp37-none-manylinux1_x86_64.whl (67.4 MB) |████████████████████████████████| 67.4 MB 26 kB/s Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.1) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.10.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (1.3.3) Installing collected packages: catboost Successfully installed catboost-0.26.1 . from catboost import CatBoostRegressor cat = CatBoostRegressor(iterations = 500,loss_function = &quot;RMSE&quot;, task_type= &quot;GPU&quot;, random_state = 243) cat.fit(X_train,y_train, eval_set=[(X_valid, y_valid)],verbose = True) y_pred2 = cat.predict(X_test) . Learning rate set to 0.152261 0: learn: 1.3121831 test: 1.1440462 best: 1.1440462 (0) total: 105ms remaining: 52.5s 1: learn: 1.2545471 test: 1.1091075 best: 1.1091075 (1) total: 203ms remaining: 50.4s 2: learn: 1.2104566 test: 1.0859787 best: 1.0859787 (2) total: 323ms remaining: 53.5s 3: learn: 1.1761175 test: 1.0692578 best: 1.0692578 (3) total: 417ms remaining: 51.7s 4: learn: 1.1506860 test: 1.0571847 best: 1.0571847 (4) total: 537ms remaining: 53.2s 5: learn: 1.1299308 test: 1.0471196 best: 1.0471196 (5) total: 631ms remaining: 52s 6: learn: 1.1149531 test: 1.0408018 best: 1.0408018 (6) total: 744ms remaining: 52.4s 7: learn: 1.1032244 test: 1.0372922 best: 1.0372922 (7) total: 829ms remaining: 51s 8: learn: 1.0940954 test: 1.0331733 best: 1.0331733 (8) total: 926ms remaining: 50.5s 9: learn: 1.0869445 test: 1.0307481 best: 1.0307481 (9) total: 1.04s remaining: 50.8s 10: learn: 1.0803131 test: 1.0283792 best: 1.0283792 (10) total: 1.13s remaining: 50.4s 11: learn: 1.0760354 test: 1.0263726 best: 1.0263726 (11) total: 1.23s remaining: 50s 12: learn: 1.0719630 test: 1.0243135 best: 1.0243135 (12) total: 1.33s remaining: 49.8s 13: learn: 1.0691187 test: 1.0229121 best: 1.0229121 (13) total: 1.41s remaining: 48.8s 14: learn: 1.0664627 test: 1.0218276 best: 1.0218276 (14) total: 1.49s remaining: 48.3s 15: learn: 1.0643853 test: 1.0213286 best: 1.0213286 (15) total: 1.58s remaining: 47.8s 16: learn: 1.0624893 test: 1.0207753 best: 1.0207753 (16) total: 1.68s remaining: 47.6s 17: learn: 1.0598284 test: 1.0197400 best: 1.0197400 (17) total: 1.76s remaining: 47.3s 18: learn: 1.0584757 test: 1.0189604 best: 1.0189604 (18) total: 1.85s remaining: 46.9s 19: learn: 1.0567621 test: 1.0185858 best: 1.0185858 (19) total: 1.95s remaining: 46.8s 20: learn: 1.0556917 test: 1.0188754 best: 1.0185858 (19) total: 2.04s remaining: 46.5s 21: learn: 1.0547697 test: 1.0187392 best: 1.0185858 (19) total: 2.13s remaining: 46.3s 22: learn: 1.0538590 test: 1.0178704 best: 1.0178704 (22) total: 2.22s remaining: 46s 23: learn: 1.0523366 test: 1.0175368 best: 1.0175368 (23) total: 2.31s remaining: 45.8s 24: learn: 1.0516654 test: 1.0177509 best: 1.0175368 (23) total: 2.43s remaining: 46.2s 25: learn: 1.0504509 test: 1.0175299 best: 1.0175299 (25) total: 2.52s remaining: 45.9s 26: learn: 1.0495113 test: 1.0175074 best: 1.0175074 (26) total: 2.6s remaining: 45.6s 27: learn: 1.0487871 test: 1.0169601 best: 1.0169601 (27) total: 2.71s remaining: 45.6s 28: learn: 1.0479913 test: 1.0166770 best: 1.0166770 (28) total: 2.79s remaining: 45.3s 29: learn: 1.0475193 test: 1.0166516 best: 1.0166516 (29) total: 2.87s remaining: 44.9s 30: learn: 1.0470397 test: 1.0161302 best: 1.0161302 (30) total: 2.97s remaining: 44.9s 31: learn: 1.0451426 test: 1.0063973 best: 1.0063973 (31) total: 3.05s remaining: 44.6s 32: learn: 1.0445320 test: 1.0062369 best: 1.0062369 (32) total: 3.15s remaining: 44.5s 33: learn: 1.0441927 test: 1.0065356 best: 1.0062369 (32) total: 3.23s remaining: 44.2s 34: learn: 1.0437508 test: 1.0064594 best: 1.0062369 (32) total: 3.32s remaining: 44.1s 35: learn: 1.0421867 test: 1.0002867 best: 1.0002867 (35) total: 3.41s remaining: 43.9s 36: learn: 1.0417778 test: 1.0003684 best: 1.0002867 (35) total: 3.5s remaining: 43.8s 37: learn: 1.0412007 test: 1.0003012 best: 1.0002867 (35) total: 3.58s remaining: 43.5s 38: learn: 1.0408579 test: 1.0007435 best: 1.0002867 (35) total: 3.66s remaining: 43.3s 39: learn: 1.0397317 test: 0.9960385 best: 0.9960385 (39) total: 3.75s remaining: 43.2s 40: learn: 1.0393233 test: 0.9959618 best: 0.9959618 (40) total: 3.84s remaining: 43s 41: learn: 1.0389183 test: 0.9955874 best: 0.9955874 (41) total: 3.94s remaining: 43s 42: learn: 1.0382574 test: 0.9955545 best: 0.9955545 (42) total: 4.02s remaining: 42.7s 43: learn: 1.0379152 test: 0.9954884 best: 0.9954884 (43) total: 4.11s remaining: 42.6s 44: learn: 1.0372483 test: 0.9956263 best: 0.9954884 (43) total: 4.2s remaining: 42.5s 45: learn: 1.0364146 test: 0.9913362 best: 0.9913362 (45) total: 4.3s remaining: 42.4s 46: learn: 1.0361682 test: 0.9909864 best: 0.9909864 (46) total: 4.38s remaining: 42.2s 47: learn: 1.0358315 test: 0.9911881 best: 0.9909864 (46) total: 4.47s remaining: 42.1s 48: learn: 1.0355774 test: 0.9908700 best: 0.9908700 (48) total: 4.56s remaining: 42s 49: learn: 1.0352345 test: 0.9907823 best: 0.9907823 (49) total: 4.66s remaining: 41.9s 50: learn: 1.0347681 test: 0.9908219 best: 0.9907823 (49) total: 4.74s remaining: 41.8s 51: learn: 1.0341119 test: 0.9888921 best: 0.9888921 (51) total: 4.84s remaining: 41.7s 52: learn: 1.0337172 test: 0.9889650 best: 0.9888921 (51) total: 4.96s remaining: 41.8s 53: learn: 1.0330004 test: 0.9889050 best: 0.9888921 (51) total: 5.04s remaining: 41.6s 54: learn: 1.0324948 test: 0.9870850 best: 0.9870850 (54) total: 5.13s remaining: 41.5s 55: learn: 1.0322901 test: 0.9869251 best: 0.9869251 (55) total: 5.21s remaining: 41.3s 56: learn: 1.0318679 test: 0.9866516 best: 0.9866516 (56) total: 5.29s remaining: 41.1s 57: learn: 1.0316264 test: 0.9865190 best: 0.9865190 (57) total: 5.4s remaining: 41.1s 58: learn: 1.0314427 test: 0.9864151 best: 0.9864151 (58) total: 5.5s remaining: 41.1s 59: learn: 1.0312213 test: 0.9861741 best: 0.9861741 (59) total: 5.61s remaining: 41.1s 60: learn: 1.0309662 test: 0.9863078 best: 0.9861741 (59) total: 5.7s remaining: 41s 61: learn: 1.0307030 test: 0.9862569 best: 0.9861741 (59) total: 5.8s remaining: 40.9s 62: learn: 1.0304316 test: 0.9862549 best: 0.9861741 (59) total: 5.9s remaining: 40.9s 63: learn: 1.0301300 test: 0.9861809 best: 0.9861741 (59) total: 5.97s remaining: 40.7s 64: learn: 1.0298363 test: 0.9860123 best: 0.9860123 (64) total: 6.07s remaining: 40.6s 65: learn: 1.0295451 test: 0.9862598 best: 0.9860123 (64) total: 6.16s remaining: 40.5s 66: learn: 1.0292930 test: 0.9860232 best: 0.9860123 (64) total: 6.24s remaining: 40.3s 67: learn: 1.0290748 test: 0.9857574 best: 0.9857574 (67) total: 6.34s remaining: 40.3s 68: learn: 1.0287312 test: 0.9854542 best: 0.9854542 (68) total: 6.44s remaining: 40.2s 69: learn: 1.0284604 test: 0.9855643 best: 0.9854542 (68) total: 6.52s remaining: 40.1s 70: learn: 1.0280034 test: 0.9853106 best: 0.9853106 (70) total: 6.61s remaining: 39.9s 71: learn: 1.0278707 test: 0.9853387 best: 0.9853106 (70) total: 6.69s remaining: 39.8s 72: learn: 1.0275561 test: 0.9853484 best: 0.9853106 (70) total: 6.77s remaining: 39.6s 73: learn: 1.0274014 test: 0.9852698 best: 0.9852698 (73) total: 6.85s remaining: 39.4s 74: learn: 1.0270551 test: 0.9836676 best: 0.9836676 (74) total: 6.93s remaining: 39.3s 75: learn: 1.0267730 test: 0.9836503 best: 0.9836503 (75) total: 7.02s remaining: 39.2s 76: learn: 1.0264986 test: 0.9835831 best: 0.9835831 (76) total: 7.11s remaining: 39s 77: learn: 1.0261436 test: 0.9837559 best: 0.9835831 (76) total: 7.21s remaining: 39s 78: learn: 1.0259999 test: 0.9836162 best: 0.9835831 (76) total: 7.3s remaining: 38.9s 79: learn: 1.0254951 test: 0.9835077 best: 0.9835077 (79) total: 7.38s remaining: 38.8s 80: learn: 1.0252754 test: 0.9836253 best: 0.9835077 (79) total: 7.48s remaining: 38.7s 81: learn: 1.0250260 test: 0.9835605 best: 0.9835077 (79) total: 7.56s remaining: 38.6s 82: learn: 1.0245136 test: 0.9835362 best: 0.9835077 (79) total: 7.65s remaining: 38.4s 83: learn: 1.0242844 test: 0.9832127 best: 0.9832127 (83) total: 7.73s remaining: 38.3s 84: learn: 1.0239118 test: 0.9832231 best: 0.9832127 (83) total: 7.81s remaining: 38.1s 85: learn: 1.0236282 test: 0.9832249 best: 0.9832127 (83) total: 7.9s remaining: 38s 86: learn: 1.0235230 test: 0.9832246 best: 0.9832127 (83) total: 7.99s remaining: 37.9s 87: learn: 1.0233417 test: 0.9832438 best: 0.9832127 (83) total: 8.07s remaining: 37.8s 88: learn: 1.0231221 test: 0.9834493 best: 0.9832127 (83) total: 8.17s remaining: 37.7s 89: learn: 1.0227563 test: 0.9833477 best: 0.9832127 (83) total: 8.26s remaining: 37.6s 90: learn: 1.0226026 test: 0.9832914 best: 0.9832127 (83) total: 8.35s remaining: 37.6s 91: learn: 1.0224296 test: 0.9832423 best: 0.9832127 (83) total: 8.45s remaining: 37.5s 92: learn: 1.0220624 test: 0.9829154 best: 0.9829154 (92) total: 8.53s remaining: 37.3s 93: learn: 1.0218658 test: 0.9831271 best: 0.9829154 (92) total: 8.63s remaining: 37.3s 94: learn: 1.0215828 test: 0.9830694 best: 0.9829154 (92) total: 8.71s remaining: 37.1s 95: learn: 1.0213723 test: 0.9829843 best: 0.9829154 (92) total: 8.79s remaining: 37s 96: learn: 1.0212427 test: 0.9829326 best: 0.9829154 (92) total: 8.88s remaining: 36.9s 97: learn: 1.0210117 test: 0.9831651 best: 0.9829154 (92) total: 8.97s remaining: 36.8s 98: learn: 1.0208629 test: 0.9830559 best: 0.9829154 (92) total: 9.06s remaining: 36.7s 99: learn: 1.0206965 test: 0.9830310 best: 0.9829154 (92) total: 9.16s remaining: 36.6s 100: learn: 1.0205247 test: 0.9828293 best: 0.9828293 (100) total: 9.26s remaining: 36.6s 101: learn: 1.0203341 test: 0.9827659 best: 0.9827659 (101) total: 9.34s remaining: 36.4s 102: learn: 1.0200978 test: 0.9823838 best: 0.9823838 (102) total: 9.44s remaining: 36.4s 103: learn: 1.0198401 test: 0.9819078 best: 0.9819078 (103) total: 9.52s remaining: 36.2s 104: learn: 1.0196611 test: 0.9819071 best: 0.9819071 (104) total: 9.6s remaining: 36.1s 105: learn: 1.0194738 test: 0.9819107 best: 0.9819071 (104) total: 9.69s remaining: 36s 106: learn: 1.0191127 test: 0.9818472 best: 0.9818472 (106) total: 9.77s remaining: 35.9s 107: learn: 1.0188548 test: 0.9817824 best: 0.9817824 (107) total: 9.86s remaining: 35.8s 108: learn: 1.0186453 test: 0.9817564 best: 0.9817564 (108) total: 9.95s remaining: 35.7s 109: learn: 1.0184320 test: 0.9816973 best: 0.9816973 (109) total: 10s remaining: 35.6s 110: learn: 1.0182552 test: 0.9816915 best: 0.9816915 (110) total: 10.1s remaining: 35.5s 111: learn: 1.0180877 test: 0.9816125 best: 0.9816125 (111) total: 10.2s remaining: 35.4s 112: learn: 1.0178598 test: 0.9815368 best: 0.9815368 (112) total: 10.3s remaining: 35.3s 113: learn: 1.0176692 test: 0.9815997 best: 0.9815368 (112) total: 10.4s remaining: 35.2s 114: learn: 1.0174699 test: 0.9815590 best: 0.9815368 (112) total: 10.5s remaining: 35.1s 115: learn: 1.0173216 test: 0.9815270 best: 0.9815270 (115) total: 10.6s remaining: 35s 116: learn: 1.0171309 test: 0.9813333 best: 0.9813333 (116) total: 10.7s remaining: 35s 117: learn: 1.0169669 test: 0.9812532 best: 0.9812532 (117) total: 10.8s remaining: 34.9s 118: learn: 1.0167651 test: 0.9812120 best: 0.9812120 (118) total: 10.9s remaining: 34.8s 119: learn: 1.0166245 test: 0.9812619 best: 0.9812120 (118) total: 10.9s remaining: 34.6s 120: learn: 1.0164858 test: 0.9812726 best: 0.9812120 (118) total: 11s remaining: 34.6s 121: learn: 1.0163512 test: 0.9812819 best: 0.9812120 (118) total: 11.1s remaining: 34.5s 122: learn: 1.0162595 test: 0.9812580 best: 0.9812120 (118) total: 11.2s remaining: 34.4s 123: learn: 1.0162085 test: 0.9810936 best: 0.9810936 (123) total: 11.3s remaining: 34.2s 124: learn: 1.0159657 test: 0.9802473 best: 0.9802473 (124) total: 11.4s remaining: 34.2s 125: learn: 1.0157701 test: 0.9801010 best: 0.9801010 (125) total: 11.5s remaining: 34.1s 126: learn: 1.0156245 test: 0.9802018 best: 0.9801010 (125) total: 11.6s remaining: 34s 127: learn: 1.0155098 test: 0.9800299 best: 0.9800299 (127) total: 11.7s remaining: 33.9s 128: learn: 1.0153749 test: 0.9798824 best: 0.9798824 (128) total: 11.8s remaining: 33.9s 129: learn: 1.0152935 test: 0.9799296 best: 0.9798824 (128) total: 11.9s remaining: 33.8s 130: learn: 1.0151935 test: 0.9798388 best: 0.9798388 (130) total: 12s remaining: 33.8s 131: learn: 1.0150700 test: 0.9797097 best: 0.9797097 (131) total: 12.1s remaining: 33.7s 132: learn: 1.0149227 test: 0.9796992 best: 0.9796992 (132) total: 12.2s remaining: 33.6s 133: learn: 1.0148321 test: 0.9796285 best: 0.9796285 (133) total: 12.2s remaining: 33.5s 134: learn: 1.0146893 test: 0.9796771 best: 0.9796285 (133) total: 12.3s remaining: 33.4s 135: learn: 1.0145558 test: 0.9797888 best: 0.9796285 (133) total: 12.4s remaining: 33.3s 136: learn: 1.0143818 test: 0.9797084 best: 0.9796285 (133) total: 12.5s remaining: 33.2s 137: learn: 1.0142074 test: 0.9797683 best: 0.9796285 (133) total: 12.6s remaining: 33.1s 138: learn: 1.0140396 test: 0.9802670 best: 0.9796285 (133) total: 12.7s remaining: 33s 139: learn: 1.0139423 test: 0.9802565 best: 0.9796285 (133) total: 12.8s remaining: 32.9s 140: learn: 1.0138455 test: 0.9801599 best: 0.9796285 (133) total: 12.9s remaining: 32.8s 141: learn: 1.0137759 test: 0.9801446 best: 0.9796285 (133) total: 13s remaining: 32.7s 142: learn: 1.0136812 test: 0.9801128 best: 0.9796285 (133) total: 13.1s remaining: 32.6s 143: learn: 1.0135207 test: 0.9802058 best: 0.9796285 (133) total: 13.2s remaining: 32.6s 144: learn: 1.0132850 test: 0.9802464 best: 0.9796285 (133) total: 13.3s remaining: 32.5s 145: learn: 1.0131191 test: 0.9801847 best: 0.9796285 (133) total: 13.4s remaining: 32.4s 146: learn: 1.0130440 test: 0.9802376 best: 0.9796285 (133) total: 13.5s remaining: 32.3s 147: learn: 1.0129565 test: 0.9800490 best: 0.9796285 (133) total: 13.5s remaining: 32.2s 148: learn: 1.0127989 test: 0.9799046 best: 0.9796285 (133) total: 13.6s remaining: 32.1s 149: learn: 1.0126500 test: 0.9798286 best: 0.9796285 (133) total: 13.7s remaining: 32s 150: learn: 1.0125182 test: 0.9797914 best: 0.9796285 (133) total: 13.8s remaining: 31.9s 151: learn: 1.0124299 test: 0.9798014 best: 0.9796285 (133) total: 13.9s remaining: 31.8s 152: learn: 1.0122821 test: 0.9798566 best: 0.9796285 (133) total: 14s remaining: 31.7s 153: learn: 1.0121648 test: 0.9796301 best: 0.9796285 (133) total: 14.1s remaining: 31.6s 154: learn: 1.0120099 test: 0.9796654 best: 0.9796285 (133) total: 14.2s remaining: 31.6s 155: learn: 1.0119300 test: 0.9796842 best: 0.9796285 (133) total: 14.3s remaining: 31.5s 156: learn: 1.0118662 test: 0.9796871 best: 0.9796285 (133) total: 14.3s remaining: 31.3s 157: learn: 1.0117339 test: 0.9797061 best: 0.9796285 (133) total: 14.5s remaining: 31.3s 158: learn: 1.0116540 test: 0.9796259 best: 0.9796259 (158) total: 14.5s remaining: 31.2s 159: learn: 1.0116051 test: 0.9796242 best: 0.9796242 (159) total: 14.6s remaining: 31s 160: learn: 1.0114420 test: 0.9796760 best: 0.9796242 (159) total: 14.7s remaining: 30.9s 161: learn: 1.0113285 test: 0.9794846 best: 0.9794846 (161) total: 14.8s remaining: 30.8s 162: learn: 1.0112912 test: 0.9794677 best: 0.9794677 (162) total: 14.8s remaining: 30.7s 163: learn: 1.0112008 test: 0.9793673 best: 0.9793673 (163) total: 14.9s remaining: 30.6s 164: learn: 1.0111229 test: 0.9791359 best: 0.9791359 (164) total: 15s remaining: 30.5s 165: learn: 1.0110165 test: 0.9789171 best: 0.9789171 (165) total: 15.1s remaining: 30.4s 166: learn: 1.0109586 test: 0.9790331 best: 0.9789171 (165) total: 15.2s remaining: 30.3s 167: learn: 1.0108317 test: 0.9788987 best: 0.9788987 (167) total: 15.3s remaining: 30.3s 168: learn: 1.0107236 test: 0.9787795 best: 0.9787795 (168) total: 15.4s remaining: 30.2s 169: learn: 1.0106283 test: 0.9788211 best: 0.9787795 (168) total: 15.5s remaining: 30.1s 170: learn: 1.0104816 test: 0.9787816 best: 0.9787795 (168) total: 15.6s remaining: 30s 171: learn: 1.0103847 test: 0.9787728 best: 0.9787728 (171) total: 15.7s remaining: 29.9s 172: learn: 1.0101876 test: 0.9787643 best: 0.9787643 (172) total: 15.7s remaining: 29.8s 173: learn: 1.0100716 test: 0.9787741 best: 0.9787643 (172) total: 15.8s remaining: 29.7s 174: learn: 1.0098167 test: 0.9781440 best: 0.9781440 (174) total: 15.9s remaining: 29.6s 175: learn: 1.0097128 test: 0.9781450 best: 0.9781440 (174) total: 16s remaining: 29.5s 176: learn: 1.0096440 test: 0.9780646 best: 0.9780646 (176) total: 16.1s remaining: 29.4s 177: learn: 1.0094343 test: 0.9780512 best: 0.9780512 (177) total: 16.2s remaining: 29.3s 178: learn: 1.0093599 test: 0.9780610 best: 0.9780512 (177) total: 16.3s remaining: 29.2s 179: learn: 1.0093056 test: 0.9779459 best: 0.9779459 (179) total: 16.4s remaining: 29.1s 180: learn: 1.0092119 test: 0.9779195 best: 0.9779195 (180) total: 16.5s remaining: 29s 181: learn: 1.0090893 test: 0.9779481 best: 0.9779195 (180) total: 16.6s remaining: 28.9s 182: learn: 1.0089534 test: 0.9778235 best: 0.9778235 (182) total: 16.6s remaining: 28.8s 183: learn: 1.0088110 test: 0.9771990 best: 0.9771990 (183) total: 16.7s remaining: 28.7s 184: learn: 1.0086402 test: 0.9771491 best: 0.9771491 (184) total: 16.8s remaining: 28.6s 185: learn: 1.0084134 test: 0.9770351 best: 0.9770351 (185) total: 16.9s remaining: 28.6s 186: learn: 1.0083069 test: 0.9770422 best: 0.9770351 (185) total: 17s remaining: 28.5s 187: learn: 1.0082084 test: 0.9770334 best: 0.9770334 (187) total: 17.1s remaining: 28.3s 188: learn: 1.0080990 test: 0.9769915 best: 0.9769915 (188) total: 17.2s remaining: 28.2s 189: learn: 1.0079254 test: 0.9763517 best: 0.9763517 (189) total: 17.3s remaining: 28.2s 190: learn: 1.0078132 test: 0.9761397 best: 0.9761397 (190) total: 17.4s remaining: 28.1s 191: learn: 1.0077224 test: 0.9768413 best: 0.9761397 (190) total: 17.4s remaining: 28s 192: learn: 1.0075953 test: 0.9768837 best: 0.9761397 (190) total: 17.5s remaining: 27.9s 193: learn: 1.0074750 test: 0.9767127 best: 0.9761397 (190) total: 17.6s remaining: 27.8s 194: learn: 1.0073943 test: 0.9765781 best: 0.9761397 (190) total: 17.7s remaining: 27.7s 195: learn: 1.0073239 test: 0.9766129 best: 0.9761397 (190) total: 17.8s remaining: 27.6s 196: learn: 1.0072983 test: 0.9766267 best: 0.9761397 (190) total: 17.9s remaining: 27.5s 197: learn: 1.0072244 test: 0.9765848 best: 0.9761397 (190) total: 18s remaining: 27.4s 198: learn: 1.0071647 test: 0.9765523 best: 0.9761397 (190) total: 18.1s remaining: 27.3s 199: learn: 1.0070718 test: 0.9764568 best: 0.9761397 (190) total: 18.1s remaining: 27.2s 200: learn: 1.0070180 test: 0.9763587 best: 0.9761397 (190) total: 18.2s remaining: 27.1s 201: learn: 1.0069242 test: 0.9763796 best: 0.9761397 (190) total: 18.3s remaining: 27s 202: learn: 1.0068414 test: 0.9763639 best: 0.9761397 (190) total: 18.4s remaining: 26.9s 203: learn: 1.0067378 test: 0.9763442 best: 0.9761397 (190) total: 18.5s remaining: 26.8s 204: learn: 1.0066697 test: 0.9763339 best: 0.9761397 (190) total: 18.6s remaining: 26.7s 205: learn: 1.0066053 test: 0.9763652 best: 0.9761397 (190) total: 18.7s remaining: 26.6s 206: learn: 1.0065303 test: 0.9763065 best: 0.9761397 (190) total: 18.8s remaining: 26.5s 207: learn: 1.0064411 test: 0.9763060 best: 0.9761397 (190) total: 18.8s remaining: 26.4s 208: learn: 1.0063288 test: 0.9764948 best: 0.9761397 (190) total: 18.9s remaining: 26.4s 209: learn: 1.0061878 test: 0.9765672 best: 0.9761397 (190) total: 19s remaining: 26.3s 210: learn: 1.0061024 test: 0.9765771 best: 0.9761397 (190) total: 19.1s remaining: 26.2s 211: learn: 1.0059788 test: 0.9765820 best: 0.9761397 (190) total: 19.2s remaining: 26.1s 212: learn: 1.0058685 test: 0.9765594 best: 0.9761397 (190) total: 19.3s remaining: 26s 213: learn: 1.0057706 test: 0.9766315 best: 0.9761397 (190) total: 19.4s remaining: 25.9s 214: learn: 1.0056606 test: 0.9768589 best: 0.9761397 (190) total: 19.5s remaining: 25.8s 215: learn: 1.0055169 test: 0.9768089 best: 0.9761397 (190) total: 19.6s remaining: 25.7s 216: learn: 1.0054182 test: 0.9766876 best: 0.9761397 (190) total: 19.7s remaining: 25.6s 217: learn: 1.0053659 test: 0.9765200 best: 0.9761397 (190) total: 19.7s remaining: 25.5s 218: learn: 1.0053173 test: 0.9765226 best: 0.9761397 (190) total: 19.8s remaining: 25.4s 219: learn: 1.0052422 test: 0.9763109 best: 0.9761397 (190) total: 19.9s remaining: 25.3s 220: learn: 1.0051417 test: 0.9762002 best: 0.9761397 (190) total: 20s remaining: 25.2s 221: learn: 1.0050389 test: 0.9762950 best: 0.9761397 (190) total: 20.1s remaining: 25.2s 222: learn: 1.0049456 test: 0.9762408 best: 0.9761397 (190) total: 20.2s remaining: 25.1s 223: learn: 1.0048813 test: 0.9761594 best: 0.9761397 (190) total: 20.3s remaining: 25s 224: learn: 1.0048244 test: 0.9761027 best: 0.9761027 (224) total: 20.4s remaining: 24.9s 225: learn: 1.0047533 test: 0.9761024 best: 0.9761024 (225) total: 20.4s remaining: 24.8s 226: learn: 1.0046589 test: 0.9760329 best: 0.9760329 (226) total: 20.5s remaining: 24.7s 227: learn: 1.0045592 test: 0.9760550 best: 0.9760329 (226) total: 20.6s remaining: 24.6s 228: learn: 1.0045082 test: 0.9759877 best: 0.9759877 (228) total: 20.7s remaining: 24.5s 229: learn: 1.0044053 test: 0.9758838 best: 0.9758838 (229) total: 20.8s remaining: 24.4s 230: learn: 1.0043550 test: 0.9757716 best: 0.9757716 (230) total: 20.9s remaining: 24.3s 231: learn: 1.0042704 test: 0.9758069 best: 0.9757716 (230) total: 21s remaining: 24.2s 232: learn: 1.0041711 test: 0.9757146 best: 0.9757146 (232) total: 21.1s remaining: 24.1s 233: learn: 1.0040681 test: 0.9757370 best: 0.9757146 (232) total: 21.1s remaining: 24s 234: learn: 1.0040109 test: 0.9757536 best: 0.9757146 (232) total: 21.2s remaining: 24s 235: learn: 1.0039470 test: 0.9757120 best: 0.9757120 (235) total: 21.3s remaining: 23.9s 236: learn: 1.0038786 test: 0.9756682 best: 0.9756682 (236) total: 21.4s remaining: 23.8s 237: learn: 1.0038003 test: 0.9757281 best: 0.9756682 (236) total: 21.5s remaining: 23.7s 238: learn: 1.0036864 test: 0.9756933 best: 0.9756682 (236) total: 21.6s remaining: 23.6s 239: learn: 1.0036435 test: 0.9756670 best: 0.9756670 (239) total: 21.7s remaining: 23.5s 240: learn: 1.0035959 test: 0.9755144 best: 0.9755144 (240) total: 21.8s remaining: 23.4s 241: learn: 1.0035045 test: 0.9754559 best: 0.9754559 (241) total: 21.9s remaining: 23.3s 242: learn: 1.0034427 test: 0.9754445 best: 0.9754445 (242) total: 22s remaining: 23.2s 243: learn: 1.0034157 test: 0.9754511 best: 0.9754445 (242) total: 22s remaining: 23.1s 244: learn: 1.0033734 test: 0.9754655 best: 0.9754445 (242) total: 22.1s remaining: 23s 245: learn: 1.0032876 test: 0.9753778 best: 0.9753778 (245) total: 22.2s remaining: 22.9s 246: learn: 1.0032098 test: 0.9754073 best: 0.9753778 (245) total: 22.3s remaining: 22.9s 247: learn: 1.0031587 test: 0.9754652 best: 0.9753778 (245) total: 22.4s remaining: 22.8s 248: learn: 1.0030871 test: 0.9753245 best: 0.9753245 (248) total: 22.5s remaining: 22.7s 249: learn: 1.0029926 test: 0.9753289 best: 0.9753245 (248) total: 22.6s remaining: 22.6s 250: learn: 1.0029549 test: 0.9755420 best: 0.9753245 (248) total: 22.6s remaining: 22.5s 251: learn: 1.0029044 test: 0.9754845 best: 0.9753245 (248) total: 22.7s remaining: 22.4s 252: learn: 1.0028677 test: 0.9754211 best: 0.9753245 (248) total: 22.8s remaining: 22.3s 253: learn: 1.0027570 test: 0.9754402 best: 0.9753245 (248) total: 22.9s remaining: 22.2s 254: learn: 1.0026625 test: 0.9753781 best: 0.9753245 (248) total: 23s remaining: 22.1s 255: learn: 1.0025871 test: 0.9752491 best: 0.9752491 (255) total: 23.1s remaining: 22s 256: learn: 1.0025552 test: 0.9752291 best: 0.9752291 (256) total: 23.2s remaining: 21.9s 257: learn: 1.0024552 test: 0.9752310 best: 0.9752291 (256) total: 23.3s remaining: 21.9s 258: learn: 1.0023825 test: 0.9751089 best: 0.9751089 (258) total: 23.4s remaining: 21.8s 259: learn: 1.0023165 test: 0.9750294 best: 0.9750294 (259) total: 23.5s remaining: 21.7s 260: learn: 1.0022691 test: 0.9750225 best: 0.9750225 (260) total: 23.6s remaining: 21.6s 261: learn: 1.0022204 test: 0.9750022 best: 0.9750022 (261) total: 23.6s remaining: 21.5s 262: learn: 1.0021634 test: 0.9749791 best: 0.9749791 (262) total: 23.8s remaining: 21.4s 263: learn: 1.0021105 test: 0.9748493 best: 0.9748493 (263) total: 23.8s remaining: 21.3s 264: learn: 1.0020689 test: 0.9747857 best: 0.9747857 (264) total: 23.9s remaining: 21.2s 265: learn: 1.0019975 test: 0.9748148 best: 0.9747857 (264) total: 24s remaining: 21.1s 266: learn: 1.0016358 test: 0.9646654 best: 0.9646654 (266) total: 24.1s remaining: 21s 267: learn: 1.0015352 test: 0.9647413 best: 0.9646654 (266) total: 24.2s remaining: 21s 268: learn: 1.0014931 test: 0.9647527 best: 0.9646654 (266) total: 24.3s remaining: 20.9s 269: learn: 1.0014094 test: 0.9647728 best: 0.9646654 (266) total: 24.4s remaining: 20.8s 270: learn: 1.0013539 test: 0.9647844 best: 0.9646654 (266) total: 24.5s remaining: 20.7s 271: learn: 1.0013059 test: 0.9647589 best: 0.9646654 (266) total: 24.6s remaining: 20.6s 272: learn: 1.0012422 test: 0.9647030 best: 0.9646654 (266) total: 24.7s remaining: 20.5s 273: learn: 1.0011980 test: 0.9645907 best: 0.9645907 (273) total: 24.8s remaining: 20.4s 274: learn: 1.0010495 test: 0.9645564 best: 0.9645564 (274) total: 24.8s remaining: 20.3s 275: learn: 1.0009815 test: 0.9643810 best: 0.9643810 (275) total: 24.9s remaining: 20.2s 276: learn: 1.0009345 test: 0.9643723 best: 0.9643723 (276) total: 25s remaining: 20.1s 277: learn: 1.0009128 test: 0.9643381 best: 0.9643381 (277) total: 25.1s remaining: 20.1s 278: learn: 1.0008942 test: 0.9643659 best: 0.9643381 (277) total: 25.2s remaining: 20s 279: learn: 1.0007478 test: 0.9644122 best: 0.9643381 (277) total: 25.3s remaining: 19.9s 280: learn: 1.0006994 test: 0.9645938 best: 0.9643381 (277) total: 25.4s remaining: 19.8s 281: learn: 1.0006016 test: 0.9645464 best: 0.9643381 (277) total: 25.5s remaining: 19.7s 282: learn: 1.0005532 test: 0.9644217 best: 0.9643381 (277) total: 25.6s remaining: 19.6s 283: learn: 1.0005280 test: 0.9643843 best: 0.9643381 (277) total: 25.7s remaining: 19.5s 284: learn: 1.0004303 test: 0.9644067 best: 0.9643381 (277) total: 25.7s remaining: 19.4s 285: learn: 1.0003347 test: 0.9643157 best: 0.9643157 (285) total: 25.9s remaining: 19.3s 286: learn: 1.0002868 test: 0.9643864 best: 0.9643157 (285) total: 26s remaining: 19.3s 287: learn: 1.0001767 test: 0.9643854 best: 0.9643157 (285) total: 26s remaining: 19.2s 288: learn: 0.9997912 test: 0.9644071 best: 0.9643157 (285) total: 26.1s remaining: 19.1s 289: learn: 0.9997657 test: 0.9643947 best: 0.9643157 (285) total: 26.2s remaining: 19s 290: learn: 0.9997112 test: 0.9643471 best: 0.9643157 (285) total: 26.3s remaining: 18.9s 291: learn: 0.9996502 test: 0.9643337 best: 0.9643157 (285) total: 26.4s remaining: 18.8s 292: learn: 0.9996049 test: 0.9642903 best: 0.9642903 (292) total: 26.5s remaining: 18.7s 293: learn: 0.9995655 test: 0.9643648 best: 0.9642903 (292) total: 26.6s remaining: 18.6s 294: learn: 0.9995334 test: 0.9643606 best: 0.9642903 (292) total: 26.7s remaining: 18.5s 295: learn: 0.9994580 test: 0.9643412 best: 0.9642903 (292) total: 26.8s remaining: 18.5s 296: learn: 0.9994006 test: 0.9643608 best: 0.9642903 (292) total: 26.9s remaining: 18.4s 297: learn: 0.9991548 test: 0.9580776 best: 0.9580776 (297) total: 27s remaining: 18.3s 298: learn: 0.9990738 test: 0.9580643 best: 0.9580643 (298) total: 27.1s remaining: 18.2s 299: learn: 0.9990235 test: 0.9580397 best: 0.9580397 (299) total: 27.1s remaining: 18.1s 300: learn: 0.9989403 test: 0.9580967 best: 0.9580397 (299) total: 27.2s remaining: 18s 301: learn: 0.9989038 test: 0.9580316 best: 0.9580316 (301) total: 27.3s remaining: 17.9s 302: learn: 0.9988027 test: 0.9579780 best: 0.9579780 (302) total: 27.4s remaining: 17.8s 303: learn: 0.9987415 test: 0.9579471 best: 0.9579471 (303) total: 27.5s remaining: 17.7s 304: learn: 0.9986871 test: 0.9579669 best: 0.9579471 (303) total: 27.6s remaining: 17.6s 305: learn: 0.9985884 test: 0.9579333 best: 0.9579333 (305) total: 27.7s remaining: 17.6s 306: learn: 0.9984803 test: 0.9580076 best: 0.9579333 (305) total: 27.8s remaining: 17.5s 307: learn: 0.9983906 test: 0.9580175 best: 0.9579333 (305) total: 27.9s remaining: 17.4s 308: learn: 0.9983480 test: 0.9579153 best: 0.9579153 (308) total: 28s remaining: 17.3s 309: learn: 0.9982845 test: 0.9579188 best: 0.9579153 (308) total: 28.1s remaining: 17.2s 310: learn: 0.9982699 test: 0.9579202 best: 0.9579153 (308) total: 28.1s remaining: 17.1s 311: learn: 0.9981955 test: 0.9579339 best: 0.9579153 (308) total: 28.2s remaining: 17s 312: learn: 0.9981037 test: 0.9579267 best: 0.9579153 (308) total: 28.3s remaining: 16.9s 313: learn: 0.9980630 test: 0.9579488 best: 0.9579153 (308) total: 28.4s remaining: 16.8s 314: learn: 0.9979871 test: 0.9579151 best: 0.9579151 (314) total: 28.5s remaining: 16.7s 315: learn: 0.9977733 test: 0.9524901 best: 0.9524901 (315) total: 28.6s remaining: 16.6s 316: learn: 0.9976916 test: 0.9524943 best: 0.9524901 (315) total: 28.7s remaining: 16.5s 317: learn: 0.9975942 test: 0.9523703 best: 0.9523703 (317) total: 28.8s remaining: 16.5s 318: learn: 0.9975705 test: 0.9523675 best: 0.9523675 (318) total: 28.8s remaining: 16.4s 319: learn: 0.9975092 test: 0.9523325 best: 0.9523325 (319) total: 28.9s remaining: 16.3s 320: learn: 0.9974375 test: 0.9523831 best: 0.9523325 (319) total: 29s remaining: 16.2s 321: learn: 0.9973784 test: 0.9524375 best: 0.9523325 (319) total: 29.1s remaining: 16.1s 322: learn: 0.9973378 test: 0.9523981 best: 0.9523325 (319) total: 29.2s remaining: 16s 323: learn: 0.9972795 test: 0.9524402 best: 0.9523325 (319) total: 29.3s remaining: 15.9s 324: learn: 0.9972568 test: 0.9526246 best: 0.9523325 (319) total: 29.4s remaining: 15.8s 325: learn: 0.9971508 test: 0.9526139 best: 0.9523325 (319) total: 29.5s remaining: 15.7s 326: learn: 0.9970760 test: 0.9526301 best: 0.9523325 (319) total: 29.6s remaining: 15.6s 327: learn: 0.9970342 test: 0.9526047 best: 0.9523325 (319) total: 29.7s remaining: 15.6s 328: learn: 0.9969858 test: 0.9526028 best: 0.9523325 (319) total: 29.8s remaining: 15.5s 329: learn: 0.9969432 test: 0.9526078 best: 0.9523325 (319) total: 29.8s remaining: 15.4s 330: learn: 0.9969012 test: 0.9526845 best: 0.9523325 (319) total: 29.9s remaining: 15.3s 331: learn: 0.9968084 test: 0.9525119 best: 0.9523325 (319) total: 30s remaining: 15.2s 332: learn: 0.9967534 test: 0.9524943 best: 0.9523325 (319) total: 30.1s remaining: 15.1s 333: learn: 0.9966734 test: 0.9524038 best: 0.9523325 (319) total: 30.2s remaining: 15s 334: learn: 0.9965888 test: 0.9524092 best: 0.9523325 (319) total: 30.3s remaining: 14.9s 335: learn: 0.9965275 test: 0.9523930 best: 0.9523325 (319) total: 30.4s remaining: 14.8s 336: learn: 0.9964640 test: 0.9523678 best: 0.9523325 (319) total: 30.5s remaining: 14.7s 337: learn: 0.9964195 test: 0.9523961 best: 0.9523325 (319) total: 30.6s remaining: 14.7s 338: learn: 0.9963493 test: 0.9523917 best: 0.9523325 (319) total: 30.7s remaining: 14.6s 339: learn: 0.9963091 test: 0.9522150 best: 0.9522150 (339) total: 30.7s remaining: 14.5s 340: learn: 0.9962735 test: 0.9521377 best: 0.9521377 (340) total: 30.8s remaining: 14.4s 341: learn: 0.9962131 test: 0.9520394 best: 0.9520394 (341) total: 30.9s remaining: 14.3s 342: learn: 0.9961588 test: 0.9519756 best: 0.9519756 (342) total: 31s remaining: 14.2s 343: learn: 0.9961027 test: 0.9519336 best: 0.9519336 (343) total: 31.1s remaining: 14.1s 344: learn: 0.9960698 test: 0.9519011 best: 0.9519011 (344) total: 31.2s remaining: 14s 345: learn: 0.9960281 test: 0.9518829 best: 0.9518829 (345) total: 31.3s remaining: 13.9s 346: learn: 0.9959697 test: 0.9516752 best: 0.9516752 (346) total: 31.3s remaining: 13.8s 347: learn: 0.9959394 test: 0.9516557 best: 0.9516557 (347) total: 31.4s remaining: 13.7s 348: learn: 0.9958773 test: 0.9516573 best: 0.9516557 (347) total: 31.5s remaining: 13.6s 349: learn: 0.9958321 test: 0.9516462 best: 0.9516462 (349) total: 31.6s remaining: 13.5s 350: learn: 0.9957375 test: 0.9516118 best: 0.9516118 (350) total: 31.7s remaining: 13.5s 351: learn: 0.9957097 test: 0.9515145 best: 0.9515145 (351) total: 31.8s remaining: 13.4s 352: learn: 0.9956678 test: 0.9515391 best: 0.9515145 (351) total: 31.9s remaining: 13.3s 353: learn: 0.9956084 test: 0.9514667 best: 0.9514667 (353) total: 32s remaining: 13.2s 354: learn: 0.9955586 test: 0.9513728 best: 0.9513728 (354) total: 32.1s remaining: 13.1s 355: learn: 0.9954900 test: 0.9514069 best: 0.9513728 (354) total: 32.2s remaining: 13s 356: learn: 0.9954386 test: 0.9514446 best: 0.9513728 (354) total: 32.3s remaining: 12.9s 357: learn: 0.9953623 test: 0.9514839 best: 0.9513728 (354) total: 32.4s remaining: 12.8s 358: learn: 0.9952628 test: 0.9514872 best: 0.9513728 (354) total: 32.4s remaining: 12.7s 359: learn: 0.9952422 test: 0.9514266 best: 0.9513728 (354) total: 32.5s remaining: 12.6s 360: learn: 0.9951849 test: 0.9515198 best: 0.9513728 (354) total: 32.6s remaining: 12.6s 361: learn: 0.9951355 test: 0.9514678 best: 0.9513728 (354) total: 32.7s remaining: 12.5s 362: learn: 0.9950920 test: 0.9520761 best: 0.9513728 (354) total: 32.8s remaining: 12.4s 363: learn: 0.9950316 test: 0.9518649 best: 0.9513728 (354) total: 32.9s remaining: 12.3s 364: learn: 0.9949973 test: 0.9518232 best: 0.9513728 (354) total: 33s remaining: 12.2s 365: learn: 0.9949618 test: 0.9518683 best: 0.9513728 (354) total: 33.1s remaining: 12.1s 366: learn: 0.9949060 test: 0.9518630 best: 0.9513728 (354) total: 33.2s remaining: 12s 367: learn: 0.9948662 test: 0.9518681 best: 0.9513728 (354) total: 33.2s remaining: 11.9s 368: learn: 0.9948289 test: 0.9518445 best: 0.9513728 (354) total: 33.3s remaining: 11.8s 369: learn: 0.9947568 test: 0.9520701 best: 0.9513728 (354) total: 33.4s remaining: 11.7s 370: learn: 0.9946955 test: 0.9520644 best: 0.9513728 (354) total: 33.5s remaining: 11.7s 371: learn: 0.9946456 test: 0.9521325 best: 0.9513728 (354) total: 33.6s remaining: 11.6s 372: learn: 0.9946247 test: 0.9521408 best: 0.9513728 (354) total: 33.7s remaining: 11.5s 373: learn: 0.9946037 test: 0.9520994 best: 0.9513728 (354) total: 33.8s remaining: 11.4s 374: learn: 0.9945630 test: 0.9519032 best: 0.9513728 (354) total: 33.9s remaining: 11.3s 375: learn: 0.9945496 test: 0.9519165 best: 0.9513728 (354) total: 34s remaining: 11.2s 376: learn: 0.9944058 test: 0.9484860 best: 0.9484860 (376) total: 34s remaining: 11.1s 377: learn: 0.9943751 test: 0.9485586 best: 0.9484860 (376) total: 34.1s remaining: 11s 378: learn: 0.9943240 test: 0.9485374 best: 0.9484860 (376) total: 34.2s remaining: 10.9s 379: learn: 0.9942545 test: 0.9485310 best: 0.9484860 (376) total: 34.3s remaining: 10.8s 380: learn: 0.9942211 test: 0.9484797 best: 0.9484797 (380) total: 34.4s remaining: 10.7s 381: learn: 0.9941714 test: 0.9485369 best: 0.9484797 (380) total: 34.5s remaining: 10.7s 382: learn: 0.9941188 test: 0.9485264 best: 0.9484797 (380) total: 34.6s remaining: 10.6s 383: learn: 0.9940790 test: 0.9484726 best: 0.9484726 (383) total: 34.7s remaining: 10.5s 384: learn: 0.9939902 test: 0.9484509 best: 0.9484509 (384) total: 34.8s remaining: 10.4s 385: learn: 0.9939647 test: 0.9484149 best: 0.9484149 (385) total: 34.8s remaining: 10.3s 386: learn: 0.9939264 test: 0.9483184 best: 0.9483184 (386) total: 34.9s remaining: 10.2s 387: learn: 0.9938901 test: 0.9483829 best: 0.9483184 (386) total: 35s remaining: 10.1s 388: learn: 0.9938414 test: 0.9485210 best: 0.9483184 (386) total: 35.1s remaining: 10s 389: learn: 0.9938311 test: 0.9484822 best: 0.9483184 (386) total: 35.2s remaining: 9.92s 390: learn: 0.9937198 test: 0.9483819 best: 0.9483184 (386) total: 35.3s remaining: 9.84s 391: learn: 0.9936816 test: 0.9483517 best: 0.9483184 (386) total: 35.4s remaining: 9.75s 392: learn: 0.9935008 test: 0.9483442 best: 0.9483184 (386) total: 35.5s remaining: 9.66s 393: learn: 0.9934554 test: 0.9483016 best: 0.9483016 (393) total: 35.6s remaining: 9.57s 394: learn: 0.9934086 test: 0.9481868 best: 0.9481868 (394) total: 35.7s remaining: 9.48s 395: learn: 0.9933755 test: 0.9482162 best: 0.9481868 (394) total: 35.8s remaining: 9.39s 396: learn: 0.9933155 test: 0.9482480 best: 0.9481868 (394) total: 35.9s remaining: 9.3s 397: learn: 0.9932759 test: 0.9483261 best: 0.9481868 (394) total: 35.9s remaining: 9.21s 398: learn: 0.9932200 test: 0.9482825 best: 0.9481868 (394) total: 36s remaining: 9.12s 399: learn: 0.9931357 test: 0.9482811 best: 0.9481868 (394) total: 36.1s remaining: 9.03s 400: learn: 0.9930766 test: 0.9482390 best: 0.9481868 (394) total: 36.2s remaining: 8.94s 401: learn: 0.9929715 test: 0.9482241 best: 0.9481868 (394) total: 36.3s remaining: 8.85s 402: learn: 0.9929495 test: 0.9482053 best: 0.9481868 (394) total: 36.4s remaining: 8.76s 403: learn: 0.9928806 test: 0.9481988 best: 0.9481868 (394) total: 36.5s remaining: 8.67s 404: learn: 0.9928563 test: 0.9482048 best: 0.9481868 (394) total: 36.6s remaining: 8.58s 405: learn: 0.9928306 test: 0.9482138 best: 0.9481868 (394) total: 36.7s remaining: 8.49s 406: learn: 0.9927779 test: 0.9481379 best: 0.9481379 (406) total: 36.8s remaining: 8.4s 407: learn: 0.9926130 test: 0.9481220 best: 0.9481220 (407) total: 36.8s remaining: 8.31s 408: learn: 0.9925871 test: 0.9481303 best: 0.9481220 (407) total: 36.9s remaining: 8.21s 409: learn: 0.9925206 test: 0.9482273 best: 0.9481220 (407) total: 37s remaining: 8.12s 410: learn: 0.9924559 test: 0.9482343 best: 0.9481220 (407) total: 37.1s remaining: 8.03s 411: learn: 0.9924091 test: 0.9482169 best: 0.9481220 (407) total: 37.2s remaining: 7.95s 412: learn: 0.9923810 test: 0.9482864 best: 0.9481220 (407) total: 37.3s remaining: 7.86s 413: learn: 0.9923197 test: 0.9483112 best: 0.9481220 (407) total: 37.4s remaining: 7.77s 414: learn: 0.9922955 test: 0.9482382 best: 0.9481220 (407) total: 37.5s remaining: 7.68s 415: learn: 0.9922044 test: 0.9482586 best: 0.9481220 (407) total: 37.6s remaining: 7.59s 416: learn: 0.9921600 test: 0.9482932 best: 0.9481220 (407) total: 37.7s remaining: 7.5s 417: learn: 0.9920977 test: 0.9482499 best: 0.9481220 (407) total: 37.8s remaining: 7.41s 418: learn: 0.9920788 test: 0.9486586 best: 0.9481220 (407) total: 37.8s remaining: 7.32s 419: learn: 0.9920374 test: 0.9487098 best: 0.9481220 (407) total: 37.9s remaining: 7.23s 420: learn: 0.9920037 test: 0.9486305 best: 0.9481220 (407) total: 38.1s remaining: 7.14s 421: learn: 0.9919387 test: 0.9486552 best: 0.9481220 (407) total: 38.2s remaining: 7.05s 422: learn: 0.9918867 test: 0.9486635 best: 0.9481220 (407) total: 38.2s remaining: 6.96s 423: learn: 0.9918208 test: 0.9475049 best: 0.9475049 (423) total: 38.3s remaining: 6.87s 424: learn: 0.9917945 test: 0.9474257 best: 0.9474257 (424) total: 38.4s remaining: 6.78s 425: learn: 0.9917608 test: 0.9474576 best: 0.9474257 (424) total: 38.5s remaining: 6.69s 426: learn: 0.9917005 test: 0.9474473 best: 0.9474257 (424) total: 38.6s remaining: 6.6s 427: learn: 0.9916646 test: 0.9474337 best: 0.9474257 (424) total: 38.7s remaining: 6.51s 428: learn: 0.9916323 test: 0.9473135 best: 0.9473135 (428) total: 38.8s remaining: 6.42s 429: learn: 0.9916236 test: 0.9473167 best: 0.9473135 (428) total: 38.9s remaining: 6.33s 430: learn: 0.9915875 test: 0.9473543 best: 0.9473135 (428) total: 39s remaining: 6.24s 431: learn: 0.9915362 test: 0.9473502 best: 0.9473135 (428) total: 39.1s remaining: 6.15s 432: learn: 0.9914652 test: 0.9471842 best: 0.9471842 (432) total: 39.2s remaining: 6.06s 433: learn: 0.9913893 test: 0.9470891 best: 0.9470891 (433) total: 39.3s remaining: 5.97s 434: learn: 0.9913759 test: 0.9470578 best: 0.9470578 (434) total: 39.3s remaining: 5.88s 435: learn: 0.9913435 test: 0.9471194 best: 0.9470578 (434) total: 39.4s remaining: 5.79s 436: learn: 0.9912866 test: 0.9471056 best: 0.9470578 (434) total: 39.5s remaining: 5.7s 437: learn: 0.9911792 test: 0.9471148 best: 0.9470578 (434) total: 39.6s remaining: 5.6s 438: learn: 0.9911433 test: 0.9470928 best: 0.9470578 (434) total: 39.7s remaining: 5.51s 439: learn: 0.9911123 test: 0.9470177 best: 0.9470177 (439) total: 39.8s remaining: 5.43s 440: learn: 0.9910718 test: 0.9469623 best: 0.9469623 (440) total: 39.9s remaining: 5.34s 441: learn: 0.9910507 test: 0.9468077 best: 0.9468077 (441) total: 40s remaining: 5.25s 442: learn: 0.9909601 test: 0.9467897 best: 0.9467897 (442) total: 40.1s remaining: 5.15s 443: learn: 0.9909219 test: 0.9467355 best: 0.9467355 (443) total: 40.1s remaining: 5.06s 444: learn: 0.9908537 test: 0.9465296 best: 0.9465296 (444) total: 40.2s remaining: 4.97s 445: learn: 0.9908070 test: 0.9465447 best: 0.9465296 (444) total: 40.3s remaining: 4.88s 446: learn: 0.9907375 test: 0.9465242 best: 0.9465242 (446) total: 40.4s remaining: 4.79s 447: learn: 0.9906993 test: 0.9465173 best: 0.9465173 (447) total: 40.5s remaining: 4.7s 448: learn: 0.9906526 test: 0.9465176 best: 0.9465173 (447) total: 40.6s remaining: 4.61s 449: learn: 0.9906260 test: 0.9465160 best: 0.9465160 (449) total: 40.7s remaining: 4.52s 450: learn: 0.9905937 test: 0.9462640 best: 0.9462640 (450) total: 40.8s remaining: 4.43s 451: learn: 0.9905720 test: 0.9463998 best: 0.9462640 (450) total: 40.9s remaining: 4.34s 452: learn: 0.9905257 test: 0.9462701 best: 0.9462640 (450) total: 40.9s remaining: 4.25s 453: learn: 0.9904954 test: 0.9462520 best: 0.9462520 (453) total: 41s remaining: 4.16s 454: learn: 0.9904582 test: 0.9462194 best: 0.9462194 (454) total: 41.1s remaining: 4.06s 455: learn: 0.9904196 test: 0.9463092 best: 0.9462194 (454) total: 41.2s remaining: 3.97s 456: learn: 0.9903834 test: 0.9462527 best: 0.9462194 (454) total: 41.3s remaining: 3.88s 457: learn: 0.9903487 test: 0.9461975 best: 0.9461975 (457) total: 41.3s remaining: 3.79s 458: learn: 0.9902746 test: 0.9462085 best: 0.9461975 (457) total: 41.4s remaining: 3.7s 459: learn: 0.9901783 test: 0.9462346 best: 0.9461975 (457) total: 41.5s remaining: 3.61s 460: learn: 0.9901299 test: 0.9461891 best: 0.9461891 (460) total: 41.6s remaining: 3.52s 461: learn: 0.9900862 test: 0.9461710 best: 0.9461710 (461) total: 41.7s remaining: 3.43s 462: learn: 0.9900088 test: 0.9460912 best: 0.9460912 (462) total: 41.8s remaining: 3.34s 463: learn: 0.9899694 test: 0.9460991 best: 0.9460912 (462) total: 41.9s remaining: 3.25s 464: learn: 0.9899428 test: 0.9460097 best: 0.9460097 (464) total: 42s remaining: 3.16s 465: learn: 0.9898889 test: 0.9461143 best: 0.9460097 (464) total: 42.1s remaining: 3.07s 466: learn: 0.9898143 test: 0.9461324 best: 0.9460097 (464) total: 42.2s remaining: 2.98s 467: learn: 0.9898068 test: 0.9459364 best: 0.9459364 (467) total: 42.2s remaining: 2.89s 468: learn: 0.9897810 test: 0.9459498 best: 0.9459364 (467) total: 42.3s remaining: 2.8s 469: learn: 0.9897330 test: 0.9458894 best: 0.9458894 (469) total: 42.4s remaining: 2.71s 470: learn: 0.9897184 test: 0.9458889 best: 0.9458889 (470) total: 42.5s remaining: 2.62s 471: learn: 0.9897005 test: 0.9459001 best: 0.9458889 (470) total: 42.6s remaining: 2.52s 472: learn: 0.9896770 test: 0.9459225 best: 0.9458889 (470) total: 42.7s remaining: 2.44s 473: learn: 0.9896442 test: 0.9458471 best: 0.9458471 (473) total: 42.8s remaining: 2.35s 474: learn: 0.9896313 test: 0.9458280 best: 0.9458280 (474) total: 42.9s remaining: 2.25s 475: learn: 0.9895991 test: 0.9458789 best: 0.9458280 (474) total: 43s remaining: 2.17s 476: learn: 0.9895432 test: 0.9458152 best: 0.9458152 (476) total: 43s remaining: 2.08s 477: learn: 0.9895102 test: 0.9457057 best: 0.9457057 (477) total: 43.1s remaining: 1.99s 478: learn: 0.9895015 test: 0.9457081 best: 0.9457057 (477) total: 43.2s remaining: 1.89s 479: learn: 0.9894746 test: 0.9457081 best: 0.9457057 (477) total: 43.3s remaining: 1.8s 480: learn: 0.9894371 test: 0.9456511 best: 0.9456511 (480) total: 43.4s remaining: 1.71s 481: learn: 0.9894064 test: 0.9456085 best: 0.9456085 (481) total: 43.5s remaining: 1.62s 482: learn: 0.9893701 test: 0.9455908 best: 0.9455908 (482) total: 43.5s remaining: 1.53s 483: learn: 0.9893201 test: 0.9456263 best: 0.9455908 (482) total: 43.6s remaining: 1.44s 484: learn: 0.9892358 test: 0.9456122 best: 0.9455908 (482) total: 43.7s remaining: 1.35s 485: learn: 0.9892046 test: 0.9456928 best: 0.9455908 (482) total: 43.8s remaining: 1.26s 486: learn: 0.9891687 test: 0.9457200 best: 0.9455908 (482) total: 43.9s remaining: 1.17s 487: learn: 0.9891465 test: 0.9457124 best: 0.9455908 (482) total: 44s remaining: 1.08s 488: learn: 0.9891153 test: 0.9461884 best: 0.9455908 (482) total: 44.1s remaining: 992ms 489: learn: 0.9890802 test: 0.9459138 best: 0.9455908 (482) total: 44.2s remaining: 901ms 490: learn: 0.9890506 test: 0.9459502 best: 0.9455908 (482) total: 44.3s remaining: 811ms 491: learn: 0.9889809 test: 0.9459261 best: 0.9455908 (482) total: 44.4s remaining: 721ms 492: learn: 0.9889467 test: 0.9458780 best: 0.9455908 (482) total: 44.4s remaining: 631ms 493: learn: 0.9888492 test: 0.9440324 best: 0.9440324 (493) total: 44.5s remaining: 541ms 494: learn: 0.9888169 test: 0.9439935 best: 0.9439935 (494) total: 44.6s remaining: 451ms 495: learn: 0.9887782 test: 0.9439918 best: 0.9439918 (495) total: 44.7s remaining: 360ms 496: learn: 0.9887605 test: 0.9439719 best: 0.9439719 (496) total: 44.8s remaining: 270ms 497: learn: 0.9887150 test: 0.9439730 best: 0.9439719 (496) total: 44.9s remaining: 180ms 498: learn: 0.9886100 test: 0.9439820 best: 0.9439719 (496) total: 44.9s remaining: 90.1ms 499: learn: 0.9885835 test: 0.9439464 best: 0.9439464 (499) total: 45s remaining: 0us bestTest = 0.9439464105 bestIteration = 499 . from sklearn.metrics import mean_squared_error RMSE = np.sqrt(mean_squared_error(y_test, y_pred2)) print(&quot;RMSE :&quot;, RMSE) . RMSE : 0.7672674456626155 . Kaggle &#51228;&#52636;&#48169;&#48277; . tt = pd.read_csv(&#39;/content/drive/MyDrive/test.csv&#39;) tt . ID shop_id item_id . 0 0 | 5 | 5037 | . 1 1 | 5 | 5320 | . 2 2 | 5 | 5233 | . 3 3 | 5 | 5232 | . 4 4 | 5 | 5268 | . ... ... | ... | ... | . 214195 214195 | 45 | 18454 | . 214196 214196 | 45 | 16188 | . 214197 214197 | 45 | 15757 | . 214198 214198 | 45 | 19648 | . 214199 214199 | 45 | 969 | . 214200 rows × 3 columns . y_pred = np.clip(y_pred,0,20) . new = pd.concat([X_test.reset_index(), pd.DataFrame(y_pred, columns =[&quot;item_cnt_month&quot;])],axis = 1) . y_pred2 = np.clip(y_pred2,0,20) . new = pd.concat([X_test.reset_index(), pd.DataFrame(y_pred2, columns =[&quot;item_cnt_month&quot;])],axis = 1) . submission = tt.merge(new, on = [&quot;shop_id&quot;, &quot;item_id&quot;])[[&quot;ID&quot;,&quot;item_cnt_month&quot;]] . submission . ID item_cnt_month . 0 0 | 0.437649 | . 1 1 | 0.125786 | . 2 2 | 1.069754 | . 3 3 | 0.279574 | . 4 4 | 0.399803 | . ... ... | ... | . 214195 214195 | 0.242378 | . 214196 214196 | 0.085225 | . 214197 214197 | 0.110761 | . 214198 214198 | 0.114991 | . 214199 214199 | 0.160666 | . 214200 rows × 2 columns . kaggle leader board score 1.00558&#51221;&#46020; . submission.to_csv(&#39;/content/drive/MyDrive/submission.csv&#39;, index=False, encoding=&#39;cp949&#39;) . kaggle leader board score 1.00094정도 . submission.to_csv(&#39;/content/drive/MyDrive/submission_cat.csv&#39;, index=False, encoding=&#39;cp949&#39;) . Trash - &#51060; &#48145;&#48512;&#53552;&#45716; &#49324;&#50857;&#50504;&#54632; . Linear Regression . from sklearn.linear_model import LinearRegression lm = LinearRegression() lm.fit(X_train, y_train) y_pred = lgbm_r.predict(X_test) . from sklearn.metrics import mean_squared_error RMSE = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;RMSE :&quot;, RMSE) . RMSE : 14.427431631612777 . from sklearn.ensemble import RandomForestRegressor rfr = RandomForestRegressor(max_depth=10,n_jobs=-1) rfr.fit(X_train,y_train) y_pred = rfr.predict(X_test) . IndexError Traceback (most recent call last) &lt;ipython-input-64-43c5e1314ece&gt; in &lt;module&gt; 4 rfr.fit(X_train,y_train) 5 -&gt; 6 y_pred = rf.predict(X_test) C: ProgramData Anaconda3 lib site-packages sklearn ensemble _forest.py in predict(self, X) 782 check_is_fitted(self) 783 # Check data --&gt; 784 X = self._validate_X_predict(X) 785 786 # Assign chunk of trees to jobs C: ProgramData Anaconda3 lib site-packages sklearn ensemble _forest.py in _validate_X_predict(self, X) 420 check_is_fitted(self) 421 --&gt; 422 return self.estimators_[0]._validate_X_predict(X, check_input=True) 423 424 @property IndexError: list index out of range . from sklearn.metrics import mean_squared_error RMSE = np.sqrt(mean_squared_error(y_test, y_pred)) print(&quot;RMSE :&quot;, RMSE) . RMSE : 17.409831607038456 . &#49884;&#44228;&#50676;&#47196; &#51217;&#44540;? . 시계열 데이터가 AR의 특성을 띄는 경우, ACF는 천천히 감소하고 PACF는 처음 시차를 제외하고 급격히 감소한다. | 반대로, MA의 특성을 띄는 경우 ACF는 급격히 감소하고 PACF는 천천히 감소한다. | 급격히 감소하는 시차를 각 AR과 MA 모형의 모수(p, q)로 사용할 수 있다. 또한 데이터를 차분하여 ACF 및 PACF 계산함으로써 적절한 차분횟수까지 구할 수 있다 | . Trash Bins . &#45936;&#51060;&#53552;&#44032; &#45320;&#47924; &#48708;&#45824;&#54644;&#51664; - &#51312;&#51221;&#54644;&#50556;&#54632;!!!(&#51060;&#50857;X) . import itertools n_df = [] add = set(sales_test.item_id).difference(set(grouped.item_id)) # train에 한개도 없는 것들 len(add) for i in range(0, 34): tmp = grouped[grouped.date_block_num == i] n_df.append(np.array(list(itertools.product([i], tmp.shop_id.unique(), tmp.item_id.unique())))) save = pd.DataFrame() for i in range(0, 34): tt = pd.DataFrame(n_df[i], columns = [&quot;date_block_num&quot;, &quot;shop_id&quot;, &quot;item_id&quot;]) save = pd.concat([save, tt]) save = save.reset_index() save.drop(&quot;index&quot;,axis = 1, inplace = True) final_train = pd.concat([save,sales_test]) final_train = final_train.reset_index() final_train.drop(&quot;index&quot;, axis = 1, inplace = True) final_train final_merge = pd.merge(final_train, grouped, on = [&quot;date_block_num&quot;, &quot;shop_id&quot;, &quot;item_id&quot;], how = &quot;left&quot;) final_merge.isna().sum() final_merge . save . date_block_num shop_id item_id . 0 0 | 0 | 32 | . 1 0 | 0 | 33 | . 2 0 | 0 | 35 | . 3 0 | 0 | 43 | . 4 0 | 0 | 51 | . ... ... | ... | ... | . 10913845 33 | 59 | 5662 | . 10913846 33 | 59 | 10068 | . 10913847 33 | 59 | 12839 | . 10913848 33 | 59 | 18275 | . 10913849 33 | 59 | 20392 | . 10913850 rows × 3 columns . test&#45936;&#51060;&#53552;&#50752; &#54633;&#52840; . 나중에 date_block_num이 34인것만 골라서 test셋의 결과를 도출한다. | . final_train = pd.concat([save,sales_test]) final_train = final_train.reset_index() final_train.drop(&quot;index&quot;, axis = 1, inplace = True) final_train final_merge = pd.merge(final_train, grouped, on = [&quot;date_block_num&quot;, &quot;shop_id&quot;, &quot;item_id&quot;], how = &quot;left&quot;) final_merge.isna().sum() final_merge . NameError Traceback (most recent call last) &lt;ipython-input-22-caeaf067f850&gt; in &lt;module&gt; -&gt; 1 final_train = pd.concat([save,sales_test]) 2 final_train = final_train.reset_index() 3 final_train.drop(&#34;index&#34;, axis = 1, inplace = True) 4 final_train NameError: name &#39;save&#39; is not defined . sales_train.groupby([&quot;shop_id&quot;,&quot;item_id&quot;]).mean() . date_block_num item_price item_cnt_day . shop_id item_id . 0 30 1.000000 | 265.000 | 3.444444 | . 31 1.000000 | 434.000 | 1.571429 | . 32 0.636364 | 221.000 | 1.454545 | . 33 0.500000 | 347.000 | 1.000000 | . 35 0.916667 | 247.000 | 1.250000 | . ... ... ... | ... | ... | . 59 22154 0.000000 | 999.000 | 1.000000 | . 22155 6.000000 | 149.000 | 1.000000 | . 22162 25.937500 | 389.625 | 1.000000 | . 22164 26.666667 | 724.000 | 1.000000 | . 22167 12.000000 | 299.000 | 1.000000 | . 424124 rows × 3 columns . tt = sales_train.groupby([&quot;date_block_num&quot;])[&quot;item_cnt_day&quot;].sum() plt.plot(tt) . [&lt;matplotlib.lines.Line2D at 0x27d9d0bc850&gt;] . sales_train = sales_train.merge(items, on = &quot;item_id&quot;) sales_train.drop([&quot;item_name&quot;], axis = 1, inplace = True) . sales_train . date date_block_num shop_id item_id item_price item_cnt_day item_category_id . 0 2013-01-01 | 0 | 18 | 5823 | 2500.0 | 1.0 | 35 | . 1 2013-01-01 | 0 | 54 | 5823 | 2499.5 | 1.0 | 35 | . 2 2013-01-02 | 0 | 54 | 5823 | 2500.0 | 1.0 | 35 | . 3 2013-01-02 | 0 | 42 | 5823 | 2490.0 | 1.0 | 35 | . 4 2013-01-02 | 0 | 56 | 5823 | 2500.0 | 1.0 | 35 | . ... ... | ... | ... | ... | ... | ... | ... | . 2935844 2015-10-31 | 33 | 55 | 6663 | 999.0 | 1.0 | 31 | . 2935845 2015-10-31 | 33 | 6 | 9826 | 1549.0 | 1.0 | 58 | . 2935846 2015-10-31 | 33 | 6 | 11905 | 2649.0 | 1.0 | 58 | . 2935847 2015-10-31 | 33 | 6 | 7136 | 3599.0 | 1.0 | 24 | . 2935848 2015-10-31 | 33 | 5 | 18723 | 549.0 | 1.0 | 40 | . 2935849 rows × 7 columns . import matplotlib.pyplot as plt monthly_count = sales_train.groupby(&quot;shop_id&quot;).sum()[&quot;item_cnt_day&quot;] plt.bar(monthly_count.index,monthly_count) . &lt;BarContainer object of 60 artists&gt; . display(grouped) . date_block_num shop_id item_id item_price item_cnt_day item_category_id shop_city . 0 0 | 0 | 32 | 221.0 | 6.0 | 40 | 0 | . 1 1 | 0 | 32 | 221.0 | 10.0 | 40 | 0 | . 2 0 | 0 | 33 | 347.0 | 3.0 | 37 | 0 | . 3 1 | 0 | 33 | 347.0 | 3.0 | 37 | 0 | . 4 0 | 0 | 35 | 247.0 | 1.0 | 40 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 1609119 33 | 55 | 10204 | 399.0 | 16.0 | 31 | 28 | . 1609120 33 | 55 | 12733 | 1490.0 | 2.0 | 76 | 28 | . 1609121 33 | 55 | 13092 | 2000.0 | 1.0 | 36 | 28 | . 1609122 33 | 55 | 16797 | 790.0 | 1.0 | 78 | 28 | . 1609123 33 | 55 | 18060 | 172.0 | 1.0 | 44 | 28 | . 1609124 rows × 7 columns . monthly_count = sales_train.groupby(&quot;shop_city&quot;).sum()[&quot;item_cnt_day&quot;] plt.bar(monthly_count.index,monthly_count) plt.xticks(rotation=90) . ([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;), Text(0, 0, &#39;&#39;)]) .",
            "url": "https://raukrauk.github.io/ML-DL/kaggle/machine%20learning/2021/10/29/Kaggle_Predict_Future_Sales.html",
            "relUrl": "/kaggle/machine%20learning/2021/10/29/Kaggle_Predict_Future_Sales.html",
            "date": " • Oct 29, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "[Kaggle] New York City Taxi Trip Duration",
            "content": "from google.colab import files files.upload() !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 /root/.kaggle/kaggle.json . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . !kaggle competitions download -c nyc-taxi-trip-duration . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) Downloading test.zip to /content 44% 9.00M/20.3M [00:01&lt;00:01, 6.68MB/s] 100% 20.3M/20.3M [00:01&lt;00:00, 16.2MB/s] Downloading train.zip to /content 79% 50.0M/62.9M [00:00&lt;00:00, 149MB/s] 100% 62.9M/62.9M [00:00&lt;00:00, 159MB/s] Downloading sample_submission.zip to /content 0% 0.00/2.49M [00:00&lt;?, ?B/s] 100% 2.49M/2.49M [00:00&lt;00:00, 66.4MB/s] . !unzip /content/sample_submission.zip !unzip /content/train.zip !unzip /content/test.zip . Archive: /content/sample_submission.zip inflating: sample_submission.csv Archive: /content/train.zip inflating: train.csv Archive: /content/test.zip inflating: test.csv . Data fields . id - a unique identifier for each trip | vendor_id - a code indicating the provider associated with the trip record | pickup_datetime - date and time when the meter was engaged (타는 시각) | dropoff_datetime - date and time when the meter was disengaged (내리는 시각) | passenger_count - the number of passengers in the vehicle (driver entered value) (승객의 수 운전자도 포함) | pickup_longitude - the longitude where the meter was engaged (탄 경도) | pickup_latitude- the latitude where the meter was engaged (탄 위도) | dropoff_longitude - the longitude where the meter was disengaged (내린 경도) | dropoff_latitude - the latitude where the meter was disengaged (내린 위도) | store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server- Y=store and forward; N=not a store and forward trip | trip_duration - duration of the trip in seconds (Target: 얼마나 여행했는가? 단위:초) | . import numpy as np import pandas as pd import matplotlib.pyplot as plt %matplotlib inline from math import radians, cos, sin, asin, sqrt from datetime import datetime . train = pd.read_csv(&quot;/content/train.csv&quot;) test = pd.read_csv(&quot;/content/test.csv&quot;) print(&#39;train:&#39;, train.shape, &#39; ntest:&#39;, test.shape) l=train.shape[0] train.head() . train: (1458644, 11) test: (625134, 9) . id vendor_id pickup_datetime dropoff_datetime passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude store_and_fwd_flag trip_duration . 0 id2875421 | 2 | 2016-03-14 17:24:55 | 2016-03-14 17:32:30 | 1 | -73.982155 | 40.767937 | -73.964630 | 40.765602 | N | 455 | . 1 id2377394 | 1 | 2016-06-12 00:43:35 | 2016-06-12 00:54:38 | 1 | -73.980415 | 40.738564 | -73.999481 | 40.731152 | N | 663 | . 2 id3858529 | 2 | 2016-01-19 11:35:24 | 2016-01-19 12:10:48 | 1 | -73.979027 | 40.763939 | -74.005333 | 40.710087 | N | 2124 | . 3 id3504673 | 2 | 2016-04-06 19:32:31 | 2016-04-06 19:39:40 | 1 | -74.010040 | 40.719971 | -74.012268 | 40.706718 | N | 429 | . 4 id2181028 | 2 | 2016-03-26 13:30:55 | 2016-03-26 13:38:10 | 1 | -73.973053 | 40.793209 | -73.972923 | 40.782520 | N | 435 | . train.dtypes . id object vendor_id int64 pickup_datetime object dropoff_datetime object passenger_count int64 pickup_longitude float64 pickup_latitude float64 dropoff_longitude float64 dropoff_latitude float64 store_and_fwd_flag object trip_duration int64 dtype: object . train.describe() . vendor_id passenger_count pickup_longitude pickup_latitude dropoff_longitude dropoff_latitude trip_duration . count 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | 1.458644e+06 | . mean 1.534950e+00 | 1.664530e+00 | -7.397349e+01 | 4.075092e+01 | -7.397342e+01 | 4.075180e+01 | 9.594923e+02 | . std 4.987772e-01 | 1.314242e+00 | 7.090186e-02 | 3.288119e-02 | 7.064327e-02 | 3.589056e-02 | 5.237432e+03 | . min 1.000000e+00 | 0.000000e+00 | -1.219333e+02 | 3.435970e+01 | -1.219333e+02 | 3.218114e+01 | 1.000000e+00 | . 25% 1.000000e+00 | 1.000000e+00 | -7.399187e+01 | 4.073735e+01 | -7.399133e+01 | 4.073588e+01 | 3.970000e+02 | . 50% 2.000000e+00 | 1.000000e+00 | -7.398174e+01 | 4.075410e+01 | -7.397975e+01 | 4.075452e+01 | 6.620000e+02 | . 75% 2.000000e+00 | 2.000000e+00 | -7.396733e+01 | 4.076836e+01 | -7.396301e+01 | 4.076981e+01 | 1.075000e+03 | . max 2.000000e+00 | 9.000000e+00 | -6.133553e+01 | 5.188108e+01 | -6.133553e+01 | 4.392103e+01 | 3.526282e+06 | . train.hist(bins=50, figsize=(20,15)) plt.show() . Data Preprocessing . trip_duration에서 말도 안되는 값들이 존재한다. | (어떻게 3500000s = 972시간?을 운행하는 미친놈이 있을까? 2000000s도 말이 안됨.) | . plt.subplots(figsize=(15,5)) train.boxplot(); . &#44536;&#47000;&#49436; trip_duration&#51060; 5000s&#48120;&#47564;&#51064; &#44050;&#46308;&#47564; &#51060;&#50857;&#54633;&#45768;&#45796;. . train = train[(train.trip_duration &lt; 5000)] . &#51060;&#46041;&#49884;&#44036;&#51060; 5000&#52488; &#48120;&#47564;&#51064; &#45936;&#51060;&#53552; &#55176;&#49828;&#53664;&#44536;&#47016; . train.loc[train[&#39;trip_duration&#39;] &lt; 5000, &#39;trip_duration&#39;].hist(edgecolor = &quot;black&quot;, color = &quot;skyblue&quot;); plt.title(&#39;trip_duration&#39;) plt.show() . &#47196;&#44536;&#48320;&#54872;&#51012; &#54644;&#51469;&#45768;&#45796;! . np.log1p(train[&#39;trip_duration&#39;]).hist(edgecolor = &quot;black&quot;, color = &quot;skyblue&quot;); plt.title(&#39;log_trip_duration&#39;) plt.show() . Pickup Location . Principal datas for &quot;pickup&quot; : longitude ∈ [-75;-73] and latitude ∈ [40;41] | 탑승하는 위도와 경도를 제한하도록한다. | . train.plot(kind=&#39;scatter&#39;, x=&#39;pickup_longitude&#39;, y=&#39;pickup_latitude&#39;, alpha=0.2); . train = train.loc[(train[&#39;pickup_longitude&#39;] &gt; -75) &amp; (train[&#39;pickup_longitude&#39;] &lt; -73)] train = train.loc[(train[&#39;pickup_latitude&#39;] &gt; 40) &amp; (train[&#39;pickup_latitude&#39;] &lt; 41)] . Dropoff Location . Principal datas for &quot;dropoff&quot; : longitude ∈ [-75;-72.5] and latitude ∈ [40;41.5]. | 내리는 위치도 제한한다. | . train.plot(kind=&#39;scatter&#39;, x=&#39;dropoff_longitude&#39;, y=&#39;dropoff_latitude&#39;, alpha=0.1); . train = train.loc[(train[&#39;dropoff_longitude&#39;] &gt; -75) &amp; (train[&#39;dropoff_longitude&#39;] &lt; -73)] train = train.loc[(train[&#39;dropoff_latitude&#39;] &gt; 40.5) &amp; (train[&#39;dropoff_latitude&#39;] &lt; 41.5)] . &#49849;&#44061; &#49688;&#46020; 6&#47749; &#51060;&#54616;&#47196; &#51228;&#54620;&#54620;&#45796;. . 위에서 운전자도 포함한다 그랬는데 0명은 뭐임? | . train[&#39;passenger_count&#39;].hist(bins=100, log=True, figsize=(10,5)); plt.title(&#39;passenger_count&#39;) plt.show() . train = train.loc[(train[&#39;passenger_count&#39;] &gt;= 0) &amp; (train[&#39;passenger_count&#39;] &lt;= 6)] . &#44208;&#52769;&#44050;&#51008; &#51316;&#51116;&#54616;&#51648; &#50506;&#45716;&#45796;. . train.isnull().sum() . id 0 vendor_id 0 pickup_datetime 0 dropoff_datetime 0 passenger_count 0 pickup_longitude 0 pickup_latitude 0 dropoff_longitude 0 dropoff_latitude 0 store_and_fwd_flag 0 trip_duration 0 dtype: int64 . &#48373;&#51228;&#46108; &#44050;&#51060; &#51080;&#45716;&#51648; &#54869;&#51064;&#54644; &#48376;&#45796;. . 5건 존재 | . train.duplicated().sum() . 0 . train = train.drop_duplicates() train.duplicated().sum() . 0 . &#48260;&#47532;&#45716; Features . train.drop([&quot;store_and_fwd_flag&quot;], axis=1, inplace=True) test.drop([&quot;store_and_fwd_flag&quot;], axis=1, inplace=True) . Preprocessing &#50756;&#47308; . train.shape, test.shape . ((1454727, 10), (625134, 8)) . Feature Engineering . DateTime과, 위경도를 만져본다! | . plg, plt = &#39;pickup_longitude&#39;, &#39;pickup_latitude&#39; dlg, dlt = &#39;dropoff_longitude&#39;, &#39;dropoff_latitude&#39; pdt, ddt = &#39;pickup_datetime&#39;, &#39;dropoff_datetime&#39; . &#44144;&#47532;&#47484; &#44228;&#49328;&#54616;&#45716; &#54632;&#49688;&#49373;&#49457; . Stackoverflow에 있다는데 링크가 없음 | haversine공식은 최단거리를 구하는 공식 | https://kayuse88.github.io/haversine/ | . def haversine(lon1, lat1, lon2, lat2): &quot;&quot;&quot; Calculate the great circle distance between two points on the earth (specified in decimal degrees) &quot;&quot;&quot; # convert decimal degrees to radians lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2]) # haversine formula dlon = lon2 - lon1 dlat = lat2 - lat1 a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2 c = 2 * asin(sqrt(a)) # Radius of earth in kilometers is 6371 km = 6371* c return km def euclidian_distance(x): x1, y1 = np.float64(x[plg]), np.float64(x[plt]) x2, y2 = np.float64(x[dlg]), np.float64(x[dlt]) return haversine(x1, y1, x2, y2) . &#44144;&#47532;&#48320;&#49688;&#47484; &#52628;&#44032;&#54644; &#51456;&#45796;. . %time train[&#39;distance&#39;] = train[[plg, plt, dlg, dlt]].apply(euclidian_distance, axis=1) . CPU times: user 4 µs, sys: 0 ns, total: 4 µs Wall time: 9.78 µs . %time test[&#39;distance&#39;] = test[[plg, plt, dlg, dlt]].apply(euclidian_distance, axis=1) . CPU times: user 5 µs, sys: 0 ns, total: 5 µs Wall time: 9.3 µs . Datetime&#48320;&#54872;&#51060; &#54596;&#50836;&#54620; &#48320;&#49688;&#46308; . train[pdt] = train[pdt].apply(lambda x : datetime.strptime(x, &quot;%Y-%m-%d %H:%M:%S&quot;)) train[ddt] = train[ddt].apply(lambda x : datetime.strptime(x, &quot;%Y-%m-%d %H:%M:%S&quot;)) . test[pdt] = test[pdt].apply(lambda x : datetime.strptime(x, &quot;%Y-%m-%d %H:%M:%S&quot;)) #test dataset has not &quot;dropoff_datetiime&quot; . Datetime&#51012; &#51328;&#45908; &#49464;&#48512;&#51201;&#51004;&#47196; &#44396;&#48516;&#54620;&#45796;. . train[&#39;month&#39;] = train[pdt].apply(lambda x : x.month) train[&#39;week_day&#39;] = train[pdt].apply(lambda x : x.weekday()) train[&#39;day_month&#39;] = train[pdt].apply(lambda x : x.day) train[&#39;pickup_time_minutes&#39;] = train[pdt].apply(lambda x : x.hour * 60.0 + x.minute) . test[&#39;month&#39;] = test[pdt].apply(lambda x : x.month) test[&#39;week_day&#39;] = test[pdt].apply(lambda x : x.weekday()) test[&#39;day_month&#39;] = test[pdt].apply(lambda x : x.day) test[&#39;pickup_time_minutes&#39;] = test[pdt].apply(lambda x : x.hour * 60.0 + x.minute) . train.shape, test.shape . ((1454727, 15), (625134, 13)) . Feature Selection . features_train = [&quot;vendor_id&quot;, &quot;passenger_count&quot;, &quot;pickup_longitude&quot;, &quot;pickup_latitude&quot;, &quot;dropoff_longitude&quot;, &quot;dropoff_latitude&quot;, &quot;distance&quot;, &quot;month&quot;, &quot;week_day&quot;, &quot;day_month&quot;, &quot;pickup_time_minutes&quot;] X_train = train[features_train] y_train = np.log1p(train[&quot;trip_duration&quot;]) features_test = [&quot;vendor_id&quot;, &quot;passenger_count&quot;, &quot;pickup_longitude&quot;, &quot;pickup_latitude&quot;, &quot;dropoff_longitude&quot;, &quot;dropoff_latitude&quot;, &quot;distance&quot;, &quot;month&quot;, &quot;week_day&quot;, &quot;day_month&quot;, &quot;pickup_time_minutes&quot;] X_test = test[features_test] . from sklearn.ensemble import RandomForestRegressor #rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, min_samples_split=15, max_depth=100, bootstrap=True, n_jobs=-1) #rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=2, max_features=0.7, max_depth=100, bootstrap=True, n_jobs=-1) #rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=10, min_samples_split=15, max_depth=100, bootstrap=True, n_jobs=-1) rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=2, max_features=&#39;auto&#39;, max_depth=50, bootstrap=True, n_jobs=-1) . rf.fit(X_train, y_train) . KeyboardInterrupt Traceback (most recent call last) &lt;ipython-input-35-168a6fc83696&gt; in &lt;module&gt;() -&gt; 1 rf.fit(X_train, y_train) /usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py in fit(self, X, y, sample_weight) 381 verbose=self.verbose, class_weight=self.class_weight, 382 n_samples_bootstrap=n_samples_bootstrap) --&gt; 383 for i, t in enumerate(trees)) 384 385 # Collect newly grown trees /usr/local/lib/python3.7/dist-packages/joblib/parallel.py in __call__(self, iterable) 1052 1053 with self._backend.retrieval_context(): -&gt; 1054 self.retrieve() 1055 # Make sure that we get a last message telling us we are done 1056 elapsed_time = time.time() - self._start_time /usr/local/lib/python3.7/dist-packages/joblib/parallel.py in retrieve(self) 931 try: 932 if getattr(self._backend, &#39;supports_timeout&#39;, False): --&gt; 933 self._output.extend(job.get(timeout=self.timeout)) 934 else: 935 self._output.extend(job.get()) /usr/lib/python3.7/multiprocessing/pool.py in get(self, timeout) 649 650 def get(self, timeout=None): --&gt; 651 self.wait(timeout) 652 if not self.ready(): 653 raise TimeoutError /usr/lib/python3.7/multiprocessing/pool.py in wait(self, timeout) 646 647 def wait(self, timeout=None): --&gt; 648 self._event.wait(timeout) 649 650 def get(self, timeout=None): /usr/lib/python3.7/threading.py in wait(self, timeout) 550 signaled = self._flag 551 if not signaled: --&gt; 552 signaled = self._cond.wait(timeout) 553 return signaled 554 /usr/lib/python3.7/threading.py in wait(self, timeout) 294 try: # restore state no matter what (e.g., KeyboardInterrupt) 295 if timeout is None: --&gt; 296 waiter.acquire() 297 gotit = True 298 else: KeyboardInterrupt: . import lightgbm as lgb hyperparameters = { &#39;n_estimators&#39;: 800, &#39;learning_rate&#39;:0.3, } model = lgb.LGBMRegressor(**hyperparameters) model.fit(X_train, y_train) . LGBMRegressor(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=1.0, importance_type=&#39;split&#39;, learning_rate=0.3, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=800, n_jobs=-1, num_leaves=31, objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0, subsample_for_bin=200000, subsample_freq=0) . log_pred = model.predict(X_test) y_pred = np.exp(log_pred) - np.ones(len(log_pred)) . my_submission = pd.DataFrame({&#39;id&#39;: test.id, &#39;trip_duration&#39;: y_pred}) my_submission.head() . id trip_duration . 0 id3004672 | 827.671755 | . 1 id3505355 | 559.265655 | . 2 id1217141 | 460.368949 | . 3 id2150126 | 962.339948 | . 4 id1598245 | 339.646166 | . my_submission.to_csv(&quot;submission.csv&quot;, index=False) . !kaggle competitions submit -c nyc-taxi-trip-duration -f submission.csv -m &quot;Message&quot; . Warning: Looks like you&#39;re using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4) 100% 16.9M/16.9M [00:10&lt;00:00, 1.69MB/s] Successfully submitted to New York City Taxi Trip Duration .",
            "url": "https://raukrauk.github.io/ML-DL/kaggle/machine%20learing/2021/10/09/New_York_City_Taxi_Trip_Duration_.html",
            "relUrl": "/kaggle/machine%20learing/2021/10/09/New_York_City_Taxi_Trip_Duration_.html",
            "date": " • Oct 9, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "[Kaggle] Heart Attack 예측",
            "content": "pip install kaggle . Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (1.5.12) Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from kaggle) (4.62.2) Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.8.2) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from kaggle) (2.23.0) Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle) (5.0.2) Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from kaggle) (2021.5.30) Requirement already satisfied: urllib3 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.24.3) Requirement already satisfied: six&gt;=1.10 in /usr/local/lib/python3.7/dist-packages (from kaggle) (1.15.0) Requirement already satisfied: text-unidecode&gt;=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify-&gt;kaggle) (1.3) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (3.0.4) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;kaggle) (2.10) . from google.colab import files files.upload() . Upload widget is only available when the cell has been executed in the current browser session. Please rerun this cell to enable. Saving kaggle.json to kaggle.json . {&#39;kaggle.json&#39;: b&#39;{&#34;username&#34;:&#34;raukrauk&#34;,&#34;key&#34;:&#34;6491880d39cd6504f06b87f5b259e40f&#34;}&#39;} . !mkdir -p ~/.kaggle !cp kaggle.json ~/.kaggle/ !chmod 600 /root/.kaggle/kaggle.json . !kaggle datasets download -d rashikrahmanpritom/heart-attack-analysis-prediction-dataset . heart-attack-analysis-prediction-dataset.zip: Skipping, found more recently modified local copy (use --force to force download) . !unzip /content/heart-attack-analysis-prediction-dataset.zip . Archive: /content/heart-attack-analysis-prediction-dataset.zip inflating: heart.csv inflating: o2Saturation.csv . HEART Attack . FEATURES: . AGE - AGE OF THE PATIENT . | SEX - SEX OF THE PATIENT , (1:MALE , 0: FEMALE) . | EXANG - EXERCISE INCLUDE ANGINA (1=YES, 0=NO) / 협심증을 가지고 있는지 여부 | CA - NUMBER OF MAJOR VESSELS (0-3) / 투시 진단(X-ray)에 의해 색칠된 주요 혈관 수(0-3) | CP - CHEST PAIN TYPE (Value 1: typical angina, Value2: atypical angina, Value 3: non-anginal pain, Value 4: asymptomatic) - 값이 커질수록 고통이 적어진다. (값을 역으로 변환하여 사용?) | TRTBPS - RESTING BLOOD PRESSURE IN (MM|HG) / 안정기 혈압 | CHOL - CHOLESTROL IN (MG|DL) FETCHED VIA BMI SENSOR / BMI 센서를 이용해 얻은 콜레스테롤 수치 | FBS - (FASTING BLOOD SUGAR &gt; 120 MG/DL) (1=TRUE, 0=FALSE) / 공복혈당이 120이 넘는지 여부 (당뇨여부와 유사하다고 생각함.) | EST-ECG -(RESTING ELECTROCARDIOGRAPHIC RESULTS) Value 0: normal, Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of &gt; 0.05 mV), Value 2: showing probable or definite left ventricular hypertrophy by Estes&#39; criteria / 휴면 심전도 결과 | THALACH -MAXIMUM HEAERT RATE ACHIEVED / 최대 심박수 TARGET -0=LESS CHANCE OF HEART ATTACK, 1= MORE CHANCE OF HEART ATTACK | ANSWER TO THE FOLLOWING QUESTIONS ARE GIVEN . Increasing in age have any effect towards heart attack. [나이가 많을 수록 심장마비의 가능성이 증대된다.] . | Does Increase in blood pressure have any relation with heart attack? [높은 혈압은 심장마비와 어떠한 관계라도 가지고 있다.] . | Does increase in cholestrol level in body have any effect towards the heart attack? [체내 콜레스테롤 수치가 높아지면 심장마비에 어떠한 영향이라도 미친다.] . | import pandas as pd import numpy as np import seaborn as sns import plotly.express as px import missingno import matplotlib.pyplot as plt from plotly.subplots import make_subplots import plotly.graph_objects as go import plotly.figure_factory as ff . &#45936;&#51060;&#53552;&#47484; &#49332;&#54196;&#48376;&#45796;. . 13 개의 feature와 1개의 target값으로 구성됨. | . df=pd.read_csv(&quot;/content/heart.csv&quot;) df.head() #Loading the First Five Rows: . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 0 63 | 1 | 3 | 145 | 233 | 1 | 0 | 150 | 0 | 2.3 | 0 | 0 | 1 | 1 | . 1 37 | 1 | 2 | 130 | 250 | 0 | 1 | 187 | 0 | 3.5 | 0 | 0 | 2 | 1 | . 2 41 | 0 | 1 | 130 | 204 | 0 | 0 | 172 | 0 | 1.4 | 2 | 0 | 2 | 1 | . 3 56 | 1 | 1 | 120 | 236 | 0 | 1 | 178 | 0 | 0.8 | 2 | 0 | 2 | 1 | . 4 57 | 0 | 0 | 120 | 354 | 0 | 1 | 163 | 1 | 0.6 | 2 | 0 | 2 | 1 | . CP &#49707;&#51088;&#44032; &#53356;&#47732; &#44256;&#53685;&#51060; &#45908; &#45458;&#44172; &#53076;&#46377;&#54644;&#51468; . df.cp = 3 - df.cp . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 303 entries, 0 to 302 Data columns (total 14 columns): # Column Non-Null Count Dtype -- -- 0 age 303 non-null int64 1 sex 303 non-null int64 2 cp 303 non-null int64 3 trtbps 303 non-null int64 4 chol 303 non-null int64 5 fbs 303 non-null int64 6 restecg 303 non-null int64 7 thalachh 303 non-null int64 8 exng 303 non-null int64 9 oldpeak 303 non-null float64 10 slp 303 non-null int64 11 caa 303 non-null int64 12 thall 303 non-null int64 13 output 303 non-null int64 dtypes: float64(1), int64(13) memory usage: 33.3 KB . df.shape . (303, 14) . plt.figure(figsize=(20,6)) sns.heatmap(df.corr(),annot=True,cmap=&quot;PuBuGn&quot;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcf311d9190&gt; . &#44208;&#52769;&#44050;&#51032; &#50668;&#48512;&#47484; &#54869;&#51064; &#54644; &#48376;&#45796;. . 결측값은 존재하지 않음 | . for col in df.columns: pct_missing = df[col].isnull().sum() print(f&#39;{col} - {pct_missing :.1%}&#39;) . age - 0.0% sex - 0.0% cp - 0.0% trtbps - 0.0% chol - 0.0% fbs - 0.0% restecg - 0.0% thalachh - 0.0% exng - 0.0% oldpeak - 0.0% slp - 0.0% caa - 0.0% thall - 0.0% output - 0.0% . &#46609;&#44057;&#51008; &#44050;&#51060; &#51080;&#45716;&#51648; (Duplicated &#46108; &#44050;)&#51060; &#51080;&#45716;&#51648; &#54869;&#51064;&#54620;&#45796;. . 164행과 같은 행이 존재한다. | 정말 우연치 않게 모두 같은 값을 가지지 않았을까? | . df[df.duplicated() == True] . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 164 38 | 1 | 1 | 138 | 175 | 0 | 1 | 173 | 0 | 0.0 | 2 | 4 | 2 | 1 | . df=df.drop_duplicates(keep=&quot;first&quot;) . df.shape . (302, 14) . plt.figure(figsize=(18,8)) sns.pairplot(df[[&#39;age&#39;,&#39;trtbps&#39;,&#39;chol&#39;,&#39;thalachh&#39;, &#39;oldpeak&#39;]]) . &lt;seaborn.axisgrid.PairGrid at 0x7f581d686c50&gt; . &lt;Figure size 1296x576 with 0 Axes&gt; . from scipy.stats import norm plt.figure(figsize=(18,8)) sns.distplot(df.age, bins = len(df[&quot;age&quot;]), fit = norm) plt.title(&quot;COUNT OF PATIENTS AGE&quot;,fontsize=20) plt.xlabel(&quot;AGE&quot;,fontsize=20) plt.ylabel(&quot;COUNT&quot;,fontsize=20) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). . s = df[&quot;output&quot;].value_counts().reset_index() explode = [0.08,0.08] colors = [&#39;#8fd9b6&#39;, &#39;#d395d0&#39;] plt.pie(s[&quot;output&quot;], labels=(&quot;Higher&quot;,&quot;Lower&quot;),autopct=&#39;%.1f%%&#39;,shadow = True, explode=explode, colors = colors, textprops={&#39;fontsize&#39;: 15} ) plt.show() . s = df[&quot;sex&quot;].value_counts().reset_index() explode = [0.08,0.08] colors = [&#39;#ff9999&#39;, &#39;#ffc000&#39;, &#39;#8fd9b6&#39;, &#39;#d395d0&#39;] plt.pie(s[&quot;sex&quot;], labels=(&quot;Male&quot;,&quot;Female&quot;),autopct=&#39;%.1f%%&#39;,shadow = True, explode=explode, colors = colors, textprops={&#39;fontsize&#39;: 15} ) plt.show() . plt.figure(figsize=(18,8)) sns.lineplot(y=&quot;thalachh&quot;,x=&quot;age&quot;,data=df) plt.title(&quot;HEART RATE WITH AGE&quot;,fontsize=20) plt.xlabel(&quot;AGE&quot;,fontsize=20) plt.ylabel(&quot;HEART RATE&quot;,fontsize=20) plt.show() . plt.figure(figsize=(18,8)) sns.lineplot(y=&quot;chol&quot;,x=&quot;age&quot;,data=df) plt.title(&quot;CHOLESTROL LEVEL WITH AGE&quot;,fontsize=20) plt.xlabel(&quot;AGE&quot;,fontsize=20) plt.ylabel(&quot;CHOLESTROL LEVEL&quot;,fontsize=20) plt.show() . plt.figure(figsize=(18,8)) sns.lineplot(y=&quot;trtbps&quot;,x=&quot;age&quot;,data=df) plt.title(&quot;BLOOD PRESSURE WITH AGE&quot;,fontsize=20) plt.xlabel(&quot;AGE&quot;,fontsize=20) plt.ylabel(&quot;BLOOD PRESSURE&quot;,fontsize=20) plt.show() . &#51060;&#49345;&#52824;&#47484; &#54869;&#51064;&#54644; &#48376;&#45796;. . q1 = df.quantile(0.25) # Q3 q3 = df.quantile(0.75) # IQR IQR = q3 - q1 # Outlier range upper = q3 + IQR * 1.5 lower = q1 - IQR * 1.5 upper_dict = dict(upper) lower_dict = dict(lower) . for i,v in df[[&#39;age&#39;,&#39;trtbps&#39;,&#39;chol&#39;,&#39;thalachh&#39;, &#39;oldpeak&#39;]].items(): v_col = v[( v&lt;= lower_dict[i]) | (v &gt;= upper_dict[i])] perc = np.shape(v_col)[0] * 100.0 / np.shape(df)[0] print(&quot;Column {} outliers = {} =&gt; {}%&quot;.format(i,len(v_col),round((perc),3))) . Column age outliers = 0 =&gt; 0.0% Column trtbps outliers = 13 =&gt; 4.29% Column chol outliers = 5 =&gt; 1.65% Column thalachh outliers = 1 =&gt; 0.33% Column oldpeak outliers = 8 =&gt; 2.64% . &#47196;&#44536;&#48320;&#54872;&#51012; &#54620;&#45796;. . oldpeak는 최솟값이 0이어서 np.log1p로 해줌 | . df[[&#39;age&#39;,&#39;trtbps&#39;,&#39;chol&#39;,&#39;thalachh&#39;, &#39;oldpeak&#39;]].describe() . age trtbps chol thalachh oldpeak . count 303.000000 | 303.000000 | 303.000000 | 303.000000 | 303.000000 | . mean 54.366337 | 131.623762 | 246.264026 | 149.646865 | 1.039604 | . std 9.082101 | 17.538143 | 51.830751 | 22.905161 | 1.161075 | . min 29.000000 | 94.000000 | 126.000000 | 71.000000 | 0.000000 | . 25% 47.500000 | 120.000000 | 211.000000 | 133.500000 | 0.000000 | . 50% 55.000000 | 130.000000 | 240.000000 | 153.000000 | 0.800000 | . 75% 61.000000 | 140.000000 | 274.500000 | 166.000000 | 1.600000 | . max 77.000000 | 200.000000 | 564.000000 | 202.000000 | 6.200000 | . df[&quot;age&quot;]= np.log(df.age) df[&quot;trtbps&quot;]= np.log(df.trtbps) df[&quot;chol&quot;]= np.log(df.chol) df[&quot;thalachh&quot;]= np.log(df.thalachh) df[&quot;oldpeak&quot;]= np.log1p(df.oldpeak) print(&quot;Log Transform performed&quot;) . Log Transform performed . Modeling . 평가척도 함수 생성 | . from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score def get_clf_eval(y_test, y_pred): confusion = confusion_matrix(y_test, y_pred) accuracy = accuracy_score(y_test, y_pred) precision = precision_score(y_test, y_pred) recall = recall_score(y_test, y_pred) F1 = f1_score(y_test, y_pred) AUC = roc_auc_score(y_test, y_pred) print(&#39;오차행렬: n&#39;, confusion) print(&#39; n정확도: {:.4f}&#39;.format(accuracy)) print(&#39;정밀도: {:.4f}&#39;.format(precision)) print(&#39;재현율: {:.4f}&#39;.format(recall)) print(&#39;F1: {:.4f}&#39;.format(F1)) print(&#39;AUC: {:.4f}&#39;.format(AUC)) . Train-test set&#49373;&#49457; . X=df.drop(&quot;output&quot;,axis=1) y=df[&quot;output&quot;] . from sklearn.preprocessing import StandardScaler scaler=StandardScaler() X = pd.DataFrame(scaler.fit_transform(X)) . from sklearn.model_selection import train_test_split X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=60) . from sklearn import metrics from sklearn.neighbors import KNeighborsClassifier knn=KNeighborsClassifier(n_neighbors=5) knn.fit(X_train,y_train) y_pred=knn.predict(X_test) print(metrics.classification_report(y_test,y_pred)) . precision recall f1-score support 0 0.95 0.68 0.79 31 1 0.74 0.97 0.84 30 accuracy 0.82 61 macro avg 0.85 0.82 0.82 61 weighted avg 0.85 0.82 0.82 61 . df.head() . age sex cp trtbps chol fbs restecg thalachh exng oldpeak slp caa thall output . 0 4.143135 | 1 | 0 | 4.976734 | 5.451038 | 1 | 0 | 5.010635 | 0 | 1.193922 | 0 | 0 | 1 | 1 | . 1 3.610918 | 1 | 1 | 4.867534 | 5.521461 | 0 | 1 | 5.231109 | 0 | 1.504077 | 0 | 0 | 2 | 1 | . 2 3.713572 | 0 | 2 | 4.867534 | 5.318120 | 0 | 0 | 5.147494 | 0 | 0.875469 | 2 | 0 | 2 | 1 | . 3 4.025352 | 1 | 2 | 4.787492 | 5.463832 | 0 | 1 | 5.181784 | 0 | 0.587787 | 2 | 0 | 2 | 1 | . 4 4.043051 | 0 | 3 | 4.787492 | 5.869297 | 0 | 1 | 5.093750 | 1 | 0.470004 | 2 | 0 | 2 | 1 | . from sklearn.neighbors import KNeighborsClassifier neig = np.arange(1, 25) train_accuracy = [] test_accuracy = [] # Loop over different values of k for i, k in enumerate(neig): # k from 1 to 25(exclude) knn = KNeighborsClassifier(n_neighbors=k) # Fit with knn knn.fit(X_train,y_train) #train accuracy train_accuracy.append(knn.score(X_train, y_train)) # test accuracy test_accuracy.append(knn.score(X_test, y_test)) # Plot plt.figure(figsize=[13,8]) plt.plot(neig, test_accuracy, label = &#39;Testing Accuracy&#39;) plt.plot(neig, train_accuracy, label = &#39;Training Accuracy&#39;) plt.legend() plt.title(&#39;-value VS Accuracy&#39;) plt.xlabel(&#39;Number of Neighbors&#39;) plt.ylabel(&#39;Accuracy&#39;) plt.xticks(neig) plt.show() print(&quot;Best accuracy is {} with K = {}&quot;.format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy)))) . Best accuracy is 0.8688524590163934 with K = 4 . Logistic Regession . from sklearn.linear_model import LogisticRegression model=LogisticRegression(random_state=60) . model.fit(X_train,y_train) y_test_pred=model.predict(X_test) . from sklearn.metrics import accuracy_score accuracy_score=accuracy_score(y_test,y_test_pred) accuracy_score . 0.8688524590163934 . sns.heatmap(metrics.confusion_matrix(y_test,y_test_pred),annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcf1573e910&gt; . get_clf_eval(y_test, y_test_pred) . 오차행렬: [[24 7] [ 1 29]] 정확도: 0.8689 정밀도: 0.8056 재현율: 0.9667 F1: 0.8788 AUC: 0.8704 . Decision Tree . from sklearn.tree import DecisionTreeClassifier model_dt=DecisionTreeClassifier(random_state=60) . model_dt.fit(X_train,y_train) dt = model_dt.predict(X_test) metrics.accuracy_score(y_test,dt) . 0.7377049180327869 . sns.heatmap(metrics.confusion_matrix(y_test,dt),annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcf2190cc90&gt; . get_clf_eval(y_test, dt) . 오차행렬: [[21 10] [ 6 24]] 정확도: 0.7377 정밀도: 0.7059 재현율: 0.8000 F1: 0.7500 AUC: 0.7387 . Random Forest . from sklearn.ensemble import RandomForestClassifier model_rf=RandomForestClassifier(random_state=60,n_estimators = 200 ,max_depth = 14, min_samples_leaf = 4) . model_rf.fit(X_train,y_train) rf=model_rf.predict(X_test) metrics.accuracy_score(y_test,rf) . 0.8852459016393442 . sns.heatmap(metrics.confusion_matrix(y_test,rf),annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcf17a03410&gt; . get_clf_eval(y_test, rf) . 오차행렬: [[25 6] [ 1 29]] 정확도: 0.8852 정밀도: 0.8286 재현율: 0.9667 F1: 0.8923 AUC: 0.8866 . from sklearn.model_selection import cross_val_score k = 7 cv_result = cross_val_score(model_rf,X,y,cv=k) print(&#39;CV Scores: &#39;,cv_result) print(&#39;CV scores average: &#39;,np.sum(cv_result)/k) . CV Scores: [0.84090909 0.81818182 0.93023256 0.86046512 0.8372093 0.81395349 0.81395349] CV scores average: 0.8449864089398972 . LGBM Classifier . 원래 10000개 이상의 데이터에서 잘 작동한다고 알려져서 정확히 안됐을수도 있음. | . from lightgbm import LGBMClassifier lgbm = LGBMClassifier(n_estimators=400) lgbm.fit(X_train, y_train, eval_metric=&#39;accuracy&#39;, verbose=True) lgbm_pred=lgbm.predict(X_test) metrics.accuracy_score(y_test,lgbm_pred) . 0.8360655737704918 . get_clf_eval(y_test, lgbm_pred) . 오차행렬: [[25 6] [ 4 26]] 정확도: 0.8361 정밀도: 0.8125 재현율: 0.8667 F1: 0.8387 AUC: 0.8366 . XGBoost Classifier . from xgboost import XGBClassifier xgb_wrapper = XGBClassifier(n_estimators = 300, learning_rate = 0.05, reg_lambda=1.5) xgb_wrapper.fit(X_train, y_train) xgb_preds = xgb_wrapper.predict(X_test) metrics.accuracy_score(y_test,xgb_preds) . 0.8032786885245902 . get_clf_eval(y_test, xgb_preds) . 오차행렬: [[26 5] [ 7 23]] 정확도: 0.8033 정밀도: 0.8214 재현율: 0.7667 F1: 0.7931 AUC: 0.8027 . Cross-Validaion . Logistic Regression | LGBM Classifier | Random Forest | . from sklearn.model_selection import cross_val_score reg = LogisticRegression() k = 7 cv_result = cross_val_score(reg ,X,y,cv=k) # uses R^2 as score print(&#39;CV Scores: &#39;,cv_result) print(&#39;CV scores average: &#39;,np.sum(cv_result)/k) . CV Scores: [0.79545455 0.79545455 0.90697674 0.8372093 0.8372093 0.76744186 0.76744186] CV scores average: 0.8153125943823617 . from sklearn.model_selection import cross_val_score k = 7 cv_result = cross_val_score(lgbm,X,y,cv=k) print(&#39;CV Scores: &#39;,cv_result) print(&#39;CV scores average: &#39;,np.sum(cv_result)/k) . CV Scores: [0.84090909 0.81818182 0.81395349 0.88372093 0.74418605 0.81395349 0.81395349] CV scores average: 0.8184083358501962 . from sklearn.model_selection import cross_val_score k = 7 cv_result = cross_val_score(model_rf,X,y,cv=k) print(&#39;CV Scores: &#39;,cv_result) print(&#39;CV scores average: &#39;,np.sum(cv_result)/k) . CV Scores: [0.84090909 0.81818182 0.93023256 0.86046512 0.8372093 0.81395349 0.81395349] CV scores average: 0.8449864089398972 . KNN Grid Search . from sklearn.model_selection import GridSearchCV grid = {&#39;n_neighbors&#39;: np.arange(1,50)} knn = KNeighborsClassifier() knn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV knn_cv.fit(X,y)# Fit # Print hyperparameter print(&quot;Tuned hyperparameter k: {}&quot;.format(knn_cv.best_params_)) print(&quot;Best score: {}&quot;.format(knn_cv.best_score_)) . Tuned hyperparameter k: {&#39;n_neighbors&#39;: 14} Best score: 0.8382838283828383 . Stacking Classifier . Random Forest, Logistic Regression, KNeighbors Classifier, light GBM | . from mlxtend.classifier import StackingCVClassifier sclf = StackingCVClassifier(classifiers = [model_rf, model,KNeighborsClassifier(n_neighbors = 4), lgbm], shuffle = False, use_probas = True, cv = 5, meta_classifier = LogisticRegression(C = 5)) . sclf.fit(X_train.to_numpy(), y_train.to_numpy()) . StackingCVClassifier(classifiers=[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None, criterion=&#39;gini&#39;, max_depth=14, max_features=&#39;auto&#39;, max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=4, min_samples_split=2, min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=None, oob_score=Fa... meta_classifier=LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class=&#39;auto&#39;, n_jobs=None, penalty=&#39;l2&#39;, random_state=None, solver=&#39;lbfgs&#39;, tol=0.0001, verbose=0, warm_start=False), shuffle=False, store_train_meta_features=False, stratify=True, use_clones=True, use_features_in_secondary=False, use_probas=True, verbose=0) . sclf_pred = sclf.predict(X_test) metrics.accuracy_score(y_test, sclf_pred) . 0.9180327868852459 . from sklearn.model_selection import cross_val_score k = 7 cv_result = cross_val_score(sclf,X.to_numpy(),y.to_numpy(),cv=k) print(&#39;CV Scores: &#39;,cv_result) print(&#39;CV scores average: &#39;,np.sum(cv_result)/k) . CV Scores: [0.86363636 0.81818182 0.93023256 0.8372093 0.8372093 0.81395349 0.81395349] CV scores average: 0.8449109030504378 . .",
            "url": "https://raukrauk.github.io/ML-DL/ssuda/kaggle/machinlearing/2021/09/24/Heart_Attack_%ED%95%84%EC%82%AC.html",
            "relUrl": "/ssuda/kaggle/machinlearing/2021/09/24/Heart_Attack_%ED%95%84%EC%82%AC.html",
            "date": " • Sep 24, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "[Kaggle] House Price Prediction",
            "content": "Google Drive 마운트 . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import numpy as np import pandas as pd import seaborn as sns import scipy.stats as stats from scipy.stats import norm import statsmodels.api as sm import matplotlib.pyplot as plt from scipy.stats import skew, norm %matplotlib inline . /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . Data fields . SalePrice - the property&#39;s sale price in dollars. This is the target variable that you&#39;re trying to predict. . MSSubClass: The building class . MSZoning: The general zoning classification . LotFrontage: Linear feet of street connected to property . LotArea: Lot size in square feet . Street: Type of road access . Alley: Type of alley access . LotShape: General shape of property . LandContour: Flatness of the property . Utilities: Type of utilities available . LotConfig: Lot configuration . LandSlope: Slope of property . Neighborhood: Physical locations within Ames city limits . Condition1: Proximity to main road or railroad . Condition2: Proximity to main road or railroad (if a second is present) . BldgType: Type of dwelling . HouseStyle: Style of dwelling . OverallQual: Overall material and finish quality . OverallCond: Overall condition rating . YearBuilt: Original construction date . YearRemodAdd: Remodel date . RoofStyle: Type of roof . RoofMatl: Roof material . Exterior1st: Exterior covering on house . Exterior2nd: Exterior covering on house (if more than one material) . MasVnrType: Masonry veneer type . MasVnrArea: Masonry veneer area in square feet . ExterQual: Exterior material quality . ExterCond: Present condition of the material on the exterior . Foundation: Type of foundation . BsmtQual: Height of the basement . BsmtCond: General condition of the basement . BsmtExposure: Walkout or garden level basement walls . BsmtFinType1: Quality of basement finished area . BsmtFinSF1: Type 1 finished square feet . BsmtFinType2: Quality of second finished area (if present) . BsmtFinSF2: Type 2 finished square feet . BsmtUnfSF: Unfinished square feet of basement area . TotalBsmtSF: Total square feet of basement area . Heating: Type of heating . HeatingQC: Heating quality and condition . CentralAir: Central air conditioning . Electrical: Electrical system . 1stFlrSF: First Floor square feet . 2ndFlrSF: Second floor square feet . LowQualFinSF: Low quality finished square feet (all floors) . GrLivArea: Above grade (ground) living area square feet . BsmtFullBath: Basement full bathrooms . BsmtHalfBath: Basement half bathrooms . FullBath: Full bathrooms above grade . HalfBath: Half baths above grade . Bedroom: Number of bedrooms above basement level . Kitchen: Number of kitchens . KitchenQual: Kitchen quality . TotRmsAbvGrd: Total rooms above grade (does not include bathrooms) . Functional: Home functionality rating . Fireplaces: Number of fireplaces . FireplaceQu: Fireplace quality . GarageType: Garage location . GarageYrBlt: Year garage was built . GarageFinish: Interior finish of the garage . GarageCars: Size of garage in car capacity . GarageArea: Size of garage in square feet . GarageQual: Garage quality . GarageCond: Garage condition . PavedDrive: Paved driveway . WoodDeckSF: Wood deck area in square feet . OpenPorchSF: Open porch area in square feet . EnclosedPorch: Enclosed porch area in square feet . 3SsnPorch: Three season porch area in square feet . ScreenPorch: Screen porch area in square feet . PoolArea: Pool area in square feet . PoolQC: Pool quality . Fence: Fence quality . MiscFeature: Miscellaneous feature not covered in other categories . MiscVal: $Value of miscellaneous feature . MoSold: Month Sold . YrSold: Year Sold . SaleType: Type of sale . SaleCondition: Condition of sale . Importing Dataset . house_data = pd.read_csv(&#39;/content/drive/MyDrive/AmesHousing.csv&#39;) test = pd.read_csv(&#39;/content/drive/MyDrive/price_test.csv&#39;) data_w = house_data.copy() data_w.columns = data_w.columns.str.replace(&#39; &#39;, &#39;&#39;) data_w.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 2930 entries, 0 to 2929 Data columns (total 82 columns): # Column Non-Null Count Dtype -- -- 0 Order 2930 non-null int64 1 PID 2930 non-null int64 2 MSSubClass 2930 non-null int64 3 MSZoning 2930 non-null object 4 LotFrontage 2440 non-null float64 5 LotArea 2930 non-null int64 6 Street 2930 non-null object 7 Alley 198 non-null object 8 LotShape 2930 non-null object 9 LandContour 2930 non-null object 10 Utilities 2930 non-null object 11 LotConfig 2930 non-null object 12 LandSlope 2930 non-null object 13 Neighborhood 2930 non-null object 14 Condition1 2930 non-null object 15 Condition2 2930 non-null object 16 BldgType 2930 non-null object 17 HouseStyle 2930 non-null object 18 OverallQual 2930 non-null int64 19 OverallCond 2930 non-null int64 20 YearBuilt 2930 non-null int64 21 YearRemod/Add 2930 non-null int64 22 RoofStyle 2930 non-null object 23 RoofMatl 2930 non-null object 24 Exterior1st 2930 non-null object 25 Exterior2nd 2930 non-null object 26 MasVnrType 2907 non-null object 27 MasVnrArea 2907 non-null float64 28 ExterQual 2930 non-null object 29 ExterCond 2930 non-null object 30 Foundation 2930 non-null object 31 BsmtQual 2850 non-null object 32 BsmtCond 2850 non-null object 33 BsmtExposure 2847 non-null object 34 BsmtFinType1 2850 non-null object 35 BsmtFinSF1 2929 non-null float64 36 BsmtFinType2 2849 non-null object 37 BsmtFinSF2 2929 non-null float64 38 BsmtUnfSF 2929 non-null float64 39 TotalBsmtSF 2929 non-null float64 40 Heating 2930 non-null object 41 HeatingQC 2930 non-null object 42 CentralAir 2930 non-null object 43 Electrical 2929 non-null object 44 1stFlrSF 2930 non-null int64 45 2ndFlrSF 2930 non-null int64 46 LowQualFinSF 2930 non-null int64 47 GrLivArea 2930 non-null int64 48 BsmtFullBath 2928 non-null float64 49 BsmtHalfBath 2928 non-null float64 50 FullBath 2930 non-null int64 51 HalfBath 2930 non-null int64 52 BedroomAbvGr 2930 non-null int64 53 KitchenAbvGr 2930 non-null int64 54 KitchenQual 2930 non-null object 55 TotRmsAbvGrd 2930 non-null int64 56 Functional 2930 non-null object 57 Fireplaces 2930 non-null int64 58 FireplaceQu 1508 non-null object 59 GarageType 2773 non-null object 60 GarageYrBlt 2771 non-null float64 61 GarageFinish 2771 non-null object 62 GarageCars 2929 non-null float64 63 GarageArea 2929 non-null float64 64 GarageQual 2771 non-null object 65 GarageCond 2771 non-null object 66 PavedDrive 2930 non-null object 67 WoodDeckSF 2930 non-null int64 68 OpenPorchSF 2930 non-null int64 69 EnclosedPorch 2930 non-null int64 70 3SsnPorch 2930 non-null int64 71 ScreenPorch 2930 non-null int64 72 PoolArea 2930 non-null int64 73 PoolQC 13 non-null object 74 Fence 572 non-null object 75 MiscFeature 106 non-null object 76 MiscVal 2930 non-null int64 77 MoSold 2930 non-null int64 78 YrSold 2930 non-null int64 79 SaleType 2930 non-null object 80 SaleCondition 2930 non-null object 81 SalePrice 2930 non-null int64 dtypes: float64(11), int64(28), object(43) memory usage: 1.8+ MB . data_w.head() . Order PID MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemod/Add RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF ... CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition SalePrice . 0 1 | 526301100 | 20 | RL | 141.0 | 31770 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 6 | 5 | 1960 | 1960 | Hip | CompShg | BrkFace | Plywood | Stone | 112.0 | TA | TA | CBlock | TA | Gd | Gd | BLQ | 639.0 | Unf | 0.0 | 441.0 | 1080.0 | ... | Y | SBrkr | 1656 | 0 | 0 | 1656 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | TA | 7 | Typ | 2 | Gd | Attchd | 1960.0 | Fin | 2.0 | 528.0 | TA | TA | P | 210 | 62 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2010 | WD | Normal | 215000 | . 1 2 | 526350040 | 20 | RH | 80.0 | 11622 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | NAmes | Feedr | Norm | 1Fam | 1Story | 5 | 6 | 1961 | 1961 | Gable | CompShg | VinylSd | VinylSd | None | 0.0 | TA | TA | CBlock | TA | TA | No | Rec | 468.0 | LwQ | 144.0 | 270.0 | 882.0 | ... | Y | SBrkr | 896 | 0 | 0 | 896 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | TA | 5 | Typ | 0 | NaN | Attchd | 1961.0 | Unf | 1.0 | 730.0 | TA | TA | Y | 140 | 0 | 0 | 0 | 120 | 0 | NaN | MnPrv | NaN | 0 | 6 | 2010 | WD | Normal | 105000 | . 2 3 | 526351010 | 20 | RL | 81.0 | 14267 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 6 | 6 | 1958 | 1958 | Hip | CompShg | Wd Sdng | Wd Sdng | BrkFace | 108.0 | TA | TA | CBlock | TA | TA | No | ALQ | 923.0 | Unf | 0.0 | 406.0 | 1329.0 | ... | Y | SBrkr | 1329 | 0 | 0 | 1329 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | Gd | 6 | Typ | 0 | NaN | Attchd | 1958.0 | Unf | 1.0 | 312.0 | TA | TA | Y | 393 | 36 | 0 | 0 | 0 | 0 | NaN | NaN | Gar2 | 12500 | 6 | 2010 | WD | Normal | 172000 | . 3 4 | 526353030 | 20 | RL | 93.0 | 11160 | Pave | NaN | Reg | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 7 | 5 | 1968 | 1968 | Hip | CompShg | BrkFace | BrkFace | None | 0.0 | Gd | TA | CBlock | TA | TA | No | ALQ | 1065.0 | Unf | 0.0 | 1045.0 | 2110.0 | ... | Y | SBrkr | 2110 | 0 | 0 | 2110 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | Ex | 8 | Typ | 2 | TA | Attchd | 1968.0 | Fin | 2.0 | 522.0 | TA | TA | Y | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 4 | 2010 | WD | Normal | 244000 | . 4 5 | 527105010 | 60 | RL | 74.0 | 13830 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | Gilbert | Norm | Norm | 1Fam | 2Story | 5 | 5 | 1997 | 1998 | Gable | CompShg | VinylSd | VinylSd | None | 0.0 | TA | TA | PConc | Gd | TA | No | GLQ | 791.0 | Unf | 0.0 | 137.0 | 928.0 | ... | Y | SBrkr | 928 | 701 | 0 | 1629 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1997.0 | Fin | 2.0 | 482.0 | TA | TA | Y | 212 | 34 | 0 | 0 | 0 | 0 | NaN | MnPrv | NaN | 0 | 3 | 2010 | WD | Normal | 189900 | . 5 rows × 82 columns . &#49688;&#52824;&#54805; &#48320;&#49688;&#50752; &#47749;&#47785;&#54805; &#48320;&#49688;&#46308;&#51012; &#44396;&#48516;&#54644; &#48376;&#45796;. . numerical_feats = data_w.dtypes[data_w.dtypes != &quot;object&quot;].index print(&quot;Number of Numerical features: &quot;, len(numerical_feats)) categorical_feats = data_w.dtypes[data_w.dtypes == &quot;object&quot;].index print(&quot;Number of Categorical features: &quot;, len(categorical_feats)) . Number of Numerical features: 39 Number of Categorical features: 43 . print(data_w[numerical_feats].columns) # 수치형 변수 print(&quot;*&quot;*80) print(data_w[categorical_feats].columns) # 명목형 변수 . Index([&#39;Order&#39;, &#39;PID&#39;, &#39;MS SubClass&#39;, &#39;Lot Frontage&#39;, &#39;Lot Area&#39;, &#39;Overall Qual&#39;, &#39;Overall Cond&#39;, &#39;Year Built&#39;, &#39;Year Remod/Add&#39;, &#39;Mas Vnr Area&#39;, &#39;BsmtFin SF 1&#39;, &#39;BsmtFin SF 2&#39;, &#39;Bsmt Unf SF&#39;, &#39;Total Bsmt SF&#39;, &#39;1st Flr SF&#39;, &#39;2nd Flr SF&#39;, &#39;Low Qual Fin SF&#39;, &#39;Gr Liv Area&#39;, &#39;Bsmt Full Bath&#39;, &#39;Bsmt Half Bath&#39;, &#39;Full Bath&#39;, &#39;Half Bath&#39;, &#39;Bedroom AbvGr&#39;, &#39;Kitchen AbvGr&#39;, &#39;TotRms AbvGrd&#39;, &#39;Fireplaces&#39;, &#39;Garage Yr Blt&#39;, &#39;Garage Cars&#39;, &#39;Garage Area&#39;, &#39;Wood Deck SF&#39;, &#39;Open Porch SF&#39;, &#39;Enclosed Porch&#39;, &#39;3Ssn Porch&#39;, &#39;Screen Porch&#39;, &#39;Pool Area&#39;, &#39;Misc Val&#39;, &#39;Mo Sold&#39;, &#39;Yr Sold&#39;, &#39;SalePrice&#39;], dtype=&#39;object&#39;) ******************************************************************************** Index([&#39;MS Zoning&#39;, &#39;Street&#39;, &#39;Alley&#39;, &#39;Lot Shape&#39;, &#39;Land Contour&#39;, &#39;Utilities&#39;, &#39;Lot Config&#39;, &#39;Land Slope&#39;, &#39;Neighborhood&#39;, &#39;Condition 1&#39;, &#39;Condition 2&#39;, &#39;Bldg Type&#39;, &#39;House Style&#39;, &#39;Roof Style&#39;, &#39;Roof Matl&#39;, &#39;Exterior 1st&#39;, &#39;Exterior 2nd&#39;, &#39;Mas Vnr Type&#39;, &#39;Exter Qual&#39;, &#39;Exter Cond&#39;, &#39;Foundation&#39;, &#39;Bsmt Qual&#39;, &#39;Bsmt Cond&#39;, &#39;Bsmt Exposure&#39;, &#39;BsmtFin Type 1&#39;, &#39;BsmtFin Type 2&#39;, &#39;Heating&#39;, &#39;Heating QC&#39;, &#39;Central Air&#39;, &#39;Electrical&#39;, &#39;Kitchen Qual&#39;, &#39;Functional&#39;, &#39;Fireplace Qu&#39;, &#39;Garage Type&#39;, &#39;Garage Finish&#39;, &#39;Garage Qual&#39;, &#39;Garage Cond&#39;, &#39;Paved Drive&#39;, &#39;Pool QC&#39;, &#39;Fence&#39;, &#39;Misc Feature&#39;, &#39;Sale Type&#39;, &#39;Sale Condition&#39;], dtype=&#39;object&#39;) . &#53456;&#49353;&#51201; &#45936;&#51060;&#53552; &#48516;&#49437; (EDA) . Target의 분포확인 | norm.fit은 scipy.stats에 존재하며 generic data에 맞는 파라미터들을 예측하는 함수 | sns.distplot에서 kde = True로 주었는데 이는 가우시안 RBF커널(방사기저함수)를 사용한다는 의미이다. | . (mu, sigma) = norm.fit(data_w[&#39;SalePrice&#39;]) plt.figure(figsize = (12,6)) sns.distplot(data_w[&#39;SalePrice&#39;], kde = True, hist=True, fit = norm) #kde: Whether to plot a gaussian kernel density estimate. plt.title(&#39;SalePrice distribution vs Normal Distribution&#39;, fontsize = 13) plt.xlabel(&quot;House&#39;s sale Price in $&quot;, fontsize = 12) plt.legend([&#39;Normal dist. ($ mu=$ {:.2f} and $ sigma=$ {:.2f} )&#39;.format(mu, sigma)], loc=&#39;best&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . Kernel Density Estimation (&#52964;&#45328; &#48128;&#46020; &#52628;&#51221;) - &#48708;&#47784;&#49688;&#51201; &#52628;&#51221;&#48169;&#48277; . 우리는 밀도추정의 가장 단순한 형태는 히스토그램(histogram) 방법이다. 히스토그램 방법은 bin의 경계에서 불연속성이 나타난다는 점, bin의 크기 및 시작 위치에 따라서 히스토그램이 달라진다는 점, 고차원(high dimension) 데이터에는 메모리 문제 등으로 사용하기 힘들다는 점 등의 문제점을 갖는다. . | 비모수적 밀도추정 방법 중 하나로서 커널함수(kernel function)를 이용하여 히스토그램 방법의 문제점을 개선한 방법이다. . | 수학적으로 커널함수는 원점을 중심으로 대칭이면서 적분값이 1인 non-negative 함수로 정의된다. . | . . . . x&#47484; &#48320;&#49688;(random variable), x1, x2, ..., xn&#51012; &#44288;&#52769;&#46108; &#49368;&#54540; &#45936;&#51060;&#53552;, K&#47484; &#52964;&#45328; &#54632;&#49688;&#46972; &#54616;&#51088;. &#51060; &#46412; KDE&#50640;&#49436;&#45716; &#47004;&#45924; &#48320;&#49688; x&#50640; &#45824;&#54620; pdf(&#54869;&#47456;&#48128;&#46020;&#54632;&#49688;)&#47484; &#45796;&#51020;&#44284; &#44057;&#51060; &#52628;&#51221;&#54620;&#45796;. . h는 커널이 뽀족한 형태(h가 작은 값)인지 완만한 형태(h가 큰 값)인지를 조절하는 파라미터이다. | . . 관측된 데이터 각각마다 해당 데이터 값을 중심으로 하는 커널 함수를 생성한다: K(x-xi) . | 이렇게 만들어진 커널 함수들을 모두 더한 후 전체 데이터 개수로 나눈다. . | . &lt;그림 4&gt; &quot;Comparison of 1D histogram and KDE&quot; by Drleft . &#50557;&#44036;&#51008;? &#48279;&#50612;&#45212; &#51060;&#50556;&#44592; . 가우시안 RBF 커널함수는 비선형 SVM에서도 이용할 수 있는데 파라미터를 조정하여 제재의 강약을 조정한다. | PDF참고 | . . SalePrice&#51032; &#50780;&#46020;&#50752; &#52392;&#46020;&#47484; &#54869;&#51064;&#54620;&#45796;. . 왜도가 -2 ~ 2 사이에 존재하면 왜도가 크지 않다고 판단한다. | 첨도는 정규분포에서 0이며 0보다크면 뾰족한 분포이며 0보다 작으면 정규분포보다 완만한 분포이다. | 치우친 분포라면 Min-Max Scaler, Standard Scaler, Log 변환을 이용 할 수 있는데 경험상 주로 Log변환이 효과적이라고 판단된다. (이정진피셜임) | . from scipy import stats shap_t,shap_p = stats.shapiro(data_w[&#39;SalePrice&#39;]) print(&quot;Skewness: %f&quot; % abs(data_w[&#39;SalePrice&#39;]).skew()) print(&quot;Kurtosis: %f&quot; % abs(data_w[&#39;SalePrice&#39;]).kurt()) print(&quot;Shapiro_Test: %f&quot; % shap_t) print(&quot;Shapiro_Test: %f&quot; % shap_p) . Skewness: 1.743500 Kurtosis: 5.118900 Shapiro_Test: 0.876261 Shapiro_Test: 0.000000 . SalePrice&#45716; &#50780;&#46020;&#45716; &#53356;&#51648;&#50506;&#51004;&#47728; &#51221;&#44508;&#48516;&#54252;&#48372;&#45796;&#45716; &#49072;&#51313;&#54620; &#54805;&#53468;&#47484; &#51648;&#45768;&#44256; &#51080;&#45796;.&#46608;&#54620; &#44536;&#47532;&#44256; &#50864;&#52769;&#51004;&#47196; &#52824;&#50864;&#52828; &#47784;&#50577;&#51012; &#44032;&#51648;&#44256; &#51080;&#45796;. . Correlation &#49345;&#44288;&#44228;&#49688; . Numeric Data에 대해서만 이루어 졌다. (문자형 데이터는 이루어지지 않음) | 상관계수는 feature들 간에 다중공선성을 확인하거나 타겟변수와의 관계를 수치적으로 확인하여 어떤 변수가 큰 영향을 줄지 예상해 볼 수 있다. | Class, Type, Condition등을 1,2,3과 같이 명목형 숫자로 분류해 놓은 경우에도 상관계수 plot에 생성 될 수 있다. 명목형은 숫자에 아무 의미가 없으므로 사용시 주의할 필요가 있다! | . f, ax = plt.subplots(figsize=(30, 25)) mat = data_w.corr(&#39;pearson&#39;) mask = np.triu(np.ones_like(mat, dtype=bool)) # triu: 삼각행렬, ones_like: matrix의 원소들을 모두 1로 바꿈 / 여기서는 True로 나타남. cmap = sns.diverging_palette(230, 20, as_cmap=True) #색관련 값 sns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0, annot = True, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) #cbar colorbar title 관련 plt.show() . &#49345;&#44288;&#44228;&#49688;&#44032; 0.8&#51060;&#49345;&#51060;&#47732; &#45458;&#51008; &#49345;&#44288;&#44288;&#44228;&#44032; &#51080;&#51004;&#47728; feature&#46308; &#44036;&#50640; &#45796;&#51473;&#44277;&#49440;&#49457;&#51060; &#48156;&#49373;&#54624; &#49688; &#51080;&#45796;. (&#45576;&#51004;&#47196; &#54869;&#51064;&#54644; &#48376;&#45796;) - [&#51228;&#44144; &#45824;&#49345;&#51060; &#46112; &#49688; &#51080;&#51020;] . 1stFlrSF와 TotalBsmtSF: 0.82 -&gt; 1층의 평방미터와 지하실 전체 평방미터(지하실을 만들 때 1층크기와 비슷하게 만드는 것 같음.) | GrLivArea와 TotRmsAbvGrd: 0.83 -&gt; 생활공간의 크기와 (욕실을 제외한 지상층의 방의 전체개수) | GarageYrBlt와 YearBuilt: 0.83 -&gt; 차고는 대부분 집을 지을때 같이 지어서 높은관계가 발생하는 것 같음. | GarageArea와 GarageCars: 0.88 -&gt; 당연히 가지고 있는 차의 대수가 많으면 차고 땅의 크기도 커짐을 생각해 볼 수 있음. | . Target&#44284; &#45458;&#51008; &#49345;&#44288;&#44288;&#44228; . OverallQual(재료와 마감의 퀄리티) - [1~10]으로 rating되어 있음. | GrLivArea(지상층 생활공간 크기) | . Pearson_GrLiv = 0.71 plt.figure(figsize = (12,6)) sns.regplot(data=data_w, x = &#39;OverallQual&#39;, y=&#39;SalePrice&#39;, scatter_kws={&#39;alpha&#39;:0.2}) plt.xlim(0.5, 10.5) plt.title(&#39;OverallQual vs SalePrice&#39;, fontsize = 12) plt.legend([&#39;$Pearson=$ {:.2f}&#39;.format(Pearson_GrLiv)], loc = &#39;best&#39;) plt.show() . Pearson_GrLiv = 0.71 plt.figure(figsize = (12,6)) sns.regplot(data=data_w, x = &#39;GrLivArea&#39;, y=&#39;SalePrice&#39;, scatter_kws={&#39;alpha&#39;:0.2}) plt.title(&#39;GrLivArea vs SalePrice&#39;, fontsize = 12) plt.legend([&#39;$Pearson=$ {:.2f}&#39;.format(Pearson_GrLiv)], loc = &#39;best&#39;) plt.show() . Pearson_TBSF = 0.63 plt.figure(figsize = (12,6)) sns.regplot(data=data_w, x = &#39;TotalBsmtSF&#39;, y=&#39;SalePrice&#39;, scatter_kws={&#39;alpha&#39;:0.2}) plt.title(&#39;TotalBsmtSF vs SalePrice&#39;, fontsize = 12) plt.legend([&#39;$Pearson=$ {:.2f}&#39;.format(Pearson_TBSF)], loc = &#39;best&#39;) plt.show() . Pearson_YrBlt = 0.56 plt.figure(figsize = (12,6)) sns.regplot(data=data_w, x = &#39;YearBuilt&#39;, y=&#39;SalePrice&#39;, scatter_kws={&#39;alpha&#39;:0.2}) plt.title(&#39;YearBuilt vs SalePrice&#39;, fontsize = 12) plt.legend([&#39;$Pearson=$ {:.2f}&#39;.format(Pearson_YrBlt)], loc = &#39;best&#39;) plt.show() . DataProcessing . Looking at potential NaN (결측값 찾기) | Dealing with categorical features (e.g. Dummy coding) (위에서는 연속형 변수들만 다뤘지만 category형 열이 훨씬 더 많음!) | Normalization (정규화) | . target = data_w[&#39;SalePrice&#39;] test_id = test[&#39;Id&#39;] test = test.drop([&#39;Id&#39;],axis = 1) data_w2 = data_w.drop([&#39;SalePrice&#39;,&#39;Order&#39;,&#39;PID&#39;], axis = 1) . train_test = pd.concat([data_w2,test], axis=0, sort=False) # train과 test를 합침 . train_test . MSSubClass MSZoning LotFrontage LotArea Street Alley LotShape LandContour Utilities LotConfig LandSlope Neighborhood Condition1 Condition2 BldgType HouseStyle OverallQual OverallCond YearBuilt YearRemod/Add RoofStyle RoofMatl Exterior1st Exterior2nd MasVnrType MasVnrArea ExterQual ExterCond Foundation BsmtQual BsmtCond BsmtExposure BsmtFinType1 BsmtFinSF1 BsmtFinType2 BsmtFinSF2 BsmtUnfSF TotalBsmtSF Heating HeatingQC CentralAir Electrical 1stFlrSF 2ndFlrSF LowQualFinSF GrLivArea BsmtFullBath BsmtHalfBath FullBath HalfBath BedroomAbvGr KitchenAbvGr KitchenQual TotRmsAbvGrd Functional Fireplaces FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars GarageArea GarageQual GarageCond PavedDrive WoodDeckSF OpenPorchSF EnclosedPorch 3SsnPorch ScreenPorch PoolArea PoolQC Fence MiscFeature MiscVal MoSold YrSold SaleType SaleCondition YearRemodAdd . 0 20 | RL | 141.0 | 31770 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 6 | 5 | 1960 | 1960.0 | Hip | CompShg | BrkFace | Plywood | Stone | 112.0 | TA | TA | CBlock | TA | Gd | Gd | BLQ | 639.0 | Unf | 0.0 | 441.0 | 1080.0 | GasA | Fa | Y | SBrkr | 1656 | 0 | 0 | 1656 | 1.0 | 0.0 | 1 | 0 | 3 | 1 | TA | 7 | Typ | 2 | Gd | Attchd | 1960.0 | Fin | 2.0 | 528.0 | TA | TA | P | 210 | 62 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 5 | 2010 | WD | Normal | NaN | . 1 20 | RH | 80.0 | 11622 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | NAmes | Feedr | Norm | 1Fam | 1Story | 5 | 6 | 1961 | 1961.0 | Gable | CompShg | VinylSd | VinylSd | None | 0.0 | TA | TA | CBlock | TA | TA | No | Rec | 468.0 | LwQ | 144.0 | 270.0 | 882.0 | GasA | TA | Y | SBrkr | 896 | 0 | 0 | 896 | 0.0 | 0.0 | 1 | 0 | 2 | 1 | TA | 5 | Typ | 0 | NaN | Attchd | 1961.0 | Unf | 1.0 | 730.0 | TA | TA | Y | 140 | 0 | 0 | 0 | 120 | 0 | NaN | MnPrv | NaN | 0 | 6 | 2010 | WD | Normal | NaN | . 2 20 | RL | 81.0 | 14267 | Pave | NaN | IR1 | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 6 | 6 | 1958 | 1958.0 | Hip | CompShg | Wd Sdng | Wd Sdng | BrkFace | 108.0 | TA | TA | CBlock | TA | TA | No | ALQ | 923.0 | Unf | 0.0 | 406.0 | 1329.0 | GasA | TA | Y | SBrkr | 1329 | 0 | 0 | 1329 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | Gd | 6 | Typ | 0 | NaN | Attchd | 1958.0 | Unf | 1.0 | 312.0 | TA | TA | Y | 393 | 36 | 0 | 0 | 0 | 0 | NaN | NaN | Gar2 | 12500 | 6 | 2010 | WD | Normal | NaN | . 3 20 | RL | 93.0 | 11160 | Pave | NaN | Reg | Lvl | AllPub | Corner | Gtl | NAmes | Norm | Norm | 1Fam | 1Story | 7 | 5 | 1968 | 1968.0 | Hip | CompShg | BrkFace | BrkFace | None | 0.0 | Gd | TA | CBlock | TA | TA | No | ALQ | 1065.0 | Unf | 0.0 | 1045.0 | 2110.0 | GasA | Ex | Y | SBrkr | 2110 | 0 | 0 | 2110 | 1.0 | 0.0 | 2 | 1 | 3 | 1 | Ex | 8 | Typ | 2 | TA | Attchd | 1968.0 | Fin | 2.0 | 522.0 | TA | TA | Y | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 4 | 2010 | WD | Normal | NaN | . 4 60 | RL | 74.0 | 13830 | Pave | NaN | IR1 | Lvl | AllPub | Inside | Gtl | Gilbert | Norm | Norm | 1Fam | 2Story | 5 | 5 | 1997 | 1998.0 | Gable | CompShg | VinylSd | VinylSd | None | 0.0 | TA | TA | PConc | Gd | TA | No | GLQ | 791.0 | Unf | 0.0 | 137.0 | 928.0 | GasA | Gd | Y | SBrkr | 928 | 701 | 0 | 1629 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | TA | 6 | Typ | 1 | TA | Attchd | 1997.0 | Fin | 2.0 | 482.0 | TA | TA | Y | 212 | 34 | 0 | 0 | 0 | 0 | NaN | MnPrv | NaN | 0 | 3 | 2010 | WD | Normal | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1454 160 | RM | 21.0 | 1936 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | MeadowV | Norm | Norm | Twnhs | 2Story | 4 | 7 | 1970 | NaN | Gable | CompShg | CemntBd | CmentBd | None | 0.0 | TA | TA | CBlock | TA | TA | No | Unf | 0.0 | Unf | 0.0 | 546.0 | 546.0 | GasA | Gd | Y | SBrkr | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | TA | 5 | Typ | 0 | NaN | NaN | NaN | NaN | 0.0 | 0.0 | NaN | NaN | Y | 0 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 6 | 2006 | WD | Normal | 1970.0 | . 1455 160 | RM | 21.0 | 1894 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | MeadowV | Norm | Norm | TwnhsE | 2Story | 4 | 5 | 1970 | NaN | Gable | CompShg | CemntBd | CmentBd | None | 0.0 | TA | TA | CBlock | TA | TA | No | Rec | 252.0 | Unf | 0.0 | 294.0 | 546.0 | GasA | TA | Y | SBrkr | 546 | 546 | 0 | 1092 | 0.0 | 0.0 | 1 | 1 | 3 | 1 | TA | 6 | Typ | 0 | NaN | CarPort | 1970.0 | Unf | 1.0 | 286.0 | TA | TA | Y | 0 | 24 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 4 | 2006 | WD | Abnorml | 1970.0 | . 1456 20 | RL | 160.0 | 20000 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | Mitchel | Norm | Norm | 1Fam | 1Story | 5 | 7 | 1960 | NaN | Gable | CompShg | VinylSd | VinylSd | None | 0.0 | TA | TA | CBlock | TA | TA | No | ALQ | 1224.0 | Unf | 0.0 | 0.0 | 1224.0 | GasA | Ex | Y | SBrkr | 1224 | 0 | 0 | 1224 | 1.0 | 0.0 | 1 | 0 | 4 | 1 | TA | 7 | Typ | 1 | TA | Detchd | 1960.0 | Unf | 2.0 | 576.0 | TA | TA | Y | 474 | 0 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 9 | 2006 | WD | Abnorml | 1996.0 | . 1457 85 | RL | 62.0 | 10441 | Pave | NaN | Reg | Lvl | AllPub | Inside | Gtl | Mitchel | Norm | Norm | 1Fam | SFoyer | 5 | 5 | 1992 | NaN | Gable | CompShg | HdBoard | Wd Shng | None | 0.0 | TA | TA | PConc | Gd | TA | Av | GLQ | 337.0 | Unf | 0.0 | 575.0 | 912.0 | GasA | TA | Y | SBrkr | 970 | 0 | 0 | 970 | 0.0 | 1.0 | 1 | 0 | 3 | 1 | TA | 6 | Typ | 0 | NaN | NaN | NaN | NaN | 0.0 | 0.0 | NaN | NaN | Y | 80 | 32 | 0 | 0 | 0 | 0 | NaN | MnPrv | Shed | 700 | 7 | 2006 | WD | Normal | 1992.0 | . 1458 60 | RL | 74.0 | 9627 | Pave | NaN | Reg | Lvl | AllPub | Inside | Mod | Mitchel | Norm | Norm | 1Fam | 2Story | 7 | 5 | 1993 | NaN | Gable | CompShg | HdBoard | HdBoard | BrkFace | 94.0 | TA | TA | PConc | Gd | TA | Av | LwQ | 758.0 | Unf | 0.0 | 238.0 | 996.0 | GasA | Ex | Y | SBrkr | 996 | 1004 | 0 | 2000 | 0.0 | 0.0 | 2 | 1 | 3 | 1 | TA | 9 | Typ | 1 | TA | Attchd | 1993.0 | Fin | 3.0 | 650.0 | TA | TA | Y | 190 | 48 | 0 | 0 | 0 | 0 | NaN | NaN | NaN | 0 | 11 | 2006 | WD | Normal | 1994.0 | . 4389 rows × 80 columns . Object&#54805; &#45936;&#51060;&#53552;&#44032; &#44032;&#51648;&#44256; &#51080;&#45716; &#50676;&#48324; &#52852;&#53580;&#44256;&#47532;&#51032; &#49688;&#47484; &#54869;&#51064;&#54620;&#45796;. . obj_col = [x for x in train_test.columns if train_test[x].dtype == &quot;O&quot;] i = 1 for k in obj_col: print(str(i) + &quot;.&quot;,k,&quot;:&quot;,train_test[k].unique()) i+=1 . 1. MSZoning : [&#39;RL&#39; &#39;RH&#39; &#39;FV&#39; &#39;RM&#39; &#39;C (all)&#39; &#39;I (all)&#39; &#39;A (agr)&#39; nan] 2. Street : [&#39;Pave&#39; &#39;Grvl&#39;] 3. Alley : [nan &#39;Pave&#39; &#39;Grvl&#39;] 4. LotShape : [&#39;IR1&#39; &#39;Reg&#39; &#39;IR2&#39; &#39;IR3&#39;] 5. LandContour : [&#39;Lvl&#39; &#39;HLS&#39; &#39;Bnk&#39; &#39;Low&#39;] 6. Utilities : [&#39;AllPub&#39; &#39;NoSewr&#39; &#39;NoSeWa&#39; nan] 7. LotConfig : [&#39;Corner&#39; &#39;Inside&#39; &#39;CulDSac&#39; &#39;FR2&#39; &#39;FR3&#39;] 8. LandSlope : [&#39;Gtl&#39; &#39;Mod&#39; &#39;Sev&#39;] 9. Neighborhood : [&#39;NAmes&#39; &#39;Gilbert&#39; &#39;StoneBr&#39; &#39;NWAmes&#39; &#39;Somerst&#39; &#39;BrDale&#39; &#39;NPkVill&#39; &#39;NridgHt&#39; &#39;Blmngtn&#39; &#39;NoRidge&#39; &#39;SawyerW&#39; &#39;Sawyer&#39; &#39;Greens&#39; &#39;BrkSide&#39; &#39;OldTown&#39; &#39;IDOTRR&#39; &#39;ClearCr&#39; &#39;SWISU&#39; &#39;Edwards&#39; &#39;CollgCr&#39; &#39;Crawfor&#39; &#39;Blueste&#39; &#39;Mitchel&#39; &#39;Timber&#39; &#39;MeadowV&#39; &#39;Veenker&#39; &#39;GrnHill&#39; &#39;Landmrk&#39;] 10. Condition1 : [&#39;Norm&#39; &#39;Feedr&#39; &#39;PosN&#39; &#39;RRNe&#39; &#39;RRAe&#39; &#39;Artery&#39; &#39;PosA&#39; &#39;RRAn&#39; &#39;RRNn&#39;] 11. Condition2 : [&#39;Norm&#39; &#39;Feedr&#39; &#39;PosA&#39; &#39;PosN&#39; &#39;Artery&#39; &#39;RRNn&#39; &#39;RRAe&#39; &#39;RRAn&#39;] 12. BldgType : [&#39;1Fam&#39; &#39;TwnhsE&#39; &#39;Twnhs&#39; &#39;Duplex&#39; &#39;2fmCon&#39;] 13. HouseStyle : [&#39;1Story&#39; &#39;2Story&#39; &#39;1.5Fin&#39; &#39;SFoyer&#39; &#39;SLvl&#39; &#39;2.5Unf&#39; &#39;1.5Unf&#39; &#39;2.5Fin&#39;] 14. RoofStyle : [&#39;Hip&#39; &#39;Gable&#39; &#39;Mansard&#39; &#39;Gambrel&#39; &#39;Shed&#39; &#39;Flat&#39;] 15. RoofMatl : [&#39;CompShg&#39; &#39;WdShake&#39; &#39;Tar&amp;Grv&#39; &#39;WdShngl&#39; &#39;Membran&#39; &#39;ClyTile&#39; &#39;Roll&#39; &#39;Metal&#39;] 16. Exterior1st : [&#39;BrkFace&#39; &#39;VinylSd&#39; &#39;Wd Sdng&#39; &#39;CemntBd&#39; &#39;HdBoard&#39; &#39;Plywood&#39; &#39;MetalSd&#39; &#39;AsbShng&#39; &#39;WdShing&#39; &#39;Stucco&#39; &#39;AsphShn&#39; &#39;BrkComm&#39; &#39;CBlock&#39; &#39;PreCast&#39; &#39;Stone&#39; &#39;ImStucc&#39; nan] 17. Exterior2nd : [&#39;Plywood&#39; &#39;VinylSd&#39; &#39;Wd Sdng&#39; &#39;BrkFace&#39; &#39;CmentBd&#39; &#39;HdBoard&#39; &#39;Wd Shng&#39; &#39;MetalSd&#39; &#39;ImStucc&#39; &#39;Brk Cmn&#39; &#39;AsbShng&#39; &#39;Stucco&#39; &#39;AsphShn&#39; &#39;CBlock&#39; &#39;Stone&#39; &#39;PreCast&#39; &#39;Other&#39; nan] 18. MasVnrType : [&#39;Stone&#39; &#39;None&#39; &#39;BrkFace&#39; nan &#39;BrkCmn&#39; &#39;CBlock&#39;] 19. ExterQual : [&#39;TA&#39; &#39;Gd&#39; &#39;Ex&#39; &#39;Fa&#39;] 20. ExterCond : [&#39;TA&#39; &#39;Gd&#39; &#39;Fa&#39; &#39;Po&#39; &#39;Ex&#39;] 21. Foundation : [&#39;CBlock&#39; &#39;PConc&#39; &#39;Wood&#39; &#39;BrkTil&#39; &#39;Slab&#39; &#39;Stone&#39;] 22. BsmtQual : [&#39;TA&#39; &#39;Gd&#39; &#39;Ex&#39; nan &#39;Fa&#39; &#39;Po&#39;] 23. BsmtCond : [&#39;Gd&#39; &#39;TA&#39; nan &#39;Po&#39; &#39;Fa&#39; &#39;Ex&#39;] 24. BsmtExposure : [&#39;Gd&#39; &#39;No&#39; &#39;Mn&#39; &#39;Av&#39; nan] 25. BsmtFinType1 : [&#39;BLQ&#39; &#39;Rec&#39; &#39;ALQ&#39; &#39;GLQ&#39; &#39;Unf&#39; &#39;LwQ&#39; nan] 26. BsmtFinType2 : [&#39;Unf&#39; &#39;LwQ&#39; &#39;BLQ&#39; &#39;Rec&#39; nan &#39;GLQ&#39; &#39;ALQ&#39;] 27. Heating : [&#39;GasA&#39; &#39;GasW&#39; &#39;Grav&#39; &#39;Wall&#39; &#39;Floor&#39; &#39;OthW&#39;] 28. HeatingQC : [&#39;Fa&#39; &#39;TA&#39; &#39;Ex&#39; &#39;Gd&#39; &#39;Po&#39;] 29. CentralAir : [&#39;Y&#39; &#39;N&#39;] 30. Electrical : [&#39;SBrkr&#39; &#39;FuseA&#39; &#39;FuseF&#39; &#39;FuseP&#39; nan &#39;Mix&#39;] 31. KitchenQual : [&#39;TA&#39; &#39;Gd&#39; &#39;Ex&#39; &#39;Fa&#39; &#39;Po&#39; nan] 32. Functional : [&#39;Typ&#39; &#39;Mod&#39; &#39;Min1&#39; &#39;Min2&#39; &#39;Maj1&#39; &#39;Maj2&#39; &#39;Sev&#39; &#39;Sal&#39; nan] 33. FireplaceQu : [&#39;Gd&#39; nan &#39;TA&#39; &#39;Po&#39; &#39;Ex&#39; &#39;Fa&#39;] 34. GarageType : [&#39;Attchd&#39; &#39;BuiltIn&#39; &#39;Basment&#39; &#39;Detchd&#39; nan &#39;CarPort&#39; &#39;2Types&#39;] 35. GarageFinish : [&#39;Fin&#39; &#39;Unf&#39; &#39;RFn&#39; nan] 36. GarageQual : [&#39;TA&#39; nan &#39;Fa&#39; &#39;Gd&#39; &#39;Ex&#39; &#39;Po&#39;] 37. GarageCond : [&#39;TA&#39; nan &#39;Fa&#39; &#39;Gd&#39; &#39;Ex&#39; &#39;Po&#39;] 38. PavedDrive : [&#39;P&#39; &#39;Y&#39; &#39;N&#39;] 39. PoolQC : [nan &#39;Ex&#39; &#39;Gd&#39; &#39;TA&#39; &#39;Fa&#39;] 40. Fence : [nan &#39;MnPrv&#39; &#39;GdPrv&#39; &#39;GdWo&#39; &#39;MnWw&#39;] 41. MiscFeature : [nan &#39;Gar2&#39; &#39;Shed&#39; &#39;Othr&#39; &#39;Elev&#39; &#39;TenC&#39;] 42. SaleType : [&#39;WD &#39; &#39;New&#39; &#39;COD&#39; &#39;ConLI&#39; &#39;Con&#39; &#39;ConLD&#39; &#39;Oth&#39; &#39;ConLw&#39; &#39;CWD&#39; &#39;VWD&#39; &#39;WD&#39; nan] 43. SaleCondition : [&#39;Normal&#39; &#39;Partial&#39; &#39;Family&#39; &#39;Abnorml&#39; &#39;Alloca&#39; &#39;AdjLand&#39;] . &#49692;&#49436;&#54805; &#48320;&#49688;&#46308;&#51008; &#51060;&#47111;&#44172; &#54644;&#48372;&#47732; &#50612;&#46504;&#44620;? . 문자로 되어있는 것들을 순서에 맞게 labeling해주면 열의 개수를 좀더 줄일 수 있지 않을까? | . ExterQual_n = train_test.ExterQual.astype(&quot;category&quot;) ExterQual_n = ExterQual_n.cat.reorder_categories([&#39;Fa&#39;, &#39;TA&#39;, &#39;Gd&#39;, &#39;Ex&#39;]) ExterQual_n = ExterQual_n.values.codes . ExterQual_n . array([1, 1, 1, ..., 1, 1, 1], dtype=int8) . NAN이 포함되어 있는 경우는? -&gt; 자동으로 -1로 처리한다. | . GarageCond_n = train_test.GarageCond.astype(&quot;category&quot;) GarageCond_n = GarageCond_n.cat.reorder_categories([&#39;Po&#39;,&#39;Fa&#39;, &#39;TA&#39;, &#39;Gd&#39;, &#39;Ex&#39;]) GarageCond_n = GarageCond_n.values.codes . GarageCond_n . array([ 2, 2, 2, ..., 2, -1, 2], dtype=int8) . &#44208;&#52769;&#52824;&#44032; &#51080;&#45716; &#50676;&#46308;&#51032; &#44208;&#52769;&#52824; &#48708;&#50984;&#51012; Checking&#54620;&#45796;. . 열별 NA개수를 얻어내어 데이터프레임을 생성 | 비율을 확인 후 결측치의 비율이 10%를 넘어가는 열은 삭제하도록하고 그렇지 않은 경우에는 가지고 간다. | train 데이터의 크기가 크지 않아 결측치를 전부 제거하게 되면 의미가 없는 분석이 될 수 있음.결측치들을 최대한 살려보도록한다. | . nan = pd.DataFrame(train_test.isna().sum(), columns = [&#39;NaN_sum&#39;]) nan[&#39;feat&#39;] = nan.index nan[&#39;Perc(%)&#39;] = (nan[&#39;NaN_sum&#39;]/len(train_test))*100 nan = nan[nan[&#39;NaN_sum&#39;] &gt; 0] nan = nan.sort_values(by = [&#39;NaN_sum&#39;]) nan[&#39;Usability&#39;] = np.where(nan[&#39;Perc(%)&#39;] &gt; 10, &#39;Discard&#39;, &#39;Keep&#39;) nan . NaN_sum feat Perc(%) Usability . Exterior1st 1 | Exterior1st | 0.022784 | Keep | . Exterior2nd 1 | Exterior2nd | 0.022784 | Keep | . KitchenQual 1 | KitchenQual | 0.022784 | Keep | . Electrical 1 | Electrical | 0.022784 | Keep | . SaleType 1 | SaleType | 0.022784 | Keep | . TotalBsmtSF 2 | TotalBsmtSF | 0.045568 | Keep | . GarageArea 2 | GarageArea | 0.045568 | Keep | . GarageCars 2 | GarageCars | 0.045568 | Keep | . Utilities 2 | Utilities | 0.045568 | Keep | . Functional 2 | Functional | 0.045568 | Keep | . BsmtUnfSF 2 | BsmtUnfSF | 0.045568 | Keep | . BsmtFinSF1 2 | BsmtFinSF1 | 0.045568 | Keep | . BsmtFinSF2 2 | BsmtFinSF2 | 0.045568 | Keep | . BsmtHalfBath 4 | BsmtHalfBath | 0.091137 | Keep | . BsmtFullBath 4 | BsmtFullBath | 0.091137 | Keep | . MSZoning 4 | MSZoning | 0.091137 | Keep | . MasVnrArea 38 | MasVnrArea | 0.865801 | Keep | . MasVnrType 39 | MasVnrType | 0.888585 | Keep | . BsmtFinType1 122 | BsmtFinType1 | 2.779676 | Keep | . BsmtFinType2 123 | BsmtFinType2 | 2.802461 | Keep | . BsmtQual 124 | BsmtQual | 2.825245 | Keep | . BsmtCond 125 | BsmtCond | 2.848029 | Keep | . BsmtExposure 127 | BsmtExposure | 2.893598 | Keep | . GarageType 233 | GarageType | 5.308726 | Keep | . GarageCond 237 | GarageCond | 5.399863 | Keep | . GarageQual 237 | GarageQual | 5.399863 | Keep | . GarageYrBlt 237 | GarageYrBlt | 5.399863 | Keep | . GarageFinish 237 | GarageFinish | 5.399863 | Keep | . LotFrontage 717 | LotFrontage | 16.336295 | Discard | . YearRemod/Add 1459 | YearRemod/Add | 33.242196 | Discard | . FireplaceQu 2152 | FireplaceQu | 49.031670 | Discard | . YearRemodAdd 2930 | YearRemodAdd | 66.757804 | Discard | . Fence 3527 | Fence | 80.359991 | Discard | . Alley 4084 | Alley | 93.050809 | Discard | . MiscFeature 4232 | MiscFeature | 96.422875 | Discard | . PoolQC 4373 | PoolQC | 99.635452 | Discard | . plt.figure(figsize = (15,5)) sns.barplot(x = nan[&#39;feat&#39;], y = nan[&#39;Perc(%)&#39;]) plt.xticks(rotation=80) # text를 80도 회전하여 표현함. plt.title(&#39;Features containing Nan&#39;) plt.xlabel(&#39;Features&#39;) plt.ylabel(&#39;% of Missing Data&#39;) plt.show() . train_test = pd.concat([data_w2,test], axis=0, sort=False) # train과 test를 합침 . NA&#52376;&#47532;1 . train_test[&#39;MSSubClass&#39;] = train_test[&#39;MSSubClass&#39;].apply(str) . train_test[&#39;Functional&#39;] = train_test[&#39;Functional&#39;].fillna(&#39;Typ&#39;) # typical functionality로 바꿔줌 / 2개 변경 train_test[&#39;Electrical&#39;] = train_test[&#39;Electrical&#39;].fillna(&quot;SBrkr&quot;) # Standard Circuit breakers로 바꿔줌 / 1개 변경 train_test[&#39;KitchenQual&#39;] = train_test[&#39;KitchenQual&#39;].fillna(&quot;TA&quot;) # TA/Average로 바꿔준다. / 1개 변경 train_test[&#39;Exterior1st&#39;] = train_test[&#39;Exterior1st&#39;].fillna(train_test[&#39;Exterior1st&#39;].mode()[0]) # 가장 많이 나온 값으로 대치한다. -&gt; VinylSd / 1개 변경 train_test[&#39;Exterior2nd&#39;] = train_test[&#39;Exterior2nd&#39;].fillna(train_test[&#39;Exterior2nd&#39;].mode()[0]) # 가장 많이 나온 값으로 대치한다. -&gt; VinylSd / 1개 변경 train_test[&#39;SaleType&#39;] = train_test[&#39;SaleType&#39;].fillna(train_test[&#39;SaleType&#39;].mode()[0]) # 가장 많이 나온 값으로 대치한다. -&gt; WD / 1개 변경 train_test[&quot;PoolQC&quot;] = train_test[&quot;PoolQC&quot;].fillna(&quot;None&quot;) train_test[&quot;Alley&quot;] = train_test[&quot;Alley&quot;].fillna(&quot;None&quot;) train_test[&#39;FireplaceQu&#39;] = train_test[&#39;FireplaceQu&#39;].fillna(&quot;None&quot;) train_test[&#39;Fence&#39;] = train_test[&#39;Fence&#39;].fillna(&quot;None&quot;) train_test[&#39;MiscFeature&#39;] = train_test[&#39;MiscFeature&#39;].fillna(&quot;None&quot;) for col in (&#39;GarageArea&#39;, &#39;GarageCars&#39;): train_test[col] = train_test[col].fillna(0) for col in [&#39;GarageType&#39;, &#39;GarageFinish&#39;, &#39;GarageQual&#39;, &#39;GarageCond&#39;]: train_test[col] = train_test[col].fillna(&#39;None&#39;) for col in (&#39;BsmtQual&#39;, &#39;BsmtCond&#39;, &#39;BsmtExposure&#39;, &#39;BsmtFinType1&#39;, &#39;BsmtFinType2&#39;): train_test[col] = train_test[col].fillna(&#39;None&#39;) # NA가 남아있는 feature들을 확인한다. for col in train_test: if train_test[col].isna().sum() &gt; 0: print(train_test[col][0]) . 0 RL 0 RH Name: MSZoning, dtype: object 0 141.0 0 80.0 Name: LotFrontage, dtype: float64 0 AllPub 0 AllPub Name: Utilities, dtype: object 0 1960.0 0 NaN Name: YearRemod/Add, dtype: float64 0 Stone 0 None Name: MasVnrType, dtype: object 0 112.0 0 0.0 Name: MasVnrArea, dtype: float64 0 639.0 0 468.0 Name: BsmtFinSF1, dtype: float64 0 0.0 0 144.0 Name: BsmtFinSF2, dtype: float64 0 441.0 0 270.0 Name: BsmtUnfSF, dtype: float64 0 1080.0 0 882.0 Name: TotalBsmtSF, dtype: float64 0 1.0 0 0.0 Name: BsmtFullBath, dtype: float64 0 0.0 0 0.0 Name: BsmtHalfBath, dtype: float64 0 1960.0 0 1961.0 Name: GarageYrBlt, dtype: float64 0 NaN 0 1961.0 Name: YearRemodAdd, dtype: float64 . NA&#52376;&#47532;2 . useless = [&#39;GarageYrBlt&#39;,&#39;YearRemodAdd&#39;] # 창고가 지어진 연도와 리모델링한 날짜를 제거한다. train_test = train_test.drop(useless, axis = 1) # Imputing with KnnRegressor (we can also use different Imputers) # 기본적으로 알고있는 평균대치, 중앙값대치등의 방법이 있음 # 수치형 변수들의 NA값들을 KNN 회귀모델로 추정하여 넣어준다. - 처음보는 방법 from sklearn.neighbors import KNeighborsRegressor #수치형 변수의 처리 방법 def impute_knn(df): ttn = train_test.select_dtypes(include=[np.number]) # 데이터 유형이 number인 것만 택함. ttc = train_test.select_dtypes(exclude=[np.number]) # 데이터 유형이 number아닌 것만 택함. cols_nan = ttn.columns[ttn.isna().any()].tolist() # 열 중에서 하나라도 NA가 있는 열을 택한다. cols_no_nan = ttn.columns.difference(cols_nan).values # 전체열과 NA가 있는 열의 차집합을 구한다 -&gt; NA가 없는 열만 택한다. for col in cols_nan: # NA가 존재하는 열들에 대해서만 실시한다. imp_test = ttn[ttn[col].isna()] # 특정열에 NA가 있는 열들을 TEST셋으로 둔다. (수치형 변수들만) imp_train = ttn.dropna() # NA를 제거한 ttn을 TRAIN SET으로 둔다. model = KNeighborsRegressor(n_neighbors=5) # KNR Unsupervised Approach knr = model.fit(imp_train[cols_no_nan], imp_train[col]) #KNR 모델에 적합한다. # x: imp_train에서 NA가 없는 열(cols_no_nan)들을 넣고, y:얻고자하는 열만 떼내서 target으로 둔다. ttn.loc[ttn[col].isna(), col] = knr.predict(imp_test[cols_no_nan]) # NA가 있는 열에 imp_test으로 추정해서 값을 넣는다. return pd.concat([ttn,ttc],axis=1) train_test = impute_knn(train_test) # 함수를 적용한다. # 명목형 변수의 처리방법 objects = [] for i in train_test.columns: # 열의 type이 object인것들만 꺼내서 objects 리스트에다가 넣어줌. if train_test[i].dtype == object: objects.append(i) train_test.update(train_test[objects].fillna(&#39;None&#39;)) # 명목형 변수에 NA를 채워준다. ## Checking NaN presence for col in train_test: if train_test[col].isna().sum() &gt; 0: print(train_test[col][0]) . /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) /usr/local/lib/python3.7/dist-packages/pandas/core/indexing.py:1743: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy isetter(ilocs[0], value) . Feature Engineering . [&#49352;&#47196;&#50868; &#48320;&#49688;&#51032; &#46020;&#51077;] . SqFtPerRoom(방당 평균 평방미터 계산) = 지상층 방의 넓이 / (욕실을 제외한 지상층의 방의 전체개수 + 반쪽짜리 욕실수 + 풀사이즈 욕실수 + 지상층 부엌수) | Total_Home_Quality = 전반적인 퀄리티 점수 + 전반적인 컨디션 점수 | Total_Bathrooms = (지상층의 화장실 + 절반크기 화장실 0.5) + (지하층의 화장실 + 절반크기 화장실 0.5) | HighQualSF = 1층의 평방미터 + 2층의 평방미터 | . train_test[&quot;SqFtPerRoom&quot;] = train_test[&quot;GrLivArea&quot;] / (train_test[&quot;TotRmsAbvGrd&quot;] + train_test[&quot;FullBath&quot;] + train_test[&quot;HalfBath&quot;] + train_test[&quot;KitchenAbvGr&quot;]) # train_test[&#39;Total_Home_Quality&#39;] = train_test[&#39;OverallQual&#39;] + train_test[&#39;OverallCond&#39;] train_test[&#39;Total_Bathrooms&#39;] = (train_test[&#39;FullBath&#39;] + (0.5 * train_test[&#39;HalfBath&#39;]) + train_test[&#39;BsmtFullBath&#39;] + (0.5 * train_test[&#39;BsmtHalfBath&#39;])) train_test[&quot;HighQualSF&quot;] = train_test[&quot;1stFlrSF&quot;] + train_test[&quot;2ndFlrSF&quot;] # Converting non-numeric predictors stored as numbers into string train_test[&#39;MSSubClass&#39;] = train_test[&#39;MSSubClass&#39;].apply(str) train_test[&#39;YrSold&#39;] = train_test[&#39;YrSold&#39;].apply(str) train_test[&#39;MoSold&#39;] = train_test[&#39;MoSold&#39;].apply(str) # 범주형 변수로부터 더미 변수를 만든다.(자동적으로 만들어준다.) # 수치형변수는 안건듦. train_test_dummy = pd.get_dummies(train_test) # Fetch all numeric features #train_test[&#39;Id&#39;] = train_test[&#39;Id&#39;].apply(str) numeric_features = train_test_dummy.dtypes[train_test_dummy.dtypes != object].index # dummy에서 type이 numeric인 feature들을 얻어냄 -&gt; 여기서는 전부다 선택된다. skewed_features = train_test_dummy[numeric_features].apply(lambda x: skew(x)).sort_values(ascending=False) # 변수들의 왜도를 얻어낸다. high_skew = skewed_features[skewed_features &gt; 0.5] # 왜도가 0.5이상인 feature을 선택한다. skew_index = high_skew.index # feature명 선택 # 로그변환을 통해 치우친 feature들을 정규화하도록한다. for i in skew_index: train_test_dummy[i] = np.log1p(train_test_dummy[i]) . &#48320;&#54872; &#51204; SalesPrice&#51032; &#51221;&#44508;&#49457;&#51012; &#54869;&#51064;&#54620;&#45796;. . fig, ax = plt.subplots(1,2, figsize= (15,5)) fig.suptitle(&quot;Q-Q plot &amp; distribution SalePrice &quot;, fontsize= 15) sm.qqplot(target, stats.t, distargs=(4,),fit=True, line=&quot;45&quot;, ax = ax[0]) sns.distplot(target, kde = True, hist=True, fit = norm, ax = ax[1]) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &#51221;&#44508;&#49457; &#48320;&#54872; &#54980; SalesPrice&#54869;&#51064; . 정규분포에 더 근사한 모습을 보인다. | . target_log = np.log1p(target) fig, ax = plt.subplots(1,2, figsize= (15,5)) fig.suptitle(&quot;qq-plot &amp; distribution SalePrice &quot;, fontsize= 15) sm.qqplot(target_log, stats.t, distargs=(4,),fit=True, line=&quot;45&quot;, ax = ax[0]) sns.distplot(target_log, kde = True, hist=True, fit = norm, ax = ax[1]) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . Modeling . pip install catboost . Requirement already satisfied: catboost in /usr/local/lib/python3.7/dist-packages (0.26.1) Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost) (4.4.1) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost) (1.15.0) Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from catboost) (3.2.2) Requirement already satisfied: numpy&gt;=1.16.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.19.5) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from catboost) (1.4.1) Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from catboost) (0.10.1) Requirement already satisfied: pandas&gt;=0.24.0 in /usr/local/lib/python3.7/dist-packages (from catboost) (1.1.5) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2.8.2) Requirement already satisfied: pytz&gt;=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.24.0-&gt;catboost) (2018.9) Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (2.4.7) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (1.3.1) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib-&gt;catboost) (0.10.0) Requirement already satisfied: retrying&gt;=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly-&gt;catboost) (1.3.3) . import xgboost as xgb from catboost import Pool from sklearn.svm import SVR from catboost import CatBoostRegressor from lightgbm import LGBMRegressor from sklearn.naive_bayes import GaussianNB from sklearn.tree import DecisionTreeRegressor from mlxtend.regressor import StackingRegressor from sklearn.linear_model import LinearRegression, BayesianRidge from sklearn.model_selection import RepeatedKFold from sklearn.model_selection import KFold, cross_val_score from sklearn.model_selection import train_test_split from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor from sklearn.model_selection import GridSearchCV, RandomizedSearchCV from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error . train = train_test_dummy[0:2930] test = train_test_dummy[2930:] #test[&#39;Id&#39;] = test_id . Stacking Ensemble&#48169;&#48277;&#51012; &#51060;&#50857;&#54616;&#50668; &#45908; &#45458;&#51008; &#44050;&#51012; &#46020;&#52636; &#54616;&#46020;&#47197; &#54644;&#48376;&#45796;. . def rmse(y, y_pred): return np.sqrt(mean_squared_error(y, y_pred)) # 교차 검증을 통해 rmse를 확인한다. def cv_rmse(model): rmse = np.sqrt(-cross_val_score(model, train, target_log, scoring=&quot;neg_mean_squared_error&quot;, cv=kf)) return (rmse) . 사이킷런의 교차 검증 기능은 scoring 매개변수에 (낮을수록 좋은) 비용 함수가 아니라 (클수록 좋은) 효용 함수를 기대합니다. . | 그래서 평균 제곱 오차(MSE)의 반댓값(즉, 음숫값)을 계산하는 neg_mean_squared_error 함수를 사용합니다. 이런 이유로 앞선 코드에서 제곱근을 계산하기 전에 -scores로 부호를 바꿨습니다. . | . kf = KFold(n_splits=10, random_state=42, shuffle=True) cv_scores = [] cv_std = [] baseline_models = [&#39;Linear_Reg.&#39;,&#39;Bayesian_Ridge_Reg.&#39;,&#39;LGBM_Reg.&#39;,&#39;SVR&#39;, &#39;Dec_Tree_Reg.&#39;,&#39;Random_Forest_Reg.&#39;, &#39;XGB_Reg.&#39;, &#39;Grad_Boost_Reg.&#39;,&#39;Cat_Boost_Reg.&#39;,&#39;Stacked_Reg.&#39;] # Linear Regression lreg = LinearRegression() score_lreg = cv_rmse(lreg) cv_scores.append(score_lreg.mean()) cv_std.append(score_lreg.std()) # Bayesian Ridge Regression brr = BayesianRidge(compute_score=True) score_brr = cv_rmse(brr) cv_scores.append(score_brr.mean()) cv_std.append(score_brr.std()) # Light Gradient Boost Regressor l_gbm = LGBMRegressor(objective=&#39;regression&#39;) score_l_gbm = cv_rmse(l_gbm) cv_scores.append(score_l_gbm.mean()) cv_std.append(score_l_gbm.std()) # Support Vector Regression svr = SVR() score_svr = cv_rmse(svr) cv_scores.append(score_svr.mean()) cv_std.append(score_svr.std()) # Decision Tree Regressor dtr = DecisionTreeRegressor() score_dtr = cv_rmse(dtr) cv_scores.append(score_dtr.mean()) cv_std.append(score_dtr.std()) # Random Forest Regressor rfr = RandomForestRegressor() score_rfr = cv_rmse(rfr) cv_scores.append(score_rfr.mean()) cv_std.append(score_rfr.std()) # XGB Regressor xgb = xgb.XGBRegressor() score_xgb = cv_rmse(xgb) cv_scores.append(score_xgb.mean()) cv_std.append(score_xgb.std()) # Gradient Boost Regressor gbr = GradientBoostingRegressor() score_gbr = cv_rmse(gbr) cv_scores.append(score_gbr.mean()) cv_std.append(score_gbr.std()) # Cat Boost Regressor catb = CatBoostRegressor() score_catb = cv_rmse(catb) cv_scores.append(score_catb.mean()) cv_std.append(score_catb.std()) # Stacking을 어떻게 사용하는지 확인해 볼것!! # Stacked Regressor stack_gen = StackingRegressor(regressors=(CatBoostRegressor(), LinearRegression(), BayesianRidge(), GradientBoostingRegressor()), meta_regressor = CatBoostRegressor(), use_features_in_secondary = True) score_stack_gen = cv_rmse(stack_gen) cv_scores.append(score_stack_gen.mean()) cv_std.append(score_stack_gen.std()) final_cv_score = pd.DataFrame(baseline_models, columns = [&#39;Regressors&#39;]) final_cv_score[&#39;RMSE_mean&#39;] = cv_scores final_cv_score[&#39;RMSE_std&#39;] = cv_std . 스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다. 4: learn: 0.3357498 total: 44.9ms remaining: 8.93s 5: learn: 0.3236582 total: 54.1ms remaining: 8.96s 6: learn: 0.3117506 total: 64ms remaining: 9.08s 7: learn: 0.3010769 total: 73.1ms remaining: 9.07s 8: learn: 0.2903882 total: 82.3ms remaining: 9.06s 9: learn: 0.2804411 total: 91.6ms remaining: 9.07s 10: learn: 0.2706180 total: 100ms remaining: 9.03s 11: learn: 0.2615951 total: 109ms remaining: 9s 12: learn: 0.2532119 total: 118ms remaining: 8.99s 13: learn: 0.2449057 total: 127ms remaining: 8.98s 14: learn: 0.2369065 total: 136ms remaining: 8.95s 15: learn: 0.2296248 total: 145ms remaining: 8.94s 16: learn: 0.2222274 total: 154ms remaining: 8.93s 17: learn: 0.2154762 total: 163ms remaining: 8.91s 18: learn: 0.2083672 total: 172ms remaining: 8.89s 19: learn: 0.2016370 total: 181ms remaining: 8.88s 20: learn: 0.1956220 total: 196ms remaining: 9.15s 21: learn: 0.1897434 total: 212ms remaining: 9.43s 22: learn: 0.1841170 total: 222ms remaining: 9.42s 23: learn: 0.1787278 total: 232ms remaining: 9.43s 24: learn: 0.1733736 total: 242ms remaining: 9.43s 25: learn: 0.1685297 total: 252ms remaining: 9.43s 26: learn: 0.1635829 total: 261ms remaining: 9.42s 27: learn: 0.1586252 total: 270ms remaining: 9.39s 28: learn: 0.1539926 total: 280ms remaining: 9.36s 29: learn: 0.1496219 total: 289ms remaining: 9.34s 30: learn: 0.1459815 total: 298ms remaining: 9.31s 31: learn: 0.1421243 total: 308ms remaining: 9.3s 32: learn: 0.1386615 total: 320ms remaining: 9.36s 33: learn: 0.1353931 total: 329ms remaining: 9.35s 34: learn: 0.1318424 total: 338ms remaining: 9.32s 35: learn: 0.1285071 total: 347ms remaining: 9.3s 36: learn: 0.1258644 total: 357ms remaining: 9.29s 37: learn: 0.1231504 total: 366ms remaining: 9.27s 38: learn: 0.1202870 total: 375ms remaining: 9.25s 39: learn: 0.1175036 total: 385ms remaining: 9.23s 40: learn: 0.1148958 total: 394ms remaining: 9.21s 41: learn: 0.1123338 total: 408ms remaining: 9.3s 42: learn: 0.1102161 total: 427ms remaining: 9.5s 43: learn: 0.1080791 total: 436ms remaining: 9.47s 44: learn: 0.1063073 total: 446ms remaining: 9.46s 45: learn: 0.1045670 total: 457ms remaining: 9.48s 46: learn: 0.1025866 total: 468ms remaining: 9.48s 47: learn: 0.1007044 total: 476ms remaining: 9.44s 48: learn: 0.0990509 total: 485ms remaining: 9.41s 49: learn: 0.0976584 total: 493ms remaining: 9.37s 50: learn: 0.0963720 total: 502ms remaining: 9.34s 51: learn: 0.0951625 total: 511ms remaining: 9.31s 52: learn: 0.0938537 total: 519ms remaining: 9.28s 53: learn: 0.0926990 total: 528ms remaining: 9.25s 54: learn: 0.0915507 total: 537ms remaining: 9.23s 55: learn: 0.0903649 total: 547ms remaining: 9.21s 56: learn: 0.0892648 total: 556ms remaining: 9.19s 57: learn: 0.0883328 total: 564ms remaining: 9.17s 58: learn: 0.0873964 total: 574ms remaining: 9.15s 59: learn: 0.0864312 total: 583ms remaining: 9.13s 60: learn: 0.0855774 total: 592ms remaining: 9.11s 61: learn: 0.0847197 total: 600ms remaining: 9.08s 62: learn: 0.0837474 total: 618ms remaining: 9.19s 63: learn: 0.0828943 total: 627ms remaining: 9.16s 64: learn: 0.0822202 total: 636ms remaining: 9.15s 65: learn: 0.0814660 total: 645ms remaining: 9.12s 66: learn: 0.0807848 total: 653ms remaining: 9.1s 67: learn: 0.0802113 total: 662ms remaining: 9.07s 68: learn: 0.0794658 total: 670ms remaining: 9.04s 69: learn: 0.0788846 total: 679ms remaining: 9.02s 70: learn: 0.0783496 total: 687ms remaining: 8.99s 71: learn: 0.0777826 total: 696ms remaining: 8.98s 72: learn: 0.0773891 total: 706ms remaining: 8.96s 73: learn: 0.0768091 total: 715ms remaining: 8.94s 74: learn: 0.0762975 total: 724ms remaining: 8.93s 75: learn: 0.0759857 total: 732ms remaining: 8.9s 76: learn: 0.0756535 total: 741ms remaining: 8.88s 77: learn: 0.0752473 total: 750ms remaining: 8.86s 78: learn: 0.0748383 total: 759ms remaining: 8.84s 79: learn: 0.0745185 total: 767ms remaining: 8.82s 80: learn: 0.0742424 total: 776ms remaining: 8.8s 81: learn: 0.0738698 total: 785ms remaining: 8.78s 82: learn: 0.0735094 total: 793ms remaining: 8.77s 83: learn: 0.0732991 total: 802ms remaining: 8.75s 84: learn: 0.0729523 total: 819ms remaining: 8.81s 85: learn: 0.0727113 total: 830ms remaining: 8.82s 86: learn: 0.0724751 total: 839ms remaining: 8.8s 87: learn: 0.0722908 total: 848ms remaining: 8.79s 88: learn: 0.0720080 total: 857ms remaining: 8.77s 89: learn: 0.0717367 total: 866ms remaining: 8.75s 90: learn: 0.0713928 total: 877ms remaining: 8.76s 91: learn: 0.0711752 total: 887ms remaining: 8.75s 92: learn: 0.0709718 total: 896ms remaining: 8.73s 93: learn: 0.0707531 total: 905ms remaining: 8.72s 94: learn: 0.0705884 total: 914ms remaining: 8.7s 95: learn: 0.0703426 total: 927ms remaining: 8.73s 96: learn: 0.0700886 total: 943ms remaining: 8.78s 97: learn: 0.0699700 total: 951ms remaining: 8.76s 98: learn: 0.0697547 total: 961ms remaining: 8.74s 99: learn: 0.0695163 total: 972ms remaining: 8.74s 100: learn: 0.0692880 total: 983ms remaining: 8.75s 101: learn: 0.0691399 total: 992ms remaining: 8.73s 102: learn: 0.0689577 total: 1s remaining: 8.72s 103: learn: 0.0687524 total: 1.01s remaining: 8.73s 104: learn: 0.0686002 total: 1.03s remaining: 8.82s 105: learn: 0.0685074 total: 1.05s remaining: 8.83s 106: learn: 0.0683491 total: 1.06s remaining: 8.82s 107: learn: 0.0682796 total: 1.07s remaining: 8.81s 108: learn: 0.0681223 total: 1.07s remaining: 8.79s 109: learn: 0.0680734 total: 1.08s remaining: 8.78s 110: learn: 0.0680453 total: 1.09s remaining: 8.76s 111: learn: 0.0678835 total: 1.1s remaining: 8.75s 112: learn: 0.0677535 total: 1.11s remaining: 8.73s 113: learn: 0.0675924 total: 1.12s remaining: 8.72s 114: learn: 0.0673877 total: 1.13s remaining: 8.71s 115: learn: 0.0672161 total: 1.14s remaining: 8.69s 116: learn: 0.0671669 total: 1.15s remaining: 8.67s 117: learn: 0.0670163 total: 1.16s remaining: 8.66s 118: learn: 0.0669203 total: 1.17s remaining: 8.65s 119: learn: 0.0668532 total: 1.18s remaining: 8.64s 120: learn: 0.0667464 total: 1.19s remaining: 8.63s 121: learn: 0.0666258 total: 1.2s remaining: 8.61s 122: learn: 0.0664973 total: 1.2s remaining: 8.59s 123: learn: 0.0663027 total: 1.21s remaining: 8.57s 124: learn: 0.0661439 total: 1.23s remaining: 8.61s 125: learn: 0.0660114 total: 1.25s remaining: 8.68s 126: learn: 0.0659382 total: 1.26s remaining: 8.66s 127: learn: 0.0658176 total: 1.27s remaining: 8.66s 128: learn: 0.0657021 total: 1.28s remaining: 8.65s 129: learn: 0.0655248 total: 1.29s remaining: 8.63s 130: learn: 0.0654777 total: 1.3s remaining: 8.62s 131: learn: 0.0654471 total: 1.31s remaining: 8.6s 132: learn: 0.0652836 total: 1.32s remaining: 8.59s 133: learn: 0.0652287 total: 1.32s remaining: 8.57s 134: learn: 0.0651044 total: 1.33s remaining: 8.55s 135: learn: 0.0649853 total: 1.34s remaining: 8.53s 136: learn: 0.0648575 total: 1.35s remaining: 8.52s 137: learn: 0.0648339 total: 1.36s remaining: 8.51s 138: learn: 0.0647612 total: 1.37s remaining: 8.49s 139: learn: 0.0647188 total: 1.38s remaining: 8.47s 140: learn: 0.0646834 total: 1.39s remaining: 8.46s 141: learn: 0.0646015 total: 1.4s remaining: 8.44s 142: learn: 0.0645159 total: 1.41s remaining: 8.43s 143: learn: 0.0643663 total: 1.41s remaining: 8.41s 144: learn: 0.0642812 total: 1.43s remaining: 8.43s 145: learn: 0.0641915 total: 1.44s remaining: 8.43s 146: learn: 0.0641143 total: 1.45s remaining: 8.41s 147: learn: 0.0639530 total: 1.46s remaining: 8.4s 148: learn: 0.0639130 total: 1.47s remaining: 8.38s 149: learn: 0.0638702 total: 1.48s remaining: 8.37s 150: learn: 0.0637587 total: 1.49s remaining: 8.35s 151: learn: 0.0636957 total: 1.49s remaining: 8.33s 152: learn: 0.0636665 total: 1.5s remaining: 8.32s 153: learn: 0.0635254 total: 1.51s remaining: 8.3s 154: learn: 0.0634179 total: 1.52s remaining: 8.29s 155: learn: 0.0633514 total: 1.53s remaining: 8.29s 156: learn: 0.0632859 total: 1.54s remaining: 8.28s 157: learn: 0.0632026 total: 1.55s remaining: 8.26s 158: learn: 0.0631424 total: 1.56s remaining: 8.25s 159: learn: 0.0631123 total: 1.57s remaining: 8.23s 160: learn: 0.0630389 total: 1.58s remaining: 8.22s 161: learn: 0.0629020 total: 1.59s remaining: 8.2s 162: learn: 0.0628340 total: 1.59s remaining: 8.19s 163: learn: 0.0627028 total: 1.6s remaining: 8.17s 164: learn: 0.0626487 total: 1.61s remaining: 8.16s 165: learn: 0.0625262 total: 1.63s remaining: 8.18s 166: learn: 0.0624956 total: 1.64s remaining: 8.17s 167: learn: 0.0624371 total: 1.65s remaining: 8.15s 168: learn: 0.0623904 total: 1.66s remaining: 8.15s 169: learn: 0.0622961 total: 1.67s remaining: 8.13s 170: learn: 0.0622181 total: 1.67s remaining: 8.12s 171: learn: 0.0621184 total: 1.68s remaining: 8.1s 172: learn: 0.0619401 total: 1.69s remaining: 8.09s 173: learn: 0.0618622 total: 1.7s remaining: 8.08s 174: learn: 0.0617461 total: 1.71s remaining: 8.06s 175: learn: 0.0616649 total: 1.72s remaining: 8.05s 176: learn: 0.0615268 total: 1.73s remaining: 8.04s 177: learn: 0.0614262 total: 1.74s remaining: 8.02s 178: learn: 0.0612686 total: 1.75s remaining: 8.01s 179: learn: 0.0610928 total: 1.76s remaining: 8s 180: learn: 0.0609840 total: 1.76s remaining: 7.99s 181: learn: 0.0608471 total: 1.77s remaining: 7.97s 182: learn: 0.0607641 total: 1.78s remaining: 7.96s 183: learn: 0.0606348 total: 1.79s remaining: 7.95s 184: learn: 0.0605231 total: 1.8s remaining: 7.93s 185: learn: 0.0604706 total: 1.81s remaining: 7.92s 186: learn: 0.0603414 total: 1.82s remaining: 7.91s 187: learn: 0.0602596 total: 1.84s remaining: 7.95s 188: learn: 0.0601151 total: 1.85s remaining: 7.93s 189: learn: 0.0599994 total: 1.86s remaining: 7.92s 190: learn: 0.0598824 total: 1.87s remaining: 7.91s 191: learn: 0.0597835 total: 1.88s remaining: 7.9s 192: learn: 0.0596834 total: 1.89s remaining: 7.88s 193: learn: 0.0596241 total: 1.9s remaining: 7.89s 194: learn: 0.0594655 total: 1.91s remaining: 7.89s 195: learn: 0.0592977 total: 1.92s remaining: 7.88s 196: learn: 0.0591829 total: 1.93s remaining: 7.87s 197: learn: 0.0590415 total: 1.94s remaining: 7.86s 198: learn: 0.0589152 total: 1.95s remaining: 7.85s 199: learn: 0.0587768 total: 1.96s remaining: 7.83s 200: learn: 0.0586716 total: 1.97s remaining: 7.82s 201: learn: 0.0585465 total: 1.98s remaining: 7.83s 202: learn: 0.0584565 total: 1.99s remaining: 7.82s 203: learn: 0.0583711 total: 2s remaining: 7.8s 204: learn: 0.0582399 total: 2.01s remaining: 7.79s 205: learn: 0.0581425 total: 2.02s remaining: 7.78s 206: learn: 0.0580377 total: 2.03s remaining: 7.77s 207: learn: 0.0579411 total: 2.05s remaining: 7.81s 208: learn: 0.0578785 total: 2.06s remaining: 7.8s 209: learn: 0.0577525 total: 2.07s remaining: 7.79s 210: learn: 0.0576453 total: 2.08s remaining: 7.78s 211: learn: 0.0575295 total: 2.09s remaining: 7.77s 212: learn: 0.0574161 total: 2.1s remaining: 7.76s 213: learn: 0.0573552 total: 2.11s remaining: 7.75s 214: learn: 0.0572045 total: 2.12s remaining: 7.74s 215: learn: 0.0571295 total: 2.13s remaining: 7.73s 216: learn: 0.0570062 total: 2.14s remaining: 7.71s 217: learn: 0.0569176 total: 2.15s remaining: 7.7s 218: learn: 0.0568142 total: 2.16s remaining: 7.69s 219: learn: 0.0567218 total: 2.17s remaining: 7.68s 220: learn: 0.0566248 total: 2.18s remaining: 7.68s 221: learn: 0.0565369 total: 2.19s remaining: 7.66s 222: learn: 0.0564486 total: 2.19s remaining: 7.65s 223: learn: 0.0563553 total: 2.2s remaining: 7.64s 224: learn: 0.0562185 total: 2.21s remaining: 7.62s 225: learn: 0.0560916 total: 2.22s remaining: 7.62s 226: learn: 0.0559472 total: 2.23s remaining: 7.61s 227: learn: 0.0558934 total: 2.24s remaining: 7.6s 228: learn: 0.0557983 total: 2.27s remaining: 7.64s 229: learn: 0.0556838 total: 2.28s remaining: 7.63s 230: learn: 0.0555480 total: 2.29s remaining: 7.62s 231: learn: 0.0553782 total: 2.3s remaining: 7.61s 232: learn: 0.0552730 total: 2.31s remaining: 7.59s 233: learn: 0.0552166 total: 2.31s remaining: 7.58s 234: learn: 0.0551659 total: 2.32s remaining: 7.57s 235: learn: 0.0550582 total: 2.33s remaining: 7.55s 236: learn: 0.0549876 total: 2.34s remaining: 7.54s 237: learn: 0.0549365 total: 2.35s remaining: 7.53s 238: learn: 0.0548888 total: 2.36s remaining: 7.52s 239: learn: 0.0547431 total: 2.37s remaining: 7.51s 240: learn: 0.0546475 total: 2.38s remaining: 7.49s 241: learn: 0.0545547 total: 2.39s remaining: 7.48s 242: learn: 0.0544934 total: 2.4s remaining: 7.47s 243: learn: 0.0544113 total: 2.41s remaining: 7.46s 244: learn: 0.0543006 total: 2.42s remaining: 7.45s 245: learn: 0.0542449 total: 2.42s remaining: 7.43s 246: learn: 0.0541010 total: 2.43s remaining: 7.42s 247: learn: 0.0540149 total: 2.44s remaining: 7.41s 248: learn: 0.0539097 total: 2.46s remaining: 7.43s 249: learn: 0.0537818 total: 2.47s remaining: 7.42s 250: learn: 0.0536939 total: 2.48s remaining: 7.4s 251: learn: 0.0536296 total: 2.49s remaining: 7.39s 252: learn: 0.0534914 total: 2.5s remaining: 7.38s 253: learn: 0.0534285 total: 2.51s remaining: 7.36s 254: learn: 0.0533303 total: 2.52s remaining: 7.35s 255: learn: 0.0532431 total: 2.52s remaining: 7.34s 256: learn: 0.0530980 total: 2.53s remaining: 7.33s 257: learn: 0.0530099 total: 2.54s remaining: 7.32s 258: learn: 0.0529326 total: 2.55s remaining: 7.3s 259: learn: 0.0528360 total: 2.56s remaining: 7.29s 260: learn: 0.0527555 total: 2.57s remaining: 7.28s 261: learn: 0.0526877 total: 2.58s remaining: 7.27s 262: learn: 0.0526212 total: 2.59s remaining: 7.26s 263: learn: 0.0525651 total: 2.6s remaining: 7.24s 264: learn: 0.0524504 total: 2.61s remaining: 7.23s 265: learn: 0.0523806 total: 2.62s remaining: 7.22s 266: learn: 0.0522723 total: 2.63s remaining: 7.21s 267: learn: 0.0521884 total: 2.63s remaining: 7.2s 268: learn: 0.0521137 total: 2.64s remaining: 7.18s 269: learn: 0.0520150 total: 2.65s remaining: 7.17s 270: learn: 0.0519561 total: 2.67s remaining: 7.18s 271: learn: 0.0518726 total: 2.68s remaining: 7.18s 272: learn: 0.0517803 total: 2.69s remaining: 7.16s 273: learn: 0.0517059 total: 2.7s remaining: 7.16s 274: learn: 0.0516320 total: 2.71s remaining: 7.15s 275: learn: 0.0515558 total: 2.72s remaining: 7.14s 276: learn: 0.0514503 total: 2.73s remaining: 7.13s 277: learn: 0.0513805 total: 2.74s remaining: 7.12s 278: learn: 0.0512911 total: 2.75s remaining: 7.11s 279: learn: 0.0512512 total: 2.76s remaining: 7.1s 280: learn: 0.0511720 total: 2.77s remaining: 7.09s 281: learn: 0.0510776 total: 2.78s remaining: 7.08s 282: learn: 0.0509867 total: 2.79s remaining: 7.07s 283: learn: 0.0508827 total: 2.8s remaining: 7.06s 284: learn: 0.0508275 total: 2.81s remaining: 7.05s 285: learn: 0.0507630 total: 2.82s remaining: 7.04s 286: learn: 0.0506731 total: 2.83s remaining: 7.02s 287: learn: 0.0506166 total: 2.84s remaining: 7.01s 288: learn: 0.0505278 total: 2.85s remaining: 7.01s 289: learn: 0.0504961 total: 2.86s remaining: 7s 290: learn: 0.0504053 total: 2.88s remaining: 7.02s 291: learn: 0.0503215 total: 2.89s remaining: 7.01s 292: learn: 0.0502580 total: 2.9s remaining: 7s 293: learn: 0.0501310 total: 2.91s remaining: 6.99s 294: learn: 0.0500666 total: 2.92s remaining: 6.98s 295: learn: 0.0500096 total: 2.93s remaining: 6.97s 296: learn: 0.0499339 total: 2.94s remaining: 6.96s 297: learn: 0.0498663 total: 2.95s remaining: 6.94s 298: learn: 0.0498135 total: 2.96s remaining: 6.93s 299: learn: 0.0497365 total: 2.96s remaining: 6.92s 300: learn: 0.0497017 total: 2.98s remaining: 6.91s 301: learn: 0.0496256 total: 2.98s remaining: 6.9s 302: learn: 0.0495578 total: 2.99s remaining: 6.89s 303: learn: 0.0494751 total: 3s remaining: 6.88s 304: learn: 0.0494333 total: 3.01s remaining: 6.87s 305: learn: 0.0493114 total: 3.02s remaining: 6.86s 306: learn: 0.0492460 total: 3.03s remaining: 6.85s 307: learn: 0.0491566 total: 3.04s remaining: 6.84s 308: learn: 0.0491003 total: 3.05s remaining: 6.83s 309: learn: 0.0489950 total: 3.06s remaining: 6.81s 310: learn: 0.0489428 total: 3.08s remaining: 6.82s 311: learn: 0.0488966 total: 3.09s remaining: 6.82s 312: learn: 0.0488243 total: 3.1s remaining: 6.81s 313: learn: 0.0487569 total: 3.11s remaining: 6.79s 314: learn: 0.0486729 total: 3.12s remaining: 6.78s 315: learn: 0.0486139 total: 3.13s remaining: 6.77s 316: learn: 0.0485274 total: 3.14s remaining: 6.76s 317: learn: 0.0484817 total: 3.15s remaining: 6.75s 318: learn: 0.0484215 total: 3.16s remaining: 6.74s 319: learn: 0.0483489 total: 3.17s remaining: 6.73s 320: learn: 0.0483103 total: 3.17s remaining: 6.71s 321: learn: 0.0482566 total: 3.19s remaining: 6.71s 322: learn: 0.0482075 total: 3.2s remaining: 6.7s 323: learn: 0.0481297 total: 3.21s remaining: 6.69s 324: learn: 0.0480578 total: 3.22s remaining: 6.68s 325: learn: 0.0479784 total: 3.23s remaining: 6.67s 326: learn: 0.0479111 total: 3.23s remaining: 6.66s 327: learn: 0.0478565 total: 3.24s remaining: 6.65s 328: learn: 0.0478107 total: 3.25s remaining: 6.63s 329: learn: 0.0477526 total: 3.26s remaining: 6.62s 330: learn: 0.0476906 total: 3.27s remaining: 6.61s 331: learn: 0.0476276 total: 3.28s remaining: 6.6s 332: learn: 0.0475708 total: 3.29s remaining: 6.6s 333: learn: 0.0475144 total: 3.3s remaining: 6.59s 334: learn: 0.0474066 total: 3.31s remaining: 6.58s 335: learn: 0.0473394 total: 3.32s remaining: 6.57s 336: learn: 0.0472982 total: 3.33s remaining: 6.55s 337: learn: 0.0472588 total: 3.34s remaining: 6.54s 338: learn: 0.0472228 total: 3.35s remaining: 6.53s 339: learn: 0.0472120 total: 3.36s remaining: 6.52s 340: learn: 0.0471152 total: 3.37s remaining: 6.51s 341: learn: 0.0470418 total: 3.38s remaining: 6.5s 342: learn: 0.0469682 total: 3.39s remaining: 6.49s 343: learn: 0.0469070 total: 3.4s remaining: 6.47s 344: learn: 0.0468621 total: 3.4s remaining: 6.46s 345: learn: 0.0468093 total: 3.41s remaining: 6.45s 346: learn: 0.0467506 total: 3.42s remaining: 6.44s 347: learn: 0.0466767 total: 3.43s remaining: 6.43s 348: learn: 0.0466306 total: 3.44s remaining: 6.42s 349: learn: 0.0465685 total: 3.45s remaining: 6.41s 350: learn: 0.0464984 total: 3.46s remaining: 6.4s 351: learn: 0.0464513 total: 3.47s remaining: 6.38s 352: learn: 0.0463792 total: 3.48s remaining: 6.38s 353: learn: 0.0463130 total: 3.5s remaining: 6.39s 354: learn: 0.0462294 total: 3.51s remaining: 6.38s 355: learn: 0.0461972 total: 3.52s remaining: 6.37s 356: learn: 0.0461703 total: 3.53s remaining: 6.35s 357: learn: 0.0461128 total: 3.54s remaining: 6.34s 358: learn: 0.0460615 total: 3.55s remaining: 6.33s 359: learn: 0.0459879 total: 3.56s remaining: 6.32s 360: learn: 0.0459400 total: 3.56s remaining: 6.31s 361: learn: 0.0458959 total: 3.57s remaining: 6.3s 362: learn: 0.0458451 total: 3.58s remaining: 6.29s 363: learn: 0.0458144 total: 3.59s remaining: 6.28s 364: learn: 0.0457590 total: 3.6s remaining: 6.27s 365: learn: 0.0456958 total: 3.61s remaining: 6.26s 366: learn: 0.0456408 total: 3.62s remaining: 6.25s 367: learn: 0.0455863 total: 3.63s remaining: 6.23s 368: learn: 0.0455462 total: 3.64s remaining: 6.22s 369: learn: 0.0454966 total: 3.65s remaining: 6.21s 370: learn: 0.0454547 total: 3.66s remaining: 6.2s 371: learn: 0.0453891 total: 3.67s remaining: 6.19s 372: learn: 0.0453532 total: 3.68s remaining: 6.19s 373: learn: 0.0453036 total: 3.7s remaining: 6.2s 374: learn: 0.0452509 total: 3.71s remaining: 6.19s 375: learn: 0.0451660 total: 3.72s remaining: 6.18s 376: learn: 0.0451215 total: 3.73s remaining: 6.17s 377: learn: 0.0450554 total: 3.74s remaining: 6.16s 378: learn: 0.0450271 total: 3.75s remaining: 6.15s 379: learn: 0.0449824 total: 3.76s remaining: 6.14s 380: learn: 0.0449149 total: 3.77s remaining: 6.13s 381: learn: 0.0448426 total: 3.78s remaining: 6.12s 382: learn: 0.0447484 total: 3.79s remaining: 6.11s 383: learn: 0.0446856 total: 3.8s remaining: 6.1s 384: learn: 0.0446508 total: 3.81s remaining: 6.09s 385: learn: 0.0445835 total: 3.82s remaining: 6.08s 386: learn: 0.0445470 total: 3.83s remaining: 6.07s 387: learn: 0.0444881 total: 3.84s remaining: 6.06s 388: learn: 0.0444483 total: 3.86s remaining: 6.06s 389: learn: 0.0443851 total: 3.87s remaining: 6.05s 390: learn: 0.0443444 total: 3.9s remaining: 6.07s 391: learn: 0.0442729 total: 3.9s remaining: 6.06s 392: learn: 0.0442079 total: 3.91s remaining: 6.05s 393: learn: 0.0441402 total: 3.92s remaining: 6.03s 394: learn: 0.0440802 total: 3.93s remaining: 6.02s 395: learn: 0.0440342 total: 3.94s remaining: 6.01s 396: learn: 0.0439508 total: 3.95s remaining: 6s 397: learn: 0.0438925 total: 3.96s remaining: 5.99s 398: learn: 0.0438634 total: 3.97s remaining: 5.98s 399: learn: 0.0438280 total: 3.98s remaining: 5.96s 400: learn: 0.0437936 total: 3.98s remaining: 5.95s 401: learn: 0.0437288 total: 4s remaining: 5.94s 402: learn: 0.0436871 total: 4s remaining: 5.93s 403: learn: 0.0436445 total: 4.01s remaining: 5.92s 404: learn: 0.0435809 total: 4.02s remaining: 5.91s 405: learn: 0.0435262 total: 4.03s remaining: 5.9s 406: learn: 0.0434685 total: 4.04s remaining: 5.89s 407: learn: 0.0434137 total: 4.05s remaining: 5.88s 408: learn: 0.0433848 total: 4.06s remaining: 5.87s 409: learn: 0.0433596 total: 4.07s remaining: 5.86s 410: learn: 0.0433057 total: 4.08s remaining: 5.84s 411: learn: 0.0432698 total: 4.09s remaining: 5.83s 412: learn: 0.0432091 total: 4.1s remaining: 5.83s 413: learn: 0.0431695 total: 4.11s remaining: 5.82s 414: learn: 0.0431391 total: 4.12s remaining: 5.81s 415: learn: 0.0431119 total: 4.13s remaining: 5.8s 416: learn: 0.0430680 total: 4.14s remaining: 5.79s 417: learn: 0.0430263 total: 4.15s remaining: 5.78s 418: learn: 0.0429769 total: 4.16s remaining: 5.77s 419: learn: 0.0429329 total: 4.17s remaining: 5.76s 420: learn: 0.0429047 total: 4.18s remaining: 5.74s 421: learn: 0.0428433 total: 4.18s remaining: 5.73s 422: learn: 0.0427922 total: 4.19s remaining: 5.72s 423: learn: 0.0427492 total: 4.2s remaining: 5.71s 424: learn: 0.0427164 total: 4.21s remaining: 5.7s 425: learn: 0.0426728 total: 4.22s remaining: 5.69s 426: learn: 0.0426185 total: 4.23s remaining: 5.68s 427: learn: 0.0425626 total: 4.24s remaining: 5.67s 428: learn: 0.0425174 total: 4.25s remaining: 5.65s 429: learn: 0.0424922 total: 4.26s remaining: 5.64s 430: learn: 0.0424393 total: 4.26s remaining: 5.63s 431: learn: 0.0423932 total: 4.27s remaining: 5.62s 432: learn: 0.0423419 total: 4.28s remaining: 5.61s 433: learn: 0.0423052 total: 4.29s remaining: 5.6s 434: learn: 0.0422682 total: 4.31s remaining: 5.6s 435: learn: 0.0422390 total: 4.33s remaining: 5.6s 436: learn: 0.0421879 total: 4.34s remaining: 5.59s 437: learn: 0.0421410 total: 4.34s remaining: 5.58s 438: learn: 0.0420818 total: 4.35s remaining: 5.56s 439: learn: 0.0420141 total: 4.36s remaining: 5.55s 440: learn: 0.0419468 total: 4.37s remaining: 5.54s 441: learn: 0.0419002 total: 4.38s remaining: 5.53s 442: learn: 0.0418712 total: 4.39s remaining: 5.52s 443: learn: 0.0418193 total: 4.4s remaining: 5.51s 444: learn: 0.0417671 total: 4.41s remaining: 5.5s 445: learn: 0.0417089 total: 4.42s remaining: 5.49s 446: learn: 0.0416400 total: 4.43s remaining: 5.48s 447: learn: 0.0416374 total: 4.44s remaining: 5.47s 448: learn: 0.0415812 total: 4.45s remaining: 5.46s 449: learn: 0.0415257 total: 4.46s remaining: 5.45s 450: learn: 0.0414893 total: 4.47s remaining: 5.44s 451: learn: 0.0414393 total: 4.48s remaining: 5.43s 452: learn: 0.0413913 total: 4.49s remaining: 5.42s 453: learn: 0.0413889 total: 4.5s remaining: 5.41s 454: learn: 0.0413249 total: 4.5s remaining: 5.39s 455: learn: 0.0412582 total: 4.52s remaining: 5.4s 456: learn: 0.0412557 total: 4.53s remaining: 5.39s 457: learn: 0.0412182 total: 4.54s remaining: 5.38s 458: learn: 0.0411798 total: 4.55s remaining: 5.36s 459: learn: 0.0411326 total: 4.56s remaining: 5.35s 460: learn: 0.0410917 total: 4.57s remaining: 5.34s 461: learn: 0.0410440 total: 4.58s remaining: 5.33s 462: learn: 0.0410025 total: 4.59s remaining: 5.32s 463: learn: 0.0409489 total: 4.6s remaining: 5.31s 464: learn: 0.0409234 total: 4.61s remaining: 5.3s 465: learn: 0.0408819 total: 4.62s remaining: 5.29s 466: learn: 0.0408797 total: 4.63s remaining: 5.28s 467: learn: 0.0408531 total: 4.63s remaining: 5.27s 468: learn: 0.0408511 total: 4.64s remaining: 5.26s 469: learn: 0.0407858 total: 4.65s remaining: 5.25s 470: learn: 0.0407200 total: 4.66s remaining: 5.24s 471: learn: 0.0406755 total: 4.67s remaining: 5.23s 472: learn: 0.0406295 total: 4.68s remaining: 5.22s 473: learn: 0.0405759 total: 4.69s remaining: 5.21s 474: learn: 0.0405373 total: 4.7s remaining: 5.2s 475: learn: 0.0404871 total: 4.71s remaining: 5.19s 476: learn: 0.0404389 total: 4.73s remaining: 5.18s 477: learn: 0.0404170 total: 4.74s remaining: 5.17s 478: learn: 0.0403718 total: 4.75s remaining: 5.16s 479: learn: 0.0403162 total: 4.76s remaining: 5.15s 480: learn: 0.0402852 total: 4.76s remaining: 5.14s 481: learn: 0.0402607 total: 4.77s remaining: 5.13s 482: learn: 0.0402367 total: 4.78s remaining: 5.12s 483: learn: 0.0402045 total: 4.79s remaining: 5.11s 484: learn: 0.0401624 total: 4.8s remaining: 5.1s 485: learn: 0.0401395 total: 4.81s remaining: 5.09s 486: learn: 0.0401014 total: 4.82s remaining: 5.08s 487: learn: 0.0400417 total: 4.84s remaining: 5.07s 488: learn: 0.0399821 total: 4.85s remaining: 5.06s 489: learn: 0.0399483 total: 4.86s remaining: 5.05s 490: learn: 0.0399307 total: 4.87s remaining: 5.05s 491: learn: 0.0398685 total: 4.88s remaining: 5.04s 492: learn: 0.0398175 total: 4.89s remaining: 5.03s 493: learn: 0.0397729 total: 4.89s remaining: 5.01s 494: learn: 0.0397318 total: 4.9s remaining: 5s 495: learn: 0.0396995 total: 4.92s remaining: 5s 496: learn: 0.0396637 total: 4.94s remaining: 5s 497: learn: 0.0396538 total: 4.95s remaining: 4.99s 498: learn: 0.0396203 total: 4.96s remaining: 4.98s 499: learn: 0.0395935 total: 4.97s remaining: 4.97s 500: learn: 0.0395466 total: 4.98s remaining: 4.96s 501: learn: 0.0394919 total: 4.99s remaining: 4.95s 502: learn: 0.0394898 total: 5s remaining: 4.94s 503: learn: 0.0394419 total: 5.01s remaining: 4.93s 504: learn: 0.0394217 total: 5.02s remaining: 4.92s 505: learn: 0.0393786 total: 5.03s remaining: 4.91s 506: learn: 0.0393373 total: 5.04s remaining: 4.9s 507: learn: 0.0392927 total: 5.05s remaining: 4.89s 508: learn: 0.0392703 total: 5.06s remaining: 4.88s 509: learn: 0.0392488 total: 5.07s remaining: 4.87s 510: learn: 0.0392095 total: 5.08s remaining: 4.86s 511: learn: 0.0391886 total: 5.08s remaining: 4.85s 512: learn: 0.0391584 total: 5.09s remaining: 4.83s 513: learn: 0.0391118 total: 5.11s remaining: 4.83s 514: learn: 0.0390760 total: 5.11s remaining: 4.82s 515: learn: 0.0390569 total: 5.12s remaining: 4.8s 516: learn: 0.0390063 total: 5.13s remaining: 4.79s 517: learn: 0.0389569 total: 5.15s remaining: 4.79s 518: learn: 0.0389082 total: 5.17s remaining: 4.79s 519: learn: 0.0388806 total: 5.17s remaining: 4.78s 520: learn: 0.0388493 total: 5.18s remaining: 4.76s 521: learn: 0.0388191 total: 5.19s remaining: 4.75s 522: learn: 0.0387756 total: 5.2s remaining: 4.74s 523: learn: 0.0387458 total: 5.21s remaining: 4.73s 524: learn: 0.0387242 total: 5.22s remaining: 4.72s 525: learn: 0.0386782 total: 5.23s remaining: 4.71s 526: learn: 0.0386606 total: 5.24s remaining: 4.71s 527: learn: 0.0386328 total: 5.25s remaining: 4.69s 528: learn: 0.0385972 total: 5.26s remaining: 4.68s 529: learn: 0.0385487 total: 5.27s remaining: 4.67s 530: learn: 0.0385243 total: 5.28s remaining: 4.66s 531: learn: 0.0384878 total: 5.29s remaining: 4.65s 532: learn: 0.0384542 total: 5.3s remaining: 4.64s 533: learn: 0.0384210 total: 5.31s remaining: 4.63s 534: learn: 0.0383792 total: 5.32s remaining: 4.62s 535: learn: 0.0383403 total: 5.33s remaining: 4.61s 536: learn: 0.0382958 total: 5.33s remaining: 4.6s 537: learn: 0.0382443 total: 5.34s remaining: 4.59s 538: learn: 0.0382179 total: 5.36s remaining: 4.58s 539: learn: 0.0381787 total: 5.37s remaining: 4.58s 540: learn: 0.0381412 total: 5.38s remaining: 4.57s 541: learn: 0.0380881 total: 5.39s remaining: 4.55s 542: learn: 0.0380558 total: 5.4s remaining: 4.54s 543: learn: 0.0380271 total: 5.41s remaining: 4.53s 544: learn: 0.0379971 total: 5.42s remaining: 4.52s 545: learn: 0.0379555 total: 5.43s remaining: 4.51s 546: learn: 0.0379038 total: 5.43s remaining: 4.5s 547: learn: 0.0379019 total: 5.44s remaining: 4.49s 548: learn: 0.0378519 total: 5.45s remaining: 4.48s 549: learn: 0.0378143 total: 5.46s remaining: 4.47s 550: learn: 0.0377872 total: 5.47s remaining: 4.46s 551: learn: 0.0377509 total: 5.48s remaining: 4.45s 552: learn: 0.0377308 total: 5.49s remaining: 4.44s 553: learn: 0.0376888 total: 5.5s remaining: 4.43s 554: learn: 0.0376457 total: 5.51s remaining: 4.42s 555: learn: 0.0376119 total: 5.52s remaining: 4.41s 556: learn: 0.0375788 total: 5.53s remaining: 4.4s 557: learn: 0.0375525 total: 5.54s remaining: 4.39s 558: learn: 0.0375195 total: 5.55s remaining: 4.38s 559: learn: 0.0374945 total: 5.55s remaining: 4.36s 560: learn: 0.0374764 total: 5.57s remaining: 4.36s 561: learn: 0.0374383 total: 5.58s remaining: 4.35s 562: learn: 0.0374107 total: 5.59s remaining: 4.34s 563: learn: 0.0373734 total: 5.6s remaining: 4.33s 564: learn: 0.0373475 total: 5.61s remaining: 4.32s 565: learn: 0.0372881 total: 5.62s remaining: 4.31s 566: learn: 0.0372468 total: 5.63s remaining: 4.3s 567: learn: 0.0372100 total: 5.64s remaining: 4.29s 568: learn: 0.0371841 total: 5.65s remaining: 4.28s 569: learn: 0.0371525 total: 5.66s remaining: 4.27s 570: learn: 0.0371102 total: 5.67s remaining: 4.26s 571: learn: 0.0371086 total: 5.68s remaining: 4.25s 572: learn: 0.0370869 total: 5.69s remaining: 4.24s 573: learn: 0.0370788 total: 5.7s remaining: 4.23s 574: learn: 0.0370546 total: 5.7s remaining: 4.22s 575: learn: 0.0370034 total: 5.71s remaining: 4.21s 576: learn: 0.0370020 total: 5.72s remaining: 4.19s 577: learn: 0.0369661 total: 5.73s remaining: 4.18s 578: learn: 0.0369225 total: 5.74s remaining: 4.18s 579: learn: 0.0369002 total: 5.75s remaining: 4.17s 580: learn: 0.0368675 total: 5.76s remaining: 4.16s 581: learn: 0.0368661 total: 5.77s remaining: 4.15s 582: learn: 0.0368115 total: 5.79s remaining: 4.14s 583: learn: 0.0367767 total: 5.81s remaining: 4.14s 584: learn: 0.0367482 total: 5.82s remaining: 4.13s 585: learn: 0.0367138 total: 5.83s remaining: 4.12s 586: learn: 0.0367124 total: 5.84s remaining: 4.11s 587: learn: 0.0366923 total: 5.85s remaining: 4.1s 588: learn: 0.0366472 total: 5.86s remaining: 4.09s 589: learn: 0.0366167 total: 5.87s remaining: 4.08s 590: learn: 0.0365738 total: 5.88s remaining: 4.07s 591: learn: 0.0365370 total: 5.88s remaining: 4.05s 592: learn: 0.0365247 total: 5.89s remaining: 4.04s 593: learn: 0.0365060 total: 5.9s remaining: 4.03s 594: learn: 0.0364569 total: 5.91s remaining: 4.02s 595: learn: 0.0364258 total: 5.92s remaining: 4.01s 596: learn: 0.0363880 total: 5.93s remaining: 4s 597: learn: 0.0363717 total: 5.94s remaining: 3.99s 598: learn: 0.0363697 total: 5.95s remaining: 3.98s 599: learn: 0.0363395 total: 5.96s remaining: 3.97s 600: learn: 0.0363051 total: 5.98s remaining: 3.97s 601: learn: 0.0362621 total: 5.99s remaining: 3.96s 602: learn: 0.0362277 total: 6s remaining: 3.95s 603: learn: 0.0361912 total: 6.01s remaining: 3.94s 604: learn: 0.0361619 total: 6.02s remaining: 3.93s 605: learn: 0.0361348 total: 6.03s remaining: 3.92s 606: learn: 0.0361064 total: 6.04s remaining: 3.91s 607: learn: 0.0360836 total: 6.05s remaining: 3.9s 608: learn: 0.0360822 total: 6.06s remaining: 3.89s 609: learn: 0.0360514 total: 6.07s remaining: 3.88s 610: learn: 0.0360498 total: 6.08s remaining: 3.87s 611: learn: 0.0360241 total: 6.08s remaining: 3.86s 612: learn: 0.0359915 total: 6.09s remaining: 3.85s 613: learn: 0.0359628 total: 6.1s remaining: 3.84s 614: learn: 0.0359395 total: 6.11s remaining: 3.83s 615: learn: 0.0358981 total: 6.12s remaining: 3.81s 616: learn: 0.0358811 total: 6.13s remaining: 3.8s 617: learn: 0.0358443 total: 6.14s remaining: 3.79s 618: learn: 0.0358403 total: 6.14s remaining: 3.78s 619: learn: 0.0358023 total: 6.15s remaining: 3.77s 620: learn: 0.0357712 total: 6.16s remaining: 3.76s 621: learn: 0.0357503 total: 6.18s remaining: 3.76s 622: learn: 0.0357172 total: 6.2s remaining: 3.75s 623: learn: 0.0357026 total: 6.21s remaining: 3.74s 624: learn: 0.0356529 total: 6.22s remaining: 3.73s 625: learn: 0.0356167 total: 6.23s remaining: 3.72s 626: learn: 0.0355791 total: 6.24s remaining: 3.71s 627: learn: 0.0355394 total: 6.24s remaining: 3.7s 628: learn: 0.0355000 total: 6.25s remaining: 3.69s 629: learn: 0.0354972 total: 6.26s remaining: 3.68s 630: learn: 0.0354660 total: 6.27s remaining: 3.67s 631: learn: 0.0354199 total: 6.28s remaining: 3.66s 632: learn: 0.0353941 total: 6.29s remaining: 3.65s 633: learn: 0.0353645 total: 6.3s remaining: 3.64s 634: learn: 0.0353283 total: 6.31s remaining: 3.63s 635: learn: 0.0352907 total: 6.32s remaining: 3.62s 636: learn: 0.0352602 total: 6.33s remaining: 3.6s 637: learn: 0.0352442 total: 6.33s remaining: 3.59s 638: learn: 0.0352247 total: 6.34s remaining: 3.58s 639: learn: 0.0351820 total: 6.35s remaining: 3.57s 640: learn: 0.0351559 total: 6.36s remaining: 3.56s 641: learn: 0.0351338 total: 6.37s remaining: 3.55s 642: learn: 0.0350899 total: 6.39s remaining: 3.54s 643: learn: 0.0350591 total: 6.41s remaining: 3.54s 644: learn: 0.0350281 total: 6.41s remaining: 3.53s 645: learn: 0.0349919 total: 6.42s remaining: 3.52s 646: learn: 0.0349369 total: 6.43s remaining: 3.51s 647: learn: 0.0348928 total: 6.44s remaining: 3.5s 648: learn: 0.0348625 total: 6.45s remaining: 3.49s 649: learn: 0.0348217 total: 6.46s remaining: 3.48s 650: learn: 0.0347853 total: 6.47s remaining: 3.47s 651: learn: 0.0347542 total: 6.48s remaining: 3.46s 652: learn: 0.0347477 total: 6.49s remaining: 3.45s 653: learn: 0.0347229 total: 6.5s remaining: 3.44s 654: learn: 0.0346938 total: 6.5s remaining: 3.42s 655: learn: 0.0346681 total: 6.51s remaining: 3.42s 656: learn: 0.0346267 total: 6.52s remaining: 3.4s 657: learn: 0.0345964 total: 6.53s remaining: 3.39s 658: learn: 0.0345667 total: 6.54s remaining: 3.38s 659: learn: 0.0345180 total: 6.55s remaining: 3.37s 660: learn: 0.0344759 total: 6.56s remaining: 3.36s 661: learn: 0.0344305 total: 6.57s remaining: 3.35s 662: learn: 0.0344048 total: 6.58s remaining: 3.34s 663: learn: 0.0343819 total: 6.59s remaining: 3.34s 664: learn: 0.0343456 total: 6.61s remaining: 3.33s 665: learn: 0.0343282 total: 6.62s remaining: 3.32s 666: learn: 0.0343137 total: 6.63s remaining: 3.31s 667: learn: 0.0342889 total: 6.64s remaining: 3.3s 668: learn: 0.0342585 total: 6.64s remaining: 3.29s 669: learn: 0.0342571 total: 6.65s remaining: 3.28s 670: learn: 0.0342422 total: 6.66s remaining: 3.27s 671: learn: 0.0341988 total: 6.67s remaining: 3.26s 672: learn: 0.0341975 total: 6.68s remaining: 3.25s 673: learn: 0.0341651 total: 6.69s remaining: 3.24s 674: learn: 0.0341323 total: 6.7s remaining: 3.23s 675: learn: 0.0341028 total: 6.71s remaining: 3.21s 676: learn: 0.0340817 total: 6.72s remaining: 3.21s 677: learn: 0.0340510 total: 6.73s remaining: 3.19s 678: learn: 0.0340229 total: 6.74s remaining: 3.19s 679: learn: 0.0340018 total: 6.75s remaining: 3.17s 680: learn: 0.0339668 total: 6.76s remaining: 3.17s 681: learn: 0.0339336 total: 6.77s remaining: 3.16s 682: learn: 0.0339176 total: 6.79s remaining: 3.15s 683: learn: 0.0338770 total: 6.79s remaining: 3.14s 684: learn: 0.0338757 total: 6.81s remaining: 3.13s 685: learn: 0.0338520 total: 6.82s remaining: 3.12s 686: learn: 0.0338396 total: 6.83s remaining: 3.11s 687: learn: 0.0337991 total: 6.84s remaining: 3.1s 688: learn: 0.0337663 total: 6.85s remaining: 3.09s 689: learn: 0.0337507 total: 6.86s remaining: 3.08s 690: learn: 0.0337348 total: 6.87s remaining: 3.07s 691: learn: 0.0337023 total: 6.88s remaining: 3.06s 692: learn: 0.0336731 total: 6.89s remaining: 3.05s 693: learn: 0.0336714 total: 6.9s remaining: 3.04s 694: learn: 0.0336582 total: 6.91s remaining: 3.03s 695: learn: 0.0336398 total: 6.92s remaining: 3.02s 696: learn: 0.0336173 total: 6.93s remaining: 3.01s 697: learn: 0.0335947 total: 6.93s remaining: 3s 698: learn: 0.0335658 total: 6.94s remaining: 2.99s 699: learn: 0.0335269 total: 6.95s remaining: 2.98s 700: learn: 0.0335107 total: 6.96s remaining: 2.97s 701: learn: 0.0334849 total: 6.97s remaining: 2.96s 702: learn: 0.0334691 total: 6.98s remaining: 2.95s 703: learn: 0.0334496 total: 7s remaining: 2.94s 704: learn: 0.0334300 total: 7.01s remaining: 2.93s 705: learn: 0.0334033 total: 7.02s remaining: 2.92s 706: learn: 0.0333730 total: 7.03s remaining: 2.91s 707: learn: 0.0333514 total: 7.04s remaining: 2.9s 708: learn: 0.0333141 total: 7.05s remaining: 2.89s 709: learn: 0.0332907 total: 7.06s remaining: 2.88s 710: learn: 0.0332709 total: 7.07s remaining: 2.87s 711: learn: 0.0332554 total: 7.08s remaining: 2.86s 712: learn: 0.0332295 total: 7.09s remaining: 2.85s 713: learn: 0.0332058 total: 7.1s remaining: 2.84s 714: learn: 0.0331825 total: 7.11s remaining: 2.83s 715: learn: 0.0331741 total: 7.12s remaining: 2.82s 716: learn: 0.0331489 total: 7.13s remaining: 2.81s 717: learn: 0.0331237 total: 7.13s remaining: 2.8s 718: learn: 0.0331134 total: 7.14s remaining: 2.79s 719: learn: 0.0331022 total: 7.15s remaining: 2.78s 720: learn: 0.0330711 total: 7.16s remaining: 2.77s 721: learn: 0.0330591 total: 7.17s remaining: 2.76s 722: learn: 0.0330342 total: 7.19s remaining: 2.75s 723: learn: 0.0330145 total: 7.2s remaining: 2.75s 724: learn: 0.0330131 total: 7.22s remaining: 2.74s 725: learn: 0.0329839 total: 7.23s remaining: 2.73s 726: learn: 0.0329573 total: 7.24s remaining: 2.72s 727: learn: 0.0329214 total: 7.25s remaining: 2.71s 728: learn: 0.0328843 total: 7.26s remaining: 2.7s 729: learn: 0.0328782 total: 7.27s remaining: 2.69s 730: learn: 0.0328524 total: 7.27s remaining: 2.68s 731: learn: 0.0328334 total: 7.28s remaining: 2.67s 732: learn: 0.0328312 total: 7.29s remaining: 2.66s 733: learn: 0.0328286 total: 7.3s remaining: 2.65s 734: learn: 0.0328050 total: 7.31s remaining: 2.63s 735: learn: 0.0327809 total: 7.32s remaining: 2.63s 736: learn: 0.0327687 total: 7.33s remaining: 2.61s 737: learn: 0.0327439 total: 7.33s remaining: 2.6s 738: learn: 0.0327046 total: 7.34s remaining: 2.59s 739: learn: 0.0326576 total: 7.35s remaining: 2.58s 740: learn: 0.0326221 total: 7.36s remaining: 2.57s 741: learn: 0.0325966 total: 7.37s remaining: 2.56s 742: learn: 0.0325649 total: 7.38s remaining: 2.55s 743: learn: 0.0325636 total: 7.39s remaining: 2.54s 744: learn: 0.0325282 total: 7.4s remaining: 2.53s 745: learn: 0.0324981 total: 7.42s remaining: 2.53s 746: learn: 0.0324739 total: 7.43s remaining: 2.52s 747: learn: 0.0324729 total: 7.44s remaining: 2.51s 748: learn: 0.0324368 total: 7.45s remaining: 2.5s 749: learn: 0.0324044 total: 7.46s remaining: 2.49s 750: learn: 0.0323762 total: 7.47s remaining: 2.48s 751: learn: 0.0323497 total: 7.48s remaining: 2.47s 752: learn: 0.0323229 total: 7.49s remaining: 2.46s 753: learn: 0.0322898 total: 7.5s remaining: 2.45s 754: learn: 0.0322639 total: 7.51s remaining: 2.44s 755: learn: 0.0322376 total: 7.52s remaining: 2.43s 756: learn: 0.0322166 total: 7.53s remaining: 2.42s 757: learn: 0.0321879 total: 7.54s remaining: 2.41s 758: learn: 0.0321871 total: 7.55s remaining: 2.4s 759: learn: 0.0321632 total: 7.56s remaining: 2.39s 760: learn: 0.0321473 total: 7.57s remaining: 2.38s 761: learn: 0.0321181 total: 7.58s remaining: 2.37s 762: learn: 0.0320869 total: 7.59s remaining: 2.36s 763: learn: 0.0320681 total: 7.6s remaining: 2.35s 764: learn: 0.0320479 total: 7.61s remaining: 2.34s 765: learn: 0.0320167 total: 7.62s remaining: 2.33s 766: learn: 0.0319844 total: 7.64s remaining: 2.32s 767: learn: 0.0319639 total: 7.65s remaining: 2.31s 768: learn: 0.0319626 total: 7.66s remaining: 2.3s 769: learn: 0.0319384 total: 7.67s remaining: 2.29s 770: learn: 0.0319149 total: 7.68s remaining: 2.28s 771: learn: 0.0318898 total: 7.69s remaining: 2.27s 772: learn: 0.0318663 total: 7.7s remaining: 2.26s 773: learn: 0.0318475 total: 7.71s remaining: 2.25s 774: learn: 0.0318346 total: 7.71s remaining: 2.24s 775: learn: 0.0318064 total: 7.72s remaining: 2.23s 776: learn: 0.0317739 total: 7.73s remaining: 2.22s 777: learn: 0.0317506 total: 7.76s remaining: 2.21s 778: learn: 0.0317364 total: 7.77s remaining: 2.2s 779: learn: 0.0317206 total: 7.78s remaining: 2.19s 780: learn: 0.0317036 total: 7.79s remaining: 2.18s 781: learn: 0.0316821 total: 7.8s remaining: 2.17s 782: learn: 0.0316539 total: 7.8s remaining: 2.16s 783: learn: 0.0316325 total: 7.82s remaining: 2.15s 784: learn: 0.0316169 total: 7.83s remaining: 2.15s 785: learn: 0.0315866 total: 7.84s remaining: 2.14s 786: learn: 0.0315568 total: 7.86s remaining: 2.13s 787: learn: 0.0315423 total: 7.87s remaining: 2.12s 788: learn: 0.0315296 total: 7.87s remaining: 2.1s 789: learn: 0.0315069 total: 7.88s remaining: 2.1s 790: learn: 0.0314766 total: 7.89s remaining: 2.08s 791: learn: 0.0314398 total: 7.9s remaining: 2.08s 792: learn: 0.0314062 total: 7.91s remaining: 2.06s 793: learn: 0.0313834 total: 7.92s remaining: 2.06s 794: learn: 0.0313501 total: 7.93s remaining: 2.04s 795: learn: 0.0313236 total: 7.94s remaining: 2.03s 796: learn: 0.0312897 total: 7.95s remaining: 2.02s 797: learn: 0.0312611 total: 7.96s remaining: 2.02s 798: learn: 0.0312425 total: 7.97s remaining: 2.01s 799: learn: 0.0312191 total: 7.98s remaining: 2s 800: learn: 0.0311942 total: 7.99s remaining: 1.99s 801: learn: 0.0311828 total: 8s remaining: 1.98s 802: learn: 0.0311497 total: 8.02s remaining: 1.97s 803: learn: 0.0311325 total: 8.03s remaining: 1.96s 804: learn: 0.0311096 total: 8.04s remaining: 1.95s 805: learn: 0.0310990 total: 8.05s remaining: 1.94s 806: learn: 0.0310779 total: 8.06s remaining: 1.93s 807: learn: 0.0310373 total: 8.07s remaining: 1.92s 808: learn: 0.0309983 total: 8.07s remaining: 1.91s 809: learn: 0.0309577 total: 8.08s remaining: 1.9s 810: learn: 0.0309266 total: 8.09s remaining: 1.89s 811: learn: 0.0309151 total: 8.1s remaining: 1.88s 812: learn: 0.0309140 total: 8.11s remaining: 1.86s 813: learn: 0.0309044 total: 8.12s remaining: 1.85s 814: learn: 0.0308941 total: 8.13s remaining: 1.84s 815: learn: 0.0308568 total: 8.14s remaining: 1.83s 816: learn: 0.0308258 total: 8.15s remaining: 1.82s 817: learn: 0.0308028 total: 8.15s remaining: 1.81s 818: learn: 0.0307831 total: 8.16s remaining: 1.8s 819: learn: 0.0307542 total: 8.17s remaining: 1.79s 820: learn: 0.0307469 total: 8.18s remaining: 1.78s 821: learn: 0.0307132 total: 8.19s remaining: 1.77s 822: learn: 0.0306837 total: 8.2s remaining: 1.76s 823: learn: 0.0306640 total: 8.21s remaining: 1.75s 824: learn: 0.0306571 total: 8.22s remaining: 1.74s 825: learn: 0.0306431 total: 8.24s remaining: 1.74s 826: learn: 0.0306174 total: 8.24s remaining: 1.72s 827: learn: 0.0305983 total: 8.25s remaining: 1.71s 828: learn: 0.0305830 total: 8.26s remaining: 1.7s 829: learn: 0.0305557 total: 8.27s remaining: 1.69s 830: learn: 0.0305199 total: 8.28s remaining: 1.68s 831: learn: 0.0304956 total: 8.29s remaining: 1.67s 832: learn: 0.0304787 total: 8.3s remaining: 1.66s 833: learn: 0.0304539 total: 8.31s remaining: 1.65s 834: learn: 0.0304166 total: 8.32s remaining: 1.64s 835: learn: 0.0303880 total: 8.32s remaining: 1.63s 836: learn: 0.0303625 total: 8.33s remaining: 1.62s 837: learn: 0.0303489 total: 8.34s remaining: 1.61s 838: learn: 0.0303292 total: 8.35s remaining: 1.6s 839: learn: 0.0303053 total: 8.36s remaining: 1.59s 840: learn: 0.0302897 total: 8.37s remaining: 1.58s 841: learn: 0.0302700 total: 8.38s remaining: 1.57s 842: learn: 0.0302438 total: 8.39s remaining: 1.56s 843: learn: 0.0302227 total: 8.4s remaining: 1.55s 844: learn: 0.0302048 total: 8.41s remaining: 1.54s 845: learn: 0.0301746 total: 8.42s remaining: 1.53s 846: learn: 0.0301520 total: 8.43s remaining: 1.52s 847: learn: 0.0301509 total: 8.44s remaining: 1.51s 848: learn: 0.0301471 total: 8.45s remaining: 1.5s 849: learn: 0.0301201 total: 8.46s remaining: 1.49s 850: learn: 0.0300886 total: 8.47s remaining: 1.48s 851: learn: 0.0300792 total: 8.48s remaining: 1.47s 852: learn: 0.0300662 total: 8.5s remaining: 1.46s 853: learn: 0.0300604 total: 8.52s remaining: 1.46s 854: learn: 0.0300436 total: 8.53s remaining: 1.45s 855: learn: 0.0300185 total: 8.54s remaining: 1.44s 856: learn: 0.0300083 total: 8.55s remaining: 1.43s 857: learn: 0.0299948 total: 8.56s remaining: 1.42s 858: learn: 0.0299934 total: 8.57s remaining: 1.41s 859: learn: 0.0299687 total: 8.58s remaining: 1.4s 860: learn: 0.0299636 total: 8.59s remaining: 1.39s 861: learn: 0.0299375 total: 8.6s remaining: 1.38s 862: learn: 0.0299048 total: 8.61s remaining: 1.37s 863: learn: 0.0298838 total: 8.63s remaining: 1.36s 864: learn: 0.0298640 total: 8.64s remaining: 1.35s 865: learn: 0.0298317 total: 8.65s remaining: 1.34s 866: learn: 0.0298135 total: 8.66s remaining: 1.33s 867: learn: 0.0297960 total: 8.67s remaining: 1.32s 868: learn: 0.0297668 total: 8.68s remaining: 1.31s 869: learn: 0.0297436 total: 8.69s remaining: 1.3s 870: learn: 0.0297423 total: 8.7s remaining: 1.29s 871: learn: 0.0297146 total: 8.71s remaining: 1.28s 872: learn: 0.0296828 total: 8.73s remaining: 1.27s 873: learn: 0.0296510 total: 8.74s remaining: 1.26s 874: learn: 0.0296225 total: 8.75s remaining: 1.25s 875: learn: 0.0296217 total: 8.76s remaining: 1.24s 876: learn: 0.0296197 total: 8.77s remaining: 1.23s 877: learn: 0.0296038 total: 8.78s remaining: 1.22s 878: learn: 0.0295758 total: 8.79s remaining: 1.21s 879: learn: 0.0295562 total: 8.8s remaining: 1.2s 880: learn: 0.0295350 total: 8.8s remaining: 1.19s 881: learn: 0.0295233 total: 8.81s remaining: 1.18s 882: learn: 0.0295053 total: 8.82s remaining: 1.17s 883: learn: 0.0294805 total: 8.84s remaining: 1.16s 884: learn: 0.0294437 total: 8.86s remaining: 1.15s 885: learn: 0.0294429 total: 8.87s remaining: 1.14s 886: learn: 0.0294157 total: 8.88s remaining: 1.13s 887: learn: 0.0294142 total: 8.89s remaining: 1.12s 888: learn: 0.0293964 total: 8.9s remaining: 1.11s 889: learn: 0.0293681 total: 8.91s remaining: 1.1s 890: learn: 0.0293429 total: 8.92s remaining: 1.09s 891: learn: 0.0293119 total: 8.93s remaining: 1.08s 892: learn: 0.0292803 total: 8.94s remaining: 1.07s 893: learn: 0.0292788 total: 8.95s remaining: 1.06s 894: learn: 0.0292561 total: 8.96s remaining: 1.05s 895: learn: 0.0292380 total: 8.97s remaining: 1.04s 896: learn: 0.0292161 total: 8.98s remaining: 1.03s 897: learn: 0.0292153 total: 8.99s remaining: 1.02s 898: learn: 0.0291978 total: 8.99s remaining: 1.01s 899: learn: 0.0291840 total: 9s remaining: 1s 900: learn: 0.0291639 total: 9.01s remaining: 991ms 901: learn: 0.0291584 total: 9.02s remaining: 980ms 902: learn: 0.0291350 total: 9.03s remaining: 970ms 903: learn: 0.0291342 total: 9.04s remaining: 960ms 904: learn: 0.0290989 total: 9.05s remaining: 950ms 905: learn: 0.0290821 total: 9.07s remaining: 941ms 906: learn: 0.0290633 total: 9.08s remaining: 931ms 907: learn: 0.0290464 total: 9.09s remaining: 921ms 908: learn: 0.0290230 total: 9.1s remaining: 911ms 909: learn: 0.0290120 total: 9.11s remaining: 901ms 910: learn: 0.0289849 total: 9.12s remaining: 891ms 911: learn: 0.0289676 total: 9.13s remaining: 881ms 912: learn: 0.0289548 total: 9.13s remaining: 870ms 913: learn: 0.0289369 total: 9.14s remaining: 860ms 914: learn: 0.0289361 total: 9.15s remaining: 850ms 915: learn: 0.0289122 total: 9.16s remaining: 840ms 916: learn: 0.0288867 total: 9.17s remaining: 830ms 917: learn: 0.0288686 total: 9.18s remaining: 820ms 918: learn: 0.0288439 total: 9.19s remaining: 810ms 919: learn: 0.0288272 total: 9.2s remaining: 800ms 920: learn: 0.0288091 total: 9.21s remaining: 790ms 921: learn: 0.0287811 total: 9.22s remaining: 780ms 922: learn: 0.0287679 total: 9.22s remaining: 770ms 923: learn: 0.0287371 total: 9.23s remaining: 760ms 924: learn: 0.0287164 total: 9.24s remaining: 750ms 925: learn: 0.0287005 total: 9.26s remaining: 740ms 926: learn: 0.0286763 total: 9.27s remaining: 730ms 927: learn: 0.0286535 total: 9.28s remaining: 720ms 928: learn: 0.0286361 total: 9.29s remaining: 710ms 929: learn: 0.0286329 total: 9.3s remaining: 700ms 930: learn: 0.0286109 total: 9.31s remaining: 690ms 931: learn: 0.0285961 total: 9.32s remaining: 680ms 932: learn: 0.0285950 total: 9.33s remaining: 670ms 933: learn: 0.0285826 total: 9.34s remaining: 660ms 934: learn: 0.0285802 total: 9.35s remaining: 650ms 935: learn: 0.0285447 total: 9.36s remaining: 640ms 936: learn: 0.0285439 total: 9.37s remaining: 630ms 937: learn: 0.0285200 total: 9.38s remaining: 620ms 938: learn: 0.0285015 total: 9.39s remaining: 610ms 939: learn: 0.0284971 total: 9.4s remaining: 600ms 940: learn: 0.0284655 total: 9.4s remaining: 590ms 941: learn: 0.0284462 total: 9.41s remaining: 580ms 942: learn: 0.0284353 total: 9.42s remaining: 570ms 943: learn: 0.0284130 total: 9.43s remaining: 560ms 944: learn: 0.0284122 total: 9.44s remaining: 550ms 945: learn: 0.0283945 total: 9.45s remaining: 539ms 946: learn: 0.0283774 total: 9.46s remaining: 529ms 947: learn: 0.0283513 total: 9.48s remaining: 520ms 948: learn: 0.0283331 total: 9.49s remaining: 510ms 949: learn: 0.0283156 total: 9.5s remaining: 500ms 950: learn: 0.0283149 total: 9.51s remaining: 490ms 951: learn: 0.0282907 total: 9.52s remaining: 480ms 952: learn: 0.0282780 total: 9.53s remaining: 470ms 953: learn: 0.0282483 total: 9.54s remaining: 460ms 954: learn: 0.0282300 total: 9.54s remaining: 450ms 955: learn: 0.0282071 total: 9.55s remaining: 440ms 956: learn: 0.0281847 total: 9.56s remaining: 430ms 957: learn: 0.0281821 total: 9.57s remaining: 420ms 958: learn: 0.0281598 total: 9.58s remaining: 410ms 959: learn: 0.0281295 total: 9.59s remaining: 400ms 960: learn: 0.0281288 total: 9.6s remaining: 390ms 961: learn: 0.0281081 total: 9.61s remaining: 380ms 962: learn: 0.0280894 total: 9.62s remaining: 370ms 963: learn: 0.0280686 total: 9.63s remaining: 360ms 964: learn: 0.0280555 total: 9.64s remaining: 350ms 965: learn: 0.0280239 total: 9.65s remaining: 340ms 966: learn: 0.0280043 total: 9.66s remaining: 330ms 967: learn: 0.0279981 total: 9.67s remaining: 320ms 968: learn: 0.0279733 total: 9.68s remaining: 310ms 969: learn: 0.0279519 total: 9.7s remaining: 300ms 970: learn: 0.0279300 total: 9.71s remaining: 290ms 971: learn: 0.0279293 total: 9.72s remaining: 280ms 972: learn: 0.0279250 total: 9.73s remaining: 270ms 973: learn: 0.0279063 total: 9.74s remaining: 260ms 974: learn: 0.0279018 total: 9.75s remaining: 250ms 975: learn: 0.0278898 total: 9.76s remaining: 240ms 976: learn: 0.0278848 total: 9.76s remaining: 230ms 977: learn: 0.0278704 total: 9.77s remaining: 220ms 978: learn: 0.0278666 total: 9.78s remaining: 210ms 979: learn: 0.0278659 total: 9.79s remaining: 200ms 980: learn: 0.0278526 total: 9.8s remaining: 190ms 981: learn: 0.0278335 total: 9.81s remaining: 180ms 982: learn: 0.0278183 total: 9.82s remaining: 170ms 983: learn: 0.0278015 total: 9.83s remaining: 160ms 984: learn: 0.0277901 total: 9.84s remaining: 150ms 985: learn: 0.0277721 total: 9.85s remaining: 140ms 986: learn: 0.0277654 total: 9.86s remaining: 130ms 987: learn: 0.0277375 total: 9.87s remaining: 120ms 988: learn: 0.0277038 total: 9.88s remaining: 110ms 989: learn: 0.0276805 total: 9.9s remaining: 100ms 990: learn: 0.0276794 total: 9.91s remaining: 90ms 991: learn: 0.0276769 total: 9.91s remaining: 80ms 992: learn: 0.0276606 total: 9.93s remaining: 70ms 993: learn: 0.0276488 total: 9.94s remaining: 60ms 994: learn: 0.0276430 total: 9.95s remaining: 50ms 995: learn: 0.0276244 total: 9.95s remaining: 40ms 996: learn: 0.0276067 total: 9.96s remaining: 30ms 997: learn: 0.0275969 total: 9.97s remaining: 20ms 998: learn: 0.0275751 total: 9.98s remaining: 9.99ms 999: learn: 0.0275571 total: 9.99s remaining: 0us Learning rate set to 0.046383 0: learn: 0.3978027 total: 8.14ms remaining: 8.13s 1: learn: 0.3856309 total: 16.2ms remaining: 8.11s 2: learn: 0.3736411 total: 24.2ms remaining: 8.04s 3: learn: 0.3628654 total: 32.2ms remaining: 8.03s 4: learn: 0.3513531 total: 43ms remaining: 8.56s 5: learn: 0.3406106 total: 53ms remaining: 8.77s 6: learn: 0.3309112 total: 61ms remaining: 8.66s 7: learn: 0.3218435 total: 69.2ms remaining: 8.58s 8: learn: 0.3127239 total: 76.9ms remaining: 8.47s 9: learn: 0.3038344 total: 84.6ms remaining: 8.38s 10: learn: 0.2958913 total: 93ms remaining: 8.37s 11: learn: 0.2878396 total: 101ms remaining: 8.33s 12: learn: 0.2800094 total: 109ms remaining: 8.27s 13: learn: 0.2729406 total: 117ms remaining: 8.22s 14: learn: 0.2660320 total: 125ms remaining: 8.21s 15: learn: 0.2596497 total: 133ms remaining: 8.21s 16: learn: 0.2533620 total: 142ms remaining: 8.2s 17: learn: 0.2475873 total: 150ms remaining: 8.16s 18: learn: 0.2420137 total: 158ms remaining: 8.15s 19: learn: 0.2364475 total: 165ms remaining: 8.11s 20: learn: 0.2310151 total: 174ms remaining: 8.09s 21: learn: 0.2262713 total: 182ms remaining: 8.08s 22: learn: 0.2214571 total: 193ms remaining: 8.18s 23: learn: 0.2169928 total: 217ms remaining: 8.83s 24: learn: 0.2128096 total: 237ms remaining: 9.22s 25: learn: 0.2085066 total: 245ms remaining: 9.18s 26: learn: 0.2048417 total: 257ms remaining: 9.28s 27: learn: 0.2010391 total: 266ms remaining: 9.25s 28: learn: 0.1975640 total: 275ms remaining: 9.21s 29: learn: 0.1938161 total: 284ms remaining: 9.18s 30: learn: 0.1903469 total: 293ms remaining: 9.16s 31: learn: 0.1874091 total: 303ms remaining: 9.17s 32: learn: 0.1845234 total: 312ms remaining: 9.13s 33: learn: 0.1816245 total: 320ms remaining: 9.1s 34: learn: 0.1786122 total: 328ms remaining: 9.06s 35: learn: 0.1759695 total: 337ms remaining: 9.01s 36: learn: 0.1734475 total: 345ms remaining: 8.98s 37: learn: 0.1711627 total: 353ms remaining: 8.94s 38: learn: 0.1687270 total: 361ms remaining: 8.9s 39: learn: 0.1665850 total: 369ms remaining: 8.86s 40: learn: 0.1644942 total: 377ms remaining: 8.83s 41: learn: 0.1626747 total: 386ms remaining: 8.79s 42: learn: 0.1605346 total: 397ms remaining: 8.83s 43: learn: 0.1588621 total: 414ms remaining: 9s 44: learn: 0.1572605 total: 426ms remaining: 9.03s 45: learn: 0.1555272 total: 433ms remaining: 8.99s 46: learn: 0.1539929 total: 441ms remaining: 8.94s 47: learn: 0.1524642 total: 449ms remaining: 8.9s 48: learn: 0.1510802 total: 456ms remaining: 8.86s 49: learn: 0.1497354 total: 464ms remaining: 8.82s 50: learn: 0.1483050 total: 472ms remaining: 8.79s 51: learn: 0.1469238 total: 480ms remaining: 8.76s 52: learn: 0.1454729 total: 488ms remaining: 8.72s 53: learn: 0.1444121 total: 497ms remaining: 8.71s 54: learn: 0.1431429 total: 505ms remaining: 8.68s 55: learn: 0.1421017 total: 513ms remaining: 8.64s 56: learn: 0.1409155 total: 521ms remaining: 8.61s 57: learn: 0.1396975 total: 528ms remaining: 8.58s 58: learn: 0.1386427 total: 537ms remaining: 8.56s 59: learn: 0.1378437 total: 545ms remaining: 8.53s 60: learn: 0.1368687 total: 552ms remaining: 8.5s 61: learn: 0.1360283 total: 560ms remaining: 8.47s 62: learn: 0.1351414 total: 568ms remaining: 8.45s 63: learn: 0.1342792 total: 576ms remaining: 8.43s 64: learn: 0.1331953 total: 584ms remaining: 8.4s 65: learn: 0.1324761 total: 592ms remaining: 8.38s 66: learn: 0.1316896 total: 600ms remaining: 8.35s 67: learn: 0.1307402 total: 611ms remaining: 8.37s 68: learn: 0.1299608 total: 628ms remaining: 8.47s 69: learn: 0.1291604 total: 641ms remaining: 8.51s 70: learn: 0.1284911 total: 649ms remaining: 8.49s 71: learn: 0.1278145 total: 657ms remaining: 8.47s 72: learn: 0.1272602 total: 665ms remaining: 8.45s 73: learn: 0.1267246 total: 674ms remaining: 8.43s 74: learn: 0.1261868 total: 682ms remaining: 8.41s 75: learn: 0.1254631 total: 690ms remaining: 8.39s 76: learn: 0.1248610 total: 698ms remaining: 8.37s 77: learn: 0.1242221 total: 707ms remaining: 8.36s 78: learn: 0.1237245 total: 717ms remaining: 8.36s 79: learn: 0.1232068 total: 727ms remaining: 8.36s 80: learn: 0.1225944 total: 735ms remaining: 8.34s 81: learn: 0.1223052 total: 743ms remaining: 8.32s 82: learn: 0.1216958 total: 752ms remaining: 8.3s 83: learn: 0.1211472 total: 760ms remaining: 8.28s 84: learn: 0.1206933 total: 768ms remaining: 8.26s 85: learn: 0.1201908 total: 776ms remaining: 8.24s 86: learn: 0.1198247 total: 784ms remaining: 8.23s 87: learn: 0.1192570 total: 792ms remaining: 8.21s 88: learn: 0.1187077 total: 800ms remaining: 8.19s 89: learn: 0.1182343 total: 808ms remaining: 8.17s 90: learn: 0.1178149 total: 822ms remaining: 8.21s 91: learn: 0.1174391 total: 831ms remaining: 8.2s 92: learn: 0.1170273 total: 838ms remaining: 8.18s 93: learn: 0.1168476 total: 846ms remaining: 8.15s 94: learn: 0.1165291 total: 853ms remaining: 8.13s 95: learn: 0.1161593 total: 861ms remaining: 8.11s 96: learn: 0.1158412 total: 869ms remaining: 8.09s 97: learn: 0.1155680 total: 876ms remaining: 8.07s 98: learn: 0.1151994 total: 884ms remaining: 8.04s 99: learn: 0.1148023 total: 892ms remaining: 8.03s 100: learn: 0.1145456 total: 900ms remaining: 8.01s 101: learn: 0.1143716 total: 908ms remaining: 7.99s 102: learn: 0.1140343 total: 915ms remaining: 7.97s 103: learn: 0.1136609 total: 923ms remaining: 7.95s 104: learn: 0.1134191 total: 931ms remaining: 7.93s 105: learn: 0.1130602 total: 939ms remaining: 7.92s 106: learn: 0.1128489 total: 947ms remaining: 7.9s 107: learn: 0.1126029 total: 955ms remaining: 7.88s 108: learn: 0.1123781 total: 962ms remaining: 7.87s 109: learn: 0.1120421 total: 970ms remaining: 7.85s 110: learn: 0.1117445 total: 978ms remaining: 7.83s 111: learn: 0.1114484 total: 986ms remaining: 7.82s 112: learn: 0.1113817 total: 993ms remaining: 7.8s 113: learn: 0.1111449 total: 1s remaining: 7.78s 114: learn: 0.1108926 total: 1.01s remaining: 7.76s 115: learn: 0.1105525 total: 1.03s remaining: 7.82s 116: learn: 0.1102119 total: 1.04s remaining: 7.82s 117: learn: 0.1099669 total: 1.04s remaining: 7.81s 118: learn: 0.1097029 total: 1.05s remaining: 7.79s 119: learn: 0.1094791 total: 1.06s remaining: 7.78s 120: learn: 0.1091944 total: 1.07s remaining: 7.79s 121: learn: 0.1090129 total: 1.08s remaining: 7.77s 122: learn: 0.1088086 total: 1.09s remaining: 7.76s 123: learn: 0.1085872 total: 1.1s remaining: 7.74s 124: learn: 0.1083479 total: 1.1s remaining: 7.73s 125: learn: 0.1081507 total: 1.11s remaining: 7.71s 126: learn: 0.1079681 total: 1.12s remaining: 7.7s 127: learn: 0.1078701 total: 1.13s remaining: 7.68s 128: learn: 0.1076274 total: 1.14s remaining: 7.73s 129: learn: 0.1074181 total: 1.15s remaining: 7.72s 130: learn: 0.1071886 total: 1.16s remaining: 7.71s 131: learn: 0.1069418 total: 1.17s remaining: 7.69s 132: learn: 0.1066931 total: 1.18s remaining: 7.68s 133: learn: 0.1065336 total: 1.19s remaining: 7.66s 134: learn: 0.1063466 total: 1.2s remaining: 7.69s 135: learn: 0.1062856 total: 1.21s remaining: 7.7s 136: learn: 0.1060653 total: 1.23s remaining: 7.73s 137: learn: 0.1058403 total: 1.24s remaining: 7.73s 138: learn: 0.1056683 total: 1.25s remaining: 7.72s 139: learn: 0.1054479 total: 1.25s remaining: 7.71s 140: learn: 0.1052291 total: 1.26s remaining: 7.7s 141: learn: 0.1050491 total: 1.27s remaining: 7.69s 142: learn: 0.1048226 total: 1.28s remaining: 7.67s 143: learn: 0.1045996 total: 1.29s remaining: 7.66s 144: learn: 0.1045206 total: 1.3s remaining: 7.64s 145: learn: 0.1043602 total: 1.3s remaining: 7.63s 146: learn: 0.1041123 total: 1.31s remaining: 7.62s 147: learn: 0.1038654 total: 1.32s remaining: 7.6s 148: learn: 0.1036972 total: 1.33s remaining: 7.59s 149: learn: 0.1035315 total: 1.34s remaining: 7.57s 150: learn: 0.1033568 total: 1.34s remaining: 7.56s 151: learn: 0.1032968 total: 1.35s remaining: 7.56s 152: learn: 0.1030904 total: 1.36s remaining: 7.55s 153: learn: 0.1030498 total: 1.37s remaining: 7.54s 154: learn: 0.1027980 total: 1.38s remaining: 7.52s 155: learn: 0.1026547 total: 1.39s remaining: 7.51s 156: learn: 0.1025361 total: 1.4s remaining: 7.49s 157: learn: 0.1024683 total: 1.4s remaining: 7.48s 158: learn: 0.1022966 total: 1.41s remaining: 7.46s 159: learn: 0.1021109 total: 1.42s remaining: 7.45s 160: learn: 0.1018792 total: 1.44s remaining: 7.49s 161: learn: 0.1016665 total: 1.45s remaining: 7.49s 162: learn: 0.1015624 total: 1.46s remaining: 7.48s 163: learn: 0.1014734 total: 1.46s remaining: 7.46s 164: learn: 0.1012674 total: 1.47s remaining: 7.45s 165: learn: 0.1011661 total: 1.48s remaining: 7.44s 166: learn: 0.1011041 total: 1.49s remaining: 7.42s 167: learn: 0.1010612 total: 1.5s remaining: 7.41s 168: learn: 0.1009099 total: 1.5s remaining: 7.39s 169: learn: 0.1006599 total: 1.51s remaining: 7.38s 170: learn: 0.1006253 total: 1.52s remaining: 7.37s 171: learn: 0.1005032 total: 1.53s remaining: 7.35s 172: learn: 0.1002798 total: 1.53s remaining: 7.34s 173: learn: 0.1002329 total: 1.54s remaining: 7.33s 174: learn: 0.1000527 total: 1.55s remaining: 7.31s 175: learn: 0.0999097 total: 1.56s remaining: 7.3s 176: learn: 0.0998381 total: 1.57s remaining: 7.28s 177: learn: 0.0995945 total: 1.57s remaining: 7.27s 178: learn: 0.0995448 total: 1.58s remaining: 7.26s 179: learn: 0.0995241 total: 1.59s remaining: 7.25s 180: learn: 0.0993731 total: 1.6s remaining: 7.23s 181: learn: 0.0991983 total: 1.61s remaining: 7.22s 182: learn: 0.0990888 total: 1.61s remaining: 7.21s 183: learn: 0.0988972 total: 1.62s remaining: 7.19s 184: learn: 0.0988489 total: 1.63s remaining: 7.18s 185: learn: 0.0985475 total: 1.65s remaining: 7.21s 186: learn: 0.0985063 total: 1.66s remaining: 7.2s 187: learn: 0.0982940 total: 1.67s remaining: 7.2s 188: learn: 0.0981916 total: 1.67s remaining: 7.18s 189: learn: 0.0980376 total: 1.68s remaining: 7.17s 190: learn: 0.0979642 total: 1.69s remaining: 7.15s 191: learn: 0.0977845 total: 1.7s remaining: 7.14s 192: learn: 0.0977514 total: 1.7s remaining: 7.13s 193: learn: 0.0977198 total: 1.71s remaining: 7.11s 194: learn: 0.0975312 total: 1.72s remaining: 7.1s 195: learn: 0.0974156 total: 1.73s remaining: 7.08s 196: learn: 0.0972283 total: 1.73s remaining: 7.07s 197: learn: 0.0970436 total: 1.74s remaining: 7.06s 198: learn: 0.0969742 total: 1.75s remaining: 7.06s 199: learn: 0.0969458 total: 1.76s remaining: 7.05s 200: learn: 0.0967784 total: 1.77s remaining: 7.04s 201: learn: 0.0967551 total: 1.78s remaining: 7.02s 202: learn: 0.0967274 total: 1.78s remaining: 7.01s 203: learn: 0.0966991 total: 1.79s remaining: 7s 204: learn: 0.0966686 total: 1.8s remaining: 6.99s 205: learn: 0.0965095 total: 1.81s remaining: 6.97s 206: learn: 0.0963435 total: 1.82s remaining: 6.96s 207: learn: 0.0962673 total: 1.82s remaining: 6.95s 208: learn: 0.0960689 total: 1.83s remaining: 6.94s 209: learn: 0.0959334 total: 1.85s remaining: 6.97s 210: learn: 0.0957435 total: 1.86s remaining: 6.96s 211: learn: 0.0955840 total: 1.87s remaining: 6.94s 212: learn: 0.0955407 total: 1.88s remaining: 6.93s 213: learn: 0.0955164 total: 1.88s remaining: 6.92s 214: learn: 0.0953190 total: 1.89s remaining: 6.9s 215: learn: 0.0951511 total: 1.9s remaining: 6.89s 216: learn: 0.0950566 total: 1.91s remaining: 6.88s 217: learn: 0.0949041 total: 1.92s remaining: 6.87s 218: learn: 0.0947437 total: 1.92s remaining: 6.86s 219: learn: 0.0945598 total: 1.93s remaining: 6.85s 220: learn: 0.0943308 total: 1.94s remaining: 6.85s 221: learn: 0.0941814 total: 1.95s remaining: 6.84s 222: learn: 0.0939751 total: 1.96s remaining: 6.83s 223: learn: 0.0938040 total: 1.97s remaining: 6.82s 224: learn: 0.0936921 total: 1.98s remaining: 6.81s 225: learn: 0.0936132 total: 1.99s remaining: 6.8s 226: learn: 0.0935899 total: 2s remaining: 6.79s 227: learn: 0.0935662 total: 2s remaining: 6.78s 228: learn: 0.0933598 total: 2.01s remaining: 6.77s 229: learn: 0.0932536 total: 2.02s remaining: 6.76s 230: learn: 0.0930998 total: 2.03s remaining: 6.75s 231: learn: 0.0929240 total: 2.04s remaining: 6.74s 232: learn: 0.0927597 total: 2.05s remaining: 6.74s 233: learn: 0.0927372 total: 2.06s remaining: 6.75s 234: learn: 0.0925808 total: 2.07s remaining: 6.74s 235: learn: 0.0924362 total: 2.08s remaining: 6.73s 236: learn: 0.0922884 total: 2.09s remaining: 6.72s 237: learn: 0.0921612 total: 2.09s remaining: 6.7s 238: learn: 0.0919999 total: 2.1s remaining: 6.69s 239: learn: 0.0919781 total: 2.11s remaining: 6.68s 240: learn: 0.0919569 total: 2.12s remaining: 6.66s 241: learn: 0.0919363 total: 2.12s remaining: 6.65s 242: learn: 0.0918049 total: 2.13s remaining: 6.64s 243: learn: 0.0916453 total: 2.14s remaining: 6.63s 244: learn: 0.0916206 total: 2.15s remaining: 6.62s 245: learn: 0.0916012 total: 2.15s remaining: 6.6s 246: learn: 0.0915823 total: 2.17s remaining: 6.6s 247: learn: 0.0914340 total: 2.18s remaining: 6.61s 248: learn: 0.0912973 total: 2.19s remaining: 6.6s 249: learn: 0.0912789 total: 2.19s remaining: 6.58s 250: learn: 0.0911621 total: 2.2s remaining: 6.57s 251: learn: 0.0910079 total: 2.21s remaining: 6.56s 252: learn: 0.0908462 total: 2.22s remaining: 6.55s 253: learn: 0.0907316 total: 2.23s remaining: 6.54s 254: learn: 0.0906219 total: 2.23s remaining: 6.53s 255: learn: 0.0905035 total: 2.25s remaining: 6.54s 256: learn: 0.0904864 total: 2.26s remaining: 6.53s 257: learn: 0.0903886 total: 2.27s remaining: 6.52s 258: learn: 0.0902606 total: 2.28s remaining: 6.51s 259: learn: 0.0900968 total: 2.29s remaining: 6.51s 260: learn: 0.0899455 total: 2.3s remaining: 6.5s 261: learn: 0.0898226 total: 2.31s remaining: 6.5s 262: learn: 0.0898055 total: 2.31s remaining: 6.49s 263: learn: 0.0897888 total: 2.32s remaining: 6.48s 264: learn: 0.0897727 total: 2.33s remaining: 6.47s 265: learn: 0.0896253 total: 2.34s remaining: 6.46s 266: learn: 0.0894987 total: 2.35s remaining: 6.45s 267: learn: 0.0893605 total: 2.36s remaining: 6.44s 268: learn: 0.0892200 total: 2.37s remaining: 6.43s 269: learn: 0.0892044 total: 2.37s remaining: 6.42s 270: learn: 0.0891892 total: 2.38s remaining: 6.4s 271: learn: 0.0890583 total: 2.39s remaining: 6.39s 272: learn: 0.0889351 total: 2.4s remaining: 6.38s 273: learn: 0.0888553 total: 2.4s remaining: 6.37s 274: learn: 0.0887289 total: 2.41s remaining: 6.37s 275: learn: 0.0886168 total: 2.42s remaining: 6.36s 276: learn: 0.0885203 total: 2.43s remaining: 6.35s 277: learn: 0.0883897 total: 2.44s remaining: 6.34s 278: learn: 0.0882513 total: 2.46s remaining: 6.35s 279: learn: 0.0881995 total: 2.46s remaining: 6.34s 280: learn: 0.0880607 total: 2.47s remaining: 6.33s 281: learn: 0.0879459 total: 2.48s remaining: 6.32s 282: learn: 0.0878425 total: 2.49s remaining: 6.31s 283: learn: 0.0877197 total: 2.5s remaining: 6.3s 284: learn: 0.0875971 total: 2.51s remaining: 6.29s 285: learn: 0.0874960 total: 2.51s remaining: 6.28s 286: learn: 0.0873851 total: 2.52s remaining: 6.26s 287: learn: 0.0872368 total: 2.53s remaining: 6.25s 288: learn: 0.0871324 total: 2.54s remaining: 6.25s 289: learn: 0.0869987 total: 2.55s remaining: 6.24s 290: learn: 0.0868441 total: 2.56s remaining: 6.23s 291: learn: 0.0867579 total: 2.56s remaining: 6.22s 292: learn: 0.0866699 total: 2.57s remaining: 6.21s 293: learn: 0.0866227 total: 2.58s remaining: 6.2s 294: learn: 0.0864797 total: 2.59s remaining: 6.19s 295: learn: 0.0863743 total: 2.6s remaining: 6.18s 296: learn: 0.0862771 total: 2.6s remaining: 6.17s 297: learn: 0.0861929 total: 2.61s remaining: 6.16s 298: learn: 0.0860733 total: 2.62s remaining: 6.15s 299: learn: 0.0859250 total: 2.63s remaining: 6.14s 300: learn: 0.0857860 total: 2.64s remaining: 6.13s 301: learn: 0.0857156 total: 2.65s remaining: 6.12s 302: learn: 0.0856082 total: 2.67s remaining: 6.14s 303: learn: 0.0854232 total: 2.68s remaining: 6.13s 304: learn: 0.0853356 total: 2.69s remaining: 6.12s 305: learn: 0.0851978 total: 2.69s remaining: 6.11s 306: learn: 0.0851649 total: 2.7s remaining: 6.09s 307: learn: 0.0850685 total: 2.71s remaining: 6.08s 308: learn: 0.0849696 total: 2.71s remaining: 6.07s 309: learn: 0.0848508 total: 2.72s remaining: 6.06s 310: learn: 0.0847811 total: 2.73s remaining: 6.05s 311: learn: 0.0846840 total: 2.74s remaining: 6.04s 312: learn: 0.0845991 total: 2.75s remaining: 6.03s 313: learn: 0.0845757 total: 2.75s remaining: 6.02s 314: learn: 0.0844045 total: 2.76s remaining: 6.01s 315: learn: 0.0842446 total: 2.77s remaining: 6s 316: learn: 0.0842213 total: 2.78s remaining: 5.99s 317: learn: 0.0841134 total: 2.79s remaining: 5.98s 318: learn: 0.0840270 total: 2.8s remaining: 5.98s 319: learn: 0.0839041 total: 2.81s remaining: 5.97s 320: learn: 0.0837815 total: 2.82s remaining: 5.96s 321: learn: 0.0837190 total: 2.82s remaining: 5.95s 322: learn: 0.0836023 total: 2.83s remaining: 5.94s 323: learn: 0.0834958 total: 2.84s remaining: 5.92s 324: learn: 0.0833956 total: 2.85s remaining: 5.92s 325: learn: 0.0833288 total: 2.86s remaining: 5.91s 326: learn: 0.0832267 total: 2.88s remaining: 5.92s 327: learn: 0.0831306 total: 2.89s remaining: 5.92s 328: learn: 0.0830160 total: 2.9s remaining: 5.91s 329: learn: 0.0829382 total: 2.9s remaining: 5.89s 330: learn: 0.0828846 total: 2.91s remaining: 5.88s 331: learn: 0.0827903 total: 2.92s remaining: 5.87s 332: learn: 0.0827247 total: 2.93s remaining: 5.86s 333: learn: 0.0825996 total: 2.94s remaining: 5.85s 334: learn: 0.0825850 total: 2.94s remaining: 5.84s 335: learn: 0.0824930 total: 2.95s remaining: 5.83s 336: learn: 0.0823666 total: 2.96s remaining: 5.82s 337: learn: 0.0822552 total: 2.97s remaining: 5.81s 338: learn: 0.0821883 total: 2.98s remaining: 5.8s 339: learn: 0.0820940 total: 2.98s remaining: 5.79s 340: learn: 0.0820717 total: 3s remaining: 5.79s 341: learn: 0.0819683 total: 3s remaining: 5.78s 342: learn: 0.0818477 total: 3.01s remaining: 5.77s 343: learn: 0.0817168 total: 3.02s remaining: 5.76s 344: learn: 0.0816221 total: 3.03s remaining: 5.75s 345: learn: 0.0814788 total: 3.04s remaining: 5.74s 346: learn: 0.0814326 total: 3.05s remaining: 5.73s 347: learn: 0.0813932 total: 3.05s remaining: 5.72s 348: learn: 0.0812820 total: 3.06s remaining: 5.71s 349: learn: 0.0811653 total: 3.07s remaining: 5.7s 350: learn: 0.0811533 total: 3.09s remaining: 5.71s 351: learn: 0.0810043 total: 3.1s remaining: 5.7s 352: learn: 0.0809585 total: 3.1s remaining: 5.69s 353: learn: 0.0809385 total: 3.11s remaining: 5.68s 354: learn: 0.0808629 total: 3.12s remaining: 5.67s 355: learn: 0.0807610 total: 3.13s remaining: 5.66s 356: learn: 0.0806585 total: 3.14s remaining: 5.65s 357: learn: 0.0805763 total: 3.15s remaining: 5.65s 358: learn: 0.0804641 total: 3.16s remaining: 5.64s 359: learn: 0.0804508 total: 3.17s remaining: 5.63s 360: learn: 0.0804370 total: 3.18s remaining: 5.62s 361: learn: 0.0803357 total: 3.19s remaining: 5.62s 362: learn: 0.0802505 total: 3.19s remaining: 5.6s 363: learn: 0.0801995 total: 3.2s remaining: 5.59s 364: learn: 0.0801141 total: 3.21s remaining: 5.58s 365: learn: 0.0800509 total: 3.22s remaining: 5.57s 366: learn: 0.0799483 total: 3.22s remaining: 5.56s 367: learn: 0.0798682 total: 3.23s remaining: 5.55s 368: learn: 0.0798126 total: 3.24s remaining: 5.54s 369: learn: 0.0797540 total: 3.25s remaining: 5.53s 370: learn: 0.0797421 total: 3.26s remaining: 5.52s 371: learn: 0.0796676 total: 3.27s remaining: 5.51s 372: learn: 0.0795691 total: 3.27s remaining: 5.5s 373: learn: 0.0795548 total: 3.28s remaining: 5.49s 374: learn: 0.0794492 total: 3.3s remaining: 5.5s 375: learn: 0.0793753 total: 3.31s remaining: 5.49s 376: learn: 0.0792906 total: 3.32s remaining: 5.48s 377: learn: 0.0792114 total: 3.32s remaining: 5.47s 378: learn: 0.0790842 total: 3.33s remaining: 5.46s 379: learn: 0.0789767 total: 3.34s remaining: 5.45s 380: learn: 0.0788825 total: 3.35s remaining: 5.44s 381: learn: 0.0788238 total: 3.36s remaining: 5.43s 382: learn: 0.0787998 total: 3.37s remaining: 5.42s 383: learn: 0.0786864 total: 3.37s remaining: 5.41s 384: learn: 0.0785670 total: 3.38s remaining: 5.4s 385: learn: 0.0784928 total: 3.39s remaining: 5.39s 386: learn: 0.0784063 total: 3.4s remaining: 5.38s 387: learn: 0.0782960 total: 3.41s remaining: 5.37s 388: learn: 0.0782578 total: 3.41s remaining: 5.36s 389: learn: 0.0781710 total: 3.42s remaining: 5.35s 390: learn: 0.0780795 total: 3.43s remaining: 5.34s 391: learn: 0.0780168 total: 3.44s remaining: 5.33s 392: learn: 0.0779336 total: 3.44s remaining: 5.32s 393: learn: 0.0778237 total: 3.45s remaining: 5.31s 394: learn: 0.0777625 total: 3.46s remaining: 5.3s 395: learn: 0.0776637 total: 3.47s remaining: 5.29s 396: learn: 0.0776035 total: 3.48s remaining: 5.28s 397: learn: 0.0775196 total: 3.48s remaining: 5.27s 398: learn: 0.0774171 total: 3.5s remaining: 5.27s 399: learn: 0.0773265 total: 3.51s remaining: 5.26s 400: learn: 0.0772744 total: 3.52s remaining: 5.25s 401: learn: 0.0771846 total: 3.52s remaining: 5.24s 402: learn: 0.0771065 total: 3.53s remaining: 5.23s 403: learn: 0.0770946 total: 3.54s remaining: 5.22s 404: learn: 0.0770157 total: 3.56s remaining: 5.22s 405: learn: 0.0769127 total: 3.56s remaining: 5.21s 406: learn: 0.0768629 total: 3.57s remaining: 5.2s 407: learn: 0.0767682 total: 3.58s remaining: 5.2s 408: learn: 0.0767041 total: 3.59s remaining: 5.18s 409: learn: 0.0766559 total: 3.6s remaining: 5.17s 410: learn: 0.0765523 total: 3.6s remaining: 5.17s 411: learn: 0.0764220 total: 3.61s remaining: 5.16s 412: learn: 0.0763455 total: 3.62s remaining: 5.14s 413: learn: 0.0762569 total: 3.63s remaining: 5.13s 414: learn: 0.0761547 total: 3.64s remaining: 5.13s 415: learn: 0.0760954 total: 3.64s remaining: 5.12s 416: learn: 0.0759778 total: 3.65s remaining: 5.11s 417: learn: 0.0758786 total: 3.66s remaining: 5.1s 418: learn: 0.0758157 total: 3.67s remaining: 5.09s 419: learn: 0.0756957 total: 3.68s remaining: 5.08s 420: learn: 0.0756034 total: 3.69s remaining: 5.07s 421: learn: 0.0755477 total: 3.7s remaining: 5.07s 422: learn: 0.0754375 total: 3.71s remaining: 5.07s 423: learn: 0.0753543 total: 3.72s remaining: 5.06s 424: learn: 0.0753178 total: 3.73s remaining: 5.04s 425: learn: 0.0752119 total: 3.74s remaining: 5.04s 426: learn: 0.0751216 total: 3.75s remaining: 5.03s 427: learn: 0.0750611 total: 3.75s remaining: 5.01s 428: learn: 0.0750442 total: 3.76s remaining: 5s 429: learn: 0.0749755 total: 3.77s remaining: 5s 430: learn: 0.0748978 total: 3.78s remaining: 4.99s 431: learn: 0.0747672 total: 3.78s remaining: 4.98s 432: learn: 0.0747302 total: 3.79s remaining: 4.96s 433: learn: 0.0746559 total: 3.8s remaining: 4.96s 434: learn: 0.0745770 total: 3.81s remaining: 4.95s 435: learn: 0.0745173 total: 3.82s remaining: 4.94s 436: learn: 0.0744820 total: 3.82s remaining: 4.93s 437: learn: 0.0743702 total: 3.83s remaining: 4.92s 438: learn: 0.0742694 total: 3.84s remaining: 4.91s 439: learn: 0.0741803 total: 3.85s remaining: 4.9s 440: learn: 0.0741186 total: 3.85s remaining: 4.89s 441: learn: 0.0740540 total: 3.86s remaining: 4.88s 442: learn: 0.0739815 total: 3.87s remaining: 4.87s 443: learn: 0.0739172 total: 3.88s remaining: 4.86s 444: learn: 0.0738088 total: 3.89s remaining: 4.85s 445: learn: 0.0737561 total: 3.89s remaining: 4.84s 446: learn: 0.0736918 total: 3.91s remaining: 4.84s 447: learn: 0.0736582 total: 3.92s remaining: 4.83s 448: learn: 0.0735720 total: 3.93s remaining: 4.82s 449: learn: 0.0735523 total: 3.94s remaining: 4.81s 450: learn: 0.0734736 total: 3.94s remaining: 4.8s 451: learn: 0.0733754 total: 3.96s remaining: 4.8s 452: learn: 0.0732983 total: 3.96s remaining: 4.79s 453: learn: 0.0732416 total: 3.97s remaining: 4.78s 454: learn: 0.0732062 total: 3.98s remaining: 4.77s 455: learn: 0.0731908 total: 3.99s remaining: 4.76s 456: learn: 0.0730994 total: 4s remaining: 4.75s 457: learn: 0.0730420 total: 4s remaining: 4.74s 458: learn: 0.0730269 total: 4.01s remaining: 4.73s 459: learn: 0.0729674 total: 4.02s remaining: 4.72s 460: learn: 0.0729200 total: 4.03s remaining: 4.71s 461: learn: 0.0728184 total: 4.04s remaining: 4.7s 462: learn: 0.0727420 total: 4.04s remaining: 4.69s 463: learn: 0.0727213 total: 4.05s remaining: 4.68s 464: learn: 0.0726632 total: 4.06s remaining: 4.67s 465: learn: 0.0725984 total: 4.07s remaining: 4.66s 466: learn: 0.0725872 total: 4.08s remaining: 4.65s 467: learn: 0.0725760 total: 4.08s remaining: 4.64s 468: learn: 0.0725170 total: 4.09s remaining: 4.63s 469: learn: 0.0724927 total: 4.1s remaining: 4.62s 470: learn: 0.0724794 total: 4.11s remaining: 4.62s 471: learn: 0.0724500 total: 4.14s remaining: 4.63s 472: learn: 0.0724044 total: 4.15s remaining: 4.62s 473: learn: 0.0723373 total: 4.16s remaining: 4.61s 474: learn: 0.0722635 total: 4.16s remaining: 4.6s 475: learn: 0.0721795 total: 4.17s remaining: 4.59s 476: learn: 0.0721253 total: 4.18s remaining: 4.58s 477: learn: 0.0720248 total: 4.19s remaining: 4.57s 478: learn: 0.0720112 total: 4.2s remaining: 4.56s 479: learn: 0.0719551 total: 4.21s remaining: 4.56s 480: learn: 0.0719067 total: 4.21s remaining: 4.55s 481: learn: 0.0718648 total: 4.22s remaining: 4.54s 482: learn: 0.0718273 total: 4.23s remaining: 4.53s 483: learn: 0.0717490 total: 4.24s remaining: 4.52s 484: learn: 0.0716825 total: 4.25s remaining: 4.51s 485: learn: 0.0716029 total: 4.25s remaining: 4.5s 486: learn: 0.0715463 total: 4.26s remaining: 4.49s 487: learn: 0.0714923 total: 4.27s remaining: 4.48s 488: learn: 0.0714714 total: 4.28s remaining: 4.47s 489: learn: 0.0713867 total: 4.29s remaining: 4.46s 490: learn: 0.0713219 total: 4.3s remaining: 4.45s 491: learn: 0.0712879 total: 4.3s remaining: 4.45s 492: learn: 0.0712050 total: 4.31s remaining: 4.43s 493: learn: 0.0711502 total: 4.32s remaining: 4.43s 494: learn: 0.0710907 total: 4.34s remaining: 4.43s 495: learn: 0.0709930 total: 4.35s remaining: 4.42s 496: learn: 0.0709210 total: 4.36s remaining: 4.41s 497: learn: 0.0708313 total: 4.37s remaining: 4.41s 498: learn: 0.0707610 total: 4.38s remaining: 4.4s 499: learn: 0.0707252 total: 4.39s remaining: 4.39s 500: learn: 0.0706590 total: 4.39s remaining: 4.38s 501: learn: 0.0706141 total: 4.4s remaining: 4.37s 502: learn: 0.0705290 total: 4.41s remaining: 4.36s 503: learn: 0.0704766 total: 4.42s remaining: 4.35s 504: learn: 0.0703952 total: 4.43s remaining: 4.34s 505: learn: 0.0703513 total: 4.43s remaining: 4.33s 506: learn: 0.0702562 total: 4.44s remaining: 4.32s 507: learn: 0.0701716 total: 4.45s remaining: 4.31s 508: learn: 0.0700862 total: 4.46s remaining: 4.3s 509: learn: 0.0700516 total: 4.47s remaining: 4.29s 510: learn: 0.0699732 total: 4.47s remaining: 4.28s 511: learn: 0.0699618 total: 4.48s remaining: 4.27s 512: learn: 0.0698908 total: 4.49s remaining: 4.26s 513: learn: 0.0698499 total: 4.5s remaining: 4.25s 514: learn: 0.0697614 total: 4.51s remaining: 4.25s 515: learn: 0.0696819 total: 4.52s remaining: 4.24s 516: learn: 0.0695866 total: 4.53s remaining: 4.23s 517: learn: 0.0695704 total: 4.53s remaining: 4.22s 518: learn: 0.0695246 total: 4.55s remaining: 4.22s 519: learn: 0.0694488 total: 4.56s remaining: 4.21s 520: learn: 0.0693763 total: 4.57s remaining: 4.2s 521: learn: 0.0693652 total: 4.58s remaining: 4.19s 522: learn: 0.0693528 total: 4.58s remaining: 4.18s 523: learn: 0.0692880 total: 4.59s remaining: 4.17s 524: learn: 0.0692311 total: 4.6s remaining: 4.16s 525: learn: 0.0692224 total: 4.61s remaining: 4.15s 526: learn: 0.0691248 total: 4.62s remaining: 4.14s 527: learn: 0.0690595 total: 4.62s remaining: 4.13s 528: learn: 0.0689980 total: 4.63s remaining: 4.12s 529: learn: 0.0689274 total: 4.64s remaining: 4.11s 530: learn: 0.0688593 total: 4.65s remaining: 4.1s 531: learn: 0.0688498 total: 4.65s remaining: 4.09s 532: learn: 0.0687644 total: 4.66s remaining: 4.08s 533: learn: 0.0686778 total: 4.67s remaining: 4.07s 534: learn: 0.0686494 total: 4.68s remaining: 4.07s 535: learn: 0.0685760 total: 4.68s remaining: 4.05s 536: learn: 0.0685010 total: 4.69s remaining: 4.05s 537: learn: 0.0684614 total: 4.7s remaining: 4.04s 538: learn: 0.0683985 total: 4.71s remaining: 4.03s 539: learn: 0.0683181 total: 4.72s remaining: 4.02s 540: learn: 0.0682592 total: 4.72s remaining: 4.01s 541: learn: 0.0681958 total: 4.73s remaining: 4s 542: learn: 0.0681434 total: 4.74s remaining: 3.99s 543: learn: 0.0680812 total: 4.75s remaining: 3.98s 544: learn: 0.0680206 total: 4.77s remaining: 3.98s 545: learn: 0.0680077 total: 4.77s remaining: 3.97s 546: learn: 0.0679151 total: 4.78s remaining: 3.96s 547: learn: 0.0678585 total: 4.79s remaining: 3.95s 548: learn: 0.0678148 total: 4.8s remaining: 3.94s 549: learn: 0.0677492 total: 4.81s remaining: 3.93s 550: learn: 0.0676584 total: 4.82s remaining: 3.92s 551: learn: 0.0675744 total: 4.83s remaining: 3.92s 552: learn: 0.0675552 total: 4.83s remaining: 3.91s 553: learn: 0.0674670 total: 4.84s remaining: 3.9s 554: learn: 0.0673916 total: 4.85s remaining: 3.89s 555: learn: 0.0673134 total: 4.86s remaining: 3.88s 556: learn: 0.0672516 total: 4.86s remaining: 3.87s 557: learn: 0.0671791 total: 4.87s remaining: 3.86s 558: learn: 0.0671433 total: 4.88s remaining: 3.85s 559: learn: 0.0670717 total: 4.89s remaining: 3.84s 560: learn: 0.0670281 total: 4.89s remaining: 3.83s 561: learn: 0.0669983 total: 4.9s remaining: 3.82s 562: learn: 0.0669265 total: 4.91s remaining: 3.81s 563: learn: 0.0668166 total: 4.92s remaining: 3.81s 564: learn: 0.0667147 total: 4.93s remaining: 3.8s 565: learn: 0.0666928 total: 4.94s remaining: 3.79s 566: learn: 0.0665863 total: 4.95s remaining: 3.78s 567: learn: 0.0665258 total: 4.97s remaining: 3.78s 568: learn: 0.0664458 total: 4.98s remaining: 3.77s 569: learn: 0.0663849 total: 4.99s remaining: 3.76s 570: learn: 0.0663236 total: 5s remaining: 3.75s 571: learn: 0.0662468 total: 5s remaining: 3.74s 572: learn: 0.0661815 total: 5.01s remaining: 3.73s 573: learn: 0.0661061 total: 5.02s remaining: 3.72s 574: learn: 0.0660419 total: 5.03s remaining: 3.71s 575: learn: 0.0659804 total: 5.03s remaining: 3.71s 576: learn: 0.0659415 total: 5.04s remaining: 3.69s 577: learn: 0.0659197 total: 5.05s remaining: 3.69s 578: learn: 0.0658445 total: 5.06s remaining: 3.68s 579: learn: 0.0658041 total: 5.06s remaining: 3.67s 580: learn: 0.0657545 total: 5.07s remaining: 3.66s 581: learn: 0.0657465 total: 5.08s remaining: 3.65s 582: learn: 0.0656953 total: 5.09s remaining: 3.64s 583: learn: 0.0656388 total: 5.1s remaining: 3.63s 584: learn: 0.0655622 total: 5.11s remaining: 3.63s 585: learn: 0.0655367 total: 5.12s remaining: 3.62s 586: learn: 0.0654418 total: 5.13s remaining: 3.61s 587: learn: 0.0653898 total: 5.14s remaining: 3.6s 588: learn: 0.0653791 total: 5.14s remaining: 3.59s 589: learn: 0.0653495 total: 5.15s remaining: 3.58s 590: learn: 0.0652782 total: 5.16s remaining: 3.57s 591: learn: 0.0652228 total: 5.18s remaining: 3.57s 592: learn: 0.0651336 total: 5.19s remaining: 3.56s 593: learn: 0.0650743 total: 5.2s remaining: 3.55s 594: learn: 0.0650218 total: 5.2s remaining: 3.54s 595: learn: 0.0649960 total: 5.21s remaining: 3.53s 596: learn: 0.0649429 total: 5.22s remaining: 3.52s 597: learn: 0.0648443 total: 5.23s remaining: 3.51s 598: learn: 0.0647484 total: 5.23s remaining: 3.5s 599: learn: 0.0646856 total: 5.24s remaining: 3.5s 600: learn: 0.0646357 total: 5.25s remaining: 3.49s 601: learn: 0.0645722 total: 5.26s remaining: 3.48s 602: learn: 0.0645312 total: 5.27s remaining: 3.47s 603: learn: 0.0645165 total: 5.27s remaining: 3.46s 604: learn: 0.0644743 total: 5.28s remaining: 3.45s 605: learn: 0.0643972 total: 5.29s remaining: 3.44s 606: learn: 0.0643894 total: 5.3s remaining: 3.43s 607: learn: 0.0643478 total: 5.3s remaining: 3.42s 608: learn: 0.0642772 total: 5.31s remaining: 3.41s 609: learn: 0.0642370 total: 5.32s remaining: 3.4s 610: learn: 0.0641806 total: 5.33s remaining: 3.39s 611: learn: 0.0641303 total: 5.33s remaining: 3.38s 612: learn: 0.0641236 total: 5.34s remaining: 3.37s 613: learn: 0.0641189 total: 5.35s remaining: 3.36s 614: learn: 0.0641076 total: 5.36s remaining: 3.35s 615: learn: 0.0640572 total: 5.37s remaining: 3.35s 616: learn: 0.0639987 total: 5.38s remaining: 3.34s 617: learn: 0.0639430 total: 5.39s remaining: 3.33s 618: learn: 0.0638801 total: 5.4s remaining: 3.32s 619: learn: 0.0638197 total: 5.41s remaining: 3.31s 620: learn: 0.0637419 total: 5.42s remaining: 3.31s 621: learn: 0.0637055 total: 5.42s remaining: 3.29s 622: learn: 0.0636809 total: 5.43s remaining: 3.29s 623: learn: 0.0636125 total: 5.44s remaining: 3.28s 624: learn: 0.0635724 total: 5.45s remaining: 3.27s 625: learn: 0.0635661 total: 5.46s remaining: 3.26s 626: learn: 0.0635163 total: 5.46s remaining: 3.25s 627: learn: 0.0634674 total: 5.47s remaining: 3.24s 628: learn: 0.0634356 total: 5.48s remaining: 3.23s 629: learn: 0.0633374 total: 5.49s remaining: 3.22s 630: learn: 0.0632818 total: 5.5s remaining: 3.21s 631: learn: 0.0632739 total: 5.51s remaining: 3.21s 632: learn: 0.0632157 total: 5.52s remaining: 3.2s 633: learn: 0.0631734 total: 5.53s remaining: 3.19s 634: learn: 0.0631397 total: 5.53s remaining: 3.18s 635: learn: 0.0630768 total: 5.54s remaining: 3.17s 636: learn: 0.0630119 total: 5.55s remaining: 3.16s 637: learn: 0.0629674 total: 5.56s remaining: 3.15s 638: learn: 0.0629122 total: 5.58s remaining: 3.15s 639: learn: 0.0628474 total: 5.59s remaining: 3.15s 640: learn: 0.0627974 total: 5.6s remaining: 3.14s 641: learn: 0.0627143 total: 5.61s remaining: 3.13s 642: learn: 0.0626595 total: 5.62s remaining: 3.12s 643: learn: 0.0626138 total: 5.62s remaining: 3.11s 644: learn: 0.0625714 total: 5.63s remaining: 3.1s 645: learn: 0.0625152 total: 5.64s remaining: 3.09s 646: learn: 0.0624720 total: 5.65s remaining: 3.08s 647: learn: 0.0624129 total: 5.66s remaining: 3.07s 648: learn: 0.0623452 total: 5.66s remaining: 3.06s 649: learn: 0.0622701 total: 5.67s remaining: 3.05s 650: learn: 0.0622271 total: 5.68s remaining: 3.05s 651: learn: 0.0621996 total: 5.69s remaining: 3.04s 652: learn: 0.0621483 total: 5.7s remaining: 3.03s 653: learn: 0.0621021 total: 5.71s remaining: 3.02s 654: learn: 0.0620709 total: 5.71s remaining: 3.01s 655: learn: 0.0620493 total: 5.72s remaining: 3s 656: learn: 0.0620434 total: 5.73s remaining: 2.99s 657: learn: 0.0619825 total: 5.74s remaining: 2.98s 658: learn: 0.0619353 total: 5.75s remaining: 2.97s 659: learn: 0.0618931 total: 5.76s remaining: 2.96s 660: learn: 0.0618527 total: 5.77s remaining: 2.96s 661: learn: 0.0617741 total: 5.78s remaining: 2.95s 662: learn: 0.0617097 total: 5.79s remaining: 2.94s 663: learn: 0.0616737 total: 5.8s remaining: 2.94s 664: learn: 0.0616198 total: 5.81s remaining: 2.92s 665: learn: 0.0615630 total: 5.82s remaining: 2.92s 666: learn: 0.0615417 total: 5.82s remaining: 2.91s 667: learn: 0.0615300 total: 5.83s remaining: 2.9s 668: learn: 0.0614726 total: 5.84s remaining: 2.89s 669: learn: 0.0613824 total: 5.85s remaining: 2.88s 670: learn: 0.0613320 total: 5.86s remaining: 2.87s 671: learn: 0.0612824 total: 5.86s remaining: 2.86s 672: learn: 0.0611586 total: 5.87s remaining: 2.85s 673: learn: 0.0611143 total: 5.88s remaining: 2.84s 674: learn: 0.0610801 total: 5.89s remaining: 2.83s 675: learn: 0.0610046 total: 5.9s remaining: 2.83s 676: learn: 0.0609305 total: 5.9s remaining: 2.82s 677: learn: 0.0608438 total: 5.91s remaining: 2.81s 678: learn: 0.0608333 total: 5.92s remaining: 2.8s 679: learn: 0.0608164 total: 5.93s remaining: 2.79s 680: learn: 0.0607608 total: 5.93s remaining: 2.78s 681: learn: 0.0607192 total: 5.94s remaining: 2.77s 682: learn: 0.0607145 total: 5.95s remaining: 2.76s 683: learn: 0.0607050 total: 5.96s remaining: 2.75s 684: learn: 0.0606610 total: 5.96s remaining: 2.74s 685: learn: 0.0606007 total: 5.98s remaining: 2.74s 686: learn: 0.0605255 total: 5.99s remaining: 2.73s 687: learn: 0.0604603 total: 6s remaining: 2.72s 688: learn: 0.0604017 total: 6.01s remaining: 2.71s 689: learn: 0.0603843 total: 6.01s remaining: 2.7s 690: learn: 0.0603468 total: 6.02s remaining: 2.69s 691: learn: 0.0602665 total: 6.03s remaining: 2.68s 692: learn: 0.0602141 total: 6.04s remaining: 2.67s 693: learn: 0.0601794 total: 6.05s remaining: 2.67s 694: learn: 0.0601504 total: 6.06s remaining: 2.66s 695: learn: 0.0600997 total: 6.07s remaining: 2.65s 696: learn: 0.0600436 total: 6.09s remaining: 2.65s 697: learn: 0.0600392 total: 6.1s remaining: 2.64s 698: learn: 0.0599796 total: 6.1s remaining: 2.63s 699: learn: 0.0599329 total: 6.11s remaining: 2.62s 700: learn: 0.0598864 total: 6.12s remaining: 2.61s 701: learn: 0.0598581 total: 6.13s remaining: 2.6s 702: learn: 0.0597956 total: 6.14s remaining: 2.59s 703: learn: 0.0597678 total: 6.15s remaining: 2.58s 704: learn: 0.0597245 total: 6.16s remaining: 2.58s 705: learn: 0.0596733 total: 6.16s remaining: 2.57s 706: learn: 0.0596656 total: 6.17s remaining: 2.56s 707: learn: 0.0596099 total: 6.19s remaining: 2.55s 708: learn: 0.0595586 total: 6.2s remaining: 2.54s 709: learn: 0.0595096 total: 6.2s remaining: 2.53s 710: learn: 0.0594453 total: 6.21s remaining: 2.52s 711: learn: 0.0593783 total: 6.22s remaining: 2.52s 712: learn: 0.0593100 total: 6.23s remaining: 2.51s 713: learn: 0.0592701 total: 6.24s remaining: 2.5s 714: learn: 0.0592617 total: 6.25s remaining: 2.49s 715: learn: 0.0592177 total: 6.25s remaining: 2.48s 716: learn: 0.0591759 total: 6.26s remaining: 2.47s 717: learn: 0.0591260 total: 6.27s remaining: 2.46s 718: learn: 0.0590835 total: 6.28s remaining: 2.45s 719: learn: 0.0590056 total: 6.29s remaining: 2.44s 720: learn: 0.0589986 total: 6.29s remaining: 2.44s 721: learn: 0.0589350 total: 6.3s remaining: 2.43s 722: learn: 0.0589310 total: 6.31s remaining: 2.42s 723: learn: 0.0588787 total: 6.32s remaining: 2.41s 724: learn: 0.0588395 total: 6.33s remaining: 2.4s 725: learn: 0.0587988 total: 6.33s remaining: 2.39s 726: learn: 0.0587357 total: 6.34s remaining: 2.38s 727: learn: 0.0587047 total: 6.35s remaining: 2.37s 728: learn: 0.0586597 total: 6.36s remaining: 2.36s 729: learn: 0.0586082 total: 6.37s remaining: 2.35s 730: learn: 0.0585746 total: 6.38s remaining: 2.35s 731: learn: 0.0585022 total: 6.39s remaining: 2.34s 732: learn: 0.0584447 total: 6.41s remaining: 2.33s 733: learn: 0.0583777 total: 6.42s remaining: 2.33s 734: learn: 0.0583237 total: 6.42s remaining: 2.32s 735: learn: 0.0582778 total: 6.43s remaining: 2.31s 736: learn: 0.0582240 total: 6.44s remaining: 2.3s 737: learn: 0.0581872 total: 6.45s remaining: 2.29s 738: learn: 0.0581125 total: 6.46s remaining: 2.28s 739: learn: 0.0580694 total: 6.46s remaining: 2.27s 740: learn: 0.0580351 total: 6.47s remaining: 2.26s 741: learn: 0.0580111 total: 6.48s remaining: 2.25s 742: learn: 0.0579885 total: 6.49s remaining: 2.24s 743: learn: 0.0579362 total: 6.49s remaining: 2.23s 744: learn: 0.0579237 total: 6.5s remaining: 2.23s 745: learn: 0.0578748 total: 6.51s remaining: 2.22s 746: learn: 0.0578558 total: 6.52s remaining: 2.21s 747: learn: 0.0578182 total: 6.52s remaining: 2.2s 748: learn: 0.0577702 total: 6.53s remaining: 2.19s 749: learn: 0.0577382 total: 6.54s remaining: 2.18s 750: learn: 0.0576869 total: 6.55s remaining: 2.17s 751: learn: 0.0576773 total: 6.55s remaining: 2.16s 752: learn: 0.0576131 total: 6.56s remaining: 2.15s 753: learn: 0.0575702 total: 6.57s remaining: 2.14s 754: learn: 0.0575226 total: 6.58s remaining: 2.13s 755: learn: 0.0574741 total: 6.59s remaining: 2.13s 756: learn: 0.0574708 total: 6.6s remaining: 2.12s 757: learn: 0.0574168 total: 6.61s remaining: 2.11s 758: learn: 0.0573725 total: 6.62s remaining: 2.1s 759: learn: 0.0573476 total: 6.63s remaining: 2.09s 760: learn: 0.0573215 total: 6.63s remaining: 2.08s 761: learn: 0.0572918 total: 6.64s remaining: 2.07s 762: learn: 0.0572239 total: 6.65s remaining: 2.06s 763: learn: 0.0572009 total: 6.66s remaining: 2.06s 764: learn: 0.0571258 total: 6.67s remaining: 2.05s 765: learn: 0.0570676 total: 6.68s remaining: 2.04s 766: learn: 0.0570295 total: 6.69s remaining: 2.03s 767: learn: 0.0569624 total: 6.7s remaining: 2.02s 768: learn: 0.0569430 total: 6.7s remaining: 2.01s 769: learn: 0.0568949 total: 6.71s remaining: 2s 770: learn: 0.0568455 total: 6.72s remaining: 2s 771: learn: 0.0568376 total: 6.73s remaining: 1.99s 772: learn: 0.0567725 total: 6.74s remaining: 1.98s 773: learn: 0.0567069 total: 6.75s remaining: 1.97s 774: learn: 0.0566892 total: 6.75s remaining: 1.96s 775: learn: 0.0566408 total: 6.76s remaining: 1.95s 776: learn: 0.0565934 total: 6.77s remaining: 1.94s 777: learn: 0.0565065 total: 6.78s remaining: 1.93s 778: learn: 0.0564657 total: 6.79s remaining: 1.93s 779: learn: 0.0564374 total: 6.81s remaining: 1.92s 780: learn: 0.0563987 total: 6.82s remaining: 1.91s 781: learn: 0.0563723 total: 6.83s remaining: 1.9s 782: learn: 0.0562965 total: 6.84s remaining: 1.89s 783: learn: 0.0562358 total: 6.85s remaining: 1.89s 784: learn: 0.0561829 total: 6.86s remaining: 1.88s 785: learn: 0.0561466 total: 6.86s remaining: 1.87s 786: learn: 0.0560854 total: 6.87s remaining: 1.86s 787: learn: 0.0560535 total: 6.88s remaining: 1.85s 788: learn: 0.0560201 total: 6.89s remaining: 1.84s 789: learn: 0.0559718 total: 6.89s remaining: 1.83s 790: learn: 0.0559171 total: 6.9s remaining: 1.82s 791: learn: 0.0558577 total: 6.91s remaining: 1.81s 792: learn: 0.0558315 total: 6.92s remaining: 1.81s 793: learn: 0.0558085 total: 6.93s remaining: 1.8s 794: learn: 0.0557646 total: 6.93s remaining: 1.79s 795: learn: 0.0557370 total: 6.94s remaining: 1.78s 796: learn: 0.0557334 total: 6.95s remaining: 1.77s 797: learn: 0.0556891 total: 6.96s remaining: 1.76s 798: learn: 0.0556718 total: 6.97s remaining: 1.75s 799: learn: 0.0556349 total: 6.97s remaining: 1.74s 800: learn: 0.0556031 total: 6.98s remaining: 1.73s 801: learn: 0.0555646 total: 6.99s remaining: 1.73s 802: learn: 0.0555153 total: 7.01s remaining: 1.72s 803: learn: 0.0554452 total: 7.02s remaining: 1.71s 804: learn: 0.0554000 total: 7.03s remaining: 1.7s 805: learn: 0.0553440 total: 7.04s remaining: 1.69s 806: learn: 0.0553007 total: 7.05s remaining: 1.69s 807: learn: 0.0552423 total: 7.06s remaining: 1.68s 808: learn: 0.0552178 total: 7.07s remaining: 1.67s 809: learn: 0.0551872 total: 7.08s remaining: 1.66s 810: learn: 0.0551410 total: 7.09s remaining: 1.65s 811: learn: 0.0551074 total: 7.1s remaining: 1.64s 812: learn: 0.0550650 total: 7.1s remaining: 1.63s 813: learn: 0.0550451 total: 7.11s remaining: 1.63s 814: learn: 0.0549810 total: 7.12s remaining: 1.62s 815: learn: 0.0549435 total: 7.13s remaining: 1.61s 816: learn: 0.0549295 total: 7.14s remaining: 1.6s 817: learn: 0.0549021 total: 7.14s remaining: 1.59s 818: learn: 0.0548983 total: 7.15s remaining: 1.58s 819: learn: 0.0548517 total: 7.16s remaining: 1.57s 820: learn: 0.0548096 total: 7.17s remaining: 1.56s 821: learn: 0.0547474 total: 7.17s remaining: 1.55s 822: learn: 0.0547093 total: 7.19s remaining: 1.54s 823: learn: 0.0546836 total: 7.19s remaining: 1.54s 824: learn: 0.0546646 total: 7.2s remaining: 1.53s 825: learn: 0.0546439 total: 7.22s remaining: 1.52s 826: learn: 0.0545833 total: 7.24s remaining: 1.51s 827: learn: 0.0545271 total: 7.25s remaining: 1.5s 828: learn: 0.0545011 total: 7.25s remaining: 1.5s 829: learn: 0.0544400 total: 7.26s remaining: 1.49s 830: learn: 0.0543707 total: 7.27s remaining: 1.48s 831: learn: 0.0543509 total: 7.28s remaining: 1.47s 832: learn: 0.0542724 total: 7.29s remaining: 1.46s 833: learn: 0.0542690 total: 7.29s remaining: 1.45s 834: learn: 0.0542089 total: 7.3s remaining: 1.44s 835: learn: 0.0541716 total: 7.31s remaining: 1.43s 836: learn: 0.0541279 total: 7.32s remaining: 1.43s 837: learn: 0.0540704 total: 7.33s remaining: 1.42s 838: learn: 0.0540327 total: 7.33s remaining: 1.41s 839: learn: 0.0539591 total: 7.34s remaining: 1.4s 840: learn: 0.0539113 total: 7.35s remaining: 1.39s 841: learn: 0.0538867 total: 7.36s remaining: 1.38s 842: learn: 0.0538372 total: 7.37s remaining: 1.37s 843: learn: 0.0538069 total: 7.37s remaining: 1.36s 844: learn: 0.0537797 total: 7.38s remaining: 1.35s 845: learn: 0.0537520 total: 7.39s remaining: 1.34s 846: learn: 0.0537218 total: 7.4s remaining: 1.34s 847: learn: 0.0536787 total: 7.41s remaining: 1.33s 848: learn: 0.0536394 total: 7.42s remaining: 1.32s 849: learn: 0.0536359 total: 7.43s remaining: 1.31s 850: learn: 0.0535803 total: 7.44s remaining: 1.3s 851: learn: 0.0535184 total: 7.45s remaining: 1.29s 852: learn: 0.0535039 total: 7.46s remaining: 1.28s 853: learn: 0.0534789 total: 7.46s remaining: 1.27s 854: learn: 0.0534322 total: 7.47s remaining: 1.27s 855: learn: 0.0533517 total: 7.48s remaining: 1.26s 856: learn: 0.0533077 total: 7.49s remaining: 1.25s 857: learn: 0.0532627 total: 7.49s remaining: 1.24s 858: learn: 0.0532113 total: 7.5s remaining: 1.23s 859: learn: 0.0531840 total: 7.51s remaining: 1.22s 860: learn: 0.0531607 total: 7.52s remaining: 1.21s 861: learn: 0.0530977 total: 7.53s remaining: 1.2s 862: learn: 0.0530259 total: 7.53s remaining: 1.2s 863: learn: 0.0529844 total: 7.54s remaining: 1.19s 864: learn: 0.0529552 total: 7.55s remaining: 1.18s 865: learn: 0.0528890 total: 7.56s remaining: 1.17s 866: learn: 0.0528853 total: 7.57s remaining: 1.16s 867: learn: 0.0528314 total: 7.57s remaining: 1.15s 868: learn: 0.0527843 total: 7.58s remaining: 1.14s 869: learn: 0.0527388 total: 7.59s remaining: 1.13s 870: learn: 0.0527147 total: 7.6s remaining: 1.13s 871: learn: 0.0526628 total: 7.6s remaining: 1.12s 872: learn: 0.0526340 total: 7.61s remaining: 1.11s 873: learn: 0.0526172 total: 7.63s remaining: 1.1s 874: learn: 0.0525765 total: 7.64s remaining: 1.09s 875: learn: 0.0525566 total: 7.64s remaining: 1.08s 876: learn: 0.0524926 total: 7.65s remaining: 1.07s 877: learn: 0.0524547 total: 7.66s remaining: 1.06s 878: learn: 0.0524040 total: 7.67s remaining: 1.05s 879: learn: 0.0523714 total: 7.68s remaining: 1.05s 880: learn: 0.0523411 total: 7.68s remaining: 1.04s 881: learn: 0.0522850 total: 7.69s remaining: 1.03s 882: learn: 0.0522820 total: 7.7s remaining: 1.02s 883: learn: 0.0522341 total: 7.71s remaining: 1.01s 884: learn: 0.0521952 total: 7.72s remaining: 1s 885: learn: 0.0521426 total: 7.72s remaining: 994ms 886: learn: 0.0521032 total: 7.73s remaining: 985ms 887: learn: 0.0520847 total: 7.75s remaining: 977ms 888: learn: 0.0520324 total: 7.75s remaining: 968ms 889: learn: 0.0519849 total: 7.76s remaining: 959ms 890: learn: 0.0519366 total: 7.77s remaining: 951ms 891: learn: 0.0518936 total: 7.78s remaining: 942ms 892: learn: 0.0518627 total: 7.79s remaining: 933ms 893: learn: 0.0518487 total: 7.79s remaining: 924ms 894: learn: 0.0518303 total: 7.8s remaining: 915ms 895: learn: 0.0517882 total: 7.81s remaining: 907ms 896: learn: 0.0517361 total: 7.82s remaining: 898ms 897: learn: 0.0517172 total: 7.83s remaining: 890ms 898: learn: 0.0516825 total: 7.84s remaining: 881ms 899: learn: 0.0516579 total: 7.85s remaining: 872ms 900: learn: 0.0516375 total: 7.86s remaining: 864ms 901: learn: 0.0516121 total: 7.87s remaining: 855ms 902: learn: 0.0515766 total: 7.88s remaining: 846ms 903: learn: 0.0515365 total: 7.88s remaining: 837ms 904: learn: 0.0514761 total: 7.89s remaining: 828ms 905: learn: 0.0514285 total: 7.9s remaining: 820ms 906: learn: 0.0513940 total: 7.91s remaining: 811ms 907: learn: 0.0513700 total: 7.91s remaining: 802ms 908: learn: 0.0513031 total: 7.92s remaining: 793ms 909: learn: 0.0512522 total: 7.93s remaining: 784ms 910: learn: 0.0512058 total: 7.94s remaining: 776ms 911: learn: 0.0511517 total: 7.95s remaining: 767ms 912: learn: 0.0511057 total: 7.95s remaining: 758ms 913: learn: 0.0510810 total: 7.96s remaining: 749ms 914: learn: 0.0510430 total: 7.97s remaining: 741ms 915: learn: 0.0510069 total: 7.98s remaining: 732ms 916: learn: 0.0509447 total: 7.99s remaining: 723ms 917: learn: 0.0508921 total: 8s remaining: 714ms 918: learn: 0.0508496 total: 8s remaining: 705ms 919: learn: 0.0508107 total: 8.02s remaining: 698ms 920: learn: 0.0507661 total: 8.04s remaining: 690ms 921: learn: 0.0507362 total: 8.05s remaining: 681ms 922: learn: 0.0507009 total: 8.05s remaining: 672ms 923: learn: 0.0506590 total: 8.06s remaining: 663ms 924: learn: 0.0506514 total: 8.07s remaining: 654ms 925: learn: 0.0506118 total: 8.08s remaining: 646ms 926: learn: 0.0505761 total: 8.09s remaining: 637ms 927: learn: 0.0505386 total: 8.09s remaining: 628ms 928: learn: 0.0505077 total: 8.1s remaining: 619ms 929: learn: 0.0504810 total: 8.11s remaining: 610ms 930: learn: 0.0504262 total: 8.12s remaining: 602ms 931: learn: 0.0503945 total: 8.12s remaining: 593ms 932: learn: 0.0503566 total: 8.13s remaining: 584ms 933: learn: 0.0503207 total: 8.14s remaining: 575ms 934: learn: 0.0502839 total: 8.15s remaining: 566ms 935: learn: 0.0502544 total: 8.15s remaining: 558ms 936: learn: 0.0502293 total: 8.16s remaining: 549ms 937: learn: 0.0501784 total: 8.17s remaining: 540ms 938: learn: 0.0501415 total: 8.18s remaining: 531ms 939: learn: 0.0501179 total: 8.19s remaining: 523ms 940: learn: 0.0500799 total: 8.19s remaining: 514ms 941: learn: 0.0500587 total: 8.2s remaining: 505ms 942: learn: 0.0500121 total: 8.21s remaining: 496ms 943: learn: 0.0499904 total: 8.22s remaining: 487ms 944: learn: 0.0499427 total: 8.22s remaining: 479ms 945: learn: 0.0499260 total: 8.23s remaining: 470ms 946: learn: 0.0499203 total: 8.25s remaining: 462ms 947: learn: 0.0498706 total: 8.26s remaining: 453ms 948: learn: 0.0498562 total: 8.27s remaining: 444ms 949: learn: 0.0498197 total: 8.27s remaining: 435ms 950: learn: 0.0497701 total: 8.28s remaining: 427ms 951: learn: 0.0496890 total: 8.29s remaining: 418ms 952: learn: 0.0496491 total: 8.3s remaining: 409ms 953: learn: 0.0495973 total: 8.31s remaining: 401ms 954: learn: 0.0495715 total: 8.31s remaining: 392ms 955: learn: 0.0495154 total: 8.32s remaining: 383ms 956: learn: 0.0494862 total: 8.33s remaining: 374ms 957: learn: 0.0494558 total: 8.34s remaining: 366ms 958: learn: 0.0494227 total: 8.35s remaining: 357ms 959: learn: 0.0494197 total: 8.35s remaining: 348ms 960: learn: 0.0493662 total: 8.36s remaining: 339ms 961: learn: 0.0493244 total: 8.37s remaining: 331ms 962: learn: 0.0492657 total: 8.38s remaining: 322ms 963: learn: 0.0492467 total: 8.39s remaining: 313ms 964: learn: 0.0492091 total: 8.4s remaining: 305ms 965: learn: 0.0492038 total: 8.4s remaining: 296ms 966: learn: 0.0492015 total: 8.41s remaining: 287ms 967: learn: 0.0491446 total: 8.42s remaining: 278ms 968: learn: 0.0491412 total: 8.43s remaining: 270ms 969: learn: 0.0491353 total: 8.44s remaining: 261ms 970: learn: 0.0490845 total: 8.45s remaining: 252ms 971: learn: 0.0490821 total: 8.46s remaining: 244ms 972: learn: 0.0490750 total: 8.46s remaining: 235ms 973: learn: 0.0490587 total: 8.47s remaining: 226ms 974: learn: 0.0490475 total: 8.48s remaining: 217ms 975: learn: 0.0490053 total: 8.49s remaining: 209ms 976: learn: 0.0489980 total: 8.49s remaining: 200ms 977: learn: 0.0489732 total: 8.5s remaining: 191ms 978: learn: 0.0489709 total: 8.51s remaining: 183ms 979: learn: 0.0489529 total: 8.52s remaining: 174ms 980: learn: 0.0489173 total: 8.53s remaining: 165ms 981: learn: 0.0488777 total: 8.54s remaining: 156ms 982: learn: 0.0488758 total: 8.54s remaining: 148ms 983: learn: 0.0488505 total: 8.56s remaining: 139ms 984: learn: 0.0488443 total: 8.56s remaining: 130ms 985: learn: 0.0487945 total: 8.57s remaining: 122ms 986: learn: 0.0487812 total: 8.58s remaining: 113ms 987: learn: 0.0487268 total: 8.59s remaining: 104ms 988: learn: 0.0486950 total: 8.6s remaining: 95.6ms 989: learn: 0.0486555 total: 8.61s remaining: 86.9ms 990: learn: 0.0486297 total: 8.61s remaining: 78.2ms 991: learn: 0.0486280 total: 8.62s remaining: 69.6ms 992: learn: 0.0485911 total: 8.64s remaining: 60.9ms 993: learn: 0.0485351 total: 8.65s remaining: 52.2ms 994: learn: 0.0485036 total: 8.65s remaining: 43.5ms 995: learn: 0.0484888 total: 8.66s remaining: 34.8ms 996: learn: 0.0484615 total: 8.67s remaining: 26.1ms 997: learn: 0.0484365 total: 8.68s remaining: 17.4ms 998: learn: 0.0483822 total: 8.69s remaining: 8.7ms 999: learn: 0.0483804 total: 8.7s remaining: 0us Learning rate set to 0.046383 0: learn: 0.3943991 total: 9.21ms remaining: 9.2s 1: learn: 0.3789991 total: 23.4ms remaining: 11.7s 2: learn: 0.3644885 total: 32.9ms remaining: 10.9s 3: learn: 0.3512887 total: 42.1ms remaining: 10.5s 4: learn: 0.3384099 total: 51.9ms remaining: 10.3s 5: learn: 0.3263068 total: 61.2ms remaining: 10.1s 6: learn: 0.3148149 total: 70.7ms remaining: 10s 7: learn: 0.3033741 total: 79.7ms remaining: 9.88s 8: learn: 0.2929932 total: 89.1ms remaining: 9.81s 9: learn: 0.2832654 total: 99.7ms remaining: 9.87s 10: learn: 0.2735330 total: 109ms remaining: 9.77s 11: learn: 0.2638036 total: 118ms remaining: 9.68s 12: learn: 0.2548762 total: 126ms remaining: 9.59s 13: learn: 0.2463958 total: 135ms remaining: 9.53s 14: learn: 0.2378950 total: 144ms remaining: 9.46s 15: learn: 0.2303647 total: 153ms remaining: 9.4s 16: learn: 0.2221171 total: 162ms remaining: 9.35s 17: learn: 0.2146976 total: 173ms remaining: 9.43s 18: learn: 0.2082400 total: 182ms remaining: 9.38s 19: learn: 0.2013332 total: 198ms remaining: 9.71s 20: learn: 0.1952954 total: 214ms remaining: 10s 21: learn: 0.1896632 total: 227ms remaining: 10.1s 22: learn: 0.1838553 total: 238ms remaining: 10.1s 23: learn: 0.1786054 total: 252ms remaining: 10.2s 24: learn: 0.1734639 total: 261ms remaining: 10.2s 25: learn: 0.1682165 total: 272ms remaining: 10.2s 26: learn: 0.1632800 total: 282ms remaining: 10.2s 27: learn: 0.1584008 total: 291ms remaining: 10.1s 28: learn: 0.1537021 total: 300ms remaining: 10s 29: learn: 0.1497188 total: 309ms remaining: 9.98s 30: learn: 0.1459408 total: 317ms remaining: 9.92s 31: learn: 0.1417995 total: 326ms remaining: 9.87s 32: learn: 0.1382092 total: 335ms remaining: 9.81s 33: learn: 0.1345952 total: 343ms remaining: 9.76s 34: learn: 0.1311113 total: 352ms remaining: 9.71s 35: learn: 0.1280546 total: 361ms remaining: 9.67s 36: learn: 0.1249494 total: 372ms remaining: 9.69s 37: learn: 0.1222542 total: 381ms remaining: 9.65s 38: learn: 0.1198251 total: 390ms remaining: 9.62s 39: learn: 0.1172223 total: 409ms remaining: 9.83s 40: learn: 0.1146249 total: 419ms remaining: 9.81s 41: learn: 0.1121293 total: 429ms remaining: 9.78s 42: learn: 0.1098262 total: 438ms remaining: 9.74s 43: learn: 0.1081469 total: 447ms remaining: 9.71s 44: learn: 0.1063289 total: 456ms remaining: 9.68s 45: learn: 0.1045217 total: 466ms remaining: 9.65s 46: learn: 0.1027972 total: 475ms remaining: 9.63s 47: learn: 0.1012871 total: 484ms remaining: 9.6s 48: learn: 0.0997457 total: 493ms remaining: 9.57s 49: learn: 0.0981154 total: 502ms remaining: 9.54s 50: learn: 0.0965902 total: 512ms remaining: 9.52s 51: learn: 0.0954016 total: 521ms remaining: 9.5s 52: learn: 0.0938927 total: 530ms remaining: 9.47s 53: learn: 0.0925547 total: 540ms remaining: 9.45s 54: learn: 0.0912303 total: 550ms remaining: 9.45s 55: learn: 0.0900715 total: 560ms remaining: 9.43s 56: learn: 0.0889332 total: 571ms remaining: 9.45s 57: learn: 0.0879121 total: 582ms remaining: 9.46s 58: learn: 0.0869771 total: 591ms remaining: 9.43s 59: learn: 0.0861142 total: 601ms remaining: 9.41s 60: learn: 0.0851577 total: 619ms remaining: 9.53s 61: learn: 0.0845059 total: 629ms remaining: 9.51s 62: learn: 0.0836586 total: 638ms remaining: 9.48s 63: learn: 0.0828915 total: 646ms remaining: 9.45s 64: learn: 0.0823616 total: 655ms remaining: 9.42s 65: learn: 0.0816667 total: 664ms remaining: 9.39s 66: learn: 0.0809559 total: 673ms remaining: 9.37s 67: learn: 0.0803545 total: 682ms remaining: 9.34s 68: learn: 0.0797424 total: 691ms remaining: 9.32s 69: learn: 0.0791548 total: 700ms remaining: 9.3s 70: learn: 0.0786746 total: 709ms remaining: 9.27s 71: learn: 0.0781858 total: 718ms remaining: 9.25s 72: learn: 0.0777038 total: 728ms remaining: 9.24s 73: learn: 0.0771954 total: 737ms remaining: 9.22s 74: learn: 0.0765945 total: 746ms remaining: 9.2s 75: learn: 0.0761692 total: 756ms remaining: 9.19s 76: learn: 0.0758415 total: 765ms remaining: 9.17s 77: learn: 0.0755772 total: 774ms remaining: 9.15s 78: learn: 0.0753715 total: 783ms remaining: 9.13s 79: learn: 0.0750725 total: 793ms remaining: 9.12s 80: learn: 0.0746999 total: 801ms remaining: 9.09s 81: learn: 0.0743287 total: 811ms remaining: 9.08s 82: learn: 0.0741194 total: 827ms remaining: 9.13s 83: learn: 0.0738259 total: 836ms remaining: 9.12s 84: learn: 0.0735267 total: 850ms remaining: 9.15s 85: learn: 0.0733862 total: 858ms remaining: 9.12s 86: learn: 0.0730698 total: 867ms remaining: 9.1s 87: learn: 0.0728153 total: 876ms remaining: 9.07s 88: learn: 0.0725500 total: 885ms remaining: 9.06s 89: learn: 0.0722854 total: 894ms remaining: 9.04s 90: learn: 0.0720889 total: 902ms remaining: 9.01s 91: learn: 0.0717439 total: 910ms remaining: 8.98s 92: learn: 0.0716503 total: 919ms remaining: 8.96s 93: learn: 0.0714280 total: 928ms remaining: 8.94s 94: learn: 0.0712662 total: 936ms remaining: 8.92s 95: learn: 0.0711101 total: 945ms remaining: 8.9s 96: learn: 0.0708810 total: 955ms remaining: 8.89s 97: learn: 0.0707311 total: 967ms remaining: 8.9s 98: learn: 0.0705716 total: 976ms remaining: 8.88s 99: learn: 0.0704008 total: 986ms remaining: 8.87s 100: learn: 0.0702145 total: 995ms remaining: 8.86s 101: learn: 0.0699914 total: 1s remaining: 8.84s 102: learn: 0.0697808 total: 1.02s remaining: 8.91s 103: learn: 0.0696373 total: 1.03s remaining: 8.9s 104: learn: 0.0693619 total: 1.04s remaining: 8.89s 105: learn: 0.0692197 total: 1.05s remaining: 8.87s 106: learn: 0.0691139 total: 1.06s remaining: 8.87s 107: learn: 0.0689716 total: 1.07s remaining: 8.85s 108: learn: 0.0688851 total: 1.08s remaining: 8.84s 109: learn: 0.0686917 total: 1.09s remaining: 8.82s 110: learn: 0.0684570 total: 1.1s remaining: 8.8s 111: learn: 0.0683617 total: 1.11s remaining: 8.79s 112: learn: 0.0681620 total: 1.12s remaining: 8.79s 113: learn: 0.0679745 total: 1.13s remaining: 8.78s 114: learn: 0.0677983 total: 1.14s remaining: 8.76s 115: learn: 0.0677658 total: 1.15s remaining: 8.74s 116: learn: 0.0676517 total: 1.16s remaining: 8.72s 117: learn: 0.0675852 total: 1.16s remaining: 8.7s 118: learn: 0.0674779 total: 1.17s remaining: 8.68s 119: learn: 0.0673634 total: 1.18s remaining: 8.66s 120: learn: 0.0671761 total: 1.19s remaining: 8.64s 121: learn: 0.0670808 total: 1.2s remaining: 8.63s 122: learn: 0.0669710 total: 1.22s remaining: 8.68s 123: learn: 0.0668275 total: 1.24s remaining: 8.74s 124: learn: 0.0666671 total: 1.25s remaining: 8.72s 125: learn: 0.0665171 total: 1.26s remaining: 8.71s 126: learn: 0.0663741 total: 1.26s remaining: 8.7s 127: learn: 0.0663055 total: 1.27s remaining: 8.68s 128: learn: 0.0662809 total: 1.28s remaining: 8.67s 129: learn: 0.0661956 total: 1.29s remaining: 8.65s 130: learn: 0.0660080 total: 1.3s remaining: 8.63s 131: learn: 0.0659604 total: 1.31s remaining: 8.62s 132: learn: 0.0658153 total: 1.32s remaining: 8.61s 133: learn: 0.0657118 total: 1.33s remaining: 8.6s 134: learn: 0.0655613 total: 1.34s remaining: 8.58s 135: learn: 0.0654662 total: 1.35s remaining: 8.57s 136: learn: 0.0653192 total: 1.36s remaining: 8.55s 137: learn: 0.0652041 total: 1.37s remaining: 8.54s 138: learn: 0.0650554 total: 1.38s remaining: 8.53s 139: learn: 0.0649798 total: 1.39s remaining: 8.51s 140: learn: 0.0648714 total: 1.39s remaining: 8.49s 141: learn: 0.0647809 total: 1.4s remaining: 8.48s 142: learn: 0.0647049 total: 1.41s remaining: 8.47s 143: learn: 0.0646689 total: 1.43s remaining: 8.5s 144: learn: 0.0646128 total: 1.44s remaining: 8.49s 145: learn: 0.0644817 total: 1.45s remaining: 8.48s 146: learn: 0.0644258 total: 1.46s remaining: 8.46s 147: learn: 0.0643108 total: 1.47s remaining: 8.45s 148: learn: 0.0642032 total: 1.48s remaining: 8.43s 149: learn: 0.0641027 total: 1.49s remaining: 8.42s 150: learn: 0.0639352 total: 1.49s remaining: 8.4s 151: learn: 0.0639097 total: 1.5s remaining: 8.38s 152: learn: 0.0637654 total: 1.51s remaining: 8.37s 153: learn: 0.0636831 total: 1.52s remaining: 8.35s 154: learn: 0.0635664 total: 1.53s remaining: 8.34s 155: learn: 0.0634798 total: 1.54s remaining: 8.34s 156: learn: 0.0633477 total: 1.55s remaining: 8.33s 157: learn: 0.0632180 total: 1.56s remaining: 8.31s 158: learn: 0.0631409 total: 1.57s remaining: 8.29s 159: learn: 0.0629962 total: 1.58s remaining: 8.28s 160: learn: 0.0629727 total: 1.59s remaining: 8.27s 161: learn: 0.0628806 total: 1.59s remaining: 8.25s 162: learn: 0.0627558 total: 1.6s remaining: 8.23s 163: learn: 0.0626638 total: 1.61s remaining: 8.22s 164: learn: 0.0625309 total: 1.62s remaining: 8.21s 165: learn: 0.0624127 total: 1.64s remaining: 8.25s 166: learn: 0.0622861 total: 1.65s remaining: 8.24s 167: learn: 0.0621565 total: 1.67s remaining: 8.26s 168: learn: 0.0620945 total: 1.68s remaining: 8.25s 169: learn: 0.0620594 total: 1.69s remaining: 8.24s 170: learn: 0.0619832 total: 1.7s remaining: 8.22s 171: learn: 0.0618694 total: 1.71s remaining: 8.23s 172: learn: 0.0617392 total: 1.72s remaining: 8.22s 173: learn: 0.0615901 total: 1.73s remaining: 8.21s 174: learn: 0.0614498 total: 1.74s remaining: 8.19s 175: learn: 0.0613422 total: 1.75s remaining: 8.18s 176: learn: 0.0612594 total: 1.76s remaining: 8.17s 177: learn: 0.0611963 total: 1.76s remaining: 8.15s 178: learn: 0.0610306 total: 1.77s remaining: 8.14s 179: learn: 0.0609639 total: 1.78s remaining: 8.13s 180: learn: 0.0608860 total: 1.79s remaining: 8.12s 181: learn: 0.0607588 total: 1.8s remaining: 8.11s 182: learn: 0.0606722 total: 1.81s remaining: 8.1s 183: learn: 0.0605443 total: 1.82s remaining: 8.09s 184: learn: 0.0604265 total: 1.83s remaining: 8.08s 185: learn: 0.0603743 total: 1.86s remaining: 8.12s 186: learn: 0.0603139 total: 1.86s remaining: 8.11s 187: learn: 0.0602344 total: 1.88s remaining: 8.1s 188: learn: 0.0601057 total: 1.88s remaining: 8.09s 189: learn: 0.0600324 total: 1.89s remaining: 8.07s 190: learn: 0.0599339 total: 1.9s remaining: 8.06s 191: learn: 0.0598324 total: 1.91s remaining: 8.05s 192: learn: 0.0597824 total: 1.92s remaining: 8.04s 193: learn: 0.0596667 total: 1.93s remaining: 8.03s 194: learn: 0.0595397 total: 1.94s remaining: 8.02s 195: learn: 0.0594139 total: 1.95s remaining: 8.01s 196: learn: 0.0593462 total: 1.96s remaining: 7.99s 197: learn: 0.0592726 total: 1.97s remaining: 7.98s 198: learn: 0.0591353 total: 1.98s remaining: 7.97s 199: learn: 0.0590527 total: 1.99s remaining: 7.96s 200: learn: 0.0589310 total: 2s remaining: 7.94s 201: learn: 0.0587887 total: 2.01s remaining: 7.93s 202: learn: 0.0586971 total: 2.02s remaining: 7.92s 203: learn: 0.0585753 total: 2.03s remaining: 7.91s 204: learn: 0.0584618 total: 2.04s remaining: 7.91s 205: learn: 0.0583558 total: 2.06s remaining: 7.96s 206: learn: 0.0582529 total: 2.08s remaining: 7.95s 207: learn: 0.0582088 total: 2.09s remaining: 7.94s 208: learn: 0.0581170 total: 2.09s remaining: 7.93s 209: learn: 0.0579939 total: 2.1s remaining: 7.91s 210: learn: 0.0578525 total: 2.11s remaining: 7.9s 211: learn: 0.0577153 total: 2.12s remaining: 7.89s 212: learn: 0.0576085 total: 2.13s remaining: 7.89s 213: learn: 0.0575068 total: 2.14s remaining: 7.88s 214: learn: 0.0574206 total: 2.15s remaining: 7.87s 215: learn: 0.0573082 total: 2.17s remaining: 7.86s 216: learn: 0.0572400 total: 2.18s remaining: 7.86s 217: learn: 0.0570557 total: 2.19s remaining: 7.86s 218: learn: 0.0569874 total: 2.2s remaining: 7.85s 219: learn: 0.0568524 total: 2.21s remaining: 7.84s 220: learn: 0.0567615 total: 2.22s remaining: 7.82s 221: learn: 0.0567016 total: 2.23s remaining: 7.81s 222: learn: 0.0565707 total: 2.25s remaining: 7.84s 223: learn: 0.0564812 total: 2.26s remaining: 7.82s 224: learn: 0.0563853 total: 2.27s remaining: 7.81s 225: learn: 0.0563296 total: 2.28s remaining: 7.8s 226: learn: 0.0562137 total: 2.29s remaining: 7.79s 227: learn: 0.0561450 total: 2.3s remaining: 7.78s 228: learn: 0.0559858 total: 2.31s remaining: 7.76s 229: learn: 0.0558956 total: 2.32s remaining: 7.75s 230: learn: 0.0558066 total: 2.33s remaining: 7.74s 231: learn: 0.0556757 total: 2.34s remaining: 7.74s 232: learn: 0.0555521 total: 2.35s remaining: 7.72s 233: learn: 0.0554828 total: 2.35s remaining: 7.71s 234: learn: 0.0553552 total: 2.37s remaining: 7.7s 235: learn: 0.0552684 total: 2.38s remaining: 7.7s 236: learn: 0.0551452 total: 2.39s remaining: 7.69s 237: learn: 0.0550533 total: 2.4s remaining: 7.67s 238: learn: 0.0548812 total: 2.41s remaining: 7.66s 239: learn: 0.0547694 total: 2.42s remaining: 7.65s 240: learn: 0.0546686 total: 2.42s remaining: 7.64s 241: learn: 0.0545757 total: 2.43s remaining: 7.62s 242: learn: 0.0544918 total: 2.44s remaining: 7.61s 243: learn: 0.0544215 total: 2.46s remaining: 7.62s 244: learn: 0.0542864 total: 2.47s remaining: 7.61s 245: learn: 0.0542111 total: 2.48s remaining: 7.59s 246: learn: 0.0540901 total: 2.49s remaining: 7.58s 247: learn: 0.0540026 total: 2.5s remaining: 7.57s 248: learn: 0.0539087 total: 2.5s remaining: 7.55s 249: learn: 0.0538111 total: 2.51s remaining: 7.54s 250: learn: 0.0536964 total: 2.52s remaining: 7.52s 251: learn: 0.0535989 total: 2.53s remaining: 7.51s 252: learn: 0.0535286 total: 2.54s remaining: 7.5s 253: learn: 0.0534802 total: 2.55s remaining: 7.48s 254: learn: 0.0533826 total: 2.56s remaining: 7.47s 255: learn: 0.0532757 total: 2.56s remaining: 7.46s 256: learn: 0.0531724 total: 2.58s remaining: 7.46s 257: learn: 0.0530692 total: 2.59s remaining: 7.45s 258: learn: 0.0529764 total: 2.6s remaining: 7.44s 259: learn: 0.0529021 total: 2.61s remaining: 7.43s 260: learn: 0.0528909 total: 2.62s remaining: 7.42s 261: learn: 0.0528041 total: 2.63s remaining: 7.41s 262: learn: 0.0527079 total: 2.64s remaining: 7.39s 263: learn: 0.0526428 total: 2.65s remaining: 7.38s 264: learn: 0.0525528 total: 2.66s remaining: 7.38s 265: learn: 0.0524877 total: 2.68s remaining: 7.39s 266: learn: 0.0523786 total: 2.69s remaining: 7.38s 267: learn: 0.0523682 total: 2.7s remaining: 7.37s 268: learn: 0.0523028 total: 2.71s remaining: 7.35s 269: learn: 0.0522355 total: 2.71s remaining: 7.34s 270: learn: 0.0521392 total: 2.72s remaining: 7.33s 271: learn: 0.0520599 total: 2.73s remaining: 7.32s 272: learn: 0.0519152 total: 2.74s remaining: 7.3s 273: learn: 0.0517743 total: 2.75s remaining: 7.29s 274: learn: 0.0516871 total: 2.76s remaining: 7.28s 275: learn: 0.0516108 total: 2.77s remaining: 7.27s 276: learn: 0.0515351 total: 2.78s remaining: 7.26s 277: learn: 0.0514540 total: 2.79s remaining: 7.24s 278: learn: 0.0513839 total: 2.8s remaining: 7.23s 279: learn: 0.0513740 total: 2.81s remaining: 7.22s 280: learn: 0.0512862 total: 2.81s remaining: 7.2s 281: learn: 0.0511868 total: 2.82s remaining: 7.19s 282: learn: 0.0511189 total: 2.83s remaining: 7.18s 283: learn: 0.0510225 total: 2.84s remaining: 7.17s 284: learn: 0.0509563 total: 2.85s remaining: 7.16s 285: learn: 0.0509018 total: 2.86s remaining: 7.14s 286: learn: 0.0508204 total: 2.88s remaining: 7.16s 287: learn: 0.0507770 total: 2.89s remaining: 7.14s 288: learn: 0.0507582 total: 2.9s remaining: 7.13s 289: learn: 0.0506546 total: 2.91s remaining: 7.11s 290: learn: 0.0505353 total: 2.92s remaining: 7.1s 291: learn: 0.0504928 total: 2.92s remaining: 7.09s 292: learn: 0.0504188 total: 2.93s remaining: 7.08s 293: learn: 0.0503428 total: 2.94s remaining: 7.07s 294: learn: 0.0502839 total: 2.95s remaining: 7.06s 295: learn: 0.0501514 total: 2.96s remaining: 7.05s 296: learn: 0.0501428 total: 2.97s remaining: 7.04s 297: learn: 0.0500842 total: 2.98s remaining: 7.03s 298: learn: 0.0499665 total: 2.99s remaining: 7.02s 299: learn: 0.0498816 total: 3s remaining: 7s 300: learn: 0.0498335 total: 3.01s remaining: 6.99s 301: learn: 0.0497607 total: 3.02s remaining: 6.98s 302: learn: 0.0496794 total: 3.03s remaining: 6.96s 303: learn: 0.0495907 total: 3.04s remaining: 6.96s 304: learn: 0.0495407 total: 3.05s remaining: 6.94s 305: learn: 0.0495045 total: 3.06s remaining: 6.93s 306: learn: 0.0494148 total: 3.07s remaining: 6.93s 307: learn: 0.0493644 total: 3.09s remaining: 6.93s 308: learn: 0.0492904 total: 3.1s remaining: 6.92s 309: learn: 0.0492026 total: 3.1s remaining: 6.91s 310: learn: 0.0491244 total: 3.11s remaining: 6.9s 311: learn: 0.0490680 total: 3.12s remaining: 6.89s 312: learn: 0.0489803 total: 3.13s remaining: 6.87s 313: learn: 0.0489123 total: 3.14s remaining: 6.86s 314: learn: 0.0488399 total: 3.15s remaining: 6.85s 315: learn: 0.0487599 total: 3.17s remaining: 6.86s 316: learn: 0.0487537 total: 3.18s remaining: 6.84s 317: learn: 0.0486973 total: 3.19s remaining: 6.83s 318: learn: 0.0486249 total: 3.19s remaining: 6.82s 319: learn: 0.0485547 total: 3.2s remaining: 6.81s 320: learn: 0.0484892 total: 3.21s remaining: 6.79s 321: learn: 0.0484762 total: 3.22s remaining: 6.78s 322: learn: 0.0484487 total: 3.23s remaining: 6.77s 323: learn: 0.0483406 total: 3.24s remaining: 6.75s 324: learn: 0.0482635 total: 3.25s remaining: 6.74s 325: learn: 0.0481912 total: 3.25s remaining: 6.73s 326: learn: 0.0481191 total: 3.26s remaining: 6.72s 327: learn: 0.0480339 total: 3.27s remaining: 6.7s 328: learn: 0.0479933 total: 3.29s remaining: 6.71s 329: learn: 0.0479501 total: 3.31s remaining: 6.71s 330: learn: 0.0478772 total: 3.31s remaining: 6.7s 331: learn: 0.0478142 total: 3.32s remaining: 6.69s 332: learn: 0.0477517 total: 3.33s remaining: 6.68s 333: learn: 0.0476791 total: 3.34s remaining: 6.67s 334: learn: 0.0476374 total: 3.35s remaining: 6.65s 335: learn: 0.0475794 total: 3.36s remaining: 6.64s 336: learn: 0.0475294 total: 3.37s remaining: 6.63s 337: learn: 0.0474914 total: 3.38s remaining: 6.62s 338: learn: 0.0474300 total: 3.39s remaining: 6.61s 339: learn: 0.0473732 total: 3.4s remaining: 6.6s 340: learn: 0.0473044 total: 3.41s remaining: 6.58s 341: learn: 0.0472653 total: 3.42s remaining: 6.57s 342: learn: 0.0472047 total: 3.43s remaining: 6.56s 343: learn: 0.0471411 total: 3.44s remaining: 6.55s 344: learn: 0.0471081 total: 3.44s remaining: 6.54s 345: learn: 0.0470574 total: 3.45s remaining: 6.53s 346: learn: 0.0470119 total: 3.46s remaining: 6.52s 347: learn: 0.0469431 total: 3.47s remaining: 6.5s 348: learn: 0.0468489 total: 3.48s remaining: 6.49s 349: learn: 0.0467824 total: 3.5s remaining: 6.5s 350: learn: 0.0467254 total: 3.52s remaining: 6.5s 351: learn: 0.0466667 total: 3.52s remaining: 6.49s 352: learn: 0.0465899 total: 3.53s remaining: 6.48s 353: learn: 0.0465315 total: 3.54s remaining: 6.46s 354: learn: 0.0464966 total: 3.55s remaining: 6.45s 355: learn: 0.0464404 total: 3.56s remaining: 6.44s 356: learn: 0.0463601 total: 3.57s remaining: 6.43s 357: learn: 0.0462953 total: 3.58s remaining: 6.42s 358: learn: 0.0462340 total: 3.59s remaining: 6.41s 359: learn: 0.0461666 total: 3.6s remaining: 6.4s 360: learn: 0.0461263 total: 3.61s remaining: 6.38s 361: learn: 0.0460652 total: 3.62s remaining: 6.37s 362: learn: 0.0460239 total: 3.63s remaining: 6.37s 363: learn: 0.0459897 total: 3.64s remaining: 6.36s 364: learn: 0.0459797 total: 3.65s remaining: 6.34s 365: learn: 0.0459728 total: 3.65s remaining: 6.33s 366: learn: 0.0458688 total: 3.67s remaining: 6.32s 367: learn: 0.0458642 total: 3.67s remaining: 6.31s 368: learn: 0.0458103 total: 3.68s remaining: 6.3s 369: learn: 0.0457541 total: 3.69s remaining: 6.29s 370: learn: 0.0457488 total: 3.71s remaining: 6.29s 371: learn: 0.0457000 total: 3.73s remaining: 6.29s 372: learn: 0.0456322 total: 3.74s remaining: 6.29s 373: learn: 0.0455656 total: 3.75s remaining: 6.27s 374: learn: 0.0454944 total: 3.76s remaining: 6.26s 375: learn: 0.0453989 total: 3.77s remaining: 6.25s 376: learn: 0.0453365 total: 3.78s remaining: 6.24s 377: learn: 0.0452884 total: 3.79s remaining: 6.23s 378: learn: 0.0452177 total: 3.79s remaining: 6.22s 379: learn: 0.0451817 total: 3.81s remaining: 6.21s 380: learn: 0.0451163 total: 3.81s remaining: 6.2s 381: learn: 0.0450784 total: 3.82s remaining: 6.18s 382: learn: 0.0450228 total: 3.83s remaining: 6.17s 383: learn: 0.0449891 total: 3.84s remaining: 6.16s 384: learn: 0.0449565 total: 3.85s remaining: 6.15s 385: learn: 0.0449022 total: 3.86s remaining: 6.14s 386: learn: 0.0448468 total: 3.87s remaining: 6.13s 387: learn: 0.0447929 total: 3.88s remaining: 6.12s 388: learn: 0.0447385 total: 3.89s remaining: 6.11s 389: learn: 0.0447051 total: 3.9s remaining: 6.1s 390: learn: 0.0446565 total: 3.91s remaining: 6.08s 391: learn: 0.0445975 total: 3.92s remaining: 6.09s 392: learn: 0.0445928 total: 3.94s remaining: 6.08s 393: learn: 0.0445488 total: 3.94s remaining: 6.07s 394: learn: 0.0444982 total: 3.95s remaining: 6.06s 395: learn: 0.0444545 total: 3.96s remaining: 6.04s 396: learn: 0.0444149 total: 3.97s remaining: 6.03s 397: learn: 0.0443582 total: 3.98s remaining: 6.02s 398: learn: 0.0442932 total: 3.99s remaining: 6.01s 399: learn: 0.0442384 total: 4s remaining: 6s 400: learn: 0.0442067 total: 4.01s remaining: 5.99s 401: learn: 0.0441496 total: 4.02s remaining: 5.98s 402: learn: 0.0440668 total: 4.03s remaining: 5.97s 403: learn: 0.0440114 total: 4.04s remaining: 5.96s 404: learn: 0.0439772 total: 4.05s remaining: 5.95s 405: learn: 0.0439321 total: 4.06s remaining: 5.94s 406: learn: 0.0438647 total: 4.07s remaining: 5.93s 407: learn: 0.0438319 total: 4.08s remaining: 5.92s 408: learn: 0.0438057 total: 4.08s remaining: 5.9s 409: learn: 0.0437450 total: 4.09s remaining: 5.89s 410: learn: 0.0436672 total: 4.1s remaining: 5.88s 411: learn: 0.0436331 total: 4.11s remaining: 5.87s 412: learn: 0.0435807 total: 4.14s remaining: 5.89s 413: learn: 0.0435479 total: 4.15s remaining: 5.88s 414: learn: 0.0435443 total: 4.16s remaining: 5.87s 415: learn: 0.0435122 total: 4.17s remaining: 5.85s 416: learn: 0.0434599 total: 4.18s remaining: 5.84s 417: learn: 0.0434006 total: 4.19s remaining: 5.83s 418: learn: 0.0433400 total: 4.2s remaining: 5.82s 419: learn: 0.0432949 total: 4.21s remaining: 5.81s 420: learn: 0.0432622 total: 4.22s remaining: 5.8s 421: learn: 0.0432130 total: 4.22s remaining: 5.79s 422: learn: 0.0431644 total: 4.23s remaining: 5.78s 423: learn: 0.0431300 total: 4.24s remaining: 5.76s 424: learn: 0.0430583 total: 4.25s remaining: 5.75s 425: learn: 0.0430320 total: 4.26s remaining: 5.74s 426: learn: 0.0429763 total: 4.27s remaining: 5.73s 427: learn: 0.0429210 total: 4.28s remaining: 5.72s 428: learn: 0.0428839 total: 4.29s remaining: 5.71s 429: learn: 0.0428720 total: 4.3s remaining: 5.7s 430: learn: 0.0428310 total: 4.31s remaining: 5.69s 431: learn: 0.0428279 total: 4.32s remaining: 5.67s 432: learn: 0.0428050 total: 4.33s remaining: 5.66s 433: learn: 0.0427742 total: 4.33s remaining: 5.65s 434: learn: 0.0427201 total: 4.35s remaining: 5.65s 435: learn: 0.0427099 total: 4.36s remaining: 5.64s 436: learn: 0.0426646 total: 4.37s remaining: 5.63s 437: learn: 0.0426111 total: 4.38s remaining: 5.62s 438: learn: 0.0425610 total: 4.39s remaining: 5.61s 439: learn: 0.0425045 total: 4.4s remaining: 5.59s 440: learn: 0.0424678 total: 4.41s remaining: 5.58s 441: learn: 0.0424099 total: 4.41s remaining: 5.57s 442: learn: 0.0423492 total: 4.42s remaining: 5.56s 443: learn: 0.0422926 total: 4.43s remaining: 5.55s 444: learn: 0.0422422 total: 4.44s remaining: 5.54s 445: learn: 0.0422081 total: 4.45s remaining: 5.53s 446: learn: 0.0422058 total: 4.46s remaining: 5.51s 447: learn: 0.0421640 total: 4.47s remaining: 5.5s 448: learn: 0.0421065 total: 4.48s remaining: 5.49s 449: learn: 0.0420690 total: 4.49s remaining: 5.48s 450: learn: 0.0420189 total: 4.49s remaining: 5.47s 451: learn: 0.0419587 total: 4.5s remaining: 5.46s 452: learn: 0.0419198 total: 4.51s remaining: 5.45s 453: learn: 0.0418775 total: 4.52s remaining: 5.44s 454: learn: 0.0418452 total: 4.53s remaining: 5.43s 455: learn: 0.0418415 total: 4.54s remaining: 5.42s 456: learn: 0.0417977 total: 4.55s remaining: 5.41s 457: learn: 0.0417524 total: 4.57s remaining: 5.41s 458: learn: 0.0417182 total: 4.58s remaining: 5.4s 459: learn: 0.0416746 total: 4.59s remaining: 5.39s 460: learn: 0.0416423 total: 4.61s remaining: 5.38s 461: learn: 0.0416071 total: 4.61s remaining: 5.37s 462: learn: 0.0415636 total: 4.62s remaining: 5.36s 463: learn: 0.0415580 total: 4.63s remaining: 5.35s 464: learn: 0.0414907 total: 4.64s remaining: 5.34s 465: learn: 0.0414425 total: 4.65s remaining: 5.33s 466: learn: 0.0413999 total: 4.66s remaining: 5.32s 467: learn: 0.0413790 total: 4.67s remaining: 5.31s 468: learn: 0.0413445 total: 4.68s remaining: 5.3s 469: learn: 0.0413151 total: 4.69s remaining: 5.29s 470: learn: 0.0412610 total: 4.7s remaining: 5.28s 471: learn: 0.0412318 total: 4.71s remaining: 5.27s 472: learn: 0.0411887 total: 4.73s remaining: 5.26s 473: learn: 0.0411471 total: 4.74s remaining: 5.25s 474: learn: 0.0411099 total: 4.75s remaining: 5.24s 475: learn: 0.0410867 total: 4.76s remaining: 5.24s 476: learn: 0.0410334 total: 4.78s remaining: 5.24s 477: learn: 0.0409878 total: 4.79s remaining: 5.22s 478: learn: 0.0409324 total: 4.79s remaining: 5.21s 479: learn: 0.0408666 total: 4.8s remaining: 5.21s 480: learn: 0.0408411 total: 4.81s remaining: 5.19s 481: learn: 0.0407807 total: 4.82s remaining: 5.18s 482: learn: 0.0407403 total: 4.83s remaining: 5.17s 483: learn: 0.0407362 total: 4.84s remaining: 5.16s 484: learn: 0.0407059 total: 4.85s remaining: 5.15s 485: learn: 0.0406778 total: 4.86s remaining: 5.14s 486: learn: 0.0406335 total: 4.87s remaining: 5.13s 487: learn: 0.0406002 total: 4.88s remaining: 5.12s 488: learn: 0.0405777 total: 4.89s remaining: 5.11s 489: learn: 0.0405433 total: 4.9s remaining: 5.1s 490: learn: 0.0404846 total: 4.91s remaining: 5.09s 491: learn: 0.0404825 total: 4.92s remaining: 5.08s 492: learn: 0.0404440 total: 4.93s remaining: 5.07s 493: learn: 0.0403965 total: 4.94s remaining: 5.06s 494: learn: 0.0403517 total: 4.95s remaining: 5.05s 495: learn: 0.0403120 total: 4.96s remaining: 5.04s 496: learn: 0.0403095 total: 4.97s remaining: 5.03s 497: learn: 0.0402366 total: 4.98s remaining: 5.02s 498: learn: 0.0401956 total: 4.99s remaining: 5.01s 499: learn: 0.0401573 total: 5s remaining: 5s 500: learn: 0.0401143 total: 5.01s remaining: 4.99s 501: learn: 0.0400671 total: 5.02s remaining: 4.98s 502: learn: 0.0400629 total: 5.03s remaining: 4.97s 503: learn: 0.0400288 total: 5.04s remaining: 4.96s 504: learn: 0.0399990 total: 5.05s remaining: 4.95s 505: learn: 0.0399972 total: 5.05s remaining: 4.94s 506: learn: 0.0399607 total: 5.06s remaining: 4.92s 507: learn: 0.0399247 total: 5.07s remaining: 4.91s 508: learn: 0.0399208 total: 5.08s remaining: 4.9s 509: learn: 0.0398845 total: 5.09s remaining: 4.89s 510: learn: 0.0398822 total: 5.1s remaining: 4.88s 511: learn: 0.0398532 total: 5.12s remaining: 4.88s 512: learn: 0.0398146 total: 5.13s remaining: 4.87s 513: learn: 0.0397862 total: 5.14s remaining: 4.86s 514: learn: 0.0397545 total: 5.15s remaining: 4.85s 515: learn: 0.0397524 total: 5.15s remaining: 4.83s 516: learn: 0.0397503 total: 5.17s remaining: 4.83s 517: learn: 0.0397057 total: 5.18s remaining: 4.82s 518: learn: 0.0396693 total: 5.19s remaining: 4.81s 519: learn: 0.0396378 total: 5.2s remaining: 4.8s 520: learn: 0.0395906 total: 5.21s remaining: 4.79s 521: learn: 0.0395483 total: 5.22s remaining: 4.78s 522: learn: 0.0395161 total: 5.23s remaining: 4.77s 523: learn: 0.0394849 total: 5.24s remaining: 4.76s 524: learn: 0.0394622 total: 5.25s remaining: 4.75s 525: learn: 0.0393983 total: 5.26s remaining: 4.74s 526: learn: 0.0393481 total: 5.27s remaining: 4.73s 527: learn: 0.0393235 total: 5.28s remaining: 4.72s 528: learn: 0.0392908 total: 5.29s remaining: 4.71s 529: learn: 0.0392483 total: 5.3s remaining: 4.7s 530: learn: 0.0392238 total: 5.31s remaining: 4.69s 531: learn: 0.0391946 total: 5.32s remaining: 4.68s 532: learn: 0.0391901 total: 5.33s remaining: 4.67s 533: learn: 0.0391543 total: 5.34s remaining: 4.66s 534: learn: 0.0391270 total: 5.34s remaining: 4.64s 535: learn: 0.0391096 total: 5.36s remaining: 4.64s 536: learn: 0.0390742 total: 5.37s remaining: 4.63s 537: learn: 0.0390584 total: 5.38s remaining: 4.62s 538: learn: 0.0390264 total: 5.39s remaining: 4.61s 539: learn: 0.0389896 total: 5.4s remaining: 4.6s 540: learn: 0.0389378 total: 5.41s remaining: 4.59s 541: learn: 0.0389104 total: 5.42s remaining: 4.58s 542: learn: 0.0388763 total: 5.43s remaining: 4.57s 543: learn: 0.0388463 total: 5.43s remaining: 4.56s 544: learn: 0.0388010 total: 5.44s remaining: 4.54s 545: learn: 0.0387692 total: 5.45s remaining: 4.53s 546: learn: 0.0387314 total: 5.46s remaining: 4.52s 547: learn: 0.0387101 total: 5.47s remaining: 4.51s 548: learn: 0.0386745 total: 5.48s remaining: 4.5s 549: learn: 0.0386508 total: 5.49s remaining: 4.49s 550: learn: 0.0385840 total: 5.5s remaining: 4.48s 551: learn: 0.0385618 total: 5.51s remaining: 4.47s 552: learn: 0.0385243 total: 5.52s remaining: 4.46s 553: learn: 0.0384950 total: 5.53s remaining: 4.45s 554: learn: 0.0384554 total: 5.54s remaining: 4.44s 555: learn: 0.0384539 total: 5.55s remaining: 4.43s 556: learn: 0.0384132 total: 5.56s remaining: 4.42s 557: learn: 0.0383619 total: 5.57s remaining: 4.41s 558: learn: 0.0383322 total: 5.58s remaining: 4.41s 559: learn: 0.0382939 total: 5.59s remaining: 4.39s 560: learn: 0.0382563 total: 5.6s remaining: 4.38s 561: learn: 0.0382183 total: 5.61s remaining: 4.37s 562: learn: 0.0382009 total: 5.62s remaining: 4.36s 563: learn: 0.0381510 total: 5.63s remaining: 4.35s 564: learn: 0.0381413 total: 5.64s remaining: 4.34s 565: learn: 0.0380840 total: 5.65s remaining: 4.33s 566: learn: 0.0380573 total: 5.66s remaining: 4.32s 567: learn: 0.0380310 total: 5.67s remaining: 4.31s 568: learn: 0.0379963 total: 5.68s remaining: 4.3s 569: learn: 0.0379785 total: 5.69s remaining: 4.29s 570: learn: 0.0379505 total: 5.7s remaining: 4.28s 571: learn: 0.0379008 total: 5.71s remaining: 4.27s 572: learn: 0.0378726 total: 5.72s remaining: 4.26s 573: learn: 0.0378290 total: 5.73s remaining: 4.25s 574: learn: 0.0377850 total: 5.74s remaining: 4.24s 575: learn: 0.0377432 total: 5.75s remaining: 4.23s 576: learn: 0.0377228 total: 5.75s remaining: 4.22s 577: learn: 0.0377156 total: 5.77s remaining: 4.21s 578: learn: 0.0376818 total: 5.79s remaining: 4.21s 579: learn: 0.0376528 total: 5.8s remaining: 4.2s 580: learn: 0.0376506 total: 5.81s remaining: 4.19s 581: learn: 0.0376069 total: 5.82s remaining: 4.18s 582: learn: 0.0375665 total: 5.83s remaining: 4.17s 583: learn: 0.0375651 total: 5.84s remaining: 4.16s 584: learn: 0.0375370 total: 5.84s remaining: 4.15s 585: learn: 0.0375217 total: 5.86s remaining: 4.14s 586: learn: 0.0374915 total: 5.87s remaining: 4.13s 587: learn: 0.0374895 total: 5.88s remaining: 4.12s 588: learn: 0.0374329 total: 5.88s remaining: 4.11s 589: learn: 0.0373987 total: 5.89s remaining: 4.09s 590: learn: 0.0373655 total: 5.9s remaining: 4.08s 591: learn: 0.0373632 total: 5.91s remaining: 4.08s 592: learn: 0.0373222 total: 5.92s remaining: 4.06s 593: learn: 0.0372925 total: 5.93s remaining: 4.05s 594: learn: 0.0372627 total: 5.94s remaining: 4.04s 595: learn: 0.0372007 total: 5.95s remaining: 4.03s 596: learn: 0.0371714 total: 5.96s remaining: 4.02s 597: learn: 0.0371701 total: 5.97s remaining: 4.01s 598: learn: 0.0371468 total: 5.98s remaining: 4s 599: learn: 0.0371084 total: 6s remaining: 4s 600: learn: 0.0370813 total: 6.01s remaining: 3.99s 601: learn: 0.0370560 total: 6.02s remaining: 3.98s 602: learn: 0.0370102 total: 6.03s remaining: 3.97s 603: learn: 0.0370087 total: 6.04s remaining: 3.96s 604: learn: 0.0370048 total: 6.04s remaining: 3.95s 605: learn: 0.0369722 total: 6.05s remaining: 3.94s 606: learn: 0.0369707 total: 6.06s remaining: 3.92s 607: learn: 0.0369435 total: 6.07s remaining: 3.92s 608: learn: 0.0368964 total: 6.09s remaining: 3.91s 609: learn: 0.0368435 total: 6.1s remaining: 3.9s 610: learn: 0.0368420 total: 6.11s remaining: 3.89s 611: learn: 0.0368407 total: 6.12s remaining: 3.88s 612: learn: 0.0368179 total: 6.13s remaining: 3.87s 613: learn: 0.0367735 total: 6.14s remaining: 3.86s 614: learn: 0.0367440 total: 6.14s remaining: 3.85s 615: learn: 0.0367118 total: 6.15s remaining: 3.84s 616: learn: 0.0366631 total: 6.16s remaining: 3.83s 617: learn: 0.0366423 total: 6.17s remaining: 3.82s 618: learn: 0.0366410 total: 6.19s remaining: 3.81s 619: learn: 0.0366121 total: 6.2s remaining: 3.8s 620: learn: 0.0365779 total: 6.21s remaining: 3.79s 621: learn: 0.0365576 total: 6.22s remaining: 3.78s 622: learn: 0.0365138 total: 6.23s remaining: 3.77s 623: learn: 0.0364864 total: 6.24s remaining: 3.76s 624: learn: 0.0364841 total: 6.25s remaining: 3.75s 625: learn: 0.0364559 total: 6.26s remaining: 3.74s 626: learn: 0.0364030 total: 6.27s remaining: 3.73s 627: learn: 0.0363731 total: 6.28s remaining: 3.72s 628: learn: 0.0363220 total: 6.29s remaining: 3.71s 629: learn: 0.0362895 total: 6.29s remaining: 3.7s 630: learn: 0.0362537 total: 6.3s remaining: 3.69s 631: learn: 0.0362525 total: 6.31s remaining: 3.68s 632: learn: 0.0362181 total: 6.32s remaining: 3.67s 633: learn: 0.0361807 total: 6.33s remaining: 3.65s 634: learn: 0.0361569 total: 6.34s remaining: 3.65s 635: learn: 0.0361402 total: 6.36s remaining: 3.64s 636: learn: 0.0361163 total: 6.36s remaining: 3.63s 637: learn: 0.0361025 total: 6.37s remaining: 3.62s 638: learn: 0.0360843 total: 6.38s remaining: 3.6s 639: learn: 0.0360478 total: 6.4s remaining: 3.6s 640: learn: 0.0360133 total: 6.41s remaining: 3.59s 641: learn: 0.0359717 total: 6.42s remaining: 3.58s 642: learn: 0.0359489 total: 6.43s remaining: 3.57s 643: learn: 0.0359191 total: 6.43s remaining: 3.56s 644: learn: 0.0358844 total: 6.45s remaining: 3.55s 645: learn: 0.0358405 total: 6.46s remaining: 3.54s 646: learn: 0.0358176 total: 6.46s remaining: 3.53s 647: learn: 0.0358024 total: 6.47s remaining: 3.52s 648: learn: 0.0357640 total: 6.48s remaining: 3.51s 649: learn: 0.0357262 total: 6.49s remaining: 3.5s 650: learn: 0.0356992 total: 6.5s remaining: 3.48s 651: learn: 0.0356685 total: 6.51s remaining: 3.48s 652: learn: 0.0356352 total: 6.52s remaining: 3.46s 653: learn: 0.0356035 total: 6.53s remaining: 3.45s 654: learn: 0.0355619 total: 6.54s remaining: 3.44s 655: learn: 0.0355342 total: 6.55s remaining: 3.43s 656: learn: 0.0355061 total: 6.56s remaining: 3.42s 657: learn: 0.0355052 total: 6.57s remaining: 3.41s 658: learn: 0.0354720 total: 6.58s remaining: 3.4s 659: learn: 0.0354516 total: 6.58s remaining: 3.39s 660: learn: 0.0354135 total: 6.59s remaining: 3.38s 661: learn: 0.0353844 total: 6.61s remaining: 3.38s 662: learn: 0.0353451 total: 6.63s remaining: 3.37s 663: learn: 0.0353072 total: 6.64s remaining: 3.36s 664: learn: 0.0352702 total: 6.65s remaining: 3.35s 665: learn: 0.0352250 total: 6.66s remaining: 3.34s 666: learn: 0.0352233 total: 6.67s remaining: 3.33s 667: learn: 0.0352033 total: 6.67s remaining: 3.32s 668: learn: 0.0351770 total: 6.68s remaining: 3.31s 669: learn: 0.0351514 total: 6.69s remaining: 3.3s 670: learn: 0.0351498 total: 6.7s remaining: 3.29s 671: learn: 0.0351247 total: 6.71s remaining: 3.27s 672: learn: 0.0351076 total: 6.72s remaining: 3.26s 673: learn: 0.0350642 total: 6.73s remaining: 3.25s 674: learn: 0.0350294 total: 6.74s remaining: 3.24s 675: learn: 0.0349978 total: 6.74s remaining: 3.23s 676: learn: 0.0349816 total: 6.75s remaining: 3.22s 677: learn: 0.0349463 total: 6.76s remaining: 3.21s 678: learn: 0.0349191 total: 6.77s remaining: 3.2s 679: learn: 0.0348921 total: 6.78s remaining: 3.19s 680: learn: 0.0348544 total: 6.79s remaining: 3.18s 681: learn: 0.0348232 total: 6.81s remaining: 3.17s 682: learn: 0.0347855 total: 6.82s remaining: 3.17s 683: learn: 0.0347405 total: 6.83s remaining: 3.15s 684: learn: 0.0347256 total: 6.84s remaining: 3.14s 685: learn: 0.0347088 total: 6.85s remaining: 3.13s 686: learn: 0.0346694 total: 6.86s remaining: 3.13s 687: learn: 0.0346353 total: 6.88s remaining: 3.12s 688: learn: 0.0346122 total: 6.89s remaining: 3.11s 689: learn: 0.0346004 total: 6.91s remaining: 3.1s 690: learn: 0.0345725 total: 6.92s remaining: 3.09s 691: learn: 0.0345333 total: 6.93s remaining: 3.08s 692: learn: 0.0344965 total: 6.94s remaining: 3.07s 693: learn: 0.0344523 total: 6.95s remaining: 3.06s 694: learn: 0.0344289 total: 6.96s remaining: 3.05s 695: learn: 0.0343997 total: 6.96s remaining: 3.04s 696: learn: 0.0343605 total: 6.97s remaining: 3.03s 697: learn: 0.0343227 total: 6.99s remaining: 3.02s 698: learn: 0.0342983 total: 7.01s remaining: 3.02s 699: learn: 0.0342766 total: 7.02s remaining: 3.01s 700: learn: 0.0342487 total: 7.03s remaining: 3s 701: learn: 0.0342334 total: 7.04s remaining: 2.99s 702: learn: 0.0342012 total: 7.05s remaining: 2.98s 703: learn: 0.0341680 total: 7.07s remaining: 2.97s 704: learn: 0.0341399 total: 7.08s remaining: 2.96s 705: learn: 0.0341320 total: 7.09s remaining: 2.95s 706: learn: 0.0341128 total: 7.1s remaining: 2.94s 707: learn: 0.0340962 total: 7.11s remaining: 2.93s 708: learn: 0.0340701 total: 7.12s remaining: 2.92s 709: learn: 0.0340524 total: 7.13s remaining: 2.91s 710: learn: 0.0340321 total: 7.13s remaining: 2.9s 711: learn: 0.0340103 total: 7.14s remaining: 2.89s 712: learn: 0.0339872 total: 7.15s remaining: 2.88s 713: learn: 0.0339648 total: 7.16s remaining: 2.87s 714: learn: 0.0339335 total: 7.17s remaining: 2.86s 715: learn: 0.0339265 total: 7.18s remaining: 2.85s 716: learn: 0.0339022 total: 7.19s remaining: 2.84s 717: learn: 0.0338826 total: 7.2s remaining: 2.83s 718: learn: 0.0338561 total: 7.22s remaining: 2.82s 719: learn: 0.0338257 total: 7.23s remaining: 2.81s 720: learn: 0.0337963 total: 7.24s remaining: 2.8s 721: learn: 0.0337652 total: 7.25s remaining: 2.79s 722: learn: 0.0337380 total: 7.25s remaining: 2.78s 723: learn: 0.0337199 total: 7.26s remaining: 2.77s 724: learn: 0.0336936 total: 7.27s remaining: 2.76s 725: learn: 0.0336676 total: 7.28s remaining: 2.75s 726: learn: 0.0336440 total: 7.29s remaining: 2.74s 727: learn: 0.0336149 total: 7.3s remaining: 2.73s 728: learn: 0.0335761 total: 7.31s remaining: 2.72s 729: learn: 0.0335490 total: 7.32s remaining: 2.71s 730: learn: 0.0335406 total: 7.33s remaining: 2.7s 731: learn: 0.0335089 total: 7.34s remaining: 2.69s 732: learn: 0.0334797 total: 7.35s remaining: 2.67s 733: learn: 0.0334589 total: 7.35s remaining: 2.67s 734: learn: 0.0334541 total: 7.36s remaining: 2.65s 735: learn: 0.0334178 total: 7.37s remaining: 2.64s 736: learn: 0.0333841 total: 7.38s remaining: 2.63s 737: learn: 0.0333539 total: 7.39s remaining: 2.62s 738: learn: 0.0333160 total: 7.4s remaining: 2.61s 739: learn: 0.0332993 total: 7.41s remaining: 2.6s 740: learn: 0.0332542 total: 7.42s remaining: 2.6s 741: learn: 0.0332387 total: 7.44s remaining: 2.58s 742: learn: 0.0332223 total: 7.45s remaining: 2.58s 743: learn: 0.0332016 total: 7.45s remaining: 2.56s 744: learn: 0.0331604 total: 7.47s remaining: 2.56s 745: learn: 0.0331498 total: 7.48s remaining: 2.54s 746: learn: 0.0331295 total: 7.49s remaining: 2.54s 747: learn: 0.0331042 total: 7.5s remaining: 2.53s 748: learn: 0.0330876 total: 7.51s remaining: 2.52s 749: learn: 0.0330607 total: 7.52s remaining: 2.5s 750: learn: 0.0330263 total: 7.53s remaining: 2.5s 751: learn: 0.0329870 total: 7.54s remaining: 2.48s 752: learn: 0.0329471 total: 7.54s remaining: 2.47s 753: learn: 0.0329148 total: 7.55s remaining: 2.46s 754: learn: 0.0328875 total: 7.56s remaining: 2.45s 755: learn: 0.0328623 total: 7.57s remaining: 2.44s 756: learn: 0.0328425 total: 7.58s remaining: 2.43s 757: learn: 0.0328050 total: 7.59s remaining: 2.42s 758: learn: 0.0327807 total: 7.6s remaining: 2.41s 759: learn: 0.0327590 total: 7.61s remaining: 2.4s 760: learn: 0.0327381 total: 7.62s remaining: 2.39s 761: learn: 0.0327163 total: 7.64s remaining: 2.38s 762: learn: 0.0327024 total: 7.64s remaining: 2.37s 763: learn: 0.0326680 total: 7.65s remaining: 2.36s 764: learn: 0.0326419 total: 7.66s remaining: 2.35s 765: learn: 0.0326197 total: 7.67s remaining: 2.34s 766: learn: 0.0326010 total: 7.68s remaining: 2.33s 767: learn: 0.0325870 total: 7.69s remaining: 2.32s 768: learn: 0.0325638 total: 7.7s remaining: 2.31s 769: learn: 0.0325435 total: 7.71s remaining: 2.3s 770: learn: 0.0325026 total: 7.72s remaining: 2.29s 771: learn: 0.0324903 total: 7.73s remaining: 2.28s 772: learn: 0.0324756 total: 7.74s remaining: 2.27s 773: learn: 0.0324408 total: 7.74s remaining: 2.26s 774: learn: 0.0324253 total: 7.75s remaining: 2.25s 775: learn: 0.0323957 total: 7.76s remaining: 2.24s 776: learn: 0.0323471 total: 7.77s remaining: 2.23s 777: learn: 0.0323263 total: 7.78s remaining: 2.22s 778: learn: 0.0323192 total: 7.79s remaining: 2.21s 779: learn: 0.0323183 total: 7.8s remaining: 2.2s 780: learn: 0.0322947 total: 7.81s remaining: 2.19s 781: learn: 0.0322697 total: 7.82s remaining: 2.18s 782: learn: 0.0322624 total: 7.83s remaining: 2.17s 783: learn: 0.0322438 total: 7.86s remaining: 2.16s 784: learn: 0.0322089 total: 7.86s remaining: 2.15s 785: learn: 0.0321793 total: 7.87s remaining: 2.14s 786: learn: 0.0321686 total: 7.88s remaining: 2.13s 787: learn: 0.0321400 total: 7.89s remaining: 2.12s 788: learn: 0.0321217 total: 7.9s remaining: 2.11s 789: learn: 0.0321048 total: 7.91s remaining: 2.1s 790: learn: 0.0320978 total: 7.92s remaining: 2.09s 791: learn: 0.0320770 total: 7.93s remaining: 2.08s 792: learn: 0.0320511 total: 7.94s remaining: 2.07s 793: learn: 0.0320159 total: 7.95s remaining: 2.06s 794: learn: 0.0319919 total: 7.96s remaining: 2.05s 795: learn: 0.0319807 total: 7.97s remaining: 2.04s 796: learn: 0.0319564 total: 7.98s remaining: 2.03s 797: learn: 0.0319248 total: 7.99s remaining: 2.02s 798: learn: 0.0318922 total: 8.01s remaining: 2.01s 799: learn: 0.0318789 total: 8.02s remaining: 2s 800: learn: 0.0318640 total: 8.03s remaining: 2s 801: learn: 0.0318362 total: 8.06s remaining: 1.99s 802: learn: 0.0318178 total: 8.07s remaining: 1.98s 803: learn: 0.0317922 total: 8.08s remaining: 1.97s 804: learn: 0.0317700 total: 8.09s remaining: 1.96s 805: learn: 0.0317691 total: 8.09s remaining: 1.95s 806: learn: 0.0317386 total: 8.1s remaining: 1.94s 807: learn: 0.0317031 total: 8.11s remaining: 1.93s 808: learn: 0.0316661 total: 8.12s remaining: 1.92s 809: learn: 0.0316462 total: 8.13s remaining: 1.91s 810: learn: 0.0316223 total: 8.14s remaining: 1.9s 811: learn: 0.0315984 total: 8.15s remaining: 1.89s 812: learn: 0.0315613 total: 8.16s remaining: 1.88s 813: learn: 0.0315337 total: 8.17s remaining: 1.87s 814: learn: 0.0315075 total: 8.18s remaining: 1.86s 815: learn: 0.0314906 total: 8.19s remaining: 1.85s 816: learn: 0.0314644 total: 8.2s remaining: 1.84s 817: learn: 0.0314504 total: 8.21s remaining: 1.83s 818: learn: 0.0314298 total: 8.22s remaining: 1.82s 819: learn: 0.0314080 total: 8.23s remaining: 1.81s 820: learn: 0.0314005 total: 8.24s remaining: 1.8s 821: learn: 0.0313802 total: 8.25s remaining: 1.79s 822: learn: 0.0313793 total: 8.27s remaining: 1.78s 823: learn: 0.0313784 total: 8.28s remaining: 1.77s 824: learn: 0.0313573 total: 8.29s remaining: 1.76s 825: learn: 0.0313251 total: 8.3s remaining: 1.75s 826: learn: 0.0313008 total: 8.31s remaining: 1.74s 827: learn: 0.0312760 total: 8.32s remaining: 1.73s 828: learn: 0.0312562 total: 8.33s remaining: 1.72s 829: learn: 0.0312424 total: 8.34s remaining: 1.71s 830: learn: 0.0312197 total: 8.35s remaining: 1.7s 831: learn: 0.0311956 total: 8.36s remaining: 1.69s 832: learn: 0.0311884 total: 8.37s remaining: 1.68s 833: learn: 0.0311677 total: 8.38s remaining: 1.67s 834: learn: 0.0311411 total: 8.39s remaining: 1.66s 835: learn: 0.0311302 total: 8.4s remaining: 1.65s 836: learn: 0.0311059 total: 8.41s remaining: 1.64s 837: learn: 0.0310873 total: 8.42s remaining: 1.63s 838: learn: 0.0310555 total: 8.43s remaining: 1.62s 839: learn: 0.0310286 total: 8.44s remaining: 1.61s 840: learn: 0.0310152 total: 8.44s remaining: 1.6s 841: learn: 0.0310143 total: 8.46s remaining: 1.59s 842: learn: 0.0309939 total: 8.47s remaining: 1.58s 843: learn: 0.0309664 total: 8.48s remaining: 1.57s 844: learn: 0.0309326 total: 8.49s remaining: 1.56s 845: learn: 0.0309165 total: 8.5s remaining: 1.55s 846: learn: 0.0308962 total: 8.51s remaining: 1.54s 847: learn: 0.0308667 total: 8.52s remaining: 1.53s 848: learn: 0.0308463 total: 8.53s remaining: 1.52s 849: learn: 0.0308262 total: 8.54s remaining: 1.51s 850: learn: 0.0308102 total: 8.54s remaining: 1.5s 851: learn: 0.0307815 total: 8.55s remaining: 1.49s 852: learn: 0.0307528 total: 8.56s remaining: 1.48s 853: learn: 0.0307317 total: 8.57s remaining: 1.47s 854: learn: 0.0307308 total: 8.59s remaining: 1.46s 855: learn: 0.0307093 total: 8.6s remaining: 1.45s 856: learn: 0.0306982 total: 8.61s remaining: 1.44s 857: learn: 0.0306673 total: 8.61s remaining: 1.43s 858: learn: 0.0306483 total: 8.62s remaining: 1.42s 859: learn: 0.0306292 total: 8.63s remaining: 1.41s 860: learn: 0.0306128 total: 8.64s remaining: 1.4s 861: learn: 0.0305859 total: 8.65s remaining: 1.38s 862: learn: 0.0305742 total: 8.67s remaining: 1.38s 863: learn: 0.0305585 total: 8.68s remaining: 1.37s 864: learn: 0.0305577 total: 8.69s remaining: 1.36s 865: learn: 0.0305435 total: 8.7s remaining: 1.34s 866: learn: 0.0305225 total: 8.71s remaining: 1.33s 867: learn: 0.0305037 total: 8.72s remaining: 1.32s 868: learn: 0.0304818 total: 8.72s remaining: 1.31s 869: learn: 0.0304529 total: 8.74s remaining: 1.3s 870: learn: 0.0304290 total: 8.75s remaining: 1.29s 871: learn: 0.0304100 total: 8.76s remaining: 1.28s 872: learn: 0.0303784 total: 8.77s remaining: 1.27s 873: learn: 0.0303558 total: 8.77s remaining: 1.26s 874: learn: 0.0303391 total: 8.78s remaining: 1.25s 875: learn: 0.0303234 total: 8.79s remaining: 1.24s 876: learn: 0.0302862 total: 8.8s remaining: 1.23s 877: learn: 0.0302741 total: 8.81s remaining: 1.22s 878: learn: 0.0302597 total: 8.82s remaining: 1.21s 879: learn: 0.0302340 total: 8.83s remaining: 1.2s 880: learn: 0.0302121 total: 8.84s remaining: 1.19s 881: learn: 0.0301908 total: 8.85s remaining: 1.18s 882: learn: 0.0301899 total: 8.86s remaining: 1.17s 883: learn: 0.0301731 total: 8.87s remaining: 1.16s 884: learn: 0.0301578 total: 8.88s remaining: 1.15s 885: learn: 0.0301411 total: 8.89s remaining: 1.14s 886: learn: 0.0301154 total: 8.9s remaining: 1.13s 887: learn: 0.0300982 total: 8.91s remaining: 1.12s 888: learn: 0.0300814 total: 8.92s remaining: 1.11s 889: learn: 0.0300807 total: 8.93s remaining: 1.1s 890: learn: 0.0300486 total: 8.94s remaining: 1.09s 891: learn: 0.0300187 total: 8.94s remaining: 1.08s 892: learn: 0.0299923 total: 8.95s remaining: 1.07s 893: learn: 0.0299794 total: 8.96s remaining: 1.06s 894: learn: 0.0299594 total: 8.97s remaining: 1.05s 895: learn: 0.0299444 total: 8.98s remaining: 1.04s 896: learn: 0.0299276 total: 8.99s remaining: 1.03s 897: learn: 0.0299065 total: 9s remaining: 1.02s 898: learn: 0.0299053 total: 9.02s remaining: 1.01s 899: learn: 0.0298891 total: 9.03s remaining: 1s 900: learn: 0.0298653 total: 9.03s remaining: 993ms 901: learn: 0.0298458 total: 9.04s remaining: 982ms 902: learn: 0.0298342 total: 9.05s remaining: 972ms 903: learn: 0.0298082 total: 9.06s remaining: 963ms 904: learn: 0.0297930 total: 9.08s remaining: 953ms 905: learn: 0.0297669 total: 9.09s remaining: 943ms 906: learn: 0.0297478 total: 9.1s remaining: 933ms 907: learn: 0.0297259 total: 9.11s remaining: 923ms 908: learn: 0.0297155 total: 9.12s remaining: 913ms 909: learn: 0.0296930 total: 9.12s remaining: 902ms 910: learn: 0.0296760 total: 9.13s remaining: 892ms 911: learn: 0.0296579 total: 9.14s remaining: 882ms 912: learn: 0.0296394 total: 9.15s remaining: 872ms 913: learn: 0.0296234 total: 9.16s remaining: 862ms 914: learn: 0.0295994 total: 9.17s remaining: 852ms 915: learn: 0.0295772 total: 9.18s remaining: 842ms 916: learn: 0.0295761 total: 9.19s remaining: 832ms 917: learn: 0.0295552 total: 9.2s remaining: 821ms 918: learn: 0.0295321 total: 9.2s remaining: 811ms 919: learn: 0.0295146 total: 9.21s remaining: 801ms 920: learn: 0.0295106 total: 9.23s remaining: 791ms 921: learn: 0.0294890 total: 9.23s remaining: 781ms 922: learn: 0.0294868 total: 9.24s remaining: 771ms 923: learn: 0.0294684 total: 9.25s remaining: 761ms 924: learn: 0.0294455 total: 9.26s remaining: 751ms 925: learn: 0.0294085 total: 9.28s remaining: 742ms 926: learn: 0.0293875 total: 9.29s remaining: 732ms 927: learn: 0.0293780 total: 9.3s remaining: 722ms 928: learn: 0.0293502 total: 9.31s remaining: 711ms 929: learn: 0.0293369 total: 9.32s remaining: 701ms 930: learn: 0.0293035 total: 9.33s remaining: 691ms 931: learn: 0.0292738 total: 9.34s remaining: 681ms 932: learn: 0.0292732 total: 9.35s remaining: 671ms 933: learn: 0.0292482 total: 9.35s remaining: 661ms 934: learn: 0.0292375 total: 9.36s remaining: 651ms 935: learn: 0.0292235 total: 9.37s remaining: 641ms 936: learn: 0.0292022 total: 9.38s remaining: 631ms 937: learn: 0.0291898 total: 9.39s remaining: 621ms 938: learn: 0.0291708 total: 9.4s remaining: 611ms 939: learn: 0.0291520 total: 9.41s remaining: 601ms 940: learn: 0.0291226 total: 9.42s remaining: 590ms 941: learn: 0.0290982 total: 9.43s remaining: 580ms 942: learn: 0.0290835 total: 9.43s remaining: 570ms 943: learn: 0.0290682 total: 9.44s remaining: 560ms 944: learn: 0.0290531 total: 9.45s remaining: 550ms 945: learn: 0.0290209 total: 9.46s remaining: 540ms 946: learn: 0.0290038 total: 9.47s remaining: 530ms 947: learn: 0.0289792 total: 9.49s remaining: 521ms 948: learn: 0.0289678 total: 9.5s remaining: 511ms 949: learn: 0.0289671 total: 9.51s remaining: 501ms 950: learn: 0.0289599 total: 9.52s remaining: 491ms 951: learn: 0.0289592 total: 9.53s remaining: 481ms 952: learn: 0.0289401 total: 9.54s remaining: 471ms 953: learn: 0.0289269 total: 9.55s remaining: 461ms 954: learn: 0.0289022 total: 9.56s remaining: 451ms 955: learn: 0.0288873 total: 9.57s remaining: 440ms 956: learn: 0.0288806 total: 9.58s remaining: 430ms 957: learn: 0.0288660 total: 9.59s remaining: 420ms 958: learn: 0.0288572 total: 9.6s remaining: 410ms 959: learn: 0.0288308 total: 9.61s remaining: 400ms 960: learn: 0.0288251 total: 9.62s remaining: 390ms 961: learn: 0.0288243 total: 9.63s remaining: 380ms 962: learn: 0.0288238 total: 9.63s remaining: 370ms 963: learn: 0.0287900 total: 9.64s remaining: 360ms 964: learn: 0.0287629 total: 9.65s remaining: 350ms 965: learn: 0.0287286 total: 9.66s remaining: 340ms 966: learn: 0.0287040 total: 9.68s remaining: 330ms 967: learn: 0.0286883 total: 9.69s remaining: 320ms 968: learn: 0.0286683 total: 9.7s remaining: 310ms 969: learn: 0.0286447 total: 9.71s remaining: 300ms 970: learn: 0.0286179 total: 9.72s remaining: 290ms 971: learn: 0.0285996 total: 9.73s remaining: 280ms 972: learn: 0.0285709 total: 9.74s remaining: 270ms 973: learn: 0.0285529 total: 9.75s remaining: 260ms 974: learn: 0.0285322 total: 9.76s remaining: 250ms 975: learn: 0.0285168 total: 9.77s remaining: 240ms 976: learn: 0.0284956 total: 9.77s remaining: 230ms 977: learn: 0.0284633 total: 9.78s remaining: 220ms 978: learn: 0.0284404 total: 9.79s remaining: 210ms 979: learn: 0.0284122 total: 9.8s remaining: 200ms 980: learn: 0.0283941 total: 9.81s remaining: 190ms 981: learn: 0.0283775 total: 9.82s remaining: 180ms 982: learn: 0.0283519 total: 9.83s remaining: 170ms 983: learn: 0.0283269 total: 9.84s remaining: 160ms 984: learn: 0.0283060 total: 9.85s remaining: 150ms 985: learn: 0.0282935 total: 9.86s remaining: 140ms 986: learn: 0.0282818 total: 9.87s remaining: 130ms 987: learn: 0.0282617 total: 9.88s remaining: 120ms 988: learn: 0.0282391 total: 9.89s remaining: 110ms 989: learn: 0.0282181 total: 9.9s remaining: 100ms 990: learn: 0.0281956 total: 9.91s remaining: 90ms 991: learn: 0.0281788 total: 9.92s remaining: 80ms 992: learn: 0.0281630 total: 9.93s remaining: 70ms 993: learn: 0.0281447 total: 9.94s remaining: 60ms 994: learn: 0.0281278 total: 9.95s remaining: 50ms 995: learn: 0.0281153 total: 9.96s remaining: 40ms 996: learn: 0.0280953 total: 9.97s remaining: 30ms 997: learn: 0.0280749 total: 9.98s remaining: 20ms 998: learn: 0.0280405 total: 9.99s remaining: 10ms 999: learn: 0.0280211 total: 10s remaining: 0us Learning rate set to 0.046383 0: learn: 0.3974890 total: 8.79ms remaining: 8.78s 1: learn: 0.3856952 total: 16.8ms remaining: 8.4s 2: learn: 0.3737829 total: 24.9ms remaining: 8.28s 3: learn: 0.3630546 total: 33.5ms remaining: 8.33s 4: learn: 0.3528858 total: 41.8ms remaining: 8.32s 5: learn: 0.3421037 total: 50.2ms remaining: 8.31s 6: learn: 0.3322403 total: 58.6ms remaining: 8.31s 7: learn: 0.3225598 total: 66.8ms remaining: 8.28s 8: learn: 0.3134455 total: 74.9ms remaining: 8.24s 9: learn: 0.3047077 total: 83.1ms remaining: 8.23s 10: learn: 0.2962723 total: 91.4ms remaining: 8.22s 11: learn: 0.2884913 total: 99.7ms remaining: 8.21s 12: learn: 0.2808331 total: 107ms remaining: 8.15s 13: learn: 0.2737275 total: 115ms remaining: 8.11s 14: learn: 0.2671142 total: 124ms remaining: 8.12s 15: learn: 0.2610253 total: 131ms remaining: 8.07s 16: learn: 0.2545565 total: 139ms remaining: 8.04s 17: learn: 0.2485779 total: 147ms remaining: 8.01s 18: learn: 0.2427191 total: 155ms remaining: 7.99s 19: learn: 0.2377816 total: 163ms remaining: 8s 20: learn: 0.2328091 total: 171ms remaining: 7.99s 21: learn: 0.2279681 total: 179ms remaining: 7.97s 22: learn: 0.2234486 total: 187ms remaining: 7.94s 23: learn: 0.2184649 total: 202ms remaining: 8.22s 24: learn: 0.2139488 total: 212ms remaining: 8.28s 25: learn: 0.2095156 total: 220ms remaining: 8.24s 26: learn: 0.2052570 total: 228ms remaining: 8.21s 27: learn: 0.2016772 total: 236ms remaining: 8.19s 28: learn: 0.1980284 total: 244ms remaining: 8.16s 29: learn: 0.1947440 total: 252ms remaining: 8.14s 30: learn: 0.1916676 total: 259ms remaining: 8.11s 31: learn: 0.1886297 total: 267ms remaining: 8.07s 32: learn: 0.1855699 total: 275ms remaining: 8.05s 33: learn: 0.1827219 total: 282ms remaining: 8.02s 34: learn: 0.1797501 total: 290ms remaining: 8s 35: learn: 0.1770436 total: 298ms remaining: 7.97s 36: learn: 0.1746646 total: 305ms remaining: 7.95s 37: learn: 0.1721599 total: 313ms remaining: 7.93s 38: learn: 0.1698693 total: 321ms remaining: 7.91s 39: learn: 0.1676379 total: 328ms remaining: 7.88s 40: learn: 0.1655813 total: 336ms remaining: 7.86s 41: learn: 0.1635078 total: 344ms remaining: 7.84s 42: learn: 0.1616739 total: 352ms remaining: 7.84s 43: learn: 0.1598448 total: 360ms remaining: 7.82s 44: learn: 0.1578119 total: 368ms remaining: 7.8s 45: learn: 0.1561026 total: 375ms remaining: 7.79s 46: learn: 0.1543625 total: 383ms remaining: 7.77s 47: learn: 0.1529714 total: 391ms remaining: 7.75s 48: learn: 0.1515752 total: 402ms remaining: 7.81s 49: learn: 0.1501699 total: 420ms remaining: 7.99s 50: learn: 0.1487815 total: 428ms remaining: 7.97s 51: learn: 0.1472990 total: 436ms remaining: 7.94s 52: learn: 0.1459377 total: 443ms remaining: 7.92s 53: learn: 0.1447686 total: 451ms remaining: 7.89s 54: learn: 0.1435348 total: 458ms remaining: 7.87s 55: learn: 0.1424805 total: 473ms remaining: 7.97s 56: learn: 0.1412682 total: 484ms remaining: 8s 57: learn: 0.1401124 total: 492ms remaining: 7.98s 58: learn: 0.1390023 total: 500ms remaining: 7.98s 59: learn: 0.1378739 total: 509ms remaining: 7.97s 60: learn: 0.1368400 total: 519ms remaining: 7.99s 61: learn: 0.1359473 total: 528ms remaining: 7.99s 62: learn: 0.1349536 total: 536ms remaining: 7.97s 63: learn: 0.1341253 total: 544ms remaining: 7.96s 64: learn: 0.1332264 total: 552ms remaining: 7.93s 65: learn: 0.1324009 total: 560ms remaining: 7.92s 66: learn: 0.1315957 total: 568ms remaining: 7.91s 67: learn: 0.1309829 total: 579ms remaining: 7.93s 68: learn: 0.1302847 total: 588ms remaining: 7.93s 69: learn: 0.1294465 total: 599ms remaining: 7.96s 70: learn: 0.1287263 total: 616ms remaining: 8.06s 71: learn: 0.1280979 total: 623ms remaining: 8.04s 72: learn: 0.1274425 total: 631ms remaining: 8.02s 73: learn: 0.1268320 total: 639ms remaining: 7.99s 74: learn: 0.1259886 total: 650ms remaining: 8.02s 75: learn: 0.1254171 total: 658ms remaining: 8s 76: learn: 0.1248984 total: 665ms remaining: 7.97s 77: learn: 0.1242792 total: 673ms remaining: 7.95s 78: learn: 0.1236202 total: 681ms remaining: 7.93s 79: learn: 0.1230367 total: 689ms remaining: 7.93s 80: learn: 0.1224982 total: 697ms remaining: 7.91s 81: learn: 0.1218976 total: 705ms remaining: 7.89s 82: learn: 0.1214765 total: 712ms remaining: 7.87s 83: learn: 0.1210461 total: 720ms remaining: 7.85s 84: learn: 0.1205447 total: 727ms remaining: 7.83s 85: learn: 0.1202433 total: 735ms remaining: 7.81s 86: learn: 0.1198120 total: 742ms remaining: 7.79s 87: learn: 0.1194485 total: 750ms remaining: 7.78s 88: learn: 0.1190601 total: 758ms remaining: 7.76s 89: learn: 0.1186997 total: 766ms remaining: 7.74s 90: learn: 0.1183590 total: 774ms remaining: 7.73s 91: learn: 0.1182097 total: 781ms remaining: 7.71s 92: learn: 0.1177770 total: 789ms remaining: 7.69s 93: learn: 0.1174586 total: 807ms remaining: 7.78s 94: learn: 0.1170615 total: 815ms remaining: 7.76s 95: learn: 0.1166731 total: 822ms remaining: 7.74s 96: learn: 0.1162749 total: 834ms remaining: 7.76s 97: learn: 0.1159529 total: 841ms remaining: 7.74s 98: learn: 0.1155514 total: 849ms remaining: 7.72s 99: learn: 0.1151343 total: 856ms remaining: 7.71s 100: learn: 0.1147050 total: 864ms remaining: 7.69s 101: learn: 0.1143914 total: 871ms remaining: 7.67s 102: learn: 0.1140780 total: 879ms remaining: 7.65s 103: learn: 0.1136855 total: 886ms remaining: 7.63s 104: learn: 0.1133655 total: 893ms remaining: 7.62s 105: learn: 0.1131110 total: 901ms remaining: 7.6s 106: learn: 0.1128510 total: 908ms remaining: 7.58s 107: learn: 0.1125155 total: 916ms remaining: 7.57s 108: learn: 0.1122620 total: 925ms remaining: 7.56s 109: learn: 0.1120386 total: 932ms remaining: 7.54s 110: learn: 0.1117149 total: 939ms remaining: 7.52s 111: learn: 0.1115472 total: 946ms remaining: 7.5s 112: learn: 0.1113352 total: 953ms remaining: 7.48s 113: learn: 0.1112131 total: 961ms remaining: 7.47s 114: learn: 0.1109607 total: 969ms remaining: 7.45s 115: learn: 0.1106925 total: 977ms remaining: 7.44s 116: learn: 0.1106233 total: 984ms remaining: 7.43s 117: learn: 0.1103585 total: 993ms remaining: 7.42s 118: learn: 0.1100920 total: 1.01s remaining: 7.5s 119: learn: 0.1097979 total: 1.02s remaining: 7.48s 120: learn: 0.1095646 total: 1.03s remaining: 7.48s 121: learn: 0.1093464 total: 1.04s remaining: 7.47s 122: learn: 0.1091400 total: 1.04s remaining: 7.45s 123: learn: 0.1089535 total: 1.05s remaining: 7.44s 124: learn: 0.1088465 total: 1.06s remaining: 7.43s 125: learn: 0.1085554 total: 1.07s remaining: 7.42s 126: learn: 0.1085026 total: 1.08s remaining: 7.41s 127: learn: 0.1083721 total: 1.09s remaining: 7.42s 128: learn: 0.1081529 total: 1.1s remaining: 7.41s 129: learn: 0.1079002 total: 1.1s remaining: 7.4s 130: learn: 0.1077006 total: 1.11s remaining: 7.39s 131: learn: 0.1074498 total: 1.12s remaining: 7.38s 132: learn: 0.1072495 total: 1.13s remaining: 7.37s 133: learn: 0.1071009 total: 1.14s remaining: 7.35s 134: learn: 0.1069146 total: 1.14s remaining: 7.34s 135: learn: 0.1066990 total: 1.16s remaining: 7.34s 136: learn: 0.1064918 total: 1.16s remaining: 7.33s 137: learn: 0.1061959 total: 1.17s remaining: 7.32s 138: learn: 0.1058975 total: 1.18s remaining: 7.3s 139: learn: 0.1056632 total: 1.19s remaining: 7.32s 140: learn: 0.1054173 total: 1.21s remaining: 7.35s 141: learn: 0.1052448 total: 1.22s remaining: 7.36s 142: learn: 0.1050647 total: 1.23s remaining: 7.35s 143: learn: 0.1048602 total: 1.24s remaining: 7.37s 144: learn: 0.1047638 total: 1.25s remaining: 7.36s 145: learn: 0.1045644 total: 1.25s remaining: 7.35s 146: learn: 0.1043828 total: 1.26s remaining: 7.33s 147: learn: 0.1042127 total: 1.27s remaining: 7.32s 148: learn: 0.1041398 total: 1.28s remaining: 7.31s 149: learn: 0.1038827 total: 1.29s remaining: 7.3s 150: learn: 0.1036796 total: 1.3s remaining: 7.29s 151: learn: 0.1035285 total: 1.3s remaining: 7.28s 152: learn: 0.1032989 total: 1.31s remaining: 7.26s 153: learn: 0.1031290 total: 1.32s remaining: 7.25s 154: learn: 0.1030648 total: 1.33s remaining: 7.24s 155: learn: 0.1029661 total: 1.33s remaining: 7.22s 156: learn: 0.1028986 total: 1.34s remaining: 7.21s 157: learn: 0.1028596 total: 1.35s remaining: 7.2s 158: learn: 0.1027193 total: 1.36s remaining: 7.19s 159: learn: 0.1025176 total: 1.37s remaining: 7.17s 160: learn: 0.1023848 total: 1.37s remaining: 7.16s 161: learn: 0.1022649 total: 1.38s remaining: 7.15s 162: learn: 0.1021132 total: 1.39s remaining: 7.14s 163: learn: 0.1019295 total: 1.41s remaining: 7.18s 164: learn: 0.1017271 total: 1.43s remaining: 7.21s 165: learn: 0.1016556 total: 1.44s remaining: 7.23s 166: learn: 0.1016151 total: 1.45s remaining: 7.22s 167: learn: 0.1014437 total: 1.46s remaining: 7.21s 168: learn: 0.1013078 total: 1.46s remaining: 7.2s 169: learn: 0.1011155 total: 1.47s remaining: 7.18s 170: learn: 0.1008970 total: 1.48s remaining: 7.17s 171: learn: 0.1006815 total: 1.49s remaining: 7.16s 172: learn: 0.1005818 total: 1.5s remaining: 7.15s 173: learn: 0.1004343 total: 1.5s remaining: 7.13s 174: learn: 0.1002192 total: 1.51s remaining: 7.12s 175: learn: 0.1001894 total: 1.52s remaining: 7.11s 176: learn: 0.1000665 total: 1.53s remaining: 7.1s 177: learn: 0.1000142 total: 1.53s remaining: 7.08s 178: learn: 0.0998099 total: 1.54s remaining: 7.07s 179: learn: 0.0995953 total: 1.55s remaining: 7.06s 180: learn: 0.0993762 total: 1.56s remaining: 7.05s 181: learn: 0.0993455 total: 1.56s remaining: 7.04s 182: learn: 0.0991737 total: 1.57s remaining: 7.02s 183: learn: 0.0991324 total: 1.58s remaining: 7.01s 184: learn: 0.0989408 total: 1.59s remaining: 7.01s 185: learn: 0.0987505 total: 1.6s remaining: 7s 186: learn: 0.0986984 total: 1.62s remaining: 7.04s 187: learn: 0.0984888 total: 1.63s remaining: 7.03s 188: learn: 0.0982765 total: 1.63s remaining: 7.01s 189: learn: 0.0982185 total: 1.64s remaining: 7s 190: learn: 0.0981763 total: 1.65s remaining: 6.99s 191: learn: 0.0980118 total: 1.66s remaining: 6.98s 192: learn: 0.0979690 total: 1.67s remaining: 6.97s 193: learn: 0.0977605 total: 1.67s remaining: 6.96s 194: learn: 0.0976826 total: 1.69s remaining: 6.96s 195: learn: 0.0975328 total: 1.69s remaining: 6.95s 196: learn: 0.0974963 total: 1.7s remaining: 6.93s 197: learn: 0.0974323 total: 1.71s remaining: 6.92s 198: learn: 0.0972625 total: 1.72s remaining: 6.91s 199: learn: 0.0970765 total: 1.72s remaining: 6.9s 200: learn: 0.0969142 total: 1.73s remaining: 6.89s 201: learn: 0.0968572 total: 1.74s remaining: 6.88s 202: learn: 0.0968082 total: 1.75s remaining: 6.86s 203: learn: 0.0967788 total: 1.75s remaining: 6.85s 204: learn: 0.0966030 total: 1.76s remaining: 6.84s 205: learn: 0.0965062 total: 1.77s remaining: 6.83s 206: learn: 0.0962695 total: 1.78s remaining: 6.82s 207: learn: 0.0960477 total: 1.79s remaining: 6.82s 208: learn: 0.0958859 total: 1.8s remaining: 6.81s 209: learn: 0.0957247 total: 1.81s remaining: 6.79s 210: learn: 0.0955912 total: 1.82s remaining: 6.81s 211: learn: 0.0954240 total: 1.83s remaining: 6.81s 212: learn: 0.0953926 total: 1.84s remaining: 6.8s 213: learn: 0.0953604 total: 1.85s remaining: 6.79s 214: learn: 0.0951687 total: 1.86s remaining: 6.78s 215: learn: 0.0949713 total: 1.86s remaining: 6.77s 216: learn: 0.0948373 total: 1.87s remaining: 6.76s 217: learn: 0.0946806 total: 1.88s remaining: 6.75s 218: learn: 0.0945310 total: 1.89s remaining: 6.74s 219: learn: 0.0944464 total: 1.9s remaining: 6.72s 220: learn: 0.0942930 total: 1.9s remaining: 6.71s 221: learn: 0.0942248 total: 1.91s remaining: 6.7s 222: learn: 0.0940425 total: 1.92s remaining: 6.69s 223: learn: 0.0939182 total: 1.93s remaining: 6.68s 224: learn: 0.0937933 total: 1.94s remaining: 6.67s 225: learn: 0.0936648 total: 1.94s remaining: 6.66s 226: learn: 0.0935551 total: 1.95s remaining: 6.64s 227: learn: 0.0935282 total: 1.96s remaining: 6.63s 228: learn: 0.0933544 total: 1.97s remaining: 6.62s 229: learn: 0.0933216 total: 1.97s remaining: 6.61s 230: learn: 0.0932045 total: 1.98s remaining: 6.59s 231: learn: 0.0931797 total: 1.99s remaining: 6.58s 232: learn: 0.0929701 total: 2s remaining: 6.57s 233: learn: 0.0928260 total: 2s remaining: 6.56s 234: learn: 0.0926876 total: 2.01s remaining: 6.55s 235: learn: 0.0925844 total: 2.02s remaining: 6.55s 236: learn: 0.0925607 total: 2.04s remaining: 6.57s 237: learn: 0.0924387 total: 2.05s remaining: 6.56s 238: learn: 0.0923382 total: 2.06s remaining: 6.54s 239: learn: 0.0922335 total: 2.06s remaining: 6.53s 240: learn: 0.0921158 total: 2.07s remaining: 6.53s 241: learn: 0.0919575 total: 2.08s remaining: 6.51s 242: learn: 0.0918243 total: 2.09s remaining: 6.5s 243: learn: 0.0917719 total: 2.09s remaining: 6.49s 244: learn: 0.0916108 total: 2.1s remaining: 6.48s 245: learn: 0.0914708 total: 2.11s remaining: 6.48s 246: learn: 0.0914376 total: 2.12s remaining: 6.46s 247: learn: 0.0913025 total: 2.13s remaining: 6.45s 248: learn: 0.0911667 total: 2.13s remaining: 6.44s 249: learn: 0.0909991 total: 2.14s remaining: 6.43s 250: learn: 0.0908977 total: 2.15s remaining: 6.42s 251: learn: 0.0908711 total: 2.16s remaining: 6.4s 252: learn: 0.0907447 total: 2.17s remaining: 6.39s 253: learn: 0.0907159 total: 2.17s remaining: 6.38s 254: learn: 0.0905633 total: 2.18s remaining: 6.37s 255: learn: 0.0904588 total: 2.19s remaining: 6.36s 256: learn: 0.0903267 total: 2.19s remaining: 6.35s 257: learn: 0.0902137 total: 2.2s remaining: 6.33s 258: learn: 0.0901001 total: 2.21s remaining: 6.32s 259: learn: 0.0899368 total: 2.22s remaining: 6.31s 260: learn: 0.0898532 total: 2.23s remaining: 6.32s 261: learn: 0.0897769 total: 2.24s remaining: 6.31s 262: learn: 0.0896812 total: 2.25s remaining: 6.3s 263: learn: 0.0896652 total: 2.25s remaining: 6.29s 264: learn: 0.0895299 total: 2.26s remaining: 6.28s 265: learn: 0.0893945 total: 2.27s remaining: 6.26s 266: learn: 0.0892632 total: 2.28s remaining: 6.25s 267: learn: 0.0891453 total: 2.29s remaining: 6.24s 268: learn: 0.0889845 total: 2.29s remaining: 6.23s 269: learn: 0.0889639 total: 2.3s remaining: 6.22s 270: learn: 0.0889102 total: 2.31s remaining: 6.21s 271: learn: 0.0887956 total: 2.32s remaining: 6.2s 272: learn: 0.0887003 total: 2.33s remaining: 6.19s 273: learn: 0.0885968 total: 2.34s remaining: 6.19s 274: learn: 0.0884880 total: 2.34s remaining: 6.18s 275: learn: 0.0883250 total: 2.35s remaining: 6.17s 276: learn: 0.0882645 total: 2.36s remaining: 6.16s 277: learn: 0.0881476 total: 2.37s remaining: 6.15s 278: learn: 0.0880796 total: 2.37s remaining: 6.13s 279: learn: 0.0879575 total: 2.38s remaining: 6.13s 280: learn: 0.0878582 total: 2.39s remaining: 6.12s 281: learn: 0.0877150 total: 2.4s remaining: 6.11s 282: learn: 0.0875486 total: 2.42s remaining: 6.13s 283: learn: 0.0874153 total: 2.44s remaining: 6.15s 284: learn: 0.0873963 total: 2.45s remaining: 6.14s 285: learn: 0.0872816 total: 2.45s remaining: 6.13s 286: learn: 0.0872241 total: 2.46s remaining: 6.12s 287: learn: 0.0871552 total: 2.47s remaining: 6.1s 288: learn: 0.0870255 total: 2.48s remaining: 6.09s 289: learn: 0.0869100 total: 2.48s remaining: 6.08s 290: learn: 0.0868043 total: 2.49s remaining: 6.07s 291: learn: 0.0866926 total: 2.5s remaining: 6.06s 292: learn: 0.0866709 total: 2.51s remaining: 6.05s 293: learn: 0.0866482 total: 2.52s remaining: 6.04s 294: learn: 0.0865702 total: 2.52s remaining: 6.03s 295: learn: 0.0864693 total: 2.53s remaining: 6.02s 296: learn: 0.0863731 total: 2.54s remaining: 6.01s 297: learn: 0.0863498 total: 2.55s remaining: 6s 298: learn: 0.0862371 total: 2.56s remaining: 5.99s 299: learn: 0.0861047 total: 2.56s remaining: 5.98s 300: learn: 0.0860164 total: 2.57s remaining: 5.97s 301: learn: 0.0859104 total: 2.58s remaining: 5.96s 302: learn: 0.0858084 total: 2.59s remaining: 5.95s 303: learn: 0.0857430 total: 2.6s remaining: 5.94s 304: learn: 0.0856188 total: 2.6s remaining: 5.93s 305: learn: 0.0855565 total: 2.61s remaining: 5.92s 306: learn: 0.0854324 total: 2.63s remaining: 5.93s 307: learn: 0.0852930 total: 2.64s remaining: 5.92s 308: learn: 0.0852063 total: 2.65s remaining: 5.92s 309: learn: 0.0850963 total: 2.66s remaining: 5.91s 310: learn: 0.0849882 total: 2.66s remaining: 5.9s 311: learn: 0.0848953 total: 2.67s remaining: 5.89s 312: learn: 0.0848724 total: 2.68s remaining: 5.88s 313: learn: 0.0847905 total: 2.69s remaining: 5.87s 314: learn: 0.0846832 total: 2.69s remaining: 5.86s 315: learn: 0.0845798 total: 2.7s remaining: 5.85s 316: learn: 0.0845642 total: 2.71s remaining: 5.84s 317: learn: 0.0844028 total: 2.72s remaining: 5.83s 318: learn: 0.0842983 total: 2.72s remaining: 5.82s 319: learn: 0.0842753 total: 2.73s remaining: 5.8s 320: learn: 0.0842641 total: 2.74s remaining: 5.79s 321: learn: 0.0841588 total: 2.75s remaining: 5.78s 322: learn: 0.0840769 total: 2.75s remaining: 5.77s 323: learn: 0.0839919 total: 2.76s remaining: 5.76s 324: learn: 0.0838737 total: 2.77s remaining: 5.75s 325: learn: 0.0837816 total: 2.78s remaining: 5.74s 326: learn: 0.0836799 total: 2.78s remaining: 5.73s 327: learn: 0.0836616 total: 2.79s remaining: 5.72s 328: learn: 0.0835577 total: 2.8s remaining: 5.71s 329: learn: 0.0834192 total: 2.81s remaining: 5.7s 330: learn: 0.0833225 total: 2.82s remaining: 5.69s 331: learn: 0.0832427 total: 2.83s remaining: 5.7s 332: learn: 0.0831642 total: 2.85s remaining: 5.71s 333: learn: 0.0830798 total: 2.86s remaining: 5.7s 334: learn: 0.0829713 total: 2.87s remaining: 5.69s 335: learn: 0.0828976 total: 2.88s remaining: 5.68s 336: learn: 0.0827754 total: 2.88s remaining: 5.67s 337: learn: 0.0827561 total: 2.89s remaining: 5.67s 338: learn: 0.0826311 total: 2.9s remaining: 5.66s 339: learn: 0.0825435 total: 2.91s remaining: 5.65s 340: learn: 0.0823982 total: 2.92s remaining: 5.64s 341: learn: 0.0823313 total: 2.93s remaining: 5.63s 342: learn: 0.0822472 total: 2.93s remaining: 5.62s 343: learn: 0.0821660 total: 2.94s remaining: 5.61s 344: learn: 0.0821482 total: 2.95s remaining: 5.6s 345: learn: 0.0820560 total: 2.96s remaining: 5.59s 346: learn: 0.0819656 total: 2.96s remaining: 5.58s 347: learn: 0.0818844 total: 2.97s remaining: 5.57s 348: learn: 0.0817992 total: 2.98s remaining: 5.55s 349: learn: 0.0816674 total: 2.99s remaining: 5.55s 350: learn: 0.0815070 total: 2.99s remaining: 5.54s 351: learn: 0.0814060 total: 3s remaining: 5.53s 352: learn: 0.0813890 total: 3.01s remaining: 5.51s 353: learn: 0.0812962 total: 3.02s remaining: 5.5s 354: learn: 0.0812158 total: 3.02s remaining: 5.5s 355: learn: 0.0810921 total: 3.03s remaining: 5.49s 356: learn: 0.0809977 total: 3.05s remaining: 5.49s 357: learn: 0.0809535 total: 3.06s remaining: 5.49s 358: learn: 0.0808643 total: 3.07s remaining: 5.48s 359: learn: 0.0807954 total: 3.08s remaining: 5.47s 360: learn: 0.0806986 total: 3.09s remaining: 5.46s 361: learn: 0.0805676 total: 3.09s remaining: 5.45s 362: learn: 0.0805535 total: 3.1s remaining: 5.44s 363: learn: 0.0804793 total: 3.11s remaining: 5.43s 364: learn: 0.0804633 total: 3.12s remaining: 5.42s 365: learn: 0.0803473 total: 3.13s remaining: 5.41s 366: learn: 0.0802290 total: 3.13s remaining: 5.4s 367: learn: 0.0801667 total: 3.14s remaining: 5.39s 368: learn: 0.0800604 total: 3.15s remaining: 5.38s 369: learn: 0.0800475 total: 3.16s remaining: 5.37s 370: learn: 0.0799458 total: 3.16s remaining: 5.37s 371: learn: 0.0798718 total: 3.17s remaining: 5.36s 372: learn: 0.0797488 total: 3.18s remaining: 5.35s 373: learn: 0.0795991 total: 3.19s remaining: 5.34s 374: learn: 0.0795354 total: 3.19s remaining: 5.33s 375: learn: 0.0794794 total: 3.2s remaining: 5.32s 376: learn: 0.0794068 total: 3.21s remaining: 5.31s 377: learn: 0.0793018 total: 3.22s remaining: 5.3s 378: learn: 0.0791870 total: 3.23s remaining: 5.29s 379: learn: 0.0791233 total: 3.23s remaining: 5.28s 380: learn: 0.0790028 total: 3.24s remaining: 5.27s 381: learn: 0.0789419 total: 3.26s remaining: 5.27s 382: learn: 0.0788537 total: 3.27s remaining: 5.27s 383: learn: 0.0788155 total: 3.28s remaining: 5.26s 384: learn: 0.0788014 total: 3.28s remaining: 5.25s 385: learn: 0.0787904 total: 3.29s remaining: 5.23s 386: learn: 0.0787447 total: 3.3s remaining: 5.22s 387: learn: 0.0786426 total: 3.31s remaining: 5.21s 388: learn: 0.0785551 total: 3.31s remaining: 5.2s 389: learn: 0.0784804 total: 3.32s remaining: 5.19s 390: learn: 0.0784056 total: 3.33s remaining: 5.18s 391: learn: 0.0783098 total: 3.33s remaining: 5.17s 392: learn: 0.0782375 total: 3.34s remaining: 5.16s 393: learn: 0.0781478 total: 3.35s remaining: 5.15s 394: learn: 0.0780473 total: 3.36s remaining: 5.14s 395: learn: 0.0779635 total: 3.37s remaining: 5.14s 396: learn: 0.0779308 total: 3.38s remaining: 5.14s 397: learn: 0.0779037 total: 3.39s remaining: 5.13s 398: learn: 0.0778030 total: 3.4s remaining: 5.12s 399: learn: 0.0777260 total: 3.41s remaining: 5.11s 400: learn: 0.0776162 total: 3.42s remaining: 5.1s 401: learn: 0.0776034 total: 3.42s remaining: 5.09s 402: learn: 0.0775603 total: 3.43s remaining: 5.08s 403: learn: 0.0775140 total: 3.44s remaining: 5.08s 404: learn: 0.0773874 total: 3.45s remaining: 5.07s 405: learn: 0.0773767 total: 3.47s remaining: 5.07s 406: learn: 0.0773050 total: 3.48s remaining: 5.06s 407: learn: 0.0772218 total: 3.48s remaining: 5.05s 408: learn: 0.0772089 total: 3.49s remaining: 5.04s 409: learn: 0.0770972 total: 3.5s remaining: 5.03s 410: learn: 0.0770183 total: 3.51s remaining: 5.02s 411: learn: 0.0769285 total: 3.51s remaining: 5.01s 412: learn: 0.0768545 total: 3.52s remaining: 5.01s 413: learn: 0.0768167 total: 3.53s remaining: 5s 414: learn: 0.0767139 total: 3.54s remaining: 4.99s 415: learn: 0.0766401 total: 3.54s remaining: 4.98s 416: learn: 0.0765628 total: 3.55s remaining: 4.97s 417: learn: 0.0764682 total: 3.56s remaining: 4.96s 418: learn: 0.0763938 total: 3.57s remaining: 4.95s 419: learn: 0.0763240 total: 3.58s remaining: 4.94s 420: learn: 0.0763134 total: 3.58s remaining: 4.93s 421: learn: 0.0762337 total: 3.59s remaining: 4.92s 422: learn: 0.0761195 total: 3.6s remaining: 4.91s 423: learn: 0.0760358 total: 3.61s remaining: 4.9s 424: learn: 0.0759231 total: 3.62s remaining: 4.89s 425: learn: 0.0758581 total: 3.62s remaining: 4.88s 426: learn: 0.0757642 total: 3.63s remaining: 4.87s 427: learn: 0.0757045 total: 3.64s remaining: 4.86s 428: learn: 0.0756649 total: 3.65s remaining: 4.85s 429: learn: 0.0756521 total: 3.65s remaining: 4.84s 430: learn: 0.0755672 total: 3.66s remaining: 4.83s 431: learn: 0.0755520 total: 3.68s remaining: 4.83s 432: learn: 0.0754949 total: 3.69s remaining: 4.83s 433: learn: 0.0754853 total: 3.7s remaining: 4.82s 434: learn: 0.0754737 total: 3.71s remaining: 4.81s 435: learn: 0.0754112 total: 3.71s remaining: 4.8s 436: learn: 0.0753350 total: 3.72s remaining: 4.79s 437: learn: 0.0752524 total: 3.73s remaining: 4.78s 438: learn: 0.0751292 total: 3.74s remaining: 4.78s 439: learn: 0.0750725 total: 3.75s remaining: 4.77s 440: learn: 0.0749723 total: 3.75s remaining: 4.76s 441: learn: 0.0748739 total: 3.76s remaining: 4.75s 442: learn: 0.0748188 total: 3.77s remaining: 4.74s 443: learn: 0.0747944 total: 3.77s remaining: 4.73s 444: learn: 0.0747260 total: 3.78s remaining: 4.72s 445: learn: 0.0746666 total: 3.79s remaining: 4.71s 446: learn: 0.0745917 total: 3.8s remaining: 4.7s 447: learn: 0.0745078 total: 3.81s remaining: 4.69s 448: learn: 0.0744285 total: 3.81s remaining: 4.68s 449: learn: 0.0743281 total: 3.82s remaining: 4.67s 450: learn: 0.0742409 total: 3.83s remaining: 4.66s 451: learn: 0.0741576 total: 3.84s remaining: 4.65s 452: learn: 0.0740758 total: 3.85s remaining: 4.64s 453: learn: 0.0740650 total: 3.85s remaining: 4.64s 454: learn: 0.0740131 total: 3.87s remaining: 4.64s 455: learn: 0.0739041 total: 3.88s remaining: 4.63s 456: learn: 0.0738321 total: 3.89s remaining: 4.62s 457: learn: 0.0737640 total: 3.9s remaining: 4.61s 458: learn: 0.0737558 total: 3.9s remaining: 4.6s 459: learn: 0.0736827 total: 3.91s remaining: 4.59s 460: learn: 0.0736174 total: 3.92s remaining: 4.58s 461: learn: 0.0735667 total: 3.93s remaining: 4.57s 462: learn: 0.0734895 total: 3.93s remaining: 4.56s 463: learn: 0.0734323 total: 3.94s remaining: 4.55s 464: learn: 0.0733708 total: 3.95s remaining: 4.54s 465: learn: 0.0733080 total: 3.96s remaining: 4.53s 466: learn: 0.0732673 total: 3.96s remaining: 4.52s 467: learn: 0.0732531 total: 3.97s remaining: 4.51s 468: learn: 0.0732116 total: 3.98s remaining: 4.5s 469: learn: 0.0731293 total: 3.99s remaining: 4.5s 470: learn: 0.0730500 total: 4s remaining: 4.49s 471: learn: 0.0729454 total: 4.01s remaining: 4.48s 472: learn: 0.0728293 total: 4.01s remaining: 4.47s 473: learn: 0.0727632 total: 4.02s remaining: 4.46s 474: learn: 0.0726961 total: 4.03s remaining: 4.45s 475: learn: 0.0726378 total: 4.04s remaining: 4.45s 476: learn: 0.0726132 total: 4.05s remaining: 4.44s 477: learn: 0.0725578 total: 4.06s remaining: 4.43s 478: learn: 0.0724751 total: 4.08s remaining: 4.44s 479: learn: 0.0724555 total: 4.09s remaining: 4.43s 480: learn: 0.0724080 total: 4.09s remaining: 4.42s 481: learn: 0.0724001 total: 4.1s remaining: 4.41s 482: learn: 0.0723107 total: 4.11s remaining: 4.4s 483: learn: 0.0722534 total: 4.12s remaining: 4.39s 484: learn: 0.0721894 total: 4.13s remaining: 4.38s 485: learn: 0.0721367 total: 4.13s remaining: 4.37s 486: learn: 0.0721273 total: 4.14s remaining: 4.36s 487: learn: 0.0720335 total: 4.15s remaining: 4.35s 488: learn: 0.0719476 total: 4.16s remaining: 4.34s 489: learn: 0.0718805 total: 4.17s remaining: 4.33s 490: learn: 0.0718034 total: 4.17s remaining: 4.33s 491: learn: 0.0716969 total: 4.18s remaining: 4.32s 492: learn: 0.0716166 total: 4.19s remaining: 4.31s 493: learn: 0.0715396 total: 4.2s remaining: 4.3s 494: learn: 0.0714635 total: 4.21s remaining: 4.29s 495: learn: 0.0713774 total: 4.21s remaining: 4.28s 496: learn: 0.0713502 total: 4.22s remaining: 4.27s 497: learn: 0.0712788 total: 4.23s remaining: 4.26s 498: learn: 0.0711997 total: 4.24s remaining: 4.25s 499: learn: 0.0711380 total: 4.24s remaining: 4.24s 500: learn: 0.0710532 total: 4.25s remaining: 4.23s 501: learn: 0.0709764 total: 4.26s remaining: 4.22s 502: learn: 0.0709273 total: 4.28s remaining: 4.23s 503: learn: 0.0708719 total: 4.29s remaining: 4.22s 504: learn: 0.0708122 total: 4.29s remaining: 4.21s 505: learn: 0.0707432 total: 4.3s remaining: 4.2s 506: learn: 0.0706753 total: 4.31s remaining: 4.19s 507: learn: 0.0706114 total: 4.32s remaining: 4.18s 508: learn: 0.0705715 total: 4.32s remaining: 4.17s 509: learn: 0.0704571 total: 4.33s remaining: 4.16s 510: learn: 0.0704045 total: 4.35s remaining: 4.16s 511: learn: 0.0703967 total: 4.36s remaining: 4.16s 512: learn: 0.0703523 total: 4.37s remaining: 4.15s 513: learn: 0.0703054 total: 4.38s remaining: 4.14s 514: learn: 0.0702386 total: 4.38s remaining: 4.13s 515: learn: 0.0701822 total: 4.39s remaining: 4.12s 516: learn: 0.0701438 total: 4.4s remaining: 4.11s 517: learn: 0.0700533 total: 4.41s remaining: 4.1s 518: learn: 0.0699894 total: 4.41s remaining: 4.09s 519: learn: 0.0699303 total: 4.42s remaining: 4.08s 520: learn: 0.0698881 total: 4.43s remaining: 4.07s 521: learn: 0.0698708 total: 4.44s remaining: 4.06s 522: learn: 0.0698628 total: 4.44s remaining: 4.05s 523: learn: 0.0697892 total: 4.45s remaining: 4.04s 524: learn: 0.0697301 total: 4.46s remaining: 4.04s 525: learn: 0.0696349 total: 4.47s remaining: 4.03s 526: learn: 0.0695766 total: 4.48s remaining: 4.02s 527: learn: 0.0695159 total: 4.49s remaining: 4.02s 528: learn: 0.0694312 total: 4.5s remaining: 4.01s 529: learn: 0.0693678 total: 4.51s remaining: 4s 530: learn: 0.0693472 total: 4.52s remaining: 3.99s 531: learn: 0.0693054 total: 4.52s remaining: 3.98s 532: learn: 0.0692201 total: 4.53s remaining: 3.97s 533: learn: 0.0691360 total: 4.54s remaining: 3.97s 534: learn: 0.0691296 total: 4.55s remaining: 3.96s 535: learn: 0.0690613 total: 4.56s remaining: 3.95s 536: learn: 0.0690552 total: 4.57s remaining: 3.94s 537: learn: 0.0689948 total: 4.58s remaining: 3.93s 538: learn: 0.0689456 total: 4.59s remaining: 3.92s 539: learn: 0.0688908 total: 4.59s remaining: 3.91s 540: learn: 0.0688376 total: 4.6s remaining: 3.9s 541: learn: 0.0688249 total: 4.61s remaining: 3.9s 542: learn: 0.0687629 total: 4.62s remaining: 3.89s 543: learn: 0.0687042 total: 4.63s remaining: 3.88s 544: learn: 0.0686463 total: 4.64s remaining: 3.87s 545: learn: 0.0685715 total: 4.64s remaining: 3.86s 546: learn: 0.0685378 total: 4.65s remaining: 3.85s 547: learn: 0.0684937 total: 4.66s remaining: 3.84s 548: learn: 0.0684335 total: 4.67s remaining: 3.83s 549: learn: 0.0684079 total: 4.68s remaining: 3.83s 550: learn: 0.0683560 total: 4.69s remaining: 3.82s 551: learn: 0.0683182 total: 4.7s remaining: 3.81s 552: learn: 0.0682300 total: 4.71s remaining: 3.81s 553: learn: 0.0681847 total: 4.72s remaining: 3.8s 554: learn: 0.0681210 total: 4.72s remaining: 3.79s 555: learn: 0.0680691 total: 4.73s remaining: 3.78s 556: learn: 0.0679527 total: 4.74s remaining: 3.77s 557: learn: 0.0679018 total: 4.75s remaining: 3.76s 558: learn: 0.0678564 total: 4.76s remaining: 3.75s 559: learn: 0.0678063 total: 4.76s remaining: 3.74s 560: learn: 0.0677505 total: 4.77s remaining: 3.73s 561: learn: 0.0677097 total: 4.78s remaining: 3.73s 562: learn: 0.0676465 total: 4.79s remaining: 3.72s 563: learn: 0.0675870 total: 4.8s remaining: 3.71s 564: learn: 0.0675201 total: 4.8s remaining: 3.7s 565: learn: 0.0674794 total: 4.81s remaining: 3.69s 566: learn: 0.0674226 total: 4.82s remaining: 3.68s 567: learn: 0.0673317 total: 4.83s remaining: 3.67s 568: learn: 0.0673216 total: 4.83s remaining: 3.66s 569: learn: 0.0672615 total: 4.84s remaining: 3.65s 570: learn: 0.0672011 total: 4.85s remaining: 3.64s 571: learn: 0.0671564 total: 4.86s remaining: 3.63s 572: learn: 0.0670940 total: 4.87s remaining: 3.63s 573: learn: 0.0670772 total: 4.87s remaining: 3.62s 574: learn: 0.0670065 total: 4.89s remaining: 3.62s 575: learn: 0.0669979 total: 4.9s remaining: 3.61s 576: learn: 0.0669556 total: 4.91s remaining: 3.6s 577: learn: 0.0669493 total: 4.92s remaining: 3.59s 578: learn: 0.0668865 total: 4.93s remaining: 3.58s 579: learn: 0.0667971 total: 4.93s remaining: 3.57s 580: learn: 0.0667889 total: 4.94s remaining: 3.56s 581: learn: 0.0667059 total: 4.95s remaining: 3.56s 582: learn: 0.0666470 total: 4.96s remaining: 3.55s 583: learn: 0.0665867 total: 4.97s remaining: 3.54s 584: learn: 0.0665316 total: 4.98s remaining: 3.53s 585: learn: 0.0664823 total: 4.99s remaining: 3.52s 586: learn: 0.0664755 total: 4.99s remaining: 3.51s 587: learn: 0.0664262 total: 5s remaining: 3.5s 588: learn: 0.0663632 total: 5.01s remaining: 3.5s 589: learn: 0.0663558 total: 5.02s remaining: 3.49s 590: learn: 0.0663009 total: 5.03s remaining: 3.48s 591: learn: 0.0662433 total: 5.03s remaining: 3.47s 592: learn: 0.0661741 total: 5.04s remaining: 3.46s 593: learn: 0.0660894 total: 5.05s remaining: 3.45s 594: learn: 0.0659821 total: 5.06s remaining: 3.44s 595: learn: 0.0659012 total: 5.06s remaining: 3.43s 596: learn: 0.0658956 total: 5.07s remaining: 3.42s 597: learn: 0.0658273 total: 5.08s remaining: 3.41s 598: learn: 0.0657842 total: 5.1s remaining: 3.41s 599: learn: 0.0657450 total: 5.11s remaining: 3.4s 600: learn: 0.0656861 total: 5.11s remaining: 3.4s 601: learn: 0.0656193 total: 5.12s remaining: 3.39s 602: learn: 0.0655486 total: 5.13s remaining: 3.38s 603: learn: 0.0654966 total: 5.14s remaining: 3.37s 604: learn: 0.0654888 total: 5.14s remaining: 3.36s 605: learn: 0.0654257 total: 5.15s remaining: 3.35s 606: learn: 0.0653747 total: 5.16s remaining: 3.34s 607: learn: 0.0653693 total: 5.17s remaining: 3.33s 608: learn: 0.0653197 total: 5.17s remaining: 3.32s 609: learn: 0.0652558 total: 5.18s remaining: 3.31s 610: learn: 0.0652045 total: 5.19s remaining: 3.3s 611: learn: 0.0651612 total: 5.2s remaining: 3.29s 612: learn: 0.0650847 total: 5.21s remaining: 3.29s 613: learn: 0.0650513 total: 5.21s remaining: 3.28s 614: learn: 0.0650256 total: 5.22s remaining: 3.27s 615: learn: 0.0649712 total: 5.23s remaining: 3.26s 616: learn: 0.0649189 total: 5.24s remaining: 3.25s 617: learn: 0.0648758 total: 5.25s remaining: 3.24s 618: learn: 0.0648090 total: 5.25s remaining: 3.23s 619: learn: 0.0647621 total: 5.26s remaining: 3.22s 620: learn: 0.0647382 total: 5.27s remaining: 3.21s 621: learn: 0.0646859 total: 5.28s remaining: 3.21s 622: learn: 0.0646797 total: 5.28s remaining: 3.2s 623: learn: 0.0646441 total: 5.3s remaining: 3.2s 624: learn: 0.0645993 total: 5.33s remaining: 3.19s 625: learn: 0.0645525 total: 5.33s remaining: 3.19s 626: learn: 0.0644967 total: 5.34s remaining: 3.18s 627: learn: 0.0644391 total: 5.35s remaining: 3.17s 628: learn: 0.0643893 total: 5.36s remaining: 3.16s 629: learn: 0.0643183 total: 5.36s remaining: 3.15s 630: learn: 0.0642660 total: 5.37s remaining: 3.14s 631: learn: 0.0642258 total: 5.38s remaining: 3.13s 632: learn: 0.0641761 total: 5.39s remaining: 3.12s 633: learn: 0.0641706 total: 5.39s remaining: 3.11s 634: learn: 0.0641161 total: 5.4s remaining: 3.1s 635: learn: 0.0640562 total: 5.41s remaining: 3.1s 636: learn: 0.0640514 total: 5.42s remaining: 3.09s 637: learn: 0.0640202 total: 5.42s remaining: 3.08s 638: learn: 0.0639847 total: 5.43s remaining: 3.07s 639: learn: 0.0638989 total: 5.44s remaining: 3.06s 640: learn: 0.0638532 total: 5.45s remaining: 3.05s 641: learn: 0.0638043 total: 5.45s remaining: 3.04s 642: learn: 0.0637713 total: 5.46s remaining: 3.03s 643: learn: 0.0636923 total: 5.47s remaining: 3.02s 644: learn: 0.0636858 total: 5.48s remaining: 3.01s 645: learn: 0.0636092 total: 5.48s remaining: 3s 646: learn: 0.0635720 total: 5.49s remaining: 3s 647: learn: 0.0635227 total: 5.5s remaining: 2.99s 648: learn: 0.0634790 total: 5.51s remaining: 2.98s 649: learn: 0.0634262 total: 5.53s remaining: 2.98s 650: learn: 0.0633763 total: 5.54s remaining: 2.97s 651: learn: 0.0633291 total: 5.55s remaining: 2.96s 652: learn: 0.0632688 total: 5.56s remaining: 2.95s 653: learn: 0.0632197 total: 5.56s remaining: 2.94s 654: learn: 0.0631945 total: 5.57s remaining: 2.93s 655: learn: 0.0631331 total: 5.58s remaining: 2.92s 656: learn: 0.0631081 total: 5.59s remaining: 2.92s 657: learn: 0.0630562 total: 5.59s remaining: 2.91s 658: learn: 0.0630041 total: 5.6s remaining: 2.9s 659: learn: 0.0629469 total: 5.61s remaining: 2.89s 660: learn: 0.0629042 total: 5.62s remaining: 2.88s 661: learn: 0.0628508 total: 5.62s remaining: 2.87s 662: learn: 0.0627985 total: 5.63s remaining: 2.86s 663: learn: 0.0627639 total: 5.64s remaining: 2.86s 664: learn: 0.0627296 total: 5.65s remaining: 2.85s 665: learn: 0.0627248 total: 5.66s remaining: 2.84s 666: learn: 0.0626856 total: 5.67s remaining: 2.83s 667: learn: 0.0626122 total: 5.67s remaining: 2.82s 668: learn: 0.0625689 total: 5.68s remaining: 2.81s 669: learn: 0.0624699 total: 5.69s remaining: 2.8s 670: learn: 0.0624228 total: 5.7s remaining: 2.79s 671: learn: 0.0623574 total: 5.71s remaining: 2.79s 672: learn: 0.0623372 total: 5.72s remaining: 2.78s 673: learn: 0.0622796 total: 5.74s remaining: 2.78s 674: learn: 0.0622440 total: 5.75s remaining: 2.77s 675: learn: 0.0621525 total: 5.76s remaining: 2.76s 676: learn: 0.0620939 total: 5.76s remaining: 2.75s 677: learn: 0.0620359 total: 5.77s remaining: 2.74s 678: learn: 0.0619866 total: 5.79s remaining: 2.74s 679: learn: 0.0619820 total: 5.81s remaining: 2.73s 680: learn: 0.0619442 total: 5.82s remaining: 2.72s 681: learn: 0.0618979 total: 5.82s remaining: 2.71s 682: learn: 0.0618402 total: 5.83s remaining: 2.71s 683: learn: 0.0617844 total: 5.84s remaining: 2.7s 684: learn: 0.0617277 total: 5.85s remaining: 2.69s 685: learn: 0.0617225 total: 5.86s remaining: 2.68s 686: learn: 0.0616471 total: 5.86s remaining: 2.67s 687: learn: 0.0615861 total: 5.87s remaining: 2.66s 688: learn: 0.0615255 total: 5.88s remaining: 2.65s 689: learn: 0.0615220 total: 5.89s remaining: 2.65s 690: learn: 0.0614871 total: 5.9s remaining: 2.64s 691: learn: 0.0613881 total: 5.9s remaining: 2.63s 692: learn: 0.0613167 total: 5.91s remaining: 2.62s 693: learn: 0.0612508 total: 5.92s remaining: 2.61s 694: learn: 0.0611869 total: 5.93s remaining: 2.6s 695: learn: 0.0611268 total: 5.95s remaining: 2.6s 696: learn: 0.0610867 total: 5.96s remaining: 2.59s 697: learn: 0.0610333 total: 5.97s remaining: 2.58s 698: learn: 0.0609932 total: 5.97s remaining: 2.57s 699: learn: 0.0609402 total: 5.98s remaining: 2.56s 700: learn: 0.0608825 total: 5.99s remaining: 2.55s 701: learn: 0.0608418 total: 6s remaining: 2.54s 702: learn: 0.0607885 total: 6s remaining: 2.54s 703: learn: 0.0607057 total: 6.01s remaining: 2.53s 704: learn: 0.0606677 total: 6.02s remaining: 2.52s 705: learn: 0.0606148 total: 6.03s remaining: 2.51s 706: learn: 0.0605805 total: 6.04s remaining: 2.5s 707: learn: 0.0605182 total: 6.04s remaining: 2.49s 708: learn: 0.0604504 total: 6.05s remaining: 2.48s 709: learn: 0.0603844 total: 6.06s remaining: 2.48s 710: learn: 0.0603290 total: 6.07s remaining: 2.47s 711: learn: 0.0603241 total: 6.08s remaining: 2.46s 712: learn: 0.0603006 total: 6.08s remaining: 2.45s 713: learn: 0.0602425 total: 6.09s remaining: 2.44s 714: learn: 0.0602380 total: 6.1s remaining: 2.43s 715: learn: 0.0601944 total: 6.11s remaining: 2.42s 716: learn: 0.0601664 total: 6.11s remaining: 2.41s 717: learn: 0.0601287 total: 6.12s remaining: 2.4s 718: learn: 0.0601090 total: 6.13s remaining: 2.4s 719: learn: 0.0600508 total: 6.15s remaining: 2.39s 720: learn: 0.0599940 total: 6.16s remaining: 2.38s 721: learn: 0.0599462 total: 6.17s remaining: 2.37s 722: learn: 0.0599085 total: 6.17s remaining: 2.37s 723: learn: 0.0598841 total: 6.18s remaining: 2.36s 724: learn: 0.0598802 total: 6.19s remaining: 2.35s 725: learn: 0.0598466 total: 6.2s remaining: 2.34s 726: learn: 0.0597761 total: 6.21s remaining: 2.33s 727: learn: 0.0597731 total: 6.22s remaining: 2.32s 728: learn: 0.0597081 total: 6.22s remaining: 2.31s 729: learn: 0.0596683 total: 6.23s remaining: 2.3s 730: learn: 0.0595910 total: 6.24s remaining: 2.29s 731: learn: 0.0595498 total: 6.25s remaining: 2.29s 732: learn: 0.0594946 total: 6.25s remaining: 2.28s 733: learn: 0.0594523 total: 6.26s remaining: 2.27s 734: learn: 0.0594004 total: 6.27s remaining: 2.26s 735: learn: 0.0593954 total: 6.28s remaining: 2.25s 736: learn: 0.0593315 total: 6.3s remaining: 2.25s 737: learn: 0.0592770 total: 6.3s remaining: 2.24s 738: learn: 0.0592257 total: 6.31s remaining: 2.23s 739: learn: 0.0591558 total: 6.32s remaining: 2.22s 740: learn: 0.0590959 total: 6.33s remaining: 2.21s 741: learn: 0.0590250 total: 6.33s remaining: 2.2s 742: learn: 0.0589742 total: 6.34s remaining: 2.19s 743: learn: 0.0589337 total: 6.36s remaining: 2.19s 744: learn: 0.0588828 total: 6.36s remaining: 2.18s 745: learn: 0.0588648 total: 6.37s remaining: 2.17s 746: learn: 0.0588068 total: 6.38s remaining: 2.16s 747: learn: 0.0587825 total: 6.39s remaining: 2.15s 748: learn: 0.0587487 total: 6.39s remaining: 2.14s 749: learn: 0.0586915 total: 6.4s remaining: 2.13s 750: learn: 0.0586374 total: 6.41s remaining: 2.13s 751: learn: 0.0585743 total: 6.42s remaining: 2.12s 752: learn: 0.0585308 total: 6.42s remaining: 2.11s 753: learn: 0.0584972 total: 6.43s remaining: 2.1s 754: learn: 0.0584556 total: 6.44s remaining: 2.09s 755: learn: 0.0584514 total: 6.45s remaining: 2.08s 756: learn: 0.0583518 total: 6.46s remaining: 2.07s 757: learn: 0.0583481 total: 6.46s remaining: 2.06s 758: learn: 0.0583435 total: 6.47s remaining: 2.05s 759: learn: 0.0582802 total: 6.48s remaining: 2.04s 760: learn: 0.0582206 total: 6.49s remaining: 2.04s 761: learn: 0.0581643 total: 6.49s remaining: 2.03s 762: learn: 0.0580987 total: 6.5s remaining: 2.02s 763: learn: 0.0580491 total: 6.51s remaining: 2.01s 764: learn: 0.0580013 total: 6.52s remaining: 2s 765: learn: 0.0579428 total: 6.53s remaining: 1.99s 766: learn: 0.0579005 total: 6.53s remaining: 1.98s 767: learn: 0.0578909 total: 6.54s remaining: 1.98s 768: learn: 0.0578302 total: 6.55s remaining: 1.97s 769: learn: 0.0577695 total: 6.56s remaining: 1.96s 770: learn: 0.0577328 total: 6.57s remaining: 1.95s 771: learn: 0.0576826 total: 6.59s remaining: 1.95s 772: learn: 0.0576239 total: 6.59s remaining: 1.94s 773: learn: 0.0575792 total: 6.6s remaining: 1.93s 774: learn: 0.0575704 total: 6.61s remaining: 1.92s 775: learn: 0.0575202 total: 6.62s remaining: 1.91s 776: learn: 0.0574811 total: 6.63s remaining: 1.9s 777: learn: 0.0574285 total: 6.63s remaining: 1.89s 778: learn: 0.0574253 total: 6.64s remaining: 1.88s 779: learn: 0.0573778 total: 6.65s remaining: 1.88s 780: learn: 0.0573297 total: 6.66s remaining: 1.87s 781: learn: 0.0573070 total: 6.67s remaining: 1.86s 782: learn: 0.0572382 total: 6.67s remaining: 1.85s 783: learn: 0.0571573 total: 6.68s remaining: 1.84s 784: learn: 0.0570769 total: 6.69s remaining: 1.83s 785: learn: 0.0570413 total: 6.7s remaining: 1.82s 786: learn: 0.0570077 total: 6.71s remaining: 1.82s 787: learn: 0.0569511 total: 6.72s remaining: 1.81s 788: learn: 0.0569075 total: 6.74s remaining: 1.8s 789: learn: 0.0568685 total: 6.75s remaining: 1.79s 790: learn: 0.0568289 total: 6.77s remaining: 1.79s 791: learn: 0.0567592 total: 6.78s remaining: 1.78s 792: learn: 0.0567065 total: 6.78s remaining: 1.77s 793: learn: 0.0566854 total: 6.79s remaining: 1.76s 794: learn: 0.0566445 total: 6.8s remaining: 1.75s 795: learn: 0.0566057 total: 6.81s remaining: 1.74s 796: learn: 0.0565607 total: 6.81s remaining: 1.74s 797: learn: 0.0565141 total: 6.82s remaining: 1.73s 798: learn: 0.0565117 total: 6.83s remaining: 1.72s 799: learn: 0.0564620 total: 6.84s remaining: 1.71s 800: learn: 0.0564123 total: 6.85s remaining: 1.7s 801: learn: 0.0563773 total: 6.86s remaining: 1.69s 802: learn: 0.0563086 total: 6.87s remaining: 1.69s 803: learn: 0.0562811 total: 6.88s remaining: 1.68s 804: learn: 0.0562547 total: 6.89s remaining: 1.67s 805: learn: 0.0562369 total: 6.89s remaining: 1.66s 806: learn: 0.0561823 total: 6.9s remaining: 1.65s 807: learn: 0.0561359 total: 6.91s remaining: 1.64s 808: learn: 0.0561017 total: 6.92s remaining: 1.63s 809: learn: 0.0560524 total: 6.93s remaining: 1.62s 810: learn: 0.0559998 total: 6.93s remaining: 1.62s 811: learn: 0.0559519 total: 6.94s remaining: 1.61s 812: learn: 0.0559178 total: 6.95s remaining: 1.6s 813: learn: 0.0558679 total: 6.96s remaining: 1.59s 814: learn: 0.0557895 total: 6.98s remaining: 1.58s 815: learn: 0.0557394 total: 6.99s remaining: 1.57s 816: learn: 0.0557040 total: 6.99s remaining: 1.57s 817: learn: 0.0556417 total: 7s remaining: 1.56s 818: learn: 0.0556066 total: 7.01s remaining: 1.55s 819: learn: 0.0555572 total: 7.02s remaining: 1.54s 820: learn: 0.0555074 total: 7.02s remaining: 1.53s 821: learn: 0.0554869 total: 7.03s remaining: 1.52s 822: learn: 0.0554335 total: 7.04s remaining: 1.51s 823: learn: 0.0554002 total: 7.05s remaining: 1.5s 824: learn: 0.0553656 total: 7.05s remaining: 1.5s 825: learn: 0.0553329 total: 7.06s remaining: 1.49s 826: learn: 0.0552814 total: 7.07s remaining: 1.48s 827: learn: 0.0552287 total: 7.08s remaining: 1.47s 828: learn: 0.0551858 total: 7.09s remaining: 1.46s 829: learn: 0.0551389 total: 7.09s remaining: 1.45s 830: learn: 0.0551102 total: 7.1s remaining: 1.44s 831: learn: 0.0550759 total: 7.11s remaining: 1.44s 832: learn: 0.0550432 total: 7.12s remaining: 1.43s 833: learn: 0.0550384 total: 7.13s remaining: 1.42s 834: learn: 0.0549874 total: 7.13s remaining: 1.41s 835: learn: 0.0549527 total: 7.14s remaining: 1.4s 836: learn: 0.0549074 total: 7.15s remaining: 1.39s 837: learn: 0.0548769 total: 7.17s remaining: 1.39s 838: learn: 0.0548244 total: 7.18s remaining: 1.38s 839: learn: 0.0547780 total: 7.19s remaining: 1.37s 840: learn: 0.0547743 total: 7.2s remaining: 1.36s 841: learn: 0.0547649 total: 7.2s remaining: 1.35s 842: learn: 0.0547182 total: 7.21s remaining: 1.34s 843: learn: 0.0546630 total: 7.22s remaining: 1.33s 844: learn: 0.0546020 total: 7.23s remaining: 1.32s 845: learn: 0.0545433 total: 7.24s remaining: 1.32s 846: learn: 0.0544936 total: 7.25s remaining: 1.31s 847: learn: 0.0544438 total: 7.26s remaining: 1.3s 848: learn: 0.0544045 total: 7.27s remaining: 1.29s 849: learn: 0.0543587 total: 7.28s remaining: 1.28s 850: learn: 0.0543070 total: 7.29s remaining: 1.27s 851: learn: 0.0542415 total: 7.29s remaining: 1.27s 852: learn: 0.0541845 total: 7.3s remaining: 1.26s 853: learn: 0.0541589 total: 7.31s remaining: 1.25s 854: learn: 0.0541413 total: 7.32s remaining: 1.24s 855: learn: 0.0540809 total: 7.33s remaining: 1.23s 856: learn: 0.0540015 total: 7.33s remaining: 1.22s 857: learn: 0.0539956 total: 7.34s remaining: 1.22s 858: learn: 0.0539913 total: 7.35s remaining: 1.21s 859: learn: 0.0539660 total: 7.36s remaining: 1.2s 860: learn: 0.0539448 total: 7.38s remaining: 1.19s 861: learn: 0.0539415 total: 7.39s remaining: 1.18s 862: learn: 0.0539262 total: 7.39s remaining: 1.17s 863: learn: 0.0539238 total: 7.4s remaining: 1.17s 864: learn: 0.0538538 total: 7.41s remaining: 1.16s 865: learn: 0.0538113 total: 7.42s remaining: 1.15s 866: learn: 0.0538091 total: 7.42s remaining: 1.14s 867: learn: 0.0537732 total: 7.43s remaining: 1.13s 868: learn: 0.0537371 total: 7.44s remaining: 1.12s 869: learn: 0.0537089 total: 7.45s remaining: 1.11s 870: learn: 0.0536923 total: 7.46s remaining: 1.1s 871: learn: 0.0536523 total: 7.47s remaining: 1.1s 872: learn: 0.0536044 total: 7.47s remaining: 1.09s 873: learn: 0.0535413 total: 7.48s remaining: 1.08s 874: learn: 0.0535005 total: 7.49s remaining: 1.07s 875: learn: 0.0534676 total: 7.5s remaining: 1.06s 876: learn: 0.0534168 total: 7.51s remaining: 1.05s 877: learn: 0.0534012 total: 7.51s remaining: 1.04s 878: learn: 0.0533312 total: 7.52s remaining: 1.03s 879: learn: 0.0532745 total: 7.53s remaining: 1.03s 880: learn: 0.0532117 total: 7.54s remaining: 1.02s 881: learn: 0.0531718 total: 7.54s remaining: 1.01s 882: learn: 0.0531693 total: 7.55s remaining: 1s 883: learn: 0.0531196 total: 7.56s remaining: 992ms 884: learn: 0.0530533 total: 7.58s remaining: 985ms 885: learn: 0.0530037 total: 7.59s remaining: 977ms 886: learn: 0.0529495 total: 7.6s remaining: 968ms 887: learn: 0.0529227 total: 7.61s remaining: 960ms 888: learn: 0.0528869 total: 7.62s remaining: 951ms 889: learn: 0.0528396 total: 7.62s remaining: 942ms 890: learn: 0.0527764 total: 7.63s remaining: 934ms 891: learn: 0.0527489 total: 7.64s remaining: 925ms 892: learn: 0.0527008 total: 7.65s remaining: 916ms 893: learn: 0.0526536 total: 7.65s remaining: 908ms 894: learn: 0.0526505 total: 7.66s remaining: 899ms 895: learn: 0.0526185 total: 7.67s remaining: 890ms 896: learn: 0.0525674 total: 7.68s remaining: 882ms 897: learn: 0.0525242 total: 7.69s remaining: 873ms 898: learn: 0.0525226 total: 7.69s remaining: 864ms 899: learn: 0.0525022 total: 7.7s remaining: 856ms 900: learn: 0.0524629 total: 7.71s remaining: 847ms 901: learn: 0.0524220 total: 7.72s remaining: 838ms 902: learn: 0.0523740 total: 7.72s remaining: 830ms 903: learn: 0.0523329 total: 7.73s remaining: 821ms 904: learn: 0.0523046 total: 7.74s remaining: 813ms 905: learn: 0.0522647 total: 7.75s remaining: 804ms 906: learn: 0.0522019 total: 7.75s remaining: 795ms 907: learn: 0.0521640 total: 7.76s remaining: 787ms 908: learn: 0.0521571 total: 7.77s remaining: 778ms 909: learn: 0.0521549 total: 7.79s remaining: 770ms 910: learn: 0.0521417 total: 7.79s remaining: 761ms 911: learn: 0.0521026 total: 7.8s remaining: 753ms 912: learn: 0.0520721 total: 7.81s remaining: 744ms 913: learn: 0.0520676 total: 7.82s remaining: 736ms 914: learn: 0.0520503 total: 7.83s remaining: 727ms 915: learn: 0.0520185 total: 7.84s remaining: 719ms 916: learn: 0.0519564 total: 7.84s remaining: 710ms 917: learn: 0.0519446 total: 7.85s remaining: 701ms 918: learn: 0.0519428 total: 7.86s remaining: 693ms 919: learn: 0.0518849 total: 7.87s remaining: 684ms 920: learn: 0.0518348 total: 7.87s remaining: 675ms 921: learn: 0.0518202 total: 7.88s remaining: 667ms 922: learn: 0.0518157 total: 7.89s remaining: 658ms 923: learn: 0.0518132 total: 7.9s remaining: 650ms 924: learn: 0.0517846 total: 7.9s remaining: 641ms 925: learn: 0.0517075 total: 7.91s remaining: 632ms 926: learn: 0.0516682 total: 7.92s remaining: 624ms 927: learn: 0.0516652 total: 7.93s remaining: 615ms 928: learn: 0.0516632 total: 7.94s remaining: 607ms 929: learn: 0.0516176 total: 7.95s remaining: 598ms 930: learn: 0.0515796 total: 7.95s remaining: 590ms 931: learn: 0.0515386 total: 7.96s remaining: 581ms 932: learn: 0.0515001 total: 7.97s remaining: 572ms 933: learn: 0.0514771 total: 7.98s remaining: 564ms 934: learn: 0.0514752 total: 8s remaining: 556ms 935: learn: 0.0513993 total: 8.01s remaining: 548ms 936: learn: 0.0513397 total: 8.02s remaining: 539ms 937: learn: 0.0513085 total: 8.03s remaining: 531ms 938: learn: 0.0512789 total: 8.04s remaining: 522ms 939: learn: 0.0512293 total: 8.04s remaining: 513ms 940: learn: 0.0512276 total: 8.05s remaining: 505ms 941: learn: 0.0511932 total: 8.06s remaining: 496ms 942: learn: 0.0511512 total: 8.07s remaining: 488ms 943: learn: 0.0511048 total: 8.07s remaining: 479ms 944: learn: 0.0510778 total: 8.08s remaining: 470ms 945: learn: 0.0510232 total: 8.09s remaining: 462ms 946: learn: 0.0509664 total: 8.1s remaining: 453ms 947: learn: 0.0509171 total: 8.11s remaining: 445ms 948: learn: 0.0508704 total: 8.11s remaining: 436ms 949: learn: 0.0508146 total: 8.12s remaining: 427ms 950: learn: 0.0507758 total: 8.13s remaining: 419ms 951: learn: 0.0507363 total: 8.14s remaining: 410ms 952: learn: 0.0507340 total: 8.14s remaining: 402ms 953: learn: 0.0506973 total: 8.15s remaining: 393ms 954: learn: 0.0506754 total: 8.16s remaining: 385ms 955: learn: 0.0506723 total: 8.17s remaining: 376ms 956: learn: 0.0506348 total: 8.18s remaining: 367ms 957: learn: 0.0505742 total: 8.19s remaining: 359ms 958: learn: 0.0505298 total: 8.2s remaining: 351ms 959: learn: 0.0504947 total: 8.21s remaining: 342ms 960: learn: 0.0504592 total: 8.23s remaining: 334ms 961: learn: 0.0504342 total: 8.23s remaining: 325ms 962: learn: 0.0504001 total: 8.24s remaining: 317ms 963: learn: 0.0503591 total: 8.25s remaining: 308ms 964: learn: 0.0503051 total: 8.26s remaining: 300ms 965: learn: 0.0503020 total: 8.27s remaining: 291ms 966: learn: 0.0502623 total: 8.27s remaining: 282ms 967: learn: 0.0502248 total: 8.28s remaining: 274ms 968: learn: 0.0502109 total: 8.29s remaining: 265ms 969: learn: 0.0501712 total: 8.3s remaining: 257ms 970: learn: 0.0501115 total: 8.31s remaining: 248ms 971: learn: 0.0500808 total: 8.31s remaining: 240ms 972: learn: 0.0500535 total: 8.32s remaining: 231ms 973: learn: 0.0500174 total: 8.33s remaining: 222ms 974: learn: 0.0499799 total: 8.34s remaining: 214ms 975: learn: 0.0499583 total: 8.35s remaining: 205ms 976: learn: 0.0499163 total: 8.35s remaining: 197ms 977: learn: 0.0498803 total: 8.36s remaining: 188ms 978: learn: 0.0498502 total: 8.37s remaining: 180ms 979: learn: 0.0498207 total: 8.38s remaining: 171ms 980: learn: 0.0497803 total: 8.38s remaining: 162ms 981: learn: 0.0497628 total: 8.4s remaining: 154ms 982: learn: 0.0497357 total: 8.41s remaining: 145ms 983: learn: 0.0496896 total: 8.42s remaining: 137ms 984: learn: 0.0496807 total: 8.43s remaining: 128ms 985: learn: 0.0496266 total: 8.44s remaining: 120ms 986: learn: 0.0495853 total: 8.45s remaining: 111ms 987: learn: 0.0495283 total: 8.46s remaining: 103ms 988: learn: 0.0494976 total: 8.46s remaining: 94.2ms 989: learn: 0.0494581 total: 8.47s remaining: 85.6ms 990: learn: 0.0494183 total: 8.48s remaining: 77ms 991: learn: 0.0493890 total: 8.49s remaining: 68.5ms 992: learn: 0.0493407 total: 8.5s remaining: 59.9ms 993: learn: 0.0492897 total: 8.51s remaining: 51.3ms 994: learn: 0.0492332 total: 8.51s remaining: 42.8ms 995: learn: 0.0492101 total: 8.52s remaining: 34.2ms 996: learn: 0.0491665 total: 8.53s remaining: 25.7ms 997: learn: 0.0491346 total: 8.54s remaining: 17.1ms 998: learn: 0.0491164 total: 8.54s remaining: 8.55ms 999: learn: 0.0490739 total: 8.55s remaining: 0us Learning rate set to 0.046383 0: learn: 0.3942193 total: 9.47ms remaining: 9.46s 1: learn: 0.3791564 total: 18.2ms remaining: 9.09s 2: learn: 0.3650168 total: 26.8ms remaining: 8.91s 3: learn: 0.3519174 total: 35.3ms remaining: 8.79s 4: learn: 0.3389860 total: 44.3ms remaining: 8.81s 5: learn: 0.3270690 total: 52.6ms remaining: 8.72s 6: learn: 0.3147294 total: 61.4ms remaining: 8.71s 7: learn: 0.3038026 total: 70.1ms remaining: 8.69s 8: learn: 0.2926438 total: 78.7ms remaining: 8.66s 9: learn: 0.2818487 total: 88.5ms remaining: 8.76s 10: learn: 0.2714556 total: 97.5ms remaining: 8.77s 11: learn: 0.2621299 total: 109ms remaining: 8.98s 12: learn: 0.2530590 total: 118ms remaining: 8.97s 13: learn: 0.2441701 total: 127ms remaining: 8.93s 14: learn: 0.2362809 total: 135ms remaining: 8.89s 15: learn: 0.2285940 total: 144ms remaining: 8.88s 16: learn: 0.2215888 total: 153ms remaining: 8.86s 17: learn: 0.2146151 total: 162ms remaining: 8.85s 18: learn: 0.2075079 total: 171ms remaining: 8.83s 19: learn: 0.2005031 total: 181ms remaining: 8.86s 20: learn: 0.1939265 total: 196ms remaining: 9.15s 21: learn: 0.1879764 total: 213ms remaining: 9.49s 22: learn: 0.1820844 total: 222ms remaining: 9.44s 23: learn: 0.1761836 total: 231ms remaining: 9.39s 24: learn: 0.1710223 total: 240ms remaining: 9.35s 25: learn: 0.1663021 total: 249ms remaining: 9.31s 26: learn: 0.1616871 total: 258ms remaining: 9.28s 27: learn: 0.1576461 total: 266ms remaining: 9.24s 28: learn: 0.1533110 total: 275ms remaining: 9.21s 29: learn: 0.1489111 total: 284ms remaining: 9.19s 30: learn: 0.1450800 total: 294ms remaining: 9.18s 31: learn: 0.1415688 total: 303ms remaining: 9.15s 32: learn: 0.1382837 total: 311ms remaining: 9.12s 33: learn: 0.1347557 total: 320ms remaining: 9.1s 34: learn: 0.1312869 total: 331ms remaining: 9.12s 35: learn: 0.1283694 total: 340ms remaining: 9.1s 36: learn: 0.1252780 total: 348ms remaining: 9.07s 37: learn: 0.1228865 total: 357ms remaining: 9.05s 38: learn: 0.1203080 total: 366ms remaining: 9.03s 39: learn: 0.1174677 total: 375ms remaining: 9s 40: learn: 0.1147532 total: 384ms remaining: 8.98s 41: learn: 0.1126896 total: 397ms remaining: 9.06s 42: learn: 0.1104568 total: 408ms remaining: 9.08s 43: learn: 0.1083261 total: 417ms remaining: 9.06s 44: learn: 0.1063220 total: 426ms remaining: 9.04s 45: learn: 0.1044474 total: 434ms remaining: 9.01s 46: learn: 0.1026995 total: 443ms remaining: 8.98s 47: learn: 0.1011516 total: 452ms remaining: 8.96s 48: learn: 0.0995343 total: 460ms remaining: 8.93s 49: learn: 0.0977557 total: 469ms remaining: 8.91s 50: learn: 0.0963253 total: 477ms remaining: 8.88s 51: learn: 0.0947916 total: 487ms remaining: 8.87s 52: learn: 0.0935425 total: 496ms remaining: 8.86s 53: learn: 0.0923332 total: 505ms remaining: 8.85s 54: learn: 0.0911756 total: 514ms remaining: 8.83s 55: learn: 0.0900823 total: 525ms remaining: 8.85s 56: learn: 0.0890303 total: 537ms remaining: 8.88s 57: learn: 0.0880101 total: 546ms remaining: 8.88s 58: learn: 0.0868602 total: 556ms remaining: 8.86s 59: learn: 0.0859006 total: 565ms remaining: 8.85s 60: learn: 0.0849121 total: 574ms remaining: 8.83s 61: learn: 0.0840512 total: 596ms remaining: 9.02s 62: learn: 0.0833114 total: 618ms remaining: 9.19s 63: learn: 0.0825660 total: 627ms remaining: 9.17s 64: learn: 0.0818502 total: 636ms remaining: 9.14s 65: learn: 0.0813480 total: 644ms remaining: 9.12s 66: learn: 0.0805791 total: 653ms remaining: 9.1s 67: learn: 0.0798664 total: 662ms remaining: 9.08s 68: learn: 0.0793079 total: 671ms remaining: 9.05s 69: learn: 0.0787904 total: 680ms remaining: 9.03s 70: learn: 0.0782777 total: 689ms remaining: 9.02s 71: learn: 0.0777852 total: 698ms remaining: 8.99s 72: learn: 0.0773489 total: 707ms remaining: 8.97s 73: learn: 0.0769787 total: 715ms remaining: 8.95s 74: learn: 0.0765852 total: 724ms remaining: 8.93s 75: learn: 0.0761851 total: 733ms remaining: 8.91s 76: learn: 0.0758539 total: 742ms remaining: 8.9s 77: learn: 0.0753949 total: 752ms remaining: 8.88s 78: learn: 0.0750185 total: 760ms remaining: 8.87s 79: learn: 0.0746526 total: 769ms remaining: 8.84s 80: learn: 0.0743414 total: 778ms remaining: 8.82s 81: learn: 0.0740634 total: 787ms remaining: 8.81s 82: learn: 0.0738739 total: 805ms remaining: 8.9s 83: learn: 0.0735342 total: 818ms remaining: 8.92s 84: learn: 0.0731691 total: 827ms remaining: 8.9s 85: learn: 0.0728683 total: 839ms remaining: 8.92s 86: learn: 0.0726237 total: 848ms remaining: 8.9s 87: learn: 0.0723458 total: 857ms remaining: 8.88s 88: learn: 0.0722537 total: 865ms remaining: 8.86s 89: learn: 0.0721148 total: 874ms remaining: 8.83s 90: learn: 0.0718934 total: 882ms remaining: 8.81s 91: learn: 0.0715876 total: 891ms remaining: 8.8s 92: learn: 0.0715568 total: 900ms remaining: 8.78s 93: learn: 0.0713069 total: 910ms remaining: 8.77s 94: learn: 0.0711106 total: 919ms remaining: 8.75s 95: learn: 0.0709795 total: 927ms remaining: 8.73s 96: learn: 0.0708978 total: 937ms remaining: 8.72s 97: learn: 0.0706503 total: 946ms remaining: 8.7s 98: learn: 0.0706278 total: 955ms remaining: 8.69s 99: learn: 0.0704359 total: 965ms remaining: 8.68s 100: learn: 0.0703556 total: 973ms remaining: 8.66s 101: learn: 0.0702378 total: 982ms remaining: 8.64s 102: learn: 0.0701057 total: 990ms remaining: 8.63s 103: learn: 0.0698316 total: 1s remaining: 8.62s 104: learn: 0.0696483 total: 1.02s remaining: 8.67s 105: learn: 0.0694494 total: 1.03s remaining: 8.68s 106: learn: 0.0692729 total: 1.04s remaining: 8.67s 107: learn: 0.0691374 total: 1.05s remaining: 8.67s 108: learn: 0.0689286 total: 1.06s remaining: 8.69s 109: learn: 0.0688036 total: 1.07s remaining: 8.69s 110: learn: 0.0687033 total: 1.08s remaining: 8.68s 111: learn: 0.0685216 total: 1.09s remaining: 8.66s 112: learn: 0.0683492 total: 1.1s remaining: 8.64s 113: learn: 0.0682163 total: 1.11s remaining: 8.62s 114: learn: 0.0680497 total: 1.12s remaining: 8.61s 115: learn: 0.0678944 total: 1.13s remaining: 8.59s 116: learn: 0.0678342 total: 1.14s remaining: 8.58s 117: learn: 0.0676951 total: 1.15s remaining: 8.56s 118: learn: 0.0676715 total: 1.15s remaining: 8.54s 119: learn: 0.0675201 total: 1.16s remaining: 8.53s 120: learn: 0.0673349 total: 1.17s remaining: 8.52s 121: learn: 0.0670787 total: 1.18s remaining: 8.5s 122: learn: 0.0668688 total: 1.19s remaining: 8.5s 123: learn: 0.0668413 total: 1.21s remaining: 8.53s 124: learn: 0.0667608 total: 1.22s remaining: 8.53s 125: learn: 0.0665999 total: 1.23s remaining: 8.51s 126: learn: 0.0664686 total: 1.24s remaining: 8.5s 127: learn: 0.0664017 total: 1.24s remaining: 8.48s 128: learn: 0.0662917 total: 1.25s remaining: 8.46s 129: learn: 0.0661301 total: 1.26s remaining: 8.45s 130: learn: 0.0659230 total: 1.27s remaining: 8.43s 131: learn: 0.0656955 total: 1.28s remaining: 8.41s 132: learn: 0.0655378 total: 1.29s remaining: 8.4s 133: learn: 0.0653159 total: 1.3s remaining: 8.38s 134: learn: 0.0652064 total: 1.3s remaining: 8.37s 135: learn: 0.0651127 total: 1.31s remaining: 8.35s 136: learn: 0.0650117 total: 1.32s remaining: 8.33s 137: learn: 0.0648910 total: 1.33s remaining: 8.32s 138: learn: 0.0648491 total: 1.34s remaining: 8.3s 139: learn: 0.0647956 total: 1.35s remaining: 8.29s 140: learn: 0.0647167 total: 1.36s remaining: 8.27s 141: learn: 0.0645903 total: 1.37s remaining: 8.26s 142: learn: 0.0644639 total: 1.38s remaining: 8.24s 143: learn: 0.0643791 total: 1.38s remaining: 8.23s 144: learn: 0.0641655 total: 1.39s remaining: 8.21s 145: learn: 0.0640596 total: 1.41s remaining: 8.25s 146: learn: 0.0639934 total: 1.43s remaining: 8.3s 147: learn: 0.0639203 total: 1.44s remaining: 8.29s 148: learn: 0.0638332 total: 1.45s remaining: 8.28s 149: learn: 0.0636520 total: 1.46s remaining: 8.26s 150: learn: 0.0635376 total: 1.47s remaining: 8.25s 151: learn: 0.0634181 total: 1.48s remaining: 8.24s 152: learn: 0.0632561 total: 1.49s remaining: 8.23s 153: learn: 0.0630972 total: 1.5s remaining: 8.21s 154: learn: 0.0629916 total: 1.5s remaining: 8.2s 155: learn: 0.0629259 total: 1.51s remaining: 8.19s 156: learn: 0.0628781 total: 1.52s remaining: 8.18s 157: learn: 0.0627144 total: 1.53s remaining: 8.16s 158: learn: 0.0625456 total: 1.54s remaining: 8.15s 159: learn: 0.0623806 total: 1.56s remaining: 8.17s 160: learn: 0.0623283 total: 1.56s remaining: 8.14s 161: learn: 0.0621558 total: 1.57s remaining: 8.13s 162: learn: 0.0620881 total: 1.58s remaining: 8.12s 163: learn: 0.0620341 total: 1.59s remaining: 8.11s 164: learn: 0.0618729 total: 1.6s remaining: 8.09s 165: learn: 0.0618492 total: 1.61s remaining: 8.11s 166: learn: 0.0617242 total: 1.63s remaining: 8.15s 167: learn: 0.0616287 total: 1.64s remaining: 8.13s 168: learn: 0.0614911 total: 1.65s remaining: 8.12s 169: learn: 0.0614131 total: 1.66s remaining: 8.11s 170: learn: 0.0613704 total: 1.67s remaining: 8.09s 171: learn: 0.0612543 total: 1.68s remaining: 8.08s 172: learn: 0.0611264 total: 1.69s remaining: 8.08s 173: learn: 0.0610112 total: 1.7s remaining: 8.06s 174: learn: 0.0608607 total: 1.71s remaining: 8.05s 175: learn: 0.0607730 total: 1.72s remaining: 8.04s 176: learn: 0.0606595 total: 1.73s remaining: 8.03s 177: learn: 0.0605893 total: 1.74s remaining: 8.03s 178: learn: 0.0604896 total: 1.75s remaining: 8.02s 179: learn: 0.0604451 total: 1.76s remaining: 8.01s 180: learn: 0.0603181 total: 1.77s remaining: 7.99s 181: learn: 0.0602279 total: 1.78s remaining: 7.99s 182: learn: 0.0600862 total: 1.79s remaining: 7.98s 183: learn: 0.0600451 total: 1.8s remaining: 7.97s 184: learn: 0.0599208 total: 1.8s remaining: 7.95s 185: learn: 0.0598164 total: 1.82s remaining: 7.98s 186: learn: 0.0597761 total: 1.83s remaining: 7.97s 187: learn: 0.0596558 total: 1.84s remaining: 7.95s 188: learn: 0.0595839 total: 1.85s remaining: 7.94s 189: learn: 0.0594751 total: 1.86s remaining: 7.93s 190: learn: 0.0593804 total: 1.87s remaining: 7.92s 191: learn: 0.0592346 total: 1.88s remaining: 7.9s 192: learn: 0.0591660 total: 1.89s remaining: 7.89s 193: learn: 0.0590551 total: 1.9s remaining: 7.88s 194: learn: 0.0589885 total: 1.91s remaining: 7.87s 195: learn: 0.0587901 total: 1.92s remaining: 7.86s 196: learn: 0.0586928 total: 1.93s remaining: 7.85s 197: learn: 0.0585718 total: 1.93s remaining: 7.84s 198: learn: 0.0584653 total: 1.94s remaining: 7.83s 199: learn: 0.0583743 total: 1.95s remaining: 7.82s 200: learn: 0.0582990 total: 1.96s remaining: 7.8s 201: learn: 0.0582425 total: 1.97s remaining: 7.79s 202: learn: 0.0581517 total: 1.98s remaining: 7.78s 203: learn: 0.0580680 total: 1.99s remaining: 7.77s 204: learn: 0.0579640 total: 2s remaining: 7.76s 205: learn: 0.0578469 total: 2.02s remaining: 7.77s 206: learn: 0.0577273 total: 2.04s remaining: 7.8s 207: learn: 0.0576380 total: 2.04s remaining: 7.79s 208: learn: 0.0574925 total: 2.05s remaining: 7.77s 209: learn: 0.0573802 total: 2.06s remaining: 7.76s 210: learn: 0.0572562 total: 2.07s remaining: 7.75s 211: learn: 0.0570917 total: 2.08s remaining: 7.74s 212: learn: 0.0569726 total: 2.09s remaining: 7.73s 213: learn: 0.0568292 total: 2.1s remaining: 7.72s 214: learn: 0.0567295 total: 2.11s remaining: 7.71s 215: learn: 0.0566561 total: 2.12s remaining: 7.7s 216: learn: 0.0565737 total: 2.13s remaining: 7.69s 217: learn: 0.0564128 total: 2.14s remaining: 7.68s 218: learn: 0.0562807 total: 2.15s remaining: 7.68s 219: learn: 0.0561277 total: 2.17s remaining: 7.69s 220: learn: 0.0560122 total: 2.18s remaining: 7.68s 221: learn: 0.0559679 total: 2.19s remaining: 7.67s 222: learn: 0.0559080 total: 2.2s remaining: 7.66s 223: learn: 0.0558218 total: 2.21s remaining: 7.64s 224: learn: 0.0557510 total: 2.22s remaining: 7.65s 225: learn: 0.0556869 total: 2.24s remaining: 7.66s 226: learn: 0.0556112 total: 2.25s remaining: 7.65s 227: learn: 0.0555239 total: 2.25s remaining: 7.64s 228: learn: 0.0554112 total: 2.26s remaining: 7.62s 229: learn: 0.0553418 total: 2.27s remaining: 7.61s 230: learn: 0.0552514 total: 2.28s remaining: 7.6s 231: learn: 0.0551256 total: 2.29s remaining: 7.58s 232: learn: 0.0550426 total: 2.3s remaining: 7.57s 233: learn: 0.0549721 total: 2.31s remaining: 7.56s 234: learn: 0.0549128 total: 2.32s remaining: 7.54s 235: learn: 0.0548210 total: 2.33s remaining: 7.53s 236: learn: 0.0547404 total: 2.33s remaining: 7.52s 237: learn: 0.0546668 total: 2.34s remaining: 7.5s 238: learn: 0.0545386 total: 2.35s remaining: 7.49s 239: learn: 0.0544039 total: 2.36s remaining: 7.48s 240: learn: 0.0543268 total: 2.37s remaining: 7.47s 241: learn: 0.0541983 total: 2.38s remaining: 7.45s 242: learn: 0.0540554 total: 2.39s remaining: 7.44s 243: learn: 0.0539505 total: 2.4s remaining: 7.43s 244: learn: 0.0538680 total: 2.4s remaining: 7.41s 245: learn: 0.0538065 total: 2.41s remaining: 7.4s 246: learn: 0.0537502 total: 2.42s remaining: 7.39s 247: learn: 0.0536831 total: 2.44s remaining: 7.41s 248: learn: 0.0535548 total: 2.45s remaining: 7.39s 249: learn: 0.0534521 total: 2.46s remaining: 7.38s 250: learn: 0.0533978 total: 2.47s remaining: 7.37s 251: learn: 0.0533347 total: 2.48s remaining: 7.36s 252: learn: 0.0532448 total: 2.49s remaining: 7.34s 253: learn: 0.0531592 total: 2.5s remaining: 7.33s 254: learn: 0.0530679 total: 2.5s remaining: 7.32s 255: learn: 0.0529521 total: 2.51s remaining: 7.3s 256: learn: 0.0528653 total: 2.52s remaining: 7.29s 257: learn: 0.0527974 total: 2.54s remaining: 7.29s 258: learn: 0.0527316 total: 2.55s remaining: 7.29s 259: learn: 0.0526555 total: 2.56s remaining: 7.29s 260: learn: 0.0525785 total: 2.57s remaining: 7.28s 261: learn: 0.0524983 total: 2.58s remaining: 7.26s 262: learn: 0.0524394 total: 2.59s remaining: 7.25s 263: learn: 0.0523574 total: 2.59s remaining: 7.23s 264: learn: 0.0523041 total: 2.6s remaining: 7.22s 265: learn: 0.0521975 total: 2.61s remaining: 7.21s 266: learn: 0.0521141 total: 2.62s remaining: 7.2s 267: learn: 0.0519953 total: 2.64s remaining: 7.21s 268: learn: 0.0519109 total: 2.65s remaining: 7.2s 269: learn: 0.0518402 total: 2.66s remaining: 7.18s 270: learn: 0.0517387 total: 2.66s remaining: 7.17s 271: learn: 0.0516478 total: 2.67s remaining: 7.15s 272: learn: 0.0515565 total: 2.68s remaining: 7.14s 273: learn: 0.0514555 total: 2.69s remaining: 7.13s 274: learn: 0.0513645 total: 2.7s remaining: 7.12s 275: learn: 0.0512843 total: 2.71s remaining: 7.11s 276: learn: 0.0512558 total: 2.72s remaining: 7.11s 277: learn: 0.0511463 total: 2.73s remaining: 7.1s 278: learn: 0.0510739 total: 2.74s remaining: 7.09s 279: learn: 0.0509844 total: 2.75s remaining: 7.08s 280: learn: 0.0509181 total: 2.76s remaining: 7.06s 281: learn: 0.0508081 total: 2.77s remaining: 7.05s 282: learn: 0.0507719 total: 2.78s remaining: 7.04s 283: learn: 0.0506818 total: 2.79s remaining: 7.03s 284: learn: 0.0505821 total: 2.8s remaining: 7.02s 285: learn: 0.0505273 total: 2.81s remaining: 7.01s 286: learn: 0.0504118 total: 2.82s remaining: 7s 287: learn: 0.0503611 total: 2.83s remaining: 6.99s 288: learn: 0.0502588 total: 2.85s remaining: 7.01s 289: learn: 0.0502030 total: 2.86s remaining: 7s 290: learn: 0.0501330 total: 2.87s remaining: 6.98s 291: learn: 0.0500504 total: 2.88s remaining: 6.97s 292: learn: 0.0500031 total: 2.89s remaining: 6.97s 293: learn: 0.0498934 total: 2.9s remaining: 6.96s 294: learn: 0.0498244 total: 2.9s remaining: 6.94s 295: learn: 0.0497920 total: 2.91s remaining: 6.93s 296: learn: 0.0497116 total: 2.92s remaining: 6.92s 297: learn: 0.0496305 total: 2.93s remaining: 6.91s 298: learn: 0.0495834 total: 2.94s remaining: 6.89s 299: learn: 0.0495355 total: 2.95s remaining: 6.88s 300: learn: 0.0494328 total: 2.96s remaining: 6.87s 301: learn: 0.0493735 total: 2.97s remaining: 6.87s 302: learn: 0.0492924 total: 2.98s remaining: 6.86s 303: learn: 0.0492597 total: 2.99s remaining: 6.84s 304: learn: 0.0492260 total: 3s remaining: 6.83s 305: learn: 0.0491717 total: 3.01s remaining: 6.82s 306: learn: 0.0490846 total: 3.02s remaining: 6.81s 307: learn: 0.0490106 total: 3.02s remaining: 6.8s 308: learn: 0.0489493 total: 3.05s remaining: 6.82s 309: learn: 0.0489028 total: 3.06s remaining: 6.81s 310: learn: 0.0488211 total: 3.07s remaining: 6.8s 311: learn: 0.0487458 total: 3.08s remaining: 6.79s 312: learn: 0.0486481 total: 3.09s remaining: 6.77s 313: learn: 0.0486002 total: 3.1s remaining: 6.76s 314: learn: 0.0485358 total: 3.1s remaining: 6.75s 315: learn: 0.0484535 total: 3.11s remaining: 6.74s 316: learn: 0.0483718 total: 3.12s remaining: 6.73s 317: learn: 0.0483125 total: 3.13s remaining: 6.72s 318: learn: 0.0482383 total: 3.14s remaining: 6.71s 319: learn: 0.0481507 total: 3.15s remaining: 6.7s 320: learn: 0.0480708 total: 3.16s remaining: 6.68s 321: learn: 0.0480217 total: 3.17s remaining: 6.67s 322: learn: 0.0479850 total: 3.18s remaining: 6.66s 323: learn: 0.0479155 total: 3.19s remaining: 6.65s 324: learn: 0.0478363 total: 3.19s remaining: 6.64s 325: learn: 0.0478116 total: 3.2s remaining: 6.63s 326: learn: 0.0477378 total: 3.21s remaining: 6.62s 327: learn: 0.0476986 total: 3.22s remaining: 6.61s 328: learn: 0.0476708 total: 3.23s remaining: 6.6s 329: learn: 0.0476286 total: 3.25s remaining: 6.6s 330: learn: 0.0475492 total: 3.27s remaining: 6.6s 331: learn: 0.0474841 total: 3.27s remaining: 6.59s 332: learn: 0.0474138 total: 3.28s remaining: 6.58s 333: learn: 0.0473456 total: 3.29s remaining: 6.56s 334: learn: 0.0472529 total: 3.3s remaining: 6.55s 335: learn: 0.0471891 total: 3.31s remaining: 6.55s 336: learn: 0.0471234 total: 3.32s remaining: 6.54s 337: learn: 0.0470926 total: 3.33s remaining: 6.52s 338: learn: 0.0470187 total: 3.34s remaining: 6.51s 339: learn: 0.0469496 total: 3.35s remaining: 6.5s 340: learn: 0.0468722 total: 3.36s remaining: 6.5s 341: learn: 0.0468122 total: 3.37s remaining: 6.49s 342: learn: 0.0467499 total: 3.38s remaining: 6.47s 343: learn: 0.0466609 total: 3.39s remaining: 6.46s 344: learn: 0.0466297 total: 3.4s remaining: 6.45s 345: learn: 0.0466005 total: 3.4s remaining: 6.44s 346: learn: 0.0465423 total: 3.42s remaining: 6.43s 347: learn: 0.0464734 total: 3.42s remaining: 6.42s 348: learn: 0.0464360 total: 3.43s remaining: 6.4s 349: learn: 0.0463710 total: 3.44s remaining: 6.39s 350: learn: 0.0463180 total: 3.46s remaining: 6.41s 351: learn: 0.0462286 total: 3.48s remaining: 6.4s 352: learn: 0.0461751 total: 3.48s remaining: 6.39s 353: learn: 0.0461009 total: 3.49s remaining: 6.38s 354: learn: 0.0460486 total: 3.51s remaining: 6.37s 355: learn: 0.0460119 total: 3.52s remaining: 6.37s 356: learn: 0.0459554 total: 3.53s remaining: 6.36s 357: learn: 0.0458915 total: 3.54s remaining: 6.34s 358: learn: 0.0458244 total: 3.55s remaining: 6.33s 359: learn: 0.0457669 total: 3.56s remaining: 6.32s 360: learn: 0.0457089 total: 3.56s remaining: 6.31s 361: learn: 0.0456418 total: 3.57s remaining: 6.3s 362: learn: 0.0455972 total: 3.58s remaining: 6.29s 363: learn: 0.0455397 total: 3.59s remaining: 6.28s 364: learn: 0.0455095 total: 3.6s remaining: 6.26s 365: learn: 0.0454579 total: 3.61s remaining: 6.25s 366: learn: 0.0454040 total: 3.62s remaining: 6.24s 367: learn: 0.0453537 total: 3.63s remaining: 6.23s 368: learn: 0.0452777 total: 3.64s remaining: 6.22s 369: learn: 0.0452211 total: 3.65s remaining: 6.21s 370: learn: 0.0451703 total: 3.66s remaining: 6.2s 371: learn: 0.0451190 total: 3.67s remaining: 6.2s 372: learn: 0.0450946 total: 3.68s remaining: 6.19s 373: learn: 0.0450491 total: 3.69s remaining: 6.18s 374: learn: 0.0449916 total: 3.7s remaining: 6.17s 375: learn: 0.0449362 total: 3.71s remaining: 6.17s 376: learn: 0.0448536 total: 3.72s remaining: 6.15s 377: learn: 0.0447825 total: 3.73s remaining: 6.14s 378: learn: 0.0447344 total: 3.74s remaining: 6.13s 379: learn: 0.0446848 total: 3.75s remaining: 6.12s 380: learn: 0.0446576 total: 3.76s remaining: 6.11s 381: learn: 0.0446150 total: 3.77s remaining: 6.1s 382: learn: 0.0445644 total: 3.78s remaining: 6.09s 383: learn: 0.0445356 total: 3.79s remaining: 6.08s 384: learn: 0.0444903 total: 3.8s remaining: 6.06s 385: learn: 0.0444291 total: 3.81s remaining: 6.05s 386: learn: 0.0443619 total: 3.81s remaining: 6.04s 387: learn: 0.0443122 total: 3.82s remaining: 6.03s 388: learn: 0.0442491 total: 3.83s remaining: 6.02s 389: learn: 0.0442083 total: 3.84s remaining: 6.01s 390: learn: 0.0441490 total: 3.85s remaining: 6s 391: learn: 0.0440878 total: 3.86s remaining: 5.98s 392: learn: 0.0440650 total: 3.87s remaining: 5.97s 393: learn: 0.0440149 total: 3.89s remaining: 5.98s 394: learn: 0.0439801 total: 3.9s remaining: 5.97s 395: learn: 0.0439298 total: 3.91s remaining: 5.96s 396: learn: 0.0438837 total: 3.92s remaining: 5.95s 397: learn: 0.0438378 total: 3.93s remaining: 5.94s 398: learn: 0.0437846 total: 3.94s remaining: 5.93s 399: learn: 0.0437293 total: 3.95s remaining: 5.92s 400: learn: 0.0436602 total: 3.96s remaining: 5.91s 401: learn: 0.0436155 total: 3.96s remaining: 5.9s 402: learn: 0.0435725 total: 3.98s remaining: 5.89s 403: learn: 0.0435104 total: 3.99s remaining: 5.88s 404: learn: 0.0434757 total: 4s remaining: 5.87s 405: learn: 0.0434279 total: 4.01s remaining: 5.86s 406: learn: 0.0433814 total: 4.01s remaining: 5.85s 407: learn: 0.0433469 total: 4.02s remaining: 5.84s 408: learn: 0.0433092 total: 4.03s remaining: 5.83s 409: learn: 0.0432496 total: 4.04s remaining: 5.82s 410: learn: 0.0431949 total: 4.05s remaining: 5.8s 411: learn: 0.0431419 total: 4.06s remaining: 5.79s 412: learn: 0.0430944 total: 4.07s remaining: 5.78s 413: learn: 0.0430515 total: 4.08s remaining: 5.77s 414: learn: 0.0429864 total: 4.09s remaining: 5.77s 415: learn: 0.0429368 total: 4.11s remaining: 5.76s 416: learn: 0.0428847 total: 4.11s remaining: 5.75s 417: learn: 0.0428594 total: 4.12s remaining: 5.74s 418: learn: 0.0427994 total: 4.13s remaining: 5.73s 419: learn: 0.0427493 total: 4.14s remaining: 5.72s 420: learn: 0.0426961 total: 4.15s remaining: 5.71s 421: learn: 0.0426445 total: 4.16s remaining: 5.7s 422: learn: 0.0426072 total: 4.17s remaining: 5.68s 423: learn: 0.0425539 total: 4.17s remaining: 5.67s 424: learn: 0.0424998 total: 4.18s remaining: 5.66s 425: learn: 0.0424430 total: 4.19s remaining: 5.65s 426: learn: 0.0424190 total: 4.2s remaining: 5.64s 427: learn: 0.0423812 total: 4.21s remaining: 5.63s 428: learn: 0.0423496 total: 4.22s remaining: 5.62s 429: learn: 0.0422895 total: 4.23s remaining: 5.61s 430: learn: 0.0422592 total: 4.24s remaining: 5.59s 431: learn: 0.0422197 total: 4.25s remaining: 5.58s 432: learn: 0.0421865 total: 4.25s remaining: 5.57s 433: learn: 0.0421455 total: 4.26s remaining: 5.56s 434: learn: 0.0421057 total: 4.27s remaining: 5.55s 435: learn: 0.0420526 total: 4.28s remaining: 5.54s 436: learn: 0.0419969 total: 4.29s remaining: 5.53s 437: learn: 0.0419484 total: 4.31s remaining: 5.53s 438: learn: 0.0419152 total: 4.32s remaining: 5.52s 439: learn: 0.0418774 total: 4.33s remaining: 5.51s 440: learn: 0.0418452 total: 4.34s remaining: 5.5s 441: learn: 0.0418011 total: 4.35s remaining: 5.49s 442: learn: 0.0417672 total: 4.36s remaining: 5.48s 443: learn: 0.0417296 total: 4.37s remaining: 5.47s 444: learn: 0.0416917 total: 4.37s remaining: 5.46s 445: learn: 0.0416453 total: 4.38s remaining: 5.45s 446: learn: 0.0415986 total: 4.39s remaining: 5.43s 447: learn: 0.0415666 total: 4.4s remaining: 5.42s 448: learn: 0.0415109 total: 4.41s remaining: 5.41s 449: learn: 0.0414595 total: 4.42s remaining: 5.4s 450: learn: 0.0414004 total: 4.43s remaining: 5.39s 451: learn: 0.0413346 total: 4.44s remaining: 5.38s 452: learn: 0.0412941 total: 4.45s remaining: 5.37s 453: learn: 0.0412396 total: 4.46s remaining: 5.36s 454: learn: 0.0411993 total: 4.47s remaining: 5.35s 455: learn: 0.0411577 total: 4.47s remaining: 5.34s 456: learn: 0.0411218 total: 4.49s remaining: 5.33s 457: learn: 0.0410955 total: 4.51s remaining: 5.33s 458: learn: 0.0410688 total: 4.52s remaining: 5.33s 459: learn: 0.0410228 total: 4.53s remaining: 5.32s 460: learn: 0.0409845 total: 4.54s remaining: 5.3s 461: learn: 0.0409253 total: 4.55s remaining: 5.29s 462: learn: 0.0408884 total: 4.55s remaining: 5.28s 463: learn: 0.0408452 total: 4.56s remaining: 5.27s 464: learn: 0.0407989 total: 4.58s remaining: 5.26s 465: learn: 0.0407545 total: 4.58s remaining: 5.25s 466: learn: 0.0407072 total: 4.59s remaining: 5.24s 467: learn: 0.0406677 total: 4.6s remaining: 5.23s 468: learn: 0.0406108 total: 4.61s remaining: 5.22s 469: learn: 0.0405673 total: 4.62s remaining: 5.21s 470: learn: 0.0405099 total: 4.63s remaining: 5.2s 471: learn: 0.0404902 total: 4.64s remaining: 5.2s 472: learn: 0.0404331 total: 4.66s remaining: 5.19s 473: learn: 0.0404092 total: 4.66s remaining: 5.18s 474: learn: 0.0403854 total: 4.67s remaining: 5.17s 475: learn: 0.0403737 total: 4.68s remaining: 5.15s 476: learn: 0.0403465 total: 4.69s remaining: 5.14s 477: learn: 0.0403134 total: 4.71s remaining: 5.14s 478: learn: 0.0402787 total: 4.72s remaining: 5.13s 479: learn: 0.0402391 total: 4.73s remaining: 5.12s 480: learn: 0.0401994 total: 4.74s remaining: 5.11s 481: learn: 0.0401773 total: 4.75s remaining: 5.1s 482: learn: 0.0401220 total: 4.76s remaining: 5.09s 483: learn: 0.0400996 total: 4.77s remaining: 5.08s 484: learn: 0.0400660 total: 4.78s remaining: 5.07s 485: learn: 0.0400357 total: 4.79s remaining: 5.06s 486: learn: 0.0400121 total: 4.79s remaining: 5.05s 487: learn: 0.0399839 total: 4.8s remaining: 5.04s 488: learn: 0.0399380 total: 4.81s remaining: 5.03s 489: learn: 0.0399161 total: 4.82s remaining: 5.02s 490: learn: 0.0398819 total: 4.83s remaining: 5.01s 491: learn: 0.0398424 total: 4.84s remaining: 5s 492: learn: 0.0398085 total: 4.85s remaining: 4.99s 493: learn: 0.0397545 total: 4.86s remaining: 4.98s 494: learn: 0.0397008 total: 4.87s remaining: 4.97s 495: learn: 0.0396661 total: 4.88s remaining: 4.96s 496: learn: 0.0396274 total: 4.89s remaining: 4.95s 497: learn: 0.0395858 total: 4.9s remaining: 4.94s 498: learn: 0.0395533 total: 4.92s remaining: 4.94s 499: learn: 0.0394992 total: 4.93s remaining: 4.93s 500: learn: 0.0394510 total: 4.94s remaining: 4.92s 501: learn: 0.0394071 total: 4.95s remaining: 4.91s 502: learn: 0.0393739 total: 4.96s remaining: 4.9s 503: learn: 0.0393271 total: 4.96s remaining: 4.89s 504: learn: 0.0393175 total: 4.97s remaining: 4.88s 505: learn: 0.0392847 total: 4.98s remaining: 4.87s 506: learn: 0.0392564 total: 4.99s remaining: 4.85s 507: learn: 0.0392083 total: 5s remaining: 4.84s 508: learn: 0.0391758 total: 5.01s remaining: 4.83s 509: learn: 0.0391228 total: 5.02s remaining: 4.82s 510: learn: 0.0390891 total: 5.03s remaining: 4.81s 511: learn: 0.0390423 total: 5.04s remaining: 4.8s 512: learn: 0.0389960 total: 5.05s remaining: 4.79s 513: learn: 0.0389479 total: 5.07s remaining: 4.79s 514: learn: 0.0389054 total: 5.09s remaining: 4.79s 515: learn: 0.0388779 total: 5.1s remaining: 4.78s 516: learn: 0.0388323 total: 5.11s remaining: 4.77s 517: learn: 0.0387933 total: 5.13s remaining: 4.78s 518: learn: 0.0387534 total: 5.14s remaining: 4.76s 519: learn: 0.0387153 total: 5.15s remaining: 4.75s 520: learn: 0.0386644 total: 5.16s remaining: 4.74s 521: learn: 0.0386255 total: 5.17s remaining: 4.74s 522: learn: 0.0385827 total: 5.19s remaining: 4.73s 523: learn: 0.0385469 total: 5.2s remaining: 4.72s 524: learn: 0.0385169 total: 5.21s remaining: 4.71s 525: learn: 0.0384851 total: 5.21s remaining: 4.7s 526: learn: 0.0384420 total: 5.22s remaining: 4.69s 527: learn: 0.0384155 total: 5.24s remaining: 4.68s 528: learn: 0.0383891 total: 5.25s remaining: 4.67s 529: learn: 0.0383514 total: 5.25s remaining: 4.66s 530: learn: 0.0382952 total: 5.26s remaining: 4.65s 531: learn: 0.0382597 total: 5.27s remaining: 4.64s 532: learn: 0.0382372 total: 5.28s remaining: 4.63s 533: learn: 0.0382238 total: 5.29s remaining: 4.62s 534: learn: 0.0381948 total: 5.3s remaining: 4.61s 535: learn: 0.0381632 total: 5.32s remaining: 4.61s 536: learn: 0.0381366 total: 5.33s remaining: 4.59s 537: learn: 0.0380997 total: 5.34s remaining: 4.58s 538: learn: 0.0380686 total: 5.35s remaining: 4.57s 539: learn: 0.0380268 total: 5.36s remaining: 4.56s 540: learn: 0.0380055 total: 5.37s remaining: 4.55s 541: learn: 0.0379671 total: 5.38s remaining: 4.54s 542: learn: 0.0379380 total: 5.38s remaining: 4.53s 543: learn: 0.0378926 total: 5.39s remaining: 4.52s 544: learn: 0.0378639 total: 5.4s remaining: 4.51s 545: learn: 0.0378493 total: 5.41s remaining: 4.5s 546: learn: 0.0378107 total: 5.42s remaining: 4.49s 547: learn: 0.0377615 total: 5.43s remaining: 4.48s 548: learn: 0.0377313 total: 5.44s remaining: 4.47s 549: learn: 0.0377008 total: 5.45s remaining: 4.46s 550: learn: 0.0376553 total: 5.46s remaining: 4.45s 551: learn: 0.0376390 total: 5.48s remaining: 4.45s 552: learn: 0.0376172 total: 5.49s remaining: 4.44s 553: learn: 0.0375893 total: 5.5s remaining: 4.42s 554: learn: 0.0375608 total: 5.5s remaining: 4.41s 555: learn: 0.0375292 total: 5.51s remaining: 4.4s 556: learn: 0.0375032 total: 5.53s remaining: 4.4s 557: learn: 0.0374625 total: 5.54s remaining: 4.39s 558: learn: 0.0374303 total: 5.55s remaining: 4.38s 559: learn: 0.0374112 total: 5.56s remaining: 4.37s 560: learn: 0.0373640 total: 5.57s remaining: 4.36s 561: learn: 0.0373370 total: 5.57s remaining: 4.34s 562: learn: 0.0373097 total: 5.58s remaining: 4.33s 563: learn: 0.0372653 total: 5.59s remaining: 4.32s 564: learn: 0.0372300 total: 5.6s remaining: 4.31s 565: learn: 0.0372072 total: 5.61s remaining: 4.3s 566: learn: 0.0371621 total: 5.62s remaining: 4.29s 567: learn: 0.0371366 total: 5.63s remaining: 4.28s 568: learn: 0.0371075 total: 5.64s remaining: 4.27s 569: learn: 0.0370890 total: 5.65s remaining: 4.26s 570: learn: 0.0370592 total: 5.66s remaining: 4.25s 571: learn: 0.0370336 total: 5.66s remaining: 4.24s 572: learn: 0.0370139 total: 5.67s remaining: 4.23s 573: learn: 0.0369837 total: 5.68s remaining: 4.22s 574: learn: 0.0369647 total: 5.69s remaining: 4.21s 575: learn: 0.0369438 total: 5.7s remaining: 4.2s 576: learn: 0.0368959 total: 5.71s remaining: 4.19s 577: learn: 0.0368604 total: 5.73s remaining: 4.18s 578: learn: 0.0368287 total: 5.74s remaining: 4.17s 579: learn: 0.0367949 total: 5.75s remaining: 4.16s 580: learn: 0.0367556 total: 5.76s remaining: 4.15s 581: learn: 0.0367225 total: 5.77s remaining: 4.14s 582: learn: 0.0366908 total: 5.78s remaining: 4.13s 583: learn: 0.0366441 total: 5.79s remaining: 4.12s 584: learn: 0.0366256 total: 5.79s remaining: 4.11s 585: learn: 0.0365723 total: 5.8s remaining: 4.1s 586: learn: 0.0365459 total: 5.81s remaining: 4.09s 587: learn: 0.0365157 total: 5.82s remaining: 4.08s 588: learn: 0.0364721 total: 5.84s remaining: 4.08s 589: learn: 0.0364452 total: 5.86s remaining: 4.07s 590: learn: 0.0364067 total: 5.87s remaining: 4.06s 591: learn: 0.0363745 total: 5.88s remaining: 4.05s 592: learn: 0.0363504 total: 5.89s remaining: 4.04s 593: learn: 0.0363151 total: 5.9s remaining: 4.03s 594: learn: 0.0362800 total: 5.91s remaining: 4.02s 595: learn: 0.0362488 total: 5.92s remaining: 4.01s 596: learn: 0.0362225 total: 5.94s remaining: 4.01s 597: learn: 0.0361947 total: 5.95s remaining: 4s 598: learn: 0.0361804 total: 5.96s remaining: 3.99s 599: learn: 0.0361562 total: 5.97s remaining: 3.98s 600: learn: 0.0361363 total: 5.98s remaining: 3.97s 601: learn: 0.0360898 total: 5.99s remaining: 3.96s 602: learn: 0.0360841 total: 5.99s remaining: 3.95s 603: learn: 0.0360664 total: 6s remaining: 3.94s 604: learn: 0.0360322 total: 6.01s remaining: 3.92s 605: learn: 0.0359887 total: 6.02s remaining: 3.92s 606: learn: 0.0359503 total: 6.03s remaining: 3.9s 607: learn: 0.0359112 total: 6.04s remaining: 3.89s 608: learn: 0.0358917 total: 6.05s remaining: 3.88s 609: learn: 0.0358483 total: 6.06s remaining: 3.87s 610: learn: 0.0357963 total: 6.07s remaining: 3.86s 611: learn: 0.0357621 total: 6.08s remaining: 3.85s 612: learn: 0.0357196 total: 6.09s remaining: 3.84s 613: learn: 0.0356991 total: 6.09s remaining: 3.83s 614: learn: 0.0356893 total: 6.1s remaining: 3.82s 615: learn: 0.0356666 total: 6.11s remaining: 3.81s 616: learn: 0.0356452 total: 6.12s remaining: 3.8s 617: learn: 0.0356218 total: 6.13s remaining: 3.79s 618: learn: 0.0355967 total: 6.16s remaining: 3.79s 619: learn: 0.0355505 total: 6.17s remaining: 3.78s 620: learn: 0.0355190 total: 6.17s remaining: 3.77s 621: learn: 0.0354922 total: 6.18s remaining: 3.76s 622: learn: 0.0354601 total: 6.19s remaining: 3.75s 623: learn: 0.0354449 total: 6.2s remaining: 3.73s 624: learn: 0.0354149 total: 6.21s remaining: 3.72s 625: learn: 0.0353794 total: 6.22s remaining: 3.71s 626: learn: 0.0353419 total: 6.22s remaining: 3.7s 627: learn: 0.0353009 total: 6.23s remaining: 3.69s 628: learn: 0.0352684 total: 6.24s remaining: 3.68s 629: learn: 0.0352489 total: 6.25s remaining: 3.67s 630: learn: 0.0352228 total: 6.26s remaining: 3.66s 631: learn: 0.0351944 total: 6.27s remaining: 3.65s 632: learn: 0.0351679 total: 6.28s remaining: 3.64s 633: learn: 0.0351413 total: 6.29s remaining: 3.63s 634: learn: 0.0351067 total: 6.29s remaining: 3.62s 635: learn: 0.0350733 total: 6.3s remaining: 3.61s 636: learn: 0.0350436 total: 6.31s remaining: 3.6s 637: learn: 0.0350155 total: 6.32s remaining: 3.59s 638: learn: 0.0349924 total: 6.33s remaining: 3.58s 639: learn: 0.0349816 total: 6.34s remaining: 3.56s 640: learn: 0.0349692 total: 6.35s remaining: 3.56s 641: learn: 0.0349381 total: 6.37s remaining: 3.55s 642: learn: 0.0349159 total: 6.38s remaining: 3.54s 643: learn: 0.0348879 total: 6.39s remaining: 3.53s 644: learn: 0.0348720 total: 6.4s remaining: 3.52s 645: learn: 0.0348189 total: 6.41s remaining: 3.51s 646: learn: 0.0347961 total: 6.42s remaining: 3.5s 647: learn: 0.0347682 total: 6.43s remaining: 3.49s 648: learn: 0.0347211 total: 6.44s remaining: 3.48s 649: learn: 0.0346953 total: 6.45s remaining: 3.47s 650: learn: 0.0346634 total: 6.46s remaining: 3.46s 651: learn: 0.0346439 total: 6.47s remaining: 3.45s 652: learn: 0.0346179 total: 6.48s remaining: 3.44s 653: learn: 0.0345987 total: 6.49s remaining: 3.43s 654: learn: 0.0345789 total: 6.5s remaining: 3.42s 655: learn: 0.0345588 total: 6.51s remaining: 3.41s 656: learn: 0.0345514 total: 6.51s remaining: 3.4s 657: learn: 0.0345226 total: 6.52s remaining: 3.39s 658: learn: 0.0344938 total: 6.53s remaining: 3.38s 659: learn: 0.0344573 total: 6.54s remaining: 3.37s 660: learn: 0.0344282 total: 6.56s remaining: 3.37s 661: learn: 0.0344050 total: 6.57s remaining: 3.35s 662: learn: 0.0343950 total: 6.58s remaining: 3.34s 663: learn: 0.0343659 total: 6.59s remaining: 3.33s 664: learn: 0.0343421 total: 6.6s remaining: 3.33s 665: learn: 0.0343031 total: 6.61s remaining: 3.31s 666: learn: 0.0342779 total: 6.62s remaining: 3.3s 667: learn: 0.0342623 total: 6.63s remaining: 3.29s 668: learn: 0.0342405 total: 6.64s remaining: 3.28s 669: learn: 0.0342083 total: 6.64s remaining: 3.27s 670: learn: 0.0341859 total: 6.65s remaining: 3.26s 671: learn: 0.0341688 total: 6.66s remaining: 3.25s 672: learn: 0.0341522 total: 6.67s remaining: 3.24s 673: learn: 0.0341263 total: 6.68s remaining: 3.23s 674: learn: 0.0341013 total: 6.69s remaining: 3.22s 675: learn: 0.0340619 total: 6.7s remaining: 3.21s 676: learn: 0.0340261 total: 6.71s remaining: 3.2s 677: learn: 0.0339963 total: 6.72s remaining: 3.19s 678: learn: 0.0339859 total: 6.72s remaining: 3.18s 679: learn: 0.0339625 total: 6.73s remaining: 3.17s 680: learn: 0.0339333 total: 6.74s remaining: 3.16s 681: learn: 0.0339188 total: 6.75s remaining: 3.15s 682: learn: 0.0338950 total: 6.77s remaining: 3.14s 683: learn: 0.0338791 total: 6.78s remaining: 3.13s 684: learn: 0.0338574 total: 6.79s remaining: 3.12s 685: learn: 0.0338416 total: 6.8s remaining: 3.11s 686: learn: 0.0338159 total: 6.81s remaining: 3.1s 687: learn: 0.0337838 total: 6.82s remaining: 3.09s 688: learn: 0.0337574 total: 6.83s remaining: 3.08s 689: learn: 0.0337326 total: 6.84s remaining: 3.07s 690: learn: 0.0336972 total: 6.84s remaining: 3.06s 691: learn: 0.0336759 total: 6.85s remaining: 3.05s 692: learn: 0.0336414 total: 6.86s remaining: 3.04s 693: learn: 0.0336206 total: 6.87s remaining: 3.03s 694: learn: 0.0335983 total: 6.88s remaining: 3.02s 695: learn: 0.0335718 total: 6.89s remaining: 3.01s 696: learn: 0.0335349 total: 6.9s remaining: 3s 697: learn: 0.0335057 total: 6.91s remaining: 2.99s 698: learn: 0.0334687 total: 6.92s remaining: 2.98s 699: learn: 0.0334600 total: 6.93s remaining: 2.97s 700: learn: 0.0334425 total: 6.93s remaining: 2.96s 701: learn: 0.0334028 total: 6.94s remaining: 2.95s 702: learn: 0.0333784 total: 6.95s remaining: 2.94s 703: learn: 0.0333562 total: 6.96s remaining: 2.93s 704: learn: 0.0333254 total: 6.98s remaining: 2.92s 705: learn: 0.0332991 total: 6.99s remaining: 2.91s 706: learn: 0.0332775 total: 7s remaining: 2.9s 707: learn: 0.0332641 total: 7.01s remaining: 2.89s 708: learn: 0.0332483 total: 7.03s remaining: 2.88s 709: learn: 0.0332169 total: 7.04s remaining: 2.87s 710: learn: 0.0331957 total: 7.05s remaining: 2.86s 711: learn: 0.0331534 total: 7.06s remaining: 2.85s 712: learn: 0.0331295 total: 7.07s remaining: 2.84s 713: learn: 0.0331062 total: 7.08s remaining: 2.83s 714: learn: 0.0330814 total: 7.08s remaining: 2.82s 715: learn: 0.0330520 total: 7.09s remaining: 2.81s 716: learn: 0.0330339 total: 7.1s remaining: 2.8s 717: learn: 0.0330195 total: 7.11s remaining: 2.79s 718: learn: 0.0329941 total: 7.12s remaining: 2.78s 719: learn: 0.0329772 total: 7.13s remaining: 2.77s 720: learn: 0.0329403 total: 7.14s remaining: 2.76s 721: learn: 0.0329199 total: 7.15s remaining: 2.75s 722: learn: 0.0329023 total: 7.16s remaining: 2.74s 723: learn: 0.0328743 total: 7.17s remaining: 2.73s 724: learn: 0.0328359 total: 7.18s remaining: 2.72s 725: learn: 0.0327977 total: 7.19s remaining: 2.71s 726: learn: 0.0327721 total: 7.2s remaining: 2.71s 727: learn: 0.0327466 total: 7.21s remaining: 2.69s 728: learn: 0.0327359 total: 7.22s remaining: 2.68s 729: learn: 0.0327075 total: 7.23s remaining: 2.67s 730: learn: 0.0326751 total: 7.24s remaining: 2.66s 731: learn: 0.0326360 total: 7.25s remaining: 2.65s 732: learn: 0.0326076 total: 7.26s remaining: 2.64s 733: learn: 0.0325875 total: 7.27s remaining: 2.63s 734: learn: 0.0325707 total: 7.28s remaining: 2.62s 735: learn: 0.0325421 total: 7.28s remaining: 2.61s 736: learn: 0.0325225 total: 7.29s remaining: 2.6s 737: learn: 0.0325061 total: 7.3s remaining: 2.59s 738: learn: 0.0324929 total: 7.31s remaining: 2.58s 739: learn: 0.0324624 total: 7.32s remaining: 2.57s 740: learn: 0.0324209 total: 7.33s remaining: 2.56s 741: learn: 0.0323931 total: 7.34s remaining: 2.55s 742: learn: 0.0323636 total: 7.35s remaining: 2.54s 743: learn: 0.0323401 total: 7.36s remaining: 2.53s 744: learn: 0.0323049 total: 7.37s remaining: 2.52s 745: learn: 0.0322793 total: 7.38s remaining: 2.51s 746: learn: 0.0322678 total: 7.39s remaining: 2.5s 747: learn: 0.0322567 total: 7.41s remaining: 2.5s 748: learn: 0.0322232 total: 7.43s remaining: 2.49s 749: learn: 0.0321951 total: 7.44s remaining: 2.48s 750: learn: 0.0321726 total: 7.45s remaining: 2.47s 751: learn: 0.0321446 total: 7.45s remaining: 2.46s 752: learn: 0.0321270 total: 7.47s remaining: 2.45s 753: learn: 0.0321009 total: 7.48s remaining: 2.44s 754: learn: 0.0320843 total: 7.49s remaining: 2.43s 755: learn: 0.0320605 total: 7.5s remaining: 2.42s 756: learn: 0.0320453 total: 7.51s remaining: 2.41s 757: learn: 0.0320271 total: 7.52s remaining: 2.4s 758: learn: 0.0320075 total: 7.53s remaining: 2.39s 759: learn: 0.0319787 total: 7.54s remaining: 2.38s 760: learn: 0.0319591 total: 7.54s remaining: 2.37s 761: learn: 0.0319282 total: 7.55s remaining: 2.36s 762: learn: 0.0319270 total: 7.56s remaining: 2.35s 763: learn: 0.0318949 total: 7.57s remaining: 2.34s 764: learn: 0.0318749 total: 7.58s remaining: 2.33s 765: learn: 0.0318578 total: 7.59s remaining: 2.32s 766: learn: 0.0318374 total: 7.61s remaining: 2.31s 767: learn: 0.0318155 total: 7.62s remaining: 2.3s 768: learn: 0.0318000 total: 7.63s remaining: 2.29s 769: learn: 0.0317799 total: 7.63s remaining: 2.28s 770: learn: 0.0317432 total: 7.64s remaining: 2.27s 771: learn: 0.0317122 total: 7.65s remaining: 2.26s 772: learn: 0.0316880 total: 7.66s remaining: 2.25s 773: learn: 0.0316695 total: 7.67s remaining: 2.24s 774: learn: 0.0316424 total: 7.68s remaining: 2.23s 775: learn: 0.0316412 total: 7.69s remaining: 2.22s 776: learn: 0.0316395 total: 7.7s remaining: 2.21s 777: learn: 0.0316251 total: 7.71s remaining: 2.2s 778: learn: 0.0316019 total: 7.71s remaining: 2.19s 779: learn: 0.0315804 total: 7.72s remaining: 2.18s 780: learn: 0.0315569 total: 7.73s remaining: 2.17s 781: learn: 0.0315348 total: 7.74s remaining: 2.16s 782: learn: 0.0315242 total: 7.75s remaining: 2.15s 783: learn: 0.0315230 total: 7.76s remaining: 2.14s 784: learn: 0.0315214 total: 7.77s remaining: 2.13s 785: learn: 0.0315018 total: 7.78s remaining: 2.12s 786: learn: 0.0314850 total: 7.79s remaining: 2.11s 787: learn: 0.0314596 total: 7.81s remaining: 2.1s 788: learn: 0.0314409 total: 7.82s remaining: 2.09s 789: learn: 0.0314116 total: 7.83s remaining: 2.08s 790: learn: 0.0313728 total: 7.84s remaining: 2.07s 791: learn: 0.0313440 total: 7.85s remaining: 2.06s 792: learn: 0.0313312 total: 7.86s remaining: 2.05s 793: learn: 0.0313127 total: 7.87s remaining: 2.04s 794: learn: 0.0312983 total: 7.88s remaining: 2.03s 795: learn: 0.0312716 total: 7.88s remaining: 2.02s 796: learn: 0.0312544 total: 7.89s remaining: 2.01s 797: learn: 0.0312398 total: 7.9s remaining: 2s 798: learn: 0.0312228 total: 7.91s remaining: 1.99s 799: learn: 0.0312079 total: 7.92s remaining: 1.98s 800: learn: 0.0311890 total: 7.93s remaining: 1.97s 801: learn: 0.0311667 total: 7.94s remaining: 1.96s 802: learn: 0.0311408 total: 7.95s remaining: 1.95s 803: learn: 0.0311178 total: 7.96s remaining: 1.94s 804: learn: 0.0311014 total: 7.97s remaining: 1.93s 805: learn: 0.0310873 total: 7.98s remaining: 1.92s 806: learn: 0.0310715 total: 7.99s remaining: 1.91s 807: learn: 0.0310644 total: 7.99s remaining: 1.9s 808: learn: 0.0310611 total: 8.01s remaining: 1.89s 809: learn: 0.0310375 total: 8.02s remaining: 1.88s 810: learn: 0.0310190 total: 8.03s remaining: 1.87s 811: learn: 0.0310099 total: 8.04s remaining: 1.86s 812: learn: 0.0309893 total: 8.05s remaining: 1.85s 813: learn: 0.0309878 total: 8.06s remaining: 1.84s 814: learn: 0.0309759 total: 8.07s remaining: 1.83s 815: learn: 0.0309631 total: 8.07s remaining: 1.82s 816: learn: 0.0309396 total: 8.08s remaining: 1.81s 817: learn: 0.0309338 total: 8.09s remaining: 1.8s 818: learn: 0.0309191 total: 8.1s remaining: 1.79s 819: learn: 0.0309175 total: 8.11s remaining: 1.78s 820: learn: 0.0309024 total: 8.12s remaining: 1.77s 821: learn: 0.0308724 total: 8.13s remaining: 1.76s 822: learn: 0.0308455 total: 8.14s remaining: 1.75s 823: learn: 0.0308158 total: 8.15s remaining: 1.74s 824: learn: 0.0307977 total: 8.16s remaining: 1.73s 825: learn: 0.0307810 total: 8.17s remaining: 1.72s 826: learn: 0.0307596 total: 8.18s remaining: 1.71s 827: learn: 0.0307358 total: 8.19s remaining: 1.7s 828: learn: 0.0307251 total: 8.2s remaining: 1.69s 829: learn: 0.0307063 total: 8.21s remaining: 1.68s 830: learn: 0.0306936 total: 8.23s remaining: 1.67s 831: learn: 0.0306667 total: 8.24s remaining: 1.66s 832: learn: 0.0306519 total: 8.25s remaining: 1.65s 833: learn: 0.0306329 total: 8.26s remaining: 1.64s 834: learn: 0.0306110 total: 8.27s remaining: 1.63s 835: learn: 0.0305975 total: 8.28s remaining: 1.62s 836: learn: 0.0305696 total: 8.28s remaining: 1.61s 837: learn: 0.0305510 total: 8.29s remaining: 1.6s 838: learn: 0.0305362 total: 8.3s remaining: 1.59s 839: learn: 0.0305162 total: 8.31s remaining: 1.58s 840: learn: 0.0304994 total: 8.32s remaining: 1.57s 841: learn: 0.0304791 total: 8.33s remaining: 1.56s 842: learn: 0.0304766 total: 8.34s remaining: 1.55s 843: learn: 0.0304414 total: 8.35s remaining: 1.54s 844: learn: 0.0304290 total: 8.36s remaining: 1.53s 845: learn: 0.0304122 total: 8.37s remaining: 1.52s 846: learn: 0.0303940 total: 8.38s remaining: 1.51s 847: learn: 0.0303723 total: 8.39s remaining: 1.5s 848: learn: 0.0303444 total: 8.41s remaining: 1.5s 849: learn: 0.0303434 total: 8.42s remaining: 1.49s 850: learn: 0.0303226 total: 8.43s remaining: 1.48s 851: learn: 0.0302868 total: 8.44s remaining: 1.47s 852: learn: 0.0302504 total: 8.45s remaining: 1.46s 853: learn: 0.0302288 total: 8.46s remaining: 1.45s 854: learn: 0.0302101 total: 8.47s remaining: 1.44s 855: learn: 0.0301854 total: 8.48s remaining: 1.43s 856: learn: 0.0301781 total: 8.48s remaining: 1.42s 857: learn: 0.0301516 total: 8.49s remaining: 1.41s 858: learn: 0.0301324 total: 8.5s remaining: 1.4s 859: learn: 0.0301163 total: 8.51s remaining: 1.39s 860: learn: 0.0301051 total: 8.52s remaining: 1.38s 861: learn: 0.0301034 total: 8.53s remaining: 1.36s 862: learn: 0.0300683 total: 8.54s remaining: 1.35s 863: learn: 0.0300673 total: 8.55s remaining: 1.34s 864: learn: 0.0300512 total: 8.55s remaining: 1.33s 865: learn: 0.0300372 total: 8.56s remaining: 1.32s 866: learn: 0.0300361 total: 8.57s remaining: 1.31s 867: learn: 0.0300280 total: 8.58s remaining: 1.3s 868: learn: 0.0299977 total: 8.6s remaining: 1.29s 869: learn: 0.0299753 total: 8.6s remaining: 1.28s 870: learn: 0.0299584 total: 8.61s remaining: 1.27s 871: learn: 0.0299288 total: 8.63s remaining: 1.27s 872: learn: 0.0299106 total: 8.64s remaining: 1.26s 873: learn: 0.0299094 total: 8.65s remaining: 1.25s 874: learn: 0.0299083 total: 8.66s remaining: 1.24s 875: learn: 0.0298930 total: 8.67s remaining: 1.23s 876: learn: 0.0298647 total: 8.68s remaining: 1.22s 877: learn: 0.0298419 total: 8.69s remaining: 1.21s 878: learn: 0.0298213 total: 8.69s remaining: 1.2s 879: learn: 0.0298172 total: 8.7s remaining: 1.19s 880: learn: 0.0297975 total: 8.71s remaining: 1.18s 881: learn: 0.0297783 total: 8.72s remaining: 1.17s 882: learn: 0.0297550 total: 8.73s remaining: 1.16s 883: learn: 0.0297237 total: 8.74s remaining: 1.15s 884: learn: 0.0296984 total: 8.75s remaining: 1.14s 885: learn: 0.0296784 total: 8.76s remaining: 1.13s 886: learn: 0.0296517 total: 8.77s remaining: 1.12s 887: learn: 0.0296346 total: 8.78s remaining: 1.11s 888: learn: 0.0296219 total: 8.79s remaining: 1.1s 889: learn: 0.0295933 total: 8.8s remaining: 1.09s 890: learn: 0.0295626 total: 8.81s remaining: 1.08s 891: learn: 0.0295260 total: 8.82s remaining: 1.07s 892: learn: 0.0295246 total: 8.84s remaining: 1.06s 893: learn: 0.0295084 total: 8.85s remaining: 1.05s 894: learn: 0.0294884 total: 8.85s remaining: 1.04s 895: learn: 0.0294783 total: 8.86s remaining: 1.03s 896: learn: 0.0294530 total: 8.87s remaining: 1.02s 897: learn: 0.0294349 total: 8.88s remaining: 1.01s 898: learn: 0.0294097 total: 8.89s remaining: 999ms 899: learn: 0.0294084 total: 8.9s remaining: 989ms 900: learn: 0.0293861 total: 8.91s remaining: 979ms 901: learn: 0.0293628 total: 8.92s remaining: 969ms 902: learn: 0.0293355 total: 8.93s remaining: 959ms 903: learn: 0.0293286 total: 8.94s remaining: 949ms 904: learn: 0.0293070 total: 8.95s remaining: 939ms 905: learn: 0.0292876 total: 8.96s remaining: 929ms 906: learn: 0.0292754 total: 8.97s remaining: 919ms 907: learn: 0.0292623 total: 8.97s remaining: 909ms 908: learn: 0.0292356 total: 8.98s remaining: 899ms 909: learn: 0.0292344 total: 8.99s remaining: 889ms 910: learn: 0.0292129 total: 9s remaining: 879ms 911: learn: 0.0292040 total: 9.01s remaining: 869ms 912: learn: 0.0291759 total: 9.02s remaining: 860ms 913: learn: 0.0291535 total: 9.03s remaining: 850ms 914: learn: 0.0291441 total: 9.05s remaining: 840ms 915: learn: 0.0291043 total: 9.06s remaining: 831ms 916: learn: 0.0290919 total: 9.07s remaining: 821ms 917: learn: 0.0290737 total: 9.07s remaining: 811ms 918: learn: 0.0290643 total: 9.09s remaining: 801ms 919: learn: 0.0290480 total: 9.09s remaining: 791ms 920: learn: 0.0290296 total: 9.1s remaining: 781ms 921: learn: 0.0290107 total: 9.11s remaining: 771ms 922: learn: 0.0289795 total: 9.12s remaining: 761ms 923: learn: 0.0289539 total: 9.13s remaining: 751ms 924: learn: 0.0289391 total: 9.14s remaining: 741ms 925: learn: 0.0289202 total: 9.15s remaining: 731ms 926: learn: 0.0288971 total: 9.16s remaining: 722ms 927: learn: 0.0288720 total: 9.17s remaining: 712ms 928: learn: 0.0288523 total: 9.18s remaining: 702ms 929: learn: 0.0288306 total: 9.19s remaining: 692ms 930: learn: 0.0288296 total: 9.2s remaining: 682ms 931: learn: 0.0288138 total: 9.21s remaining: 672ms 932: learn: 0.0287927 total: 9.22s remaining: 662ms 933: learn: 0.0287759 total: 9.23s remaining: 652ms 934: learn: 0.0287476 total: 9.24s remaining: 642ms 935: learn: 0.0287309 total: 9.25s remaining: 633ms 936: learn: 0.0287120 total: 9.26s remaining: 623ms 937: learn: 0.0286817 total: 9.27s remaining: 613ms 938: learn: 0.0286561 total: 9.28s remaining: 603ms 939: learn: 0.0286323 total: 9.29s remaining: 593ms 940: learn: 0.0286174 total: 9.3s remaining: 583ms 941: learn: 0.0285973 total: 9.31s remaining: 573ms 942: learn: 0.0285789 total: 9.32s remaining: 563ms 943: learn: 0.0285580 total: 9.33s remaining: 553ms 944: learn: 0.0285386 total: 9.34s remaining: 543ms 945: learn: 0.0285331 total: 9.35s remaining: 533ms 946: learn: 0.0285030 total: 9.35s remaining: 524ms 947: learn: 0.0284901 total: 9.36s remaining: 514ms 948: learn: 0.0284824 total: 9.38s remaining: 504ms 949: learn: 0.0284742 total: 9.39s remaining: 494ms 950: learn: 0.0284538 total: 9.4s remaining: 484ms 951: learn: 0.0284522 total: 9.41s remaining: 474ms 952: learn: 0.0284431 total: 9.42s remaining: 465ms 953: learn: 0.0284243 total: 9.43s remaining: 455ms 954: learn: 0.0283994 total: 9.44s remaining: 445ms 955: learn: 0.0283877 total: 9.45s remaining: 435ms 956: learn: 0.0283772 total: 9.47s remaining: 425ms 957: learn: 0.0283450 total: 9.48s remaining: 415ms 958: learn: 0.0283404 total: 9.49s remaining: 406ms 959: learn: 0.0283156 total: 9.5s remaining: 396ms 960: learn: 0.0282978 total: 9.51s remaining: 386ms 961: learn: 0.0282839 total: 9.51s remaining: 376ms 962: learn: 0.0282645 total: 9.52s remaining: 366ms 963: learn: 0.0282399 total: 9.53s remaining: 356ms 964: learn: 0.0282279 total: 9.54s remaining: 346ms 965: learn: 0.0282126 total: 9.55s remaining: 336ms 966: learn: 0.0282045 total: 9.56s remaining: 326ms 967: learn: 0.0281938 total: 9.56s remaining: 316ms 968: learn: 0.0281719 total: 9.57s remaining: 306ms 969: learn: 0.0281577 total: 9.58s remaining: 296ms 970: learn: 0.0281302 total: 9.59s remaining: 286ms 971: learn: 0.0281113 total: 9.6s remaining: 277ms 972: learn: 0.0280976 total: 9.61s remaining: 267ms 973: learn: 0.0280882 total: 9.62s remaining: 257ms 974: learn: 0.0280676 total: 9.63s remaining: 247ms 975: learn: 0.0280485 total: 9.63s remaining: 237ms 976: learn: 0.0280345 total: 9.64s remaining: 227ms 977: learn: 0.0280235 total: 9.65s remaining: 217ms 978: learn: 0.0279917 total: 9.67s remaining: 207ms 979: learn: 0.0279635 total: 9.68s remaining: 198ms 980: learn: 0.0279544 total: 9.69s remaining: 188ms 981: learn: 0.0279402 total: 9.7s remaining: 178ms 982: learn: 0.0279196 total: 9.71s remaining: 168ms 983: learn: 0.0279033 total: 9.72s remaining: 158ms 984: learn: 0.0278850 total: 9.73s remaining: 148ms 985: learn: 0.0278761 total: 9.74s remaining: 138ms 986: learn: 0.0278530 total: 9.75s remaining: 128ms 987: learn: 0.0278280 total: 9.76s remaining: 118ms 988: learn: 0.0277963 total: 9.76s remaining: 109ms 989: learn: 0.0277694 total: 9.77s remaining: 98.7ms 990: learn: 0.0277673 total: 9.78s remaining: 88.8ms 991: learn: 0.0277441 total: 9.79s remaining: 79ms 992: learn: 0.0277343 total: 9.8s remaining: 69.1ms 993: learn: 0.0277147 total: 9.81s remaining: 59.2ms 994: learn: 0.0276958 total: 9.82s remaining: 49.3ms 995: learn: 0.0276649 total: 9.83s remaining: 39.5ms 996: learn: 0.0276333 total: 9.83s remaining: 29.6ms 997: learn: 0.0276167 total: 9.84s remaining: 19.7ms 998: learn: 0.0275896 total: 9.85s remaining: 9.86ms 999: learn: 0.0275745 total: 9.86s remaining: 0us . final_cv_score . Regressors RMSE_mean RMSE_std . 0 Linear_Reg. | 0.139996 | 0.021668 | . 1 Bayesian_Ridge_Reg. | 0.121452 | 0.018995 | . 2 LGBM_Reg. | 0.123800 | 0.013532 | . 3 SVR | 0.269460 | 0.019030 | . 4 Dec_Tree_Reg. | 0.194676 | 0.016841 | . 5 Random_Forest_Reg. | 0.136588 | 0.012856 | . 6 XGB_Reg. | 0.125529 | 0.013459 | . 7 Grad_Boost_Reg. | 0.123467 | 0.011552 | . 8 Cat_Boost_Reg. | 0.113831 | 0.016073 | . 9 Stacked_Reg. | 0.115909 | 0.015257 | . plt.figure(figsize = (12,8)) sns.barplot(final_cv_score[&#39;Regressors&#39;],final_cv_score[&#39;RMSE_mean&#39;]) plt.xlabel(&#39;Regressors&#39;, fontsize = 12) plt.ylabel(&#39;CV_Mean_RMSE&#39;, fontsize = 12) plt.xticks(rotation=90) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . &#44208;&#44284;&#44032; &#44032;&#51109; &#51339;&#51008; 4&#44060;&#51032; &#47784;&#45944;&#51012; &#48660;&#47116;&#46377;&#54620;&#45796;. . Stacked_Reg / Cat_Boost_Reg / Bayesian_Ridge_Reg / Grad_Boost_Reg . X_train,X_val,y_train,y_val = train_test_split(train,target_log,test_size = 0.1,random_state=42) . params = {&#39;iterations&#39;: 6000, &#39;learning_rate&#39;: 0.005, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 1, &#39;eval_metric&#39;:&#39;RMSE&#39;, &#39;early_stopping_rounds&#39;: 300, &#39;verbose&#39;: 200, &#39;random_seed&#39;: 42} cat_f = CatBoostRegressor(**params) cat_model_f = cat_f.fit(X_train,y_train, eval_set = (X_val,y_val), plot=True, verbose = False) catf_pred = cat_model_f.predict(X_val) catf_score = rmse(y_val, catf_pred) . catf_score . 0.09365830716688796 . stack_model = stack_gen.fit(X_train, y_train) brr_model = brr.fit(X_train, y_train) gbr_model = gbr.fit(X_train, y_train) . Learning rate set to 0.046383 0: learn: 0.3911424 total: 8.57ms remaining: 8.56s 1: learn: 0.3789780 total: 17.3ms remaining: 8.64s 2: learn: 0.3668883 total: 25.5ms remaining: 8.47s 3: learn: 0.3560675 total: 33.9ms remaining: 8.45s 4: learn: 0.3460963 total: 42.5ms remaining: 8.45s 5: learn: 0.3363543 total: 50.8ms remaining: 8.41s 6: learn: 0.3267382 total: 59.2ms remaining: 8.39s 7: learn: 0.3176820 total: 68.9ms remaining: 8.54s 8: learn: 0.3089405 total: 77.4ms remaining: 8.52s 9: learn: 0.3004778 total: 85.3ms remaining: 8.45s 10: learn: 0.2930001 total: 93.3ms remaining: 8.39s 11: learn: 0.2850753 total: 101ms remaining: 8.33s 12: learn: 0.2781380 total: 110ms remaining: 8.32s 13: learn: 0.2707586 total: 118ms remaining: 8.31s 14: learn: 0.2640472 total: 127ms remaining: 8.31s 15: learn: 0.2577807 total: 135ms remaining: 8.3s 16: learn: 0.2513260 total: 143ms remaining: 8.29s 17: learn: 0.2456723 total: 152ms remaining: 8.29s 18: learn: 0.2401821 total: 160ms remaining: 8.28s 19: learn: 0.2347673 total: 169ms remaining: 8.27s 20: learn: 0.2293512 total: 177ms remaining: 8.24s 21: learn: 0.2241978 total: 185ms remaining: 8.21s 22: learn: 0.2195587 total: 200ms remaining: 8.5s 23: learn: 0.2154664 total: 208ms remaining: 8.47s 24: learn: 0.2110710 total: 216ms remaining: 8.43s 25: learn: 0.2071459 total: 225ms remaining: 8.41s 26: learn: 0.2030718 total: 233ms remaining: 8.38s 27: learn: 0.1993045 total: 241ms remaining: 8.36s 28: learn: 0.1958238 total: 249ms remaining: 8.35s 29: learn: 0.1927383 total: 260ms remaining: 8.39s 30: learn: 0.1894540 total: 271ms remaining: 8.48s 31: learn: 0.1864103 total: 280ms remaining: 8.48s 32: learn: 0.1836215 total: 289ms remaining: 8.48s 33: learn: 0.1807823 total: 297ms remaining: 8.45s 34: learn: 0.1779393 total: 305ms remaining: 8.41s 35: learn: 0.1749431 total: 313ms remaining: 8.38s 36: learn: 0.1725136 total: 321ms remaining: 8.35s 37: learn: 0.1700700 total: 329ms remaining: 8.33s 38: learn: 0.1678114 total: 337ms remaining: 8.3s 39: learn: 0.1655213 total: 344ms remaining: 8.26s 40: learn: 0.1636029 total: 353ms remaining: 8.24s 41: learn: 0.1617440 total: 360ms remaining: 8.22s 42: learn: 0.1599672 total: 369ms remaining: 8.2s 43: learn: 0.1583790 total: 377ms remaining: 8.19s 44: learn: 0.1566574 total: 392ms remaining: 8.32s 45: learn: 0.1551301 total: 411ms remaining: 8.53s 46: learn: 0.1535430 total: 419ms remaining: 8.5s 47: learn: 0.1520700 total: 427ms remaining: 8.47s 48: learn: 0.1505501 total: 435ms remaining: 8.43s 49: learn: 0.1490726 total: 442ms remaining: 8.4s 50: learn: 0.1476832 total: 450ms remaining: 8.37s 51: learn: 0.1465878 total: 457ms remaining: 8.34s 52: learn: 0.1453717 total: 465ms remaining: 8.31s 53: learn: 0.1441590 total: 473ms remaining: 8.29s 54: learn: 0.1431420 total: 481ms remaining: 8.26s 55: learn: 0.1422000 total: 488ms remaining: 8.23s 56: learn: 0.1412166 total: 496ms remaining: 8.2s 57: learn: 0.1401542 total: 503ms remaining: 8.17s 58: learn: 0.1391272 total: 511ms remaining: 8.14s 59: learn: 0.1380901 total: 518ms remaining: 8.12s 60: learn: 0.1371980 total: 526ms remaining: 8.09s 61: learn: 0.1362527 total: 536ms remaining: 8.11s 62: learn: 0.1353885 total: 544ms remaining: 8.09s 63: learn: 0.1344294 total: 552ms remaining: 8.07s 64: learn: 0.1335231 total: 559ms remaining: 8.04s 65: learn: 0.1327455 total: 566ms remaining: 8.02s 66: learn: 0.1320318 total: 574ms remaining: 7.99s 67: learn: 0.1313153 total: 582ms remaining: 7.97s 68: learn: 0.1305656 total: 598ms remaining: 8.06s 69: learn: 0.1298839 total: 605ms remaining: 8.04s 70: learn: 0.1291554 total: 613ms remaining: 8.02s 71: learn: 0.1285792 total: 621ms remaining: 8s 72: learn: 0.1282338 total: 628ms remaining: 7.97s 73: learn: 0.1276747 total: 636ms remaining: 7.96s 74: learn: 0.1271199 total: 644ms remaining: 7.94s 75: learn: 0.1265812 total: 652ms remaining: 7.92s 76: learn: 0.1260533 total: 660ms remaining: 7.91s 77: learn: 0.1253962 total: 668ms remaining: 7.89s 78: learn: 0.1247700 total: 675ms remaining: 7.87s 79: learn: 0.1242496 total: 683ms remaining: 7.86s 80: learn: 0.1241412 total: 691ms remaining: 7.84s 81: learn: 0.1236994 total: 699ms remaining: 7.82s 82: learn: 0.1231481 total: 706ms remaining: 7.8s 83: learn: 0.1230618 total: 711ms remaining: 7.76s 84: learn: 0.1226768 total: 719ms remaining: 7.74s 85: learn: 0.1221020 total: 727ms remaining: 7.72s 86: learn: 0.1216149 total: 734ms remaining: 7.71s 87: learn: 0.1211258 total: 742ms remaining: 7.69s 88: learn: 0.1207412 total: 751ms remaining: 7.68s 89: learn: 0.1202231 total: 758ms remaining: 7.67s 90: learn: 0.1197963 total: 767ms remaining: 7.66s 91: learn: 0.1197367 total: 774ms remaining: 7.64s 92: learn: 0.1192919 total: 782ms remaining: 7.63s 93: learn: 0.1191127 total: 791ms remaining: 7.62s 94: learn: 0.1186803 total: 804ms remaining: 7.66s 95: learn: 0.1186073 total: 812ms remaining: 7.64s 96: learn: 0.1181470 total: 820ms remaining: 7.63s 97: learn: 0.1177885 total: 835ms remaining: 7.68s 98: learn: 0.1174917 total: 846ms remaining: 7.7s 99: learn: 0.1171952 total: 855ms remaining: 7.69s 100: learn: 0.1168912 total: 862ms remaining: 7.68s 101: learn: 0.1165477 total: 870ms remaining: 7.66s 102: learn: 0.1161863 total: 878ms remaining: 7.64s 103: learn: 0.1159084 total: 885ms remaining: 7.63s 104: learn: 0.1156417 total: 893ms remaining: 7.61s 105: learn: 0.1155701 total: 901ms remaining: 7.6s 106: learn: 0.1154546 total: 909ms remaining: 7.59s 107: learn: 0.1151700 total: 918ms remaining: 7.58s 108: learn: 0.1148680 total: 926ms remaining: 7.57s 109: learn: 0.1144904 total: 934ms remaining: 7.56s 110: learn: 0.1142227 total: 942ms remaining: 7.55s 111: learn: 0.1140027 total: 950ms remaining: 7.53s 112: learn: 0.1139306 total: 958ms remaining: 7.52s 113: learn: 0.1135802 total: 966ms remaining: 7.5s 114: learn: 0.1135166 total: 974ms remaining: 7.5s 115: learn: 0.1132142 total: 982ms remaining: 7.48s 116: learn: 0.1128913 total: 995ms remaining: 7.51s 117: learn: 0.1126158 total: 1s remaining: 7.5s 118: learn: 0.1124421 total: 1.01s remaining: 7.48s 119: learn: 0.1122027 total: 1.02s remaining: 7.48s 120: learn: 0.1120259 total: 1.03s remaining: 7.46s 121: learn: 0.1119851 total: 1.03s remaining: 7.45s 122: learn: 0.1116569 total: 1.04s remaining: 7.44s 123: learn: 0.1114312 total: 1.05s remaining: 7.43s 124: learn: 0.1112538 total: 1.06s remaining: 7.41s 125: learn: 0.1109939 total: 1.07s remaining: 7.4s 126: learn: 0.1107191 total: 1.07s remaining: 7.39s 127: learn: 0.1105124 total: 1.08s remaining: 7.38s 128: learn: 0.1102151 total: 1.1s remaining: 7.42s 129: learn: 0.1100145 total: 1.12s remaining: 7.53s 130: learn: 0.1097370 total: 1.13s remaining: 7.51s 131: learn: 0.1096936 total: 1.14s remaining: 7.5s 132: learn: 0.1094479 total: 1.15s remaining: 7.48s 133: learn: 0.1091653 total: 1.16s remaining: 7.47s 134: learn: 0.1090114 total: 1.16s remaining: 7.45s 135: learn: 0.1087640 total: 1.17s remaining: 7.44s 136: learn: 0.1085065 total: 1.18s remaining: 7.42s 137: learn: 0.1083016 total: 1.19s remaining: 7.41s 138: learn: 0.1081062 total: 1.2s remaining: 7.44s 139: learn: 0.1078605 total: 1.21s remaining: 7.43s 140: learn: 0.1076221 total: 1.22s remaining: 7.42s 141: learn: 0.1073233 total: 1.23s remaining: 7.4s 142: learn: 0.1070728 total: 1.23s remaining: 7.39s 143: learn: 0.1068746 total: 1.24s remaining: 7.38s 144: learn: 0.1067266 total: 1.25s remaining: 7.36s 145: learn: 0.1066121 total: 1.26s remaining: 7.35s 146: learn: 0.1063647 total: 1.26s remaining: 7.33s 147: learn: 0.1062873 total: 1.27s remaining: 7.32s 148: learn: 0.1060880 total: 1.28s remaining: 7.3s 149: learn: 0.1060498 total: 1.29s remaining: 7.29s 150: learn: 0.1058693 total: 1.29s remaining: 7.28s 151: learn: 0.1057059 total: 1.3s remaining: 7.26s 152: learn: 0.1055069 total: 1.31s remaining: 7.25s 153: learn: 0.1053451 total: 1.32s remaining: 7.24s 154: learn: 0.1052685 total: 1.32s remaining: 7.22s 155: learn: 0.1052239 total: 1.33s remaining: 7.21s 156: learn: 0.1051280 total: 1.34s remaining: 7.2s 157: learn: 0.1050771 total: 1.35s remaining: 7.2s 158: learn: 0.1048848 total: 1.37s remaining: 7.24s 159: learn: 0.1046519 total: 1.38s remaining: 7.22s 160: learn: 0.1044603 total: 1.38s remaining: 7.21s 161: learn: 0.1043261 total: 1.4s remaining: 7.27s 162: learn: 0.1041798 total: 1.41s remaining: 7.25s 163: learn: 0.1039911 total: 1.42s remaining: 7.24s 164: learn: 0.1037967 total: 1.43s remaining: 7.23s 165: learn: 0.1036502 total: 1.44s remaining: 7.22s 166: learn: 0.1034516 total: 1.45s remaining: 7.21s 167: learn: 0.1034046 total: 1.45s remaining: 7.2s 168: learn: 0.1033498 total: 1.46s remaining: 7.18s 169: learn: 0.1031730 total: 1.47s remaining: 7.17s 170: learn: 0.1030677 total: 1.48s remaining: 7.16s 171: learn: 0.1028666 total: 1.49s remaining: 7.16s 172: learn: 0.1027981 total: 1.49s remaining: 7.15s 173: learn: 0.1027508 total: 1.5s remaining: 7.13s 174: learn: 0.1025872 total: 1.51s remaining: 7.12s 175: learn: 0.1025503 total: 1.52s remaining: 7.1s 176: learn: 0.1024776 total: 1.52s remaining: 7.09s 177: learn: 0.1024031 total: 1.53s remaining: 7.08s 178: learn: 0.1021970 total: 1.54s remaining: 7.07s 179: learn: 0.1020236 total: 1.55s remaining: 7.05s 180: learn: 0.1019751 total: 1.56s remaining: 7.04s 181: learn: 0.1017789 total: 1.56s remaining: 7.03s 182: learn: 0.1015870 total: 1.57s remaining: 7.02s 183: learn: 0.1014476 total: 1.58s remaining: 7s 184: learn: 0.1012806 total: 1.59s remaining: 6.99s 185: learn: 0.1012211 total: 1.59s remaining: 6.98s 186: learn: 0.1010453 total: 1.61s remaining: 6.99s 187: learn: 0.1009936 total: 1.61s remaining: 6.97s 188: learn: 0.1008503 total: 1.62s remaining: 6.96s 189: learn: 0.1007077 total: 1.63s remaining: 6.95s 190: learn: 0.1006662 total: 1.64s remaining: 6.94s 191: learn: 0.1006039 total: 1.65s remaining: 6.92s 192: learn: 0.1005607 total: 1.65s remaining: 6.91s 193: learn: 0.1005267 total: 1.66s remaining: 6.9s 194: learn: 0.1004930 total: 1.67s remaining: 6.88s 195: learn: 0.1004603 total: 1.68s remaining: 6.87s 196: learn: 0.1002612 total: 1.68s remaining: 6.86s 197: learn: 0.1000447 total: 1.69s remaining: 6.85s 198: learn: 0.1000063 total: 1.7s remaining: 6.84s 199: learn: 0.0998234 total: 1.71s remaining: 6.83s 200: learn: 0.0996317 total: 1.71s remaining: 6.82s 201: learn: 0.0994343 total: 1.72s remaining: 6.8s 202: learn: 0.0992866 total: 1.73s remaining: 6.79s 203: learn: 0.0992612 total: 1.74s remaining: 6.78s 204: learn: 0.0990200 total: 1.75s remaining: 6.77s 205: learn: 0.0988602 total: 1.75s remaining: 6.75s 206: learn: 0.0986944 total: 1.76s remaining: 6.74s 207: learn: 0.0985303 total: 1.77s remaining: 6.73s 208: learn: 0.0984912 total: 1.77s remaining: 6.72s 209: learn: 0.0984003 total: 1.78s remaining: 6.71s 210: learn: 0.0982359 total: 1.79s remaining: 6.71s 211: learn: 0.0980860 total: 1.8s remaining: 6.7s 212: learn: 0.0979656 total: 1.81s remaining: 6.69s 213: learn: 0.0979334 total: 1.82s remaining: 6.68s 214: learn: 0.0978759 total: 1.83s remaining: 6.67s 215: learn: 0.0978452 total: 1.83s remaining: 6.66s 216: learn: 0.0976555 total: 1.84s remaining: 6.65s 217: learn: 0.0976243 total: 1.85s remaining: 6.63s 218: learn: 0.0973772 total: 1.86s remaining: 6.62s 219: learn: 0.0973478 total: 1.87s remaining: 6.63s 220: learn: 0.0972044 total: 1.88s remaining: 6.62s 221: learn: 0.0969665 total: 1.88s remaining: 6.6s 222: learn: 0.0968380 total: 1.89s remaining: 6.59s 223: learn: 0.0968094 total: 1.9s remaining: 6.58s 224: learn: 0.0966372 total: 1.91s remaining: 6.57s 225: learn: 0.0964595 total: 1.92s remaining: 6.56s 226: learn: 0.0963216 total: 1.92s remaining: 6.55s 227: learn: 0.0962000 total: 1.93s remaining: 6.54s 228: learn: 0.0960682 total: 1.94s remaining: 6.53s 229: learn: 0.0958673 total: 1.95s remaining: 6.52s 230: learn: 0.0956008 total: 1.96s remaining: 6.51s 231: learn: 0.0955743 total: 1.96s remaining: 6.5s 232: learn: 0.0953976 total: 1.97s remaining: 6.5s 233: learn: 0.0952322 total: 1.98s remaining: 6.49s 234: learn: 0.0951085 total: 2s remaining: 6.5s 235: learn: 0.0949535 total: 2s remaining: 6.49s 236: learn: 0.0948166 total: 2.02s remaining: 6.49s 237: learn: 0.0947205 total: 2.02s remaining: 6.47s 238: learn: 0.0945065 total: 2.03s remaining: 6.46s 239: learn: 0.0944822 total: 2.04s remaining: 6.45s 240: learn: 0.0943380 total: 2.04s remaining: 6.44s 241: learn: 0.0941996 total: 2.05s remaining: 6.43s 242: learn: 0.0940996 total: 2.06s remaining: 6.42s 243: learn: 0.0939671 total: 2.07s remaining: 6.41s 244: learn: 0.0938113 total: 2.08s remaining: 6.4s 245: learn: 0.0937868 total: 2.08s remaining: 6.39s 246: learn: 0.0936989 total: 2.09s remaining: 6.38s 247: learn: 0.0935185 total: 2.1s remaining: 6.37s 248: learn: 0.0934948 total: 2.11s remaining: 6.36s 249: learn: 0.0934119 total: 2.12s remaining: 6.35s 250: learn: 0.0932717 total: 2.12s remaining: 6.34s 251: learn: 0.0931098 total: 2.13s remaining: 6.33s 252: learn: 0.0930109 total: 2.14s remaining: 6.32s 253: learn: 0.0928896 total: 2.15s remaining: 6.31s 254: learn: 0.0927241 total: 2.16s remaining: 6.3s 255: learn: 0.0926608 total: 2.16s remaining: 6.29s 256: learn: 0.0925571 total: 2.17s remaining: 6.28s 257: learn: 0.0923745 total: 2.18s remaining: 6.27s 258: learn: 0.0922165 total: 2.2s remaining: 6.3s 259: learn: 0.0920951 total: 2.21s remaining: 6.29s 260: learn: 0.0920721 total: 2.22s remaining: 6.28s 261: learn: 0.0919674 total: 2.23s remaining: 6.27s 262: learn: 0.0918031 total: 2.23s remaining: 6.26s 263: learn: 0.0916832 total: 2.24s remaining: 6.25s 264: learn: 0.0916257 total: 2.25s remaining: 6.25s 265: learn: 0.0915294 total: 2.27s remaining: 6.25s 266: learn: 0.0913684 total: 2.27s remaining: 6.25s 267: learn: 0.0912614 total: 2.29s remaining: 6.24s 268: learn: 0.0911361 total: 2.3s remaining: 6.25s 269: learn: 0.0909927 total: 2.31s remaining: 6.24s 270: learn: 0.0908703 total: 2.31s remaining: 6.23s 271: learn: 0.0907384 total: 2.33s remaining: 6.23s 272: learn: 0.0906102 total: 2.34s remaining: 6.23s 273: learn: 0.0905692 total: 2.35s remaining: 6.22s 274: learn: 0.0904282 total: 2.36s remaining: 6.21s 275: learn: 0.0903806 total: 2.36s remaining: 6.2s 276: learn: 0.0902480 total: 2.37s remaining: 6.19s 277: learn: 0.0902112 total: 2.38s remaining: 6.18s 278: learn: 0.0901088 total: 2.39s remaining: 6.17s 279: learn: 0.0899773 total: 2.4s remaining: 6.18s 280: learn: 0.0898763 total: 2.41s remaining: 6.17s 281: learn: 0.0897641 total: 2.42s remaining: 6.16s 282: learn: 0.0896333 total: 2.43s remaining: 6.15s 283: learn: 0.0894987 total: 2.44s remaining: 6.14s 284: learn: 0.0894764 total: 2.44s remaining: 6.13s 285: learn: 0.0893754 total: 2.45s remaining: 6.12s 286: learn: 0.0892388 total: 2.46s remaining: 6.11s 287: learn: 0.0891737 total: 2.47s remaining: 6.1s 288: learn: 0.0890885 total: 2.48s remaining: 6.09s 289: learn: 0.0889858 total: 2.48s remaining: 6.08s 290: learn: 0.0888599 total: 2.49s remaining: 6.07s 291: learn: 0.0887329 total: 2.5s remaining: 6.06s 292: learn: 0.0886007 total: 2.51s remaining: 6.05s 293: learn: 0.0883750 total: 2.52s remaining: 6.04s 294: learn: 0.0883553 total: 2.52s remaining: 6.03s 295: learn: 0.0882631 total: 2.53s remaining: 6.02s 296: learn: 0.0881118 total: 2.54s remaining: 6.01s 297: learn: 0.0880914 total: 2.55s remaining: 6.01s 298: learn: 0.0879041 total: 2.56s remaining: 6s 299: learn: 0.0877822 total: 2.57s remaining: 5.99s 300: learn: 0.0877034 total: 2.58s remaining: 5.98s 301: learn: 0.0875910 total: 2.58s remaining: 5.97s 302: learn: 0.0874061 total: 2.59s remaining: 5.96s 303: learn: 0.0872661 total: 2.61s remaining: 5.97s 304: learn: 0.0871097 total: 2.61s remaining: 5.96s 305: learn: 0.0869755 total: 2.62s remaining: 5.95s 306: learn: 0.0868777 total: 2.63s remaining: 5.94s 307: learn: 0.0867585 total: 2.64s remaining: 5.93s 308: learn: 0.0866544 total: 2.65s remaining: 5.92s 309: learn: 0.0865470 total: 2.65s remaining: 5.91s 310: learn: 0.0864602 total: 2.66s remaining: 5.9s 311: learn: 0.0863424 total: 2.67s remaining: 5.89s 312: learn: 0.0862895 total: 2.68s remaining: 5.88s 313: learn: 0.0861898 total: 2.69s remaining: 5.87s 314: learn: 0.0860575 total: 2.69s remaining: 5.86s 315: learn: 0.0859680 total: 2.7s remaining: 5.84s 316: learn: 0.0858284 total: 2.71s remaining: 5.83s 317: learn: 0.0856979 total: 2.72s remaining: 5.83s 318: learn: 0.0856066 total: 2.72s remaining: 5.81s 319: learn: 0.0855443 total: 2.73s remaining: 5.8s 320: learn: 0.0854204 total: 2.74s remaining: 5.8s 321: learn: 0.0852750 total: 2.75s remaining: 5.79s 322: learn: 0.0851621 total: 2.76s remaining: 5.78s 323: learn: 0.0851032 total: 2.76s remaining: 5.77s 324: learn: 0.0850885 total: 2.77s remaining: 5.76s 325: learn: 0.0850092 total: 2.78s remaining: 5.75s 326: learn: 0.0849097 total: 2.79s remaining: 5.74s 327: learn: 0.0848163 total: 2.79s remaining: 5.73s 328: learn: 0.0847292 total: 2.81s remaining: 5.72s 329: learn: 0.0846672 total: 2.82s remaining: 5.73s 330: learn: 0.0845833 total: 2.83s remaining: 5.72s 331: learn: 0.0844543 total: 2.84s remaining: 5.72s 332: learn: 0.0843383 total: 2.85s remaining: 5.71s 333: learn: 0.0842404 total: 2.86s remaining: 5.7s 334: learn: 0.0841284 total: 2.87s remaining: 5.69s 335: learn: 0.0840694 total: 2.87s remaining: 5.68s 336: learn: 0.0839464 total: 2.88s remaining: 5.67s 337: learn: 0.0838572 total: 2.89s remaining: 5.66s 338: learn: 0.0837569 total: 2.9s remaining: 5.65s 339: learn: 0.0836839 total: 2.9s remaining: 5.64s 340: learn: 0.0836179 total: 2.91s remaining: 5.63s 341: learn: 0.0835287 total: 2.92s remaining: 5.62s 342: learn: 0.0835131 total: 2.93s remaining: 5.61s 343: learn: 0.0834140 total: 2.94s remaining: 5.6s 344: learn: 0.0833001 total: 2.94s remaining: 5.59s 345: learn: 0.0832837 total: 2.95s remaining: 5.58s 346: learn: 0.0831582 total: 2.96s remaining: 5.57s 347: learn: 0.0830761 total: 2.97s remaining: 5.56s 348: learn: 0.0829865 total: 2.98s remaining: 5.55s 349: learn: 0.0828660 total: 2.98s remaining: 5.54s 350: learn: 0.0827545 total: 2.99s remaining: 5.53s 351: learn: 0.0826559 total: 3s remaining: 5.53s 352: learn: 0.0825626 total: 3.02s remaining: 5.53s 353: learn: 0.0825141 total: 3.02s remaining: 5.52s 354: learn: 0.0824148 total: 3.03s remaining: 5.51s 355: learn: 0.0822966 total: 3.04s remaining: 5.5s 356: learn: 0.0822813 total: 3.05s remaining: 5.49s 357: learn: 0.0821796 total: 3.06s remaining: 5.48s 358: learn: 0.0820896 total: 3.06s remaining: 5.47s 359: learn: 0.0820009 total: 3.07s remaining: 5.46s 360: learn: 0.0819862 total: 3.08s remaining: 5.45s 361: learn: 0.0819006 total: 3.09s remaining: 5.45s 362: learn: 0.0818239 total: 3.1s remaining: 5.43s 363: learn: 0.0817465 total: 3.11s remaining: 5.43s 364: learn: 0.0816115 total: 3.11s remaining: 5.42s 365: learn: 0.0814818 total: 3.12s remaining: 5.41s 366: learn: 0.0814346 total: 3.13s remaining: 5.4s 367: learn: 0.0813530 total: 3.14s remaining: 5.39s 368: learn: 0.0812315 total: 3.15s remaining: 5.38s 369: learn: 0.0811449 total: 3.15s remaining: 5.37s 370: learn: 0.0810597 total: 3.17s remaining: 5.37s 371: learn: 0.0810474 total: 3.17s remaining: 5.36s 372: learn: 0.0809151 total: 3.18s remaining: 5.35s 373: learn: 0.0808107 total: 3.19s remaining: 5.34s 374: learn: 0.0807138 total: 3.2s remaining: 5.33s 375: learn: 0.0806299 total: 3.21s remaining: 5.33s 376: learn: 0.0805206 total: 3.22s remaining: 5.32s 377: learn: 0.0804298 total: 3.23s remaining: 5.31s 378: learn: 0.0803516 total: 3.23s remaining: 5.3s 379: learn: 0.0803402 total: 3.24s remaining: 5.29s 380: learn: 0.0802594 total: 3.25s remaining: 5.28s 381: learn: 0.0801761 total: 3.26s remaining: 5.27s 382: learn: 0.0800882 total: 3.26s remaining: 5.26s 383: learn: 0.0799903 total: 3.27s remaining: 5.25s 384: learn: 0.0799801 total: 3.28s remaining: 5.24s 385: learn: 0.0799259 total: 3.29s remaining: 5.23s 386: learn: 0.0798579 total: 3.31s remaining: 5.24s 387: learn: 0.0797746 total: 3.31s remaining: 5.23s 388: learn: 0.0796918 total: 3.32s remaining: 5.22s 389: learn: 0.0795868 total: 3.33s remaining: 5.21s 390: learn: 0.0795230 total: 3.34s remaining: 5.2s 391: learn: 0.0794363 total: 3.35s remaining: 5.19s 392: learn: 0.0793635 total: 3.35s remaining: 5.18s 393: learn: 0.0792299 total: 3.36s remaining: 5.17s 394: learn: 0.0791265 total: 3.37s remaining: 5.16s 395: learn: 0.0790388 total: 3.38s remaining: 5.15s 396: learn: 0.0789163 total: 3.38s remaining: 5.14s 397: learn: 0.0788298 total: 3.39s remaining: 5.13s 398: learn: 0.0787241 total: 3.4s remaining: 5.12s 399: learn: 0.0786307 total: 3.42s remaining: 5.13s 400: learn: 0.0786036 total: 3.43s remaining: 5.12s 401: learn: 0.0785274 total: 3.44s remaining: 5.11s 402: learn: 0.0784077 total: 3.44s remaining: 5.1s 403: learn: 0.0783220 total: 3.45s remaining: 5.09s 404: learn: 0.0782770 total: 3.46s remaining: 5.08s 405: learn: 0.0782581 total: 3.47s remaining: 5.07s 406: learn: 0.0781870 total: 3.48s remaining: 5.06s 407: learn: 0.0780570 total: 3.48s remaining: 5.05s 408: learn: 0.0779826 total: 3.49s remaining: 5.04s 409: learn: 0.0779148 total: 3.5s remaining: 5.04s 410: learn: 0.0778289 total: 3.51s remaining: 5.03s 411: learn: 0.0777579 total: 3.51s remaining: 5.02s 412: learn: 0.0776743 total: 3.52s remaining: 5.01s 413: learn: 0.0775926 total: 3.53s remaining: 5s 414: learn: 0.0775354 total: 3.54s remaining: 4.99s 415: learn: 0.0774412 total: 3.54s remaining: 4.98s 416: learn: 0.0773540 total: 3.55s remaining: 4.97s 417: learn: 0.0772857 total: 3.56s remaining: 4.96s 418: learn: 0.0772077 total: 3.57s remaining: 4.96s 419: learn: 0.0771977 total: 3.58s remaining: 4.94s 420: learn: 0.0771231 total: 3.59s remaining: 4.93s 421: learn: 0.0770573 total: 3.6s remaining: 4.92s 422: learn: 0.0769809 total: 3.6s remaining: 4.92s 423: learn: 0.0768941 total: 3.61s remaining: 4.91s 424: learn: 0.0767930 total: 3.63s remaining: 4.91s 425: learn: 0.0766981 total: 3.63s remaining: 4.9s 426: learn: 0.0766097 total: 3.64s remaining: 4.89s 427: learn: 0.0765385 total: 3.65s remaining: 4.88s 428: learn: 0.0764397 total: 3.66s remaining: 4.87s 429: learn: 0.0763393 total: 3.67s remaining: 4.86s 430: learn: 0.0762611 total: 3.67s remaining: 4.85s 431: learn: 0.0761698 total: 3.68s remaining: 4.84s 432: learn: 0.0761508 total: 3.69s remaining: 4.83s 433: learn: 0.0760883 total: 3.7s remaining: 4.82s 434: learn: 0.0760137 total: 3.71s remaining: 4.81s 435: learn: 0.0759477 total: 3.71s remaining: 4.8s 436: learn: 0.0758675 total: 3.72s remaining: 4.79s 437: learn: 0.0757682 total: 3.73s remaining: 4.79s 438: learn: 0.0756888 total: 3.74s remaining: 4.78s 439: learn: 0.0756557 total: 3.75s remaining: 4.77s 440: learn: 0.0755617 total: 3.75s remaining: 4.76s 441: learn: 0.0754882 total: 3.76s remaining: 4.75s 442: learn: 0.0754307 total: 3.77s remaining: 4.74s 443: learn: 0.0753091 total: 3.78s remaining: 4.73s 444: learn: 0.0752929 total: 3.78s remaining: 4.72s 445: learn: 0.0752297 total: 3.79s remaining: 4.71s 446: learn: 0.0751348 total: 3.8s remaining: 4.7s 447: learn: 0.0750430 total: 3.81s remaining: 4.69s 448: learn: 0.0749688 total: 3.81s remaining: 4.68s 449: learn: 0.0748882 total: 3.83s remaining: 4.68s 450: learn: 0.0748312 total: 3.84s remaining: 4.67s 451: learn: 0.0747380 total: 3.85s remaining: 4.66s 452: learn: 0.0746639 total: 3.85s remaining: 4.65s 453: learn: 0.0745782 total: 3.86s remaining: 4.64s 454: learn: 0.0745075 total: 3.87s remaining: 4.63s 455: learn: 0.0744469 total: 3.88s remaining: 4.63s 456: learn: 0.0743456 total: 3.89s remaining: 4.62s 457: learn: 0.0742561 total: 3.9s remaining: 4.62s 458: learn: 0.0741808 total: 3.91s remaining: 4.61s 459: learn: 0.0740813 total: 3.92s remaining: 4.6s 460: learn: 0.0739954 total: 3.92s remaining: 4.59s 461: learn: 0.0739781 total: 3.93s remaining: 4.58s 462: learn: 0.0739037 total: 3.94s remaining: 4.57s 463: learn: 0.0738871 total: 3.95s remaining: 4.56s 464: learn: 0.0737678 total: 3.96s remaining: 4.55s 465: learn: 0.0737555 total: 3.96s remaining: 4.54s 466: learn: 0.0736743 total: 3.97s remaining: 4.53s 467: learn: 0.0736636 total: 3.98s remaining: 4.52s 468: learn: 0.0735881 total: 3.99s remaining: 4.52s 469: learn: 0.0735789 total: 4s remaining: 4.51s 470: learn: 0.0734974 total: 4.01s remaining: 4.5s 471: learn: 0.0734022 total: 4.01s remaining: 4.49s 472: learn: 0.0733945 total: 4.02s remaining: 4.48s 473: learn: 0.0733161 total: 4.04s remaining: 4.48s 474: learn: 0.0732420 total: 4.05s remaining: 4.48s 475: learn: 0.0731839 total: 4.06s remaining: 4.47s 476: learn: 0.0731000 total: 4.07s remaining: 4.46s 477: learn: 0.0730478 total: 4.08s remaining: 4.45s 478: learn: 0.0729789 total: 4.08s remaining: 4.44s 479: learn: 0.0729077 total: 4.09s remaining: 4.43s 480: learn: 0.0728243 total: 4.1s remaining: 4.43s 481: learn: 0.0727326 total: 4.11s remaining: 4.42s 482: learn: 0.0726492 total: 4.12s remaining: 4.41s 483: learn: 0.0725523 total: 4.13s remaining: 4.4s 484: learn: 0.0724604 total: 4.13s remaining: 4.39s 485: learn: 0.0723841 total: 4.14s remaining: 4.38s 486: learn: 0.0723190 total: 4.15s remaining: 4.37s 487: learn: 0.0722152 total: 4.16s remaining: 4.36s 488: learn: 0.0721321 total: 4.17s remaining: 4.36s 489: learn: 0.0720643 total: 4.18s remaining: 4.35s 490: learn: 0.0720138 total: 4.18s remaining: 4.34s 491: learn: 0.0719710 total: 4.19s remaining: 4.33s 492: learn: 0.0719600 total: 4.2s remaining: 4.32s 493: learn: 0.0719375 total: 4.21s remaining: 4.31s 494: learn: 0.0718495 total: 4.22s remaining: 4.3s 495: learn: 0.0717855 total: 4.23s remaining: 4.3s 496: learn: 0.0716850 total: 4.24s remaining: 4.29s 497: learn: 0.0716747 total: 4.25s remaining: 4.28s 498: learn: 0.0716135 total: 4.25s remaining: 4.27s 499: learn: 0.0716050 total: 4.26s remaining: 4.26s 500: learn: 0.0715408 total: 4.28s remaining: 4.26s 501: learn: 0.0714668 total: 4.29s remaining: 4.25s 502: learn: 0.0714239 total: 4.29s remaining: 4.24s 503: learn: 0.0713764 total: 4.3s remaining: 4.23s 504: learn: 0.0712943 total: 4.31s remaining: 4.22s 505: learn: 0.0712501 total: 4.32s remaining: 4.21s 506: learn: 0.0712157 total: 4.33s remaining: 4.21s 507: learn: 0.0711401 total: 4.33s remaining: 4.2s 508: learn: 0.0710731 total: 4.34s remaining: 4.19s 509: learn: 0.0710460 total: 4.35s remaining: 4.18s 510: learn: 0.0709958 total: 4.36s remaining: 4.17s 511: learn: 0.0709116 total: 4.37s remaining: 4.16s 512: learn: 0.0708587 total: 4.37s remaining: 4.15s 513: learn: 0.0708120 total: 4.38s remaining: 4.14s 514: learn: 0.0707488 total: 4.39s remaining: 4.13s 515: learn: 0.0706533 total: 4.4s remaining: 4.13s 516: learn: 0.0705992 total: 4.41s remaining: 4.12s 517: learn: 0.0705213 total: 4.42s remaining: 4.11s 518: learn: 0.0704302 total: 4.43s remaining: 4.1s 519: learn: 0.0703751 total: 4.44s remaining: 4.1s 520: learn: 0.0702776 total: 4.45s remaining: 4.09s 521: learn: 0.0702270 total: 4.45s remaining: 4.08s 522: learn: 0.0701355 total: 4.46s remaining: 4.07s 523: learn: 0.0700650 total: 4.47s remaining: 4.06s 524: learn: 0.0700007 total: 4.48s remaining: 4.05s 525: learn: 0.0699458 total: 4.48s remaining: 4.04s 526: learn: 0.0698797 total: 4.49s remaining: 4.03s 527: learn: 0.0698269 total: 4.5s remaining: 4.02s 528: learn: 0.0697627 total: 4.51s remaining: 4.01s 529: learn: 0.0697029 total: 4.52s remaining: 4s 530: learn: 0.0696430 total: 4.52s remaining: 4s 531: learn: 0.0695897 total: 4.53s remaining: 3.99s 532: learn: 0.0695259 total: 4.54s remaining: 3.98s 533: learn: 0.0694702 total: 4.55s remaining: 3.97s 534: learn: 0.0694240 total: 4.55s remaining: 3.96s 535: learn: 0.0693556 total: 4.56s remaining: 3.95s 536: learn: 0.0692651 total: 4.57s remaining: 3.94s 537: learn: 0.0692031 total: 4.58s remaining: 3.93s 538: learn: 0.0691283 total: 4.59s remaining: 3.92s 539: learn: 0.0690678 total: 4.59s remaining: 3.91s 540: learn: 0.0690247 total: 4.6s remaining: 3.9s 541: learn: 0.0689872 total: 4.61s remaining: 3.9s 542: learn: 0.0689291 total: 4.62s remaining: 3.89s 543: learn: 0.0688718 total: 4.63s remaining: 3.88s 544: learn: 0.0688340 total: 4.64s remaining: 3.88s 545: learn: 0.0687686 total: 4.65s remaining: 3.87s 546: learn: 0.0687184 total: 4.66s remaining: 3.86s 547: learn: 0.0686632 total: 4.68s remaining: 3.87s 548: learn: 0.0686266 total: 4.7s remaining: 3.86s 549: learn: 0.0686131 total: 4.7s remaining: 3.85s 550: learn: 0.0685443 total: 4.71s remaining: 3.84s 551: learn: 0.0684725 total: 4.72s remaining: 3.83s 552: learn: 0.0683954 total: 4.73s remaining: 3.82s 553: learn: 0.0683424 total: 4.74s remaining: 3.81s 554: learn: 0.0682494 total: 4.74s remaining: 3.8s 555: learn: 0.0681896 total: 4.75s remaining: 3.79s 556: learn: 0.0681238 total: 4.76s remaining: 3.79s 557: learn: 0.0681110 total: 4.77s remaining: 3.78s 558: learn: 0.0681036 total: 4.78s remaining: 3.77s 559: learn: 0.0680347 total: 4.79s remaining: 3.76s 560: learn: 0.0680196 total: 4.79s remaining: 3.75s 561: learn: 0.0679525 total: 4.8s remaining: 3.74s 562: learn: 0.0679268 total: 4.81s remaining: 3.73s 563: learn: 0.0678514 total: 4.82s remaining: 3.72s 564: learn: 0.0677968 total: 4.83s remaining: 3.72s 565: learn: 0.0677857 total: 4.84s remaining: 3.71s 566: learn: 0.0677096 total: 4.86s remaining: 3.71s 567: learn: 0.0677037 total: 4.87s remaining: 3.7s 568: learn: 0.0676251 total: 4.87s remaining: 3.69s 569: learn: 0.0676097 total: 4.88s remaining: 3.68s 570: learn: 0.0675385 total: 4.89s remaining: 3.67s 571: learn: 0.0674541 total: 4.9s remaining: 3.67s 572: learn: 0.0674018 total: 4.91s remaining: 3.66s 573: learn: 0.0673571 total: 4.92s remaining: 3.65s 574: learn: 0.0673332 total: 4.92s remaining: 3.64s 575: learn: 0.0672703 total: 4.93s remaining: 3.63s 576: learn: 0.0672183 total: 4.94s remaining: 3.62s 577: learn: 0.0671783 total: 4.95s remaining: 3.61s 578: learn: 0.0671427 total: 4.96s remaining: 3.6s 579: learn: 0.0670776 total: 4.96s remaining: 3.6s 580: learn: 0.0670692 total: 4.97s remaining: 3.58s 581: learn: 0.0669861 total: 4.98s remaining: 3.58s 582: learn: 0.0669449 total: 4.99s remaining: 3.57s 583: learn: 0.0668293 total: 5s remaining: 3.56s 584: learn: 0.0667682 total: 5s remaining: 3.55s 585: learn: 0.0667544 total: 5.01s remaining: 3.54s 586: learn: 0.0667060 total: 5.02s remaining: 3.53s 587: learn: 0.0666451 total: 5.03s remaining: 3.52s 588: learn: 0.0665675 total: 5.04s remaining: 3.52s 589: learn: 0.0665200 total: 5.05s remaining: 3.51s 590: learn: 0.0664668 total: 5.06s remaining: 3.5s 591: learn: 0.0664112 total: 5.07s remaining: 3.49s 592: learn: 0.0663237 total: 5.07s remaining: 3.48s 593: learn: 0.0662862 total: 5.08s remaining: 3.47s 594: learn: 0.0662015 total: 5.09s remaining: 3.46s 595: learn: 0.0661319 total: 5.1s remaining: 3.46s 596: learn: 0.0660832 total: 5.11s remaining: 3.45s 597: learn: 0.0660218 total: 5.11s remaining: 3.44s 598: learn: 0.0659969 total: 5.12s remaining: 3.43s 599: learn: 0.0659455 total: 5.13s remaining: 3.42s 600: learn: 0.0659384 total: 5.14s remaining: 3.41s 601: learn: 0.0659315 total: 5.14s remaining: 3.4s 602: learn: 0.0658655 total: 5.15s remaining: 3.39s 603: learn: 0.0658111 total: 5.16s remaining: 3.38s 604: learn: 0.0657488 total: 5.17s remaining: 3.37s 605: learn: 0.0656997 total: 5.18s remaining: 3.37s 606: learn: 0.0656528 total: 5.18s remaining: 3.36s 607: learn: 0.0655781 total: 5.19s remaining: 3.35s 608: learn: 0.0655706 total: 5.2s remaining: 3.34s 609: learn: 0.0655143 total: 5.21s remaining: 3.33s 610: learn: 0.0654586 total: 5.21s remaining: 3.32s 611: learn: 0.0654086 total: 5.22s remaining: 3.31s 612: learn: 0.0653321 total: 5.24s remaining: 3.31s 613: learn: 0.0652535 total: 5.25s remaining: 3.3s 614: learn: 0.0651802 total: 5.26s remaining: 3.29s 615: learn: 0.0651104 total: 5.27s remaining: 3.29s 616: learn: 0.0650452 total: 5.28s remaining: 3.28s 617: learn: 0.0649849 total: 5.29s remaining: 3.27s 618: learn: 0.0649353 total: 5.29s remaining: 3.26s 619: learn: 0.0648882 total: 5.3s remaining: 3.25s 620: learn: 0.0648168 total: 5.31s remaining: 3.24s 621: learn: 0.0647574 total: 5.32s remaining: 3.23s 622: learn: 0.0646812 total: 5.33s remaining: 3.22s 623: learn: 0.0646477 total: 5.34s remaining: 3.21s 624: learn: 0.0645825 total: 5.34s remaining: 3.21s 625: learn: 0.0645626 total: 5.35s remaining: 3.2s 626: learn: 0.0645176 total: 5.36s remaining: 3.19s 627: learn: 0.0644558 total: 5.37s remaining: 3.18s 628: learn: 0.0643621 total: 5.38s remaining: 3.17s 629: learn: 0.0642936 total: 5.38s remaining: 3.16s 630: learn: 0.0642424 total: 5.39s remaining: 3.15s 631: learn: 0.0641846 total: 5.4s remaining: 3.14s 632: learn: 0.0641276 total: 5.41s remaining: 3.13s 633: learn: 0.0640822 total: 5.42s remaining: 3.13s 634: learn: 0.0640545 total: 5.42s remaining: 3.12s 635: learn: 0.0639904 total: 5.43s remaining: 3.11s 636: learn: 0.0639829 total: 5.45s remaining: 3.1s 637: learn: 0.0639272 total: 5.46s remaining: 3.1s 638: learn: 0.0638720 total: 5.46s remaining: 3.09s 639: learn: 0.0637930 total: 5.47s remaining: 3.08s 640: learn: 0.0637295 total: 5.48s remaining: 3.07s 641: learn: 0.0636636 total: 5.49s remaining: 3.06s 642: learn: 0.0636138 total: 5.5s remaining: 3.05s 643: learn: 0.0635439 total: 5.5s remaining: 3.04s 644: learn: 0.0634934 total: 5.51s remaining: 3.04s 645: learn: 0.0634511 total: 5.52s remaining: 3.03s 646: learn: 0.0633952 total: 5.53s remaining: 3.02s 647: learn: 0.0633123 total: 5.54s remaining: 3.01s 648: learn: 0.0632529 total: 5.55s remaining: 3s 649: learn: 0.0631758 total: 5.55s remaining: 2.99s 650: learn: 0.0631698 total: 5.56s remaining: 2.98s 651: learn: 0.0631144 total: 5.57s remaining: 2.97s 652: learn: 0.0630544 total: 5.58s remaining: 2.96s 653: learn: 0.0630126 total: 5.58s remaining: 2.96s 654: learn: 0.0629621 total: 5.59s remaining: 2.95s 655: learn: 0.0628938 total: 5.6s remaining: 2.94s 656: learn: 0.0628636 total: 5.61s remaining: 2.93s 657: learn: 0.0628323 total: 5.62s remaining: 2.92s 658: learn: 0.0627920 total: 5.63s remaining: 2.91s 659: learn: 0.0627758 total: 5.64s remaining: 2.9s 660: learn: 0.0627069 total: 5.65s remaining: 2.9s 661: learn: 0.0626354 total: 5.66s remaining: 2.89s 662: learn: 0.0625744 total: 5.67s remaining: 2.88s 663: learn: 0.0625047 total: 5.67s remaining: 2.87s 664: learn: 0.0624746 total: 5.68s remaining: 2.86s 665: learn: 0.0624682 total: 5.69s remaining: 2.85s 666: learn: 0.0623977 total: 5.71s remaining: 2.85s 667: learn: 0.0623421 total: 5.71s remaining: 2.84s 668: learn: 0.0622875 total: 5.72s remaining: 2.83s 669: learn: 0.0622358 total: 5.73s remaining: 2.82s 670: learn: 0.0621745 total: 5.74s remaining: 2.81s 671: learn: 0.0620894 total: 5.75s remaining: 2.8s 672: learn: 0.0620519 total: 5.75s remaining: 2.8s 673: learn: 0.0620081 total: 5.76s remaining: 2.79s 674: learn: 0.0619394 total: 5.77s remaining: 2.78s 675: learn: 0.0619050 total: 5.78s remaining: 2.77s 676: learn: 0.0618637 total: 5.79s remaining: 2.76s 677: learn: 0.0618202 total: 5.79s remaining: 2.75s 678: learn: 0.0617443 total: 5.8s remaining: 2.74s 679: learn: 0.0616783 total: 5.81s remaining: 2.73s 680: learn: 0.0616453 total: 5.82s remaining: 2.73s 681: learn: 0.0615969 total: 5.83s remaining: 2.72s 682: learn: 0.0615582 total: 5.84s remaining: 2.71s 683: learn: 0.0614977 total: 5.85s remaining: 2.7s 684: learn: 0.0614374 total: 5.86s remaining: 2.69s 685: learn: 0.0613993 total: 5.87s remaining: 2.68s 686: learn: 0.0613398 total: 5.87s remaining: 2.67s 687: learn: 0.0612828 total: 5.88s remaining: 2.67s 688: learn: 0.0612762 total: 5.89s remaining: 2.66s 689: learn: 0.0612706 total: 5.9s remaining: 2.65s 690: learn: 0.0612388 total: 5.91s remaining: 2.64s 691: learn: 0.0611543 total: 5.91s remaining: 2.63s 692: learn: 0.0611445 total: 5.92s remaining: 2.62s 693: learn: 0.0611175 total: 5.93s remaining: 2.61s 694: learn: 0.0610630 total: 5.94s remaining: 2.6s 695: learn: 0.0610048 total: 5.95s remaining: 2.6s 696: learn: 0.0609911 total: 5.95s remaining: 2.59s 697: learn: 0.0609033 total: 5.96s remaining: 2.58s 698: learn: 0.0608451 total: 5.97s remaining: 2.57s 699: learn: 0.0608036 total: 5.98s remaining: 2.56s 700: learn: 0.0607583 total: 5.98s remaining: 2.55s 701: learn: 0.0607305 total: 5.99s remaining: 2.54s 702: learn: 0.0606661 total: 6s remaining: 2.54s 703: learn: 0.0606434 total: 6.01s remaining: 2.53s 704: learn: 0.0605860 total: 6.02s remaining: 2.52s 705: learn: 0.0605386 total: 6.03s remaining: 2.51s 706: learn: 0.0604831 total: 6.04s remaining: 2.5s 707: learn: 0.0603998 total: 6.05s remaining: 2.49s 708: learn: 0.0603399 total: 6.05s remaining: 2.48s 709: learn: 0.0602758 total: 6.06s remaining: 2.48s 710: learn: 0.0602291 total: 6.07s remaining: 2.47s 711: learn: 0.0602248 total: 6.08s remaining: 2.46s 712: learn: 0.0601755 total: 6.09s remaining: 2.45s 713: learn: 0.0601396 total: 6.09s remaining: 2.44s 714: learn: 0.0600811 total: 6.1s remaining: 2.43s 715: learn: 0.0600487 total: 6.11s remaining: 2.42s 716: learn: 0.0599880 total: 6.12s remaining: 2.41s 717: learn: 0.0599041 total: 6.13s remaining: 2.4s 718: learn: 0.0598371 total: 6.13s remaining: 2.4s 719: learn: 0.0597920 total: 6.14s remaining: 2.39s 720: learn: 0.0597430 total: 6.15s remaining: 2.38s 721: learn: 0.0596530 total: 6.16s remaining: 2.37s 722: learn: 0.0596039 total: 6.16s remaining: 2.36s 723: learn: 0.0595577 total: 6.17s remaining: 2.35s 724: learn: 0.0595159 total: 6.18s remaining: 2.34s 725: learn: 0.0595022 total: 6.19s remaining: 2.33s 726: learn: 0.0594537 total: 6.2s remaining: 2.33s 727: learn: 0.0593920 total: 6.2s remaining: 2.32s 728: learn: 0.0593468 total: 6.22s remaining: 2.31s 729: learn: 0.0592977 total: 6.23s remaining: 2.3s 730: learn: 0.0592383 total: 6.24s remaining: 2.3s 731: learn: 0.0592078 total: 6.26s remaining: 2.29s 732: learn: 0.0591783 total: 6.26s remaining: 2.28s 733: learn: 0.0591466 total: 6.27s remaining: 2.27s 734: learn: 0.0590949 total: 6.28s remaining: 2.26s 735: learn: 0.0590484 total: 6.29s remaining: 2.25s 736: learn: 0.0589985 total: 6.3s remaining: 2.25s 737: learn: 0.0589397 total: 6.3s remaining: 2.24s 738: learn: 0.0589169 total: 6.31s remaining: 2.23s 739: learn: 0.0588703 total: 6.32s remaining: 2.22s 740: learn: 0.0588359 total: 6.33s remaining: 2.21s 741: learn: 0.0587921 total: 6.33s remaining: 2.2s 742: learn: 0.0587775 total: 6.34s remaining: 2.19s 743: learn: 0.0587080 total: 6.35s remaining: 2.19s 744: learn: 0.0586488 total: 6.36s remaining: 2.18s 745: learn: 0.0586252 total: 6.37s remaining: 2.17s 746: learn: 0.0585809 total: 6.38s remaining: 2.16s 747: learn: 0.0585572 total: 6.38s remaining: 2.15s 748: learn: 0.0585431 total: 6.39s remaining: 2.14s 749: learn: 0.0584881 total: 6.4s remaining: 2.13s 750: learn: 0.0584195 total: 6.41s remaining: 2.12s 751: learn: 0.0583591 total: 6.42s remaining: 2.12s 752: learn: 0.0582914 total: 6.42s remaining: 2.11s 753: learn: 0.0582699 total: 6.43s remaining: 2.1s 754: learn: 0.0582033 total: 6.45s remaining: 2.09s 755: learn: 0.0581756 total: 6.45s remaining: 2.08s 756: learn: 0.0581218 total: 6.46s remaining: 2.07s 757: learn: 0.0580868 total: 6.47s remaining: 2.06s 758: learn: 0.0580376 total: 6.48s remaining: 2.06s 759: learn: 0.0579718 total: 6.49s remaining: 2.05s 760: learn: 0.0579452 total: 6.49s remaining: 2.04s 761: learn: 0.0578788 total: 6.5s remaining: 2.03s 762: learn: 0.0578204 total: 6.51s remaining: 2.02s 763: learn: 0.0578107 total: 6.52s remaining: 2.01s 764: learn: 0.0577889 total: 6.53s remaining: 2s 765: learn: 0.0577621 total: 6.54s remaining: 2s 766: learn: 0.0576877 total: 6.55s remaining: 1.99s 767: learn: 0.0576195 total: 6.55s remaining: 1.98s 768: learn: 0.0575833 total: 6.56s remaining: 1.97s 769: learn: 0.0575518 total: 6.57s remaining: 1.96s 770: learn: 0.0575011 total: 6.58s remaining: 1.95s 771: learn: 0.0574401 total: 6.58s remaining: 1.94s 772: learn: 0.0573728 total: 6.59s remaining: 1.94s 773: learn: 0.0573624 total: 6.6s remaining: 1.93s 774: learn: 0.0573004 total: 6.61s remaining: 1.92s 775: learn: 0.0572557 total: 6.62s remaining: 1.91s 776: learn: 0.0572189 total: 6.62s remaining: 1.9s 777: learn: 0.0571876 total: 6.63s remaining: 1.89s 778: learn: 0.0571823 total: 6.64s remaining: 1.88s 779: learn: 0.0571306 total: 6.65s remaining: 1.88s 780: learn: 0.0570799 total: 6.66s remaining: 1.87s 781: learn: 0.0570376 total: 6.67s remaining: 1.86s 782: learn: 0.0570071 total: 6.67s remaining: 1.85s 783: learn: 0.0569884 total: 6.68s remaining: 1.84s 784: learn: 0.0569594 total: 6.69s remaining: 1.83s 785: learn: 0.0569518 total: 6.7s remaining: 1.82s 786: learn: 0.0569080 total: 6.7s remaining: 1.81s 787: learn: 0.0568998 total: 6.71s remaining: 1.8s 788: learn: 0.0568483 total: 6.72s remaining: 1.8s 789: learn: 0.0568224 total: 6.73s remaining: 1.79s 790: learn: 0.0567915 total: 6.74s remaining: 1.78s 791: learn: 0.0567864 total: 6.74s remaining: 1.77s 792: learn: 0.0567497 total: 6.75s remaining: 1.76s 793: learn: 0.0567095 total: 6.76s remaining: 1.75s 794: learn: 0.0566788 total: 6.77s remaining: 1.75s 795: learn: 0.0566255 total: 6.78s remaining: 1.74s 796: learn: 0.0565659 total: 6.79s remaining: 1.73s 797: learn: 0.0564947 total: 6.8s remaining: 1.72s 798: learn: 0.0564280 total: 6.8s remaining: 1.71s 799: learn: 0.0563490 total: 6.81s remaining: 1.7s 800: learn: 0.0563013 total: 6.82s remaining: 1.69s 801: learn: 0.0562467 total: 6.83s remaining: 1.69s 802: learn: 0.0562273 total: 6.84s remaining: 1.68s 803: learn: 0.0561709 total: 6.85s remaining: 1.67s 804: learn: 0.0561464 total: 6.86s remaining: 1.66s 805: learn: 0.0560880 total: 6.87s remaining: 1.65s 806: learn: 0.0560686 total: 6.88s remaining: 1.64s 807: learn: 0.0560255 total: 6.88s remaining: 1.64s 808: learn: 0.0560164 total: 6.89s remaining: 1.63s 809: learn: 0.0559664 total: 6.9s remaining: 1.62s 810: learn: 0.0559143 total: 6.91s remaining: 1.61s 811: learn: 0.0558751 total: 6.91s remaining: 1.6s 812: learn: 0.0558248 total: 6.92s remaining: 1.59s 813: learn: 0.0558121 total: 6.93s remaining: 1.58s 814: learn: 0.0558040 total: 6.94s remaining: 1.57s 815: learn: 0.0557356 total: 6.95s remaining: 1.57s 816: learn: 0.0557070 total: 6.95s remaining: 1.56s 817: learn: 0.0556432 total: 6.96s remaining: 1.55s 818: learn: 0.0556300 total: 6.97s remaining: 1.54s 819: learn: 0.0555933 total: 6.98s remaining: 1.53s 820: learn: 0.0555498 total: 6.98s remaining: 1.52s 821: learn: 0.0555165 total: 6.99s remaining: 1.51s 822: learn: 0.0554733 total: 7s remaining: 1.5s 823: learn: 0.0554229 total: 7.01s remaining: 1.5s 824: learn: 0.0553868 total: 7.01s remaining: 1.49s 825: learn: 0.0553457 total: 7.02s remaining: 1.48s 826: learn: 0.0552918 total: 7.03s remaining: 1.47s 827: learn: 0.0552743 total: 7.04s remaining: 1.46s 828: learn: 0.0552369 total: 7.05s remaining: 1.45s 829: learn: 0.0551939 total: 7.06s remaining: 1.45s 830: learn: 0.0551717 total: 7.07s remaining: 1.44s 831: learn: 0.0551187 total: 7.07s remaining: 1.43s 832: learn: 0.0551130 total: 7.08s remaining: 1.42s 833: learn: 0.0550500 total: 7.09s remaining: 1.41s 834: learn: 0.0549775 total: 7.1s remaining: 1.4s 835: learn: 0.0549445 total: 7.1s remaining: 1.39s 836: learn: 0.0549366 total: 7.11s remaining: 1.39s 837: learn: 0.0548672 total: 7.12s remaining: 1.38s 838: learn: 0.0548257 total: 7.13s remaining: 1.37s 839: learn: 0.0547538 total: 7.14s remaining: 1.36s 840: learn: 0.0547488 total: 7.14s remaining: 1.35s 841: learn: 0.0547248 total: 7.15s remaining: 1.34s 842: learn: 0.0546725 total: 7.16s remaining: 1.33s 843: learn: 0.0546109 total: 7.17s remaining: 1.32s 844: learn: 0.0545743 total: 7.19s remaining: 1.32s 845: learn: 0.0545428 total: 7.19s remaining: 1.31s 846: learn: 0.0544657 total: 7.21s remaining: 1.3s 847: learn: 0.0544158 total: 7.21s remaining: 1.29s 848: learn: 0.0543758 total: 7.22s remaining: 1.28s 849: learn: 0.0543392 total: 7.24s remaining: 1.28s 850: learn: 0.0542706 total: 7.24s remaining: 1.27s 851: learn: 0.0542575 total: 7.25s remaining: 1.26s 852: learn: 0.0542110 total: 7.26s remaining: 1.25s 853: learn: 0.0541776 total: 7.26s remaining: 1.24s 854: learn: 0.0541114 total: 7.27s remaining: 1.23s 855: learn: 0.0540658 total: 7.28s remaining: 1.22s 856: learn: 0.0540156 total: 7.29s remaining: 1.22s 857: learn: 0.0540036 total: 7.29s remaining: 1.21s 858: learn: 0.0539921 total: 7.3s remaining: 1.2s 859: learn: 0.0539725 total: 7.31s remaining: 1.19s 860: learn: 0.0538976 total: 7.32s remaining: 1.18s 861: learn: 0.0538924 total: 7.33s remaining: 1.17s 862: learn: 0.0538357 total: 7.33s remaining: 1.16s 863: learn: 0.0538278 total: 7.34s remaining: 1.16s 864: learn: 0.0538131 total: 7.35s remaining: 1.15s 865: learn: 0.0537712 total: 7.36s remaining: 1.14s 866: learn: 0.0537169 total: 7.36s remaining: 1.13s 867: learn: 0.0536838 total: 7.37s remaining: 1.12s 868: learn: 0.0536463 total: 7.38s remaining: 1.11s 869: learn: 0.0535832 total: 7.39s remaining: 1.1s 870: learn: 0.0535441 total: 7.4s remaining: 1.1s 871: learn: 0.0535227 total: 7.41s remaining: 1.09s 872: learn: 0.0535155 total: 7.42s remaining: 1.08s 873: learn: 0.0534823 total: 7.42s remaining: 1.07s 874: learn: 0.0534370 total: 7.44s remaining: 1.06s 875: learn: 0.0533981 total: 7.45s remaining: 1.05s 876: learn: 0.0533379 total: 7.46s remaining: 1.04s 877: learn: 0.0532937 total: 7.46s remaining: 1.04s 878: learn: 0.0532534 total: 7.47s remaining: 1.03s 879: learn: 0.0531998 total: 7.48s remaining: 1.02s 880: learn: 0.0531527 total: 7.49s remaining: 1.01s 881: learn: 0.0531101 total: 7.49s remaining: 1s 882: learn: 0.0530743 total: 7.5s remaining: 994ms 883: learn: 0.0530077 total: 7.51s remaining: 985ms 884: learn: 0.0529746 total: 7.52s remaining: 977ms 885: learn: 0.0529208 total: 7.53s remaining: 968ms 886: learn: 0.0528502 total: 7.54s remaining: 960ms 887: learn: 0.0527894 total: 7.54s remaining: 951ms 888: learn: 0.0527740 total: 7.55s remaining: 943ms 889: learn: 0.0527132 total: 7.56s remaining: 934ms 890: learn: 0.0526514 total: 7.57s remaining: 926ms 891: learn: 0.0525927 total: 7.58s remaining: 917ms 892: learn: 0.0525847 total: 7.58s remaining: 909ms 893: learn: 0.0525359 total: 7.59s remaining: 900ms 894: learn: 0.0525153 total: 7.6s remaining: 892ms 895: learn: 0.0524702 total: 7.61s remaining: 883ms 896: learn: 0.0524232 total: 7.62s remaining: 874ms 897: learn: 0.0523678 total: 7.63s remaining: 867ms 898: learn: 0.0523074 total: 7.64s remaining: 858ms 899: learn: 0.0522609 total: 7.64s remaining: 850ms 900: learn: 0.0522558 total: 7.65s remaining: 841ms 901: learn: 0.0521892 total: 7.66s remaining: 833ms 902: learn: 0.0521338 total: 7.67s remaining: 824ms 903: learn: 0.0520595 total: 7.68s remaining: 816ms 904: learn: 0.0520107 total: 7.69s remaining: 807ms 905: learn: 0.0519606 total: 7.7s remaining: 799ms 906: learn: 0.0519178 total: 7.71s remaining: 791ms 907: learn: 0.0518825 total: 7.72s remaining: 782ms 908: learn: 0.0518365 total: 7.73s remaining: 773ms 909: learn: 0.0518124 total: 7.73s remaining: 765ms 910: learn: 0.0517801 total: 7.74s remaining: 756ms 911: learn: 0.0517465 total: 7.75s remaining: 748ms 912: learn: 0.0517185 total: 7.76s remaining: 739ms 913: learn: 0.0517156 total: 7.76s remaining: 731ms 914: learn: 0.0516808 total: 7.77s remaining: 722ms 915: learn: 0.0516398 total: 7.78s remaining: 713ms 916: learn: 0.0516125 total: 7.79s remaining: 705ms 917: learn: 0.0515564 total: 7.79s remaining: 696ms 918: learn: 0.0515019 total: 7.8s remaining: 688ms 919: learn: 0.0514294 total: 7.81s remaining: 679ms 920: learn: 0.0514263 total: 7.82s remaining: 671ms 921: learn: 0.0513989 total: 7.83s remaining: 663ms 922: learn: 0.0513670 total: 7.84s remaining: 654ms 923: learn: 0.0513383 total: 7.85s remaining: 646ms 924: learn: 0.0513124 total: 7.86s remaining: 637ms 925: learn: 0.0513068 total: 7.87s remaining: 629ms 926: learn: 0.0512720 total: 7.87s remaining: 620ms 927: learn: 0.0512212 total: 7.88s remaining: 612ms 928: learn: 0.0511822 total: 7.89s remaining: 603ms 929: learn: 0.0511503 total: 7.9s remaining: 595ms 930: learn: 0.0511075 total: 7.91s remaining: 586ms 931: learn: 0.0510486 total: 7.91s remaining: 577ms 932: learn: 0.0509966 total: 7.92s remaining: 569ms 933: learn: 0.0509575 total: 7.93s remaining: 560ms 934: learn: 0.0509072 total: 7.94s remaining: 552ms 935: learn: 0.0508699 total: 7.95s remaining: 543ms 936: learn: 0.0508266 total: 7.95s remaining: 535ms 937: learn: 0.0507958 total: 7.96s remaining: 526ms 938: learn: 0.0507710 total: 7.97s remaining: 518ms 939: learn: 0.0507317 total: 7.98s remaining: 509ms 940: learn: 0.0506855 total: 7.99s remaining: 501ms 941: learn: 0.0506377 total: 7.99s remaining: 492ms 942: learn: 0.0505735 total: 8s remaining: 484ms 943: learn: 0.0505187 total: 8.01s remaining: 475ms 944: learn: 0.0504871 total: 8.02s remaining: 467ms 945: learn: 0.0504392 total: 8.04s remaining: 459ms 946: learn: 0.0504139 total: 8.04s remaining: 450ms 947: learn: 0.0503685 total: 8.05s remaining: 442ms 948: learn: 0.0503422 total: 8.06s remaining: 433ms 949: learn: 0.0503387 total: 8.07s remaining: 425ms 950: learn: 0.0503345 total: 8.08s remaining: 416ms 951: learn: 0.0503217 total: 8.09s remaining: 408ms 952: learn: 0.0502983 total: 8.09s remaining: 399ms 953: learn: 0.0502593 total: 8.1s remaining: 391ms 954: learn: 0.0502556 total: 8.11s remaining: 382ms 955: learn: 0.0502260 total: 8.12s remaining: 374ms 956: learn: 0.0502075 total: 8.13s remaining: 365ms 957: learn: 0.0501985 total: 8.14s remaining: 357ms 958: learn: 0.0501490 total: 8.15s remaining: 349ms 959: learn: 0.0501312 total: 8.16s remaining: 340ms 960: learn: 0.0500828 total: 8.17s remaining: 332ms 961: learn: 0.0500467 total: 8.18s remaining: 323ms 962: learn: 0.0500152 total: 8.18s remaining: 314ms 963: learn: 0.0499420 total: 8.19s remaining: 306ms 964: learn: 0.0499141 total: 8.2s remaining: 297ms 965: learn: 0.0499101 total: 8.21s remaining: 289ms 966: learn: 0.0498698 total: 8.21s remaining: 280ms 967: learn: 0.0498306 total: 8.23s remaining: 272ms 968: learn: 0.0498038 total: 8.24s remaining: 264ms 969: learn: 0.0497551 total: 8.24s remaining: 255ms 970: learn: 0.0497436 total: 8.25s remaining: 246ms 971: learn: 0.0497006 total: 8.26s remaining: 238ms 972: learn: 0.0496522 total: 8.27s remaining: 229ms 973: learn: 0.0496481 total: 8.28s remaining: 221ms 974: learn: 0.0496369 total: 8.29s remaining: 212ms 975: learn: 0.0496005 total: 8.29s remaining: 204ms 976: learn: 0.0495753 total: 8.3s remaining: 195ms 977: learn: 0.0495303 total: 8.31s remaining: 187ms 978: learn: 0.0494919 total: 8.32s remaining: 178ms 979: learn: 0.0494323 total: 8.33s remaining: 170ms 980: learn: 0.0494040 total: 8.33s remaining: 161ms 981: learn: 0.0494003 total: 8.34s remaining: 153ms 982: learn: 0.0493767 total: 8.35s remaining: 144ms 983: learn: 0.0493374 total: 8.36s remaining: 136ms 984: learn: 0.0492983 total: 8.37s remaining: 127ms 985: learn: 0.0492423 total: 8.37s remaining: 119ms 986: learn: 0.0492087 total: 8.38s remaining: 110ms 987: learn: 0.0491414 total: 8.39s remaining: 102ms 988: learn: 0.0491105 total: 8.4s remaining: 93.4ms 989: learn: 0.0490924 total: 8.4s remaining: 84.9ms 990: learn: 0.0490518 total: 8.41s remaining: 76.4ms 991: learn: 0.0490285 total: 8.42s remaining: 67.9ms 992: learn: 0.0489880 total: 8.44s remaining: 59.5ms 993: learn: 0.0489364 total: 8.44s remaining: 51ms 994: learn: 0.0488837 total: 8.45s remaining: 42.5ms 995: learn: 0.0488610 total: 8.46s remaining: 34ms 996: learn: 0.0488266 total: 8.47s remaining: 25.5ms 997: learn: 0.0487788 total: 8.47s remaining: 17ms 998: learn: 0.0487761 total: 8.48s remaining: 8.49ms 999: learn: 0.0487511 total: 8.49s remaining: 0us Learning rate set to 0.046383 0: learn: 0.3895099 total: 9.03ms remaining: 9.02s 1: learn: 0.3747091 total: 18.3ms remaining: 9.11s 2: learn: 0.3604959 total: 27.8ms remaining: 9.24s 3: learn: 0.3467878 total: 37ms remaining: 9.21s 4: learn: 0.3338126 total: 46.1ms remaining: 9.18s 5: learn: 0.3218844 total: 55.4ms remaining: 9.17s 6: learn: 0.3102681 total: 64.8ms remaining: 9.2s 7: learn: 0.2997175 total: 74.3ms remaining: 9.21s 8: learn: 0.2890410 total: 83.2ms remaining: 9.16s 9: learn: 0.2792306 total: 92.2ms remaining: 9.13s 10: learn: 0.2691926 total: 102ms remaining: 9.15s 11: learn: 0.2602393 total: 113ms remaining: 9.28s 12: learn: 0.2513477 total: 122ms remaining: 9.25s 13: learn: 0.2424132 total: 131ms remaining: 9.24s 14: learn: 0.2343719 total: 140ms remaining: 9.22s 15: learn: 0.2270265 total: 149ms remaining: 9.19s 16: learn: 0.2197214 total: 158ms remaining: 9.16s 17: learn: 0.2129598 total: 168ms remaining: 9.14s 18: learn: 0.2058249 total: 177ms remaining: 9.12s 19: learn: 0.1989234 total: 186ms remaining: 9.1s 20: learn: 0.1928363 total: 203ms remaining: 9.47s 21: learn: 0.1868329 total: 213ms remaining: 9.46s 22: learn: 0.1810277 total: 223ms remaining: 9.45s 23: learn: 0.1753582 total: 232ms remaining: 9.45s 24: learn: 0.1703991 total: 242ms remaining: 9.42s 25: learn: 0.1651806 total: 251ms remaining: 9.39s 26: learn: 0.1606699 total: 264ms remaining: 9.53s 27: learn: 0.1562024 total: 274ms remaining: 9.49s 28: learn: 0.1519652 total: 282ms remaining: 9.46s 29: learn: 0.1478773 total: 292ms remaining: 9.43s 30: learn: 0.1438805 total: 301ms remaining: 9.4s 31: learn: 0.1402405 total: 310ms remaining: 9.38s 32: learn: 0.1369680 total: 320ms remaining: 9.36s 33: learn: 0.1334244 total: 329ms remaining: 9.36s 34: learn: 0.1302222 total: 339ms remaining: 9.35s 35: learn: 0.1270003 total: 348ms remaining: 9.32s 36: learn: 0.1238898 total: 357ms remaining: 9.29s 37: learn: 0.1213093 total: 366ms remaining: 9.27s 38: learn: 0.1184424 total: 375ms remaining: 9.25s 39: learn: 0.1155266 total: 385ms remaining: 9.24s 40: learn: 0.1130339 total: 394ms remaining: 9.22s 41: learn: 0.1106706 total: 410ms remaining: 9.35s 42: learn: 0.1086210 total: 419ms remaining: 9.32s 43: learn: 0.1064741 total: 430ms remaining: 9.33s 44: learn: 0.1045134 total: 448ms remaining: 9.5s 45: learn: 0.1027214 total: 457ms remaining: 9.48s 46: learn: 0.1009380 total: 467ms remaining: 9.46s 47: learn: 0.0992830 total: 476ms remaining: 9.44s 48: learn: 0.0979623 total: 486ms remaining: 9.42s 49: learn: 0.0964736 total: 495ms remaining: 9.4s 50: learn: 0.0950414 total: 504ms remaining: 9.38s 51: learn: 0.0937079 total: 513ms remaining: 9.35s 52: learn: 0.0922777 total: 522ms remaining: 9.33s 53: learn: 0.0911453 total: 531ms remaining: 9.31s 54: learn: 0.0900528 total: 541ms remaining: 9.29s 55: learn: 0.0889460 total: 550ms remaining: 9.27s 56: learn: 0.0880341 total: 562ms remaining: 9.29s 57: learn: 0.0870587 total: 570ms remaining: 9.26s 58: learn: 0.0860693 total: 579ms remaining: 9.23s 59: learn: 0.0852325 total: 588ms remaining: 9.21s 60: learn: 0.0844589 total: 596ms remaining: 9.18s 61: learn: 0.0837809 total: 609ms remaining: 9.21s 62: learn: 0.0831730 total: 621ms remaining: 9.23s 63: learn: 0.0823255 total: 636ms remaining: 9.31s 64: learn: 0.0818129 total: 648ms remaining: 9.32s 65: learn: 0.0812529 total: 657ms remaining: 9.3s 66: learn: 0.0805125 total: 666ms remaining: 9.27s 67: learn: 0.0799945 total: 675ms remaining: 9.24s 68: learn: 0.0793289 total: 683ms remaining: 9.22s 69: learn: 0.0786773 total: 692ms remaining: 9.2s 70: learn: 0.0782039 total: 701ms remaining: 9.18s 71: learn: 0.0777519 total: 710ms remaining: 9.15s 72: learn: 0.0773871 total: 719ms remaining: 9.13s 73: learn: 0.0768154 total: 728ms remaining: 9.11s 74: learn: 0.0764395 total: 737ms remaining: 9.09s 75: learn: 0.0760636 total: 747ms remaining: 9.07s 76: learn: 0.0757340 total: 755ms remaining: 9.05s 77: learn: 0.0752908 total: 764ms remaining: 9.04s 78: learn: 0.0749040 total: 774ms remaining: 9.02s 79: learn: 0.0744160 total: 783ms remaining: 9s 80: learn: 0.0739983 total: 792ms remaining: 8.98s 81: learn: 0.0737597 total: 800ms remaining: 8.96s 82: learn: 0.0733999 total: 811ms remaining: 8.96s 83: learn: 0.0730313 total: 822ms remaining: 8.96s 84: learn: 0.0727859 total: 830ms remaining: 8.94s 85: learn: 0.0724544 total: 839ms remaining: 8.92s 86: learn: 0.0721342 total: 848ms remaining: 8.9s 87: learn: 0.0717774 total: 857ms remaining: 8.88s 88: learn: 0.0713949 total: 866ms remaining: 8.86s 89: learn: 0.0710827 total: 875ms remaining: 8.85s 90: learn: 0.0708199 total: 884ms remaining: 8.83s 91: learn: 0.0705890 total: 893ms remaining: 8.81s 92: learn: 0.0703755 total: 901ms remaining: 8.79s 93: learn: 0.0701955 total: 910ms remaining: 8.77s 94: learn: 0.0700531 total: 919ms remaining: 8.76s 95: learn: 0.0698909 total: 929ms remaining: 8.74s 96: learn: 0.0696541 total: 939ms remaining: 8.74s 97: learn: 0.0694862 total: 949ms remaining: 8.73s 98: learn: 0.0693436 total: 958ms remaining: 8.72s 99: learn: 0.0690717 total: 967ms remaining: 8.7s 100: learn: 0.0689396 total: 976ms remaining: 8.69s 101: learn: 0.0687997 total: 984ms remaining: 8.67s 102: learn: 0.0685943 total: 993ms remaining: 8.65s 103: learn: 0.0685123 total: 1s remaining: 8.63s 104: learn: 0.0683791 total: 1.02s remaining: 8.7s 105: learn: 0.0682912 total: 1.03s remaining: 8.69s 106: learn: 0.0681415 total: 1.04s remaining: 8.67s 107: learn: 0.0680287 total: 1.05s remaining: 8.68s 108: learn: 0.0679374 total: 1.06s remaining: 8.67s 109: learn: 0.0678154 total: 1.07s remaining: 8.67s 110: learn: 0.0676467 total: 1.08s remaining: 8.65s 111: learn: 0.0675084 total: 1.09s remaining: 8.64s 112: learn: 0.0673202 total: 1.1s remaining: 8.63s 113: learn: 0.0672649 total: 1.11s remaining: 8.61s 114: learn: 0.0670797 total: 1.12s remaining: 8.6s 115: learn: 0.0670577 total: 1.13s remaining: 8.61s 116: learn: 0.0669107 total: 1.14s remaining: 8.59s 117: learn: 0.0666876 total: 1.15s remaining: 8.58s 118: learn: 0.0666438 total: 1.16s remaining: 8.56s 119: learn: 0.0665682 total: 1.17s remaining: 8.57s 120: learn: 0.0664089 total: 1.18s remaining: 8.55s 121: learn: 0.0663631 total: 1.19s remaining: 8.54s 122: learn: 0.0662333 total: 1.2s remaining: 8.53s 123: learn: 0.0660574 total: 1.21s remaining: 8.51s 124: learn: 0.0659495 total: 1.23s remaining: 8.58s 125: learn: 0.0658444 total: 1.24s remaining: 8.57s 126: learn: 0.0657525 total: 1.24s remaining: 8.55s 127: learn: 0.0656425 total: 1.25s remaining: 8.54s 128: learn: 0.0654783 total: 1.26s remaining: 8.53s 129: learn: 0.0653203 total: 1.27s remaining: 8.52s 130: learn: 0.0652735 total: 1.28s remaining: 8.5s 131: learn: 0.0652428 total: 1.29s remaining: 8.48s 132: learn: 0.0651423 total: 1.3s remaining: 8.47s 133: learn: 0.0650267 total: 1.31s remaining: 8.46s 134: learn: 0.0649308 total: 1.32s remaining: 8.44s 135: learn: 0.0648890 total: 1.33s remaining: 8.43s 136: learn: 0.0647516 total: 1.33s remaining: 8.41s 137: learn: 0.0646540 total: 1.34s remaining: 8.4s 138: learn: 0.0645239 total: 1.35s remaining: 8.39s 139: learn: 0.0643982 total: 1.36s remaining: 8.38s 140: learn: 0.0643404 total: 1.37s remaining: 8.37s 141: learn: 0.0642680 total: 1.38s remaining: 8.36s 142: learn: 0.0640996 total: 1.39s remaining: 8.35s 143: learn: 0.0640873 total: 1.4s remaining: 8.33s 144: learn: 0.0640686 total: 1.41s remaining: 8.32s 145: learn: 0.0639889 total: 1.42s remaining: 8.32s 146: learn: 0.0639072 total: 1.44s remaining: 8.34s 147: learn: 0.0638087 total: 1.45s remaining: 8.32s 148: learn: 0.0637261 total: 1.45s remaining: 8.31s 149: learn: 0.0636402 total: 1.46s remaining: 8.29s 150: learn: 0.0635074 total: 1.48s remaining: 8.3s 151: learn: 0.0634352 total: 1.49s remaining: 8.29s 152: learn: 0.0633600 total: 1.5s remaining: 8.28s 153: learn: 0.0632935 total: 1.5s remaining: 8.27s 154: learn: 0.0632676 total: 1.52s remaining: 8.27s 155: learn: 0.0632201 total: 1.53s remaining: 8.26s 156: learn: 0.0631444 total: 1.53s remaining: 8.24s 157: learn: 0.0630858 total: 1.54s remaining: 8.23s 158: learn: 0.0629775 total: 1.55s remaining: 8.21s 159: learn: 0.0628889 total: 1.56s remaining: 8.2s 160: learn: 0.0627971 total: 1.57s remaining: 8.19s 161: learn: 0.0627695 total: 1.58s remaining: 8.17s 162: learn: 0.0626946 total: 1.59s remaining: 8.16s 163: learn: 0.0625664 total: 1.6s remaining: 8.16s 164: learn: 0.0624923 total: 1.62s remaining: 8.18s 165: learn: 0.0623686 total: 1.63s remaining: 8.2s 166: learn: 0.0622941 total: 1.64s remaining: 8.18s 167: learn: 0.0621647 total: 1.65s remaining: 8.17s 168: learn: 0.0620172 total: 1.66s remaining: 8.15s 169: learn: 0.0619748 total: 1.67s remaining: 8.14s 170: learn: 0.0618404 total: 1.68s remaining: 8.12s 171: learn: 0.0616806 total: 1.69s remaining: 8.11s 172: learn: 0.0614680 total: 1.69s remaining: 8.1s 173: learn: 0.0613313 total: 1.7s remaining: 8.09s 174: learn: 0.0611969 total: 1.71s remaining: 8.07s 175: learn: 0.0611480 total: 1.72s remaining: 8.06s 176: learn: 0.0610204 total: 1.73s remaining: 8.05s 177: learn: 0.0609705 total: 1.74s remaining: 8.05s 178: learn: 0.0608500 total: 1.75s remaining: 8.04s 179: learn: 0.0606905 total: 1.76s remaining: 8.03s 180: learn: 0.0605827 total: 1.77s remaining: 8.01s 181: learn: 0.0604621 total: 1.78s remaining: 8s 182: learn: 0.0603241 total: 1.79s remaining: 7.99s 183: learn: 0.0602028 total: 1.8s remaining: 7.98s 184: learn: 0.0601052 total: 1.81s remaining: 7.97s 185: learn: 0.0600357 total: 1.82s remaining: 7.99s 186: learn: 0.0599117 total: 1.83s remaining: 7.98s 187: learn: 0.0597976 total: 1.84s remaining: 7.97s 188: learn: 0.0596973 total: 1.85s remaining: 7.96s 189: learn: 0.0595513 total: 1.86s remaining: 7.94s 190: learn: 0.0593937 total: 1.87s remaining: 7.93s 191: learn: 0.0593152 total: 1.88s remaining: 7.92s 192: learn: 0.0591643 total: 1.89s remaining: 7.9s 193: learn: 0.0590686 total: 1.9s remaining: 7.89s 194: learn: 0.0589946 total: 1.91s remaining: 7.88s 195: learn: 0.0589263 total: 1.92s remaining: 7.86s 196: learn: 0.0588543 total: 1.93s remaining: 7.85s 197: learn: 0.0587511 total: 1.93s remaining: 7.84s 198: learn: 0.0586653 total: 1.94s remaining: 7.82s 199: learn: 0.0585680 total: 1.95s remaining: 7.81s 200: learn: 0.0583838 total: 1.96s remaining: 7.8s 201: learn: 0.0583313 total: 1.97s remaining: 7.79s 202: learn: 0.0581951 total: 1.98s remaining: 7.78s 203: learn: 0.0581239 total: 1.99s remaining: 7.77s 204: learn: 0.0580154 total: 2s remaining: 7.77s 205: learn: 0.0579445 total: 2.01s remaining: 7.76s 206: learn: 0.0578899 total: 2.03s remaining: 7.77s 207: learn: 0.0577500 total: 2.04s remaining: 7.76s 208: learn: 0.0576089 total: 2.05s remaining: 7.75s 209: learn: 0.0574853 total: 2.06s remaining: 7.74s 210: learn: 0.0573773 total: 2.07s remaining: 7.73s 211: learn: 0.0572274 total: 2.08s remaining: 7.72s 212: learn: 0.0571601 total: 2.09s remaining: 7.71s 213: learn: 0.0570846 total: 2.1s remaining: 7.71s 214: learn: 0.0569979 total: 2.11s remaining: 7.7s 215: learn: 0.0569304 total: 2.12s remaining: 7.69s 216: learn: 0.0568648 total: 2.13s remaining: 7.67s 217: learn: 0.0567332 total: 2.14s remaining: 7.66s 218: learn: 0.0566348 total: 2.15s remaining: 7.65s 219: learn: 0.0564928 total: 2.15s remaining: 7.64s 220: learn: 0.0564141 total: 2.17s remaining: 7.63s 221: learn: 0.0562901 total: 2.17s remaining: 7.62s 222: learn: 0.0561704 total: 2.18s remaining: 7.61s 223: learn: 0.0560826 total: 2.19s remaining: 7.6s 224: learn: 0.0559912 total: 2.2s remaining: 7.59s 225: learn: 0.0558603 total: 2.21s remaining: 7.58s 226: learn: 0.0557579 total: 2.22s remaining: 7.57s 227: learn: 0.0556392 total: 2.25s remaining: 7.62s 228: learn: 0.0555621 total: 2.26s remaining: 7.61s 229: learn: 0.0553987 total: 2.27s remaining: 7.59s 230: learn: 0.0552653 total: 2.28s remaining: 7.6s 231: learn: 0.0551543 total: 2.29s remaining: 7.58s 232: learn: 0.0550640 total: 2.3s remaining: 7.57s 233: learn: 0.0550061 total: 2.31s remaining: 7.56s 234: learn: 0.0549055 total: 2.32s remaining: 7.55s 235: learn: 0.0548170 total: 2.33s remaining: 7.54s 236: learn: 0.0547133 total: 2.34s remaining: 7.53s 237: learn: 0.0546560 total: 2.35s remaining: 7.52s 238: learn: 0.0545303 total: 2.36s remaining: 7.51s 239: learn: 0.0544421 total: 2.37s remaining: 7.51s 240: learn: 0.0543693 total: 2.38s remaining: 7.51s 241: learn: 0.0542393 total: 2.39s remaining: 7.5s 242: learn: 0.0541671 total: 2.4s remaining: 7.49s 243: learn: 0.0540650 total: 2.41s remaining: 7.48s 244: learn: 0.0539718 total: 2.42s remaining: 7.47s 245: learn: 0.0539247 total: 2.43s remaining: 7.46s 246: learn: 0.0538790 total: 2.45s remaining: 7.46s 247: learn: 0.0537837 total: 2.46s remaining: 7.46s 248: learn: 0.0537284 total: 2.47s remaining: 7.45s 249: learn: 0.0536286 total: 2.48s remaining: 7.44s 250: learn: 0.0535308 total: 2.49s remaining: 7.43s 251: learn: 0.0533879 total: 2.5s remaining: 7.42s 252: learn: 0.0532907 total: 2.51s remaining: 7.41s 253: learn: 0.0531711 total: 2.52s remaining: 7.39s 254: learn: 0.0530854 total: 2.53s remaining: 7.38s 255: learn: 0.0530081 total: 2.54s remaining: 7.37s 256: learn: 0.0529720 total: 2.54s remaining: 7.36s 257: learn: 0.0528857 total: 2.55s remaining: 7.34s 258: learn: 0.0528070 total: 2.56s remaining: 7.33s 259: learn: 0.0527069 total: 2.57s remaining: 7.32s 260: learn: 0.0526579 total: 2.58s remaining: 7.32s 261: learn: 0.0525909 total: 2.6s remaining: 7.33s 262: learn: 0.0524551 total: 2.61s remaining: 7.32s 263: learn: 0.0523735 total: 2.62s remaining: 7.3s 264: learn: 0.0523094 total: 2.63s remaining: 7.29s 265: learn: 0.0521872 total: 2.64s remaining: 7.28s 266: learn: 0.0521017 total: 2.65s remaining: 7.29s 267: learn: 0.0520669 total: 2.66s remaining: 7.28s 268: learn: 0.0519541 total: 2.67s remaining: 7.26s 269: learn: 0.0518569 total: 2.68s remaining: 7.25s 270: learn: 0.0518148 total: 2.69s remaining: 7.24s 271: learn: 0.0516800 total: 2.7s remaining: 7.23s 272: learn: 0.0515818 total: 2.71s remaining: 7.21s 273: learn: 0.0515257 total: 2.72s remaining: 7.2s 274: learn: 0.0514504 total: 2.73s remaining: 7.19s 275: learn: 0.0513746 total: 2.73s remaining: 7.18s 276: learn: 0.0513244 total: 2.74s remaining: 7.16s 277: learn: 0.0512491 total: 2.75s remaining: 7.15s 278: learn: 0.0511674 total: 2.76s remaining: 7.14s 279: learn: 0.0511250 total: 2.77s remaining: 7.13s 280: learn: 0.0510439 total: 2.78s remaining: 7.12s 281: learn: 0.0509947 total: 2.79s remaining: 7.1s 282: learn: 0.0508990 total: 2.8s remaining: 7.09s 283: learn: 0.0508218 total: 2.81s remaining: 7.08s 284: learn: 0.0507254 total: 2.82s remaining: 7.07s 285: learn: 0.0506130 total: 2.83s remaining: 7.06s 286: learn: 0.0505023 total: 2.84s remaining: 7.05s 287: learn: 0.0504039 total: 2.85s remaining: 7.04s 288: learn: 0.0503819 total: 2.86s remaining: 7.04s 289: learn: 0.0503086 total: 2.87s remaining: 7.03s 290: learn: 0.0502648 total: 2.88s remaining: 7.02s 291: learn: 0.0501891 total: 2.89s remaining: 7.01s 292: learn: 0.0501279 total: 2.9s remaining: 7s 293: learn: 0.0500436 total: 2.91s remaining: 6.99s 294: learn: 0.0499863 total: 2.92s remaining: 6.98s 295: learn: 0.0499121 total: 2.93s remaining: 6.97s 296: learn: 0.0498069 total: 2.94s remaining: 6.96s 297: learn: 0.0497608 total: 2.95s remaining: 6.95s 298: learn: 0.0497072 total: 2.96s remaining: 6.94s 299: learn: 0.0496101 total: 2.97s remaining: 6.93s 300: learn: 0.0495436 total: 2.98s remaining: 6.92s 301: learn: 0.0494630 total: 2.99s remaining: 6.91s 302: learn: 0.0494063 total: 3s remaining: 6.89s 303: learn: 0.0493031 total: 3.01s remaining: 6.88s 304: learn: 0.0492295 total: 3.02s remaining: 6.87s 305: learn: 0.0491518 total: 3.02s remaining: 6.86s 306: learn: 0.0491018 total: 3.03s remaining: 6.85s 307: learn: 0.0490257 total: 3.05s remaining: 6.85s 308: learn: 0.0489592 total: 3.06s remaining: 6.84s 309: learn: 0.0488711 total: 3.07s remaining: 6.83s 310: learn: 0.0488069 total: 3.08s remaining: 6.82s 311: learn: 0.0487190 total: 3.09s remaining: 6.81s 312: learn: 0.0486703 total: 3.1s remaining: 6.79s 313: learn: 0.0485944 total: 3.1s remaining: 6.78s 314: learn: 0.0485456 total: 3.11s remaining: 6.77s 315: learn: 0.0484615 total: 3.12s remaining: 6.76s 316: learn: 0.0484235 total: 3.13s remaining: 6.75s 317: learn: 0.0483540 total: 3.14s remaining: 6.74s 318: learn: 0.0482453 total: 3.15s remaining: 6.73s 319: learn: 0.0481751 total: 3.16s remaining: 6.72s 320: learn: 0.0481321 total: 3.17s remaining: 6.71s 321: learn: 0.0480968 total: 3.18s remaining: 6.7s 322: learn: 0.0480238 total: 3.19s remaining: 6.68s 323: learn: 0.0479500 total: 3.2s remaining: 6.67s 324: learn: 0.0478725 total: 3.21s remaining: 6.67s 325: learn: 0.0477695 total: 3.22s remaining: 6.66s 326: learn: 0.0477131 total: 3.23s remaining: 6.65s 327: learn: 0.0476023 total: 3.24s remaining: 6.64s 328: learn: 0.0475530 total: 3.26s remaining: 6.64s 329: learn: 0.0474872 total: 3.27s remaining: 6.63s 330: learn: 0.0473874 total: 3.27s remaining: 6.62s 331: learn: 0.0473275 total: 3.28s remaining: 6.61s 332: learn: 0.0472935 total: 3.29s remaining: 6.59s 333: learn: 0.0472193 total: 3.3s remaining: 6.58s 334: learn: 0.0471826 total: 3.31s remaining: 6.57s 335: learn: 0.0471447 total: 3.32s remaining: 6.56s 336: learn: 0.0470622 total: 3.33s remaining: 6.55s 337: learn: 0.0469903 total: 3.34s remaining: 6.54s 338: learn: 0.0469197 total: 3.35s remaining: 6.53s 339: learn: 0.0468533 total: 3.36s remaining: 6.52s 340: learn: 0.0467870 total: 3.37s remaining: 6.5s 341: learn: 0.0467102 total: 3.38s remaining: 6.5s 342: learn: 0.0466323 total: 3.38s remaining: 6.48s 343: learn: 0.0465458 total: 3.39s remaining: 6.47s 344: learn: 0.0464784 total: 3.4s remaining: 6.46s 345: learn: 0.0464417 total: 3.41s remaining: 6.45s 346: learn: 0.0463843 total: 3.42s remaining: 6.45s 347: learn: 0.0463429 total: 3.43s remaining: 6.43s 348: learn: 0.0462604 total: 3.45s remaining: 6.44s 349: learn: 0.0461732 total: 3.46s remaining: 6.43s 350: learn: 0.0461410 total: 3.47s remaining: 6.42s 351: learn: 0.0460810 total: 3.48s remaining: 6.41s 352: learn: 0.0460242 total: 3.49s remaining: 6.4s 353: learn: 0.0459794 total: 3.5s remaining: 6.39s 354: learn: 0.0459064 total: 3.51s remaining: 6.38s 355: learn: 0.0458768 total: 3.52s remaining: 6.36s 356: learn: 0.0458148 total: 3.53s remaining: 6.35s 357: learn: 0.0457523 total: 3.54s remaining: 6.34s 358: learn: 0.0456855 total: 3.55s remaining: 6.35s 359: learn: 0.0456418 total: 3.56s remaining: 6.34s 360: learn: 0.0455859 total: 3.57s remaining: 6.32s 361: learn: 0.0455202 total: 3.58s remaining: 6.31s 362: learn: 0.0454537 total: 3.59s remaining: 6.31s 363: learn: 0.0453664 total: 3.6s remaining: 6.3s 364: learn: 0.0453393 total: 3.61s remaining: 6.28s 365: learn: 0.0452643 total: 3.62s remaining: 6.27s 366: learn: 0.0452137 total: 3.64s remaining: 6.27s 367: learn: 0.0451825 total: 3.65s remaining: 6.26s 368: learn: 0.0451084 total: 3.65s remaining: 6.25s 369: learn: 0.0450698 total: 3.66s remaining: 6.24s 370: learn: 0.0450302 total: 3.67s remaining: 6.22s 371: learn: 0.0449995 total: 3.68s remaining: 6.21s 372: learn: 0.0449637 total: 3.69s remaining: 6.2s 373: learn: 0.0449070 total: 3.7s remaining: 6.19s 374: learn: 0.0448476 total: 3.71s remaining: 6.18s 375: learn: 0.0448006 total: 3.72s remaining: 6.17s 376: learn: 0.0447384 total: 3.73s remaining: 6.16s 377: learn: 0.0446652 total: 3.73s remaining: 6.15s 378: learn: 0.0446449 total: 3.74s remaining: 6.13s 379: learn: 0.0445708 total: 3.75s remaining: 6.13s 380: learn: 0.0445291 total: 3.76s remaining: 6.11s 381: learn: 0.0444659 total: 3.77s remaining: 6.1s 382: learn: 0.0444149 total: 3.78s remaining: 6.09s 383: learn: 0.0443924 total: 3.79s remaining: 6.08s 384: learn: 0.0443410 total: 3.8s remaining: 6.07s 385: learn: 0.0442918 total: 3.81s remaining: 6.06s 386: learn: 0.0442199 total: 3.82s remaining: 6.05s 387: learn: 0.0441735 total: 3.83s remaining: 6.04s 388: learn: 0.0440927 total: 3.84s remaining: 6.04s 389: learn: 0.0440314 total: 3.86s remaining: 6.03s 390: learn: 0.0440222 total: 3.87s remaining: 6.02s 391: learn: 0.0439501 total: 3.87s remaining: 6.01s 392: learn: 0.0439081 total: 3.88s remaining: 6s 393: learn: 0.0438544 total: 3.89s remaining: 5.99s 394: learn: 0.0438142 total: 3.9s remaining: 5.98s 395: learn: 0.0437820 total: 3.91s remaining: 5.97s 396: learn: 0.0437461 total: 3.92s remaining: 5.96s 397: learn: 0.0437111 total: 3.93s remaining: 5.95s 398: learn: 0.0436474 total: 3.94s remaining: 5.94s 399: learn: 0.0436117 total: 3.95s remaining: 5.93s 400: learn: 0.0435758 total: 3.96s remaining: 5.92s 401: learn: 0.0435207 total: 3.97s remaining: 5.9s 402: learn: 0.0434655 total: 3.98s remaining: 5.89s 403: learn: 0.0434080 total: 3.99s remaining: 5.89s 404: learn: 0.0433788 total: 4s remaining: 5.88s 405: learn: 0.0433030 total: 4.01s remaining: 5.87s 406: learn: 0.0432634 total: 4.02s remaining: 5.86s 407: learn: 0.0432130 total: 4.04s remaining: 5.86s 408: learn: 0.0431791 total: 4.04s remaining: 5.85s 409: learn: 0.0431206 total: 4.06s remaining: 5.84s 410: learn: 0.0430905 total: 4.07s remaining: 5.83s 411: learn: 0.0430642 total: 4.07s remaining: 5.81s 412: learn: 0.0430032 total: 4.08s remaining: 5.8s 413: learn: 0.0429473 total: 4.09s remaining: 5.79s 414: learn: 0.0429229 total: 4.1s remaining: 5.78s 415: learn: 0.0428649 total: 4.11s remaining: 5.77s 416: learn: 0.0428070 total: 4.13s remaining: 5.77s 417: learn: 0.0427610 total: 4.14s remaining: 5.76s 418: learn: 0.0427436 total: 4.15s remaining: 5.75s 419: learn: 0.0427063 total: 4.16s remaining: 5.74s 420: learn: 0.0426702 total: 4.17s remaining: 5.73s 421: learn: 0.0426290 total: 4.18s remaining: 5.72s 422: learn: 0.0425943 total: 4.19s remaining: 5.71s 423: learn: 0.0425462 total: 4.2s remaining: 5.7s 424: learn: 0.0424826 total: 4.21s remaining: 5.69s 425: learn: 0.0424387 total: 4.22s remaining: 5.68s 426: learn: 0.0423855 total: 4.23s remaining: 5.67s 427: learn: 0.0423359 total: 4.24s remaining: 5.67s 428: learn: 0.0422961 total: 4.25s remaining: 5.66s 429: learn: 0.0422609 total: 4.26s remaining: 5.65s 430: learn: 0.0422202 total: 4.27s remaining: 5.64s 431: learn: 0.0421924 total: 4.28s remaining: 5.63s 432: learn: 0.0421666 total: 4.29s remaining: 5.62s 433: learn: 0.0421336 total: 4.3s remaining: 5.61s 434: learn: 0.0420981 total: 4.31s remaining: 5.59s 435: learn: 0.0420362 total: 4.32s remaining: 5.58s 436: learn: 0.0420056 total: 4.33s remaining: 5.57s 437: learn: 0.0419410 total: 4.34s remaining: 5.56s 438: learn: 0.0418819 total: 4.34s remaining: 5.55s 439: learn: 0.0418137 total: 4.35s remaining: 5.54s 440: learn: 0.0417844 total: 4.37s remaining: 5.53s 441: learn: 0.0417382 total: 4.38s remaining: 5.53s 442: learn: 0.0417131 total: 4.39s remaining: 5.51s 443: learn: 0.0416660 total: 4.39s remaining: 5.5s 444: learn: 0.0416168 total: 4.4s remaining: 5.49s 445: learn: 0.0415735 total: 4.41s remaining: 5.48s 446: learn: 0.0415359 total: 4.42s remaining: 5.47s 447: learn: 0.0415185 total: 4.43s remaining: 5.46s 448: learn: 0.0414507 total: 4.45s remaining: 5.46s 449: learn: 0.0413988 total: 4.46s remaining: 5.45s 450: learn: 0.0413651 total: 4.47s remaining: 5.44s 451: learn: 0.0413204 total: 4.48s remaining: 5.43s 452: learn: 0.0412852 total: 4.49s remaining: 5.42s 453: learn: 0.0412333 total: 4.5s remaining: 5.41s 454: learn: 0.0411980 total: 4.52s remaining: 5.41s 455: learn: 0.0411345 total: 4.53s remaining: 5.4s 456: learn: 0.0410796 total: 4.54s remaining: 5.39s 457: learn: 0.0410263 total: 4.55s remaining: 5.38s 458: learn: 0.0409702 total: 4.56s remaining: 5.37s 459: learn: 0.0409101 total: 4.57s remaining: 5.36s 460: learn: 0.0408570 total: 4.58s remaining: 5.35s 461: learn: 0.0408168 total: 4.59s remaining: 5.34s 462: learn: 0.0407994 total: 4.6s remaining: 5.33s 463: learn: 0.0407804 total: 4.61s remaining: 5.32s 464: learn: 0.0407430 total: 4.62s remaining: 5.31s 465: learn: 0.0407085 total: 4.63s remaining: 5.3s 466: learn: 0.0406735 total: 4.64s remaining: 5.29s 467: learn: 0.0406326 total: 4.65s remaining: 5.29s 468: learn: 0.0405762 total: 4.66s remaining: 5.28s 469: learn: 0.0405240 total: 4.67s remaining: 5.27s 470: learn: 0.0404874 total: 4.68s remaining: 5.25s 471: learn: 0.0404524 total: 4.69s remaining: 5.24s 472: learn: 0.0404279 total: 4.7s remaining: 5.23s 473: learn: 0.0403601 total: 4.71s remaining: 5.22s 474: learn: 0.0403311 total: 4.72s remaining: 5.21s 475: learn: 0.0402867 total: 4.73s remaining: 5.2s 476: learn: 0.0402498 total: 4.74s remaining: 5.19s 477: learn: 0.0402066 total: 4.74s remaining: 5.18s 478: learn: 0.0401560 total: 4.75s remaining: 5.17s 479: learn: 0.0401444 total: 4.76s remaining: 5.16s 480: learn: 0.0400909 total: 4.77s remaining: 5.15s 481: learn: 0.0400619 total: 4.78s remaining: 5.14s 482: learn: 0.0400269 total: 4.79s remaining: 5.13s 483: learn: 0.0399856 total: 4.8s remaining: 5.12s 484: learn: 0.0399375 total: 4.81s remaining: 5.11s 485: learn: 0.0399113 total: 4.82s remaining: 5.09s 486: learn: 0.0398831 total: 4.83s remaining: 5.08s 487: learn: 0.0398508 total: 4.84s remaining: 5.07s 488: learn: 0.0398107 total: 4.85s remaining: 5.07s 489: learn: 0.0397834 total: 4.86s remaining: 5.06s 490: learn: 0.0397458 total: 4.87s remaining: 5.05s 491: learn: 0.0397157 total: 4.88s remaining: 5.04s 492: learn: 0.0396680 total: 4.9s remaining: 5.04s 493: learn: 0.0396128 total: 4.91s remaining: 5.03s 494: learn: 0.0395734 total: 4.91s remaining: 5.01s 495: learn: 0.0395417 total: 4.92s remaining: 5s 496: learn: 0.0394997 total: 4.93s remaining: 4.99s 497: learn: 0.0394570 total: 4.94s remaining: 4.98s 498: learn: 0.0394301 total: 4.95s remaining: 4.97s 499: learn: 0.0393956 total: 4.96s remaining: 4.96s 500: learn: 0.0393490 total: 4.97s remaining: 4.95s 501: learn: 0.0393189 total: 4.98s remaining: 4.94s 502: learn: 0.0392774 total: 4.99s remaining: 4.93s 503: learn: 0.0392354 total: 5s remaining: 4.92s 504: learn: 0.0391886 total: 5.01s remaining: 4.91s 505: learn: 0.0391674 total: 5.01s remaining: 4.9s 506: learn: 0.0391418 total: 5.02s remaining: 4.89s 507: learn: 0.0390885 total: 5.03s remaining: 4.88s 508: learn: 0.0390330 total: 5.05s remaining: 4.87s 509: learn: 0.0389815 total: 5.06s remaining: 4.86s 510: learn: 0.0389372 total: 5.07s remaining: 4.85s 511: learn: 0.0388929 total: 5.08s remaining: 4.84s 512: learn: 0.0388518 total: 5.09s remaining: 4.83s 513: learn: 0.0388155 total: 5.1s remaining: 4.82s 514: learn: 0.0387670 total: 5.11s remaining: 4.81s 515: learn: 0.0387305 total: 5.12s remaining: 4.8s 516: learn: 0.0386744 total: 5.13s remaining: 4.79s 517: learn: 0.0386345 total: 5.13s remaining: 4.78s 518: learn: 0.0386083 total: 5.15s remaining: 4.77s 519: learn: 0.0385640 total: 5.16s remaining: 4.76s 520: learn: 0.0385397 total: 5.17s remaining: 4.75s 521: learn: 0.0384980 total: 5.18s remaining: 4.74s 522: learn: 0.0384576 total: 5.19s remaining: 4.73s 523: learn: 0.0384196 total: 5.2s remaining: 4.72s 524: learn: 0.0383749 total: 5.2s remaining: 4.71s 525: learn: 0.0383385 total: 5.21s remaining: 4.7s 526: learn: 0.0382974 total: 5.22s remaining: 4.69s 527: learn: 0.0382523 total: 5.23s remaining: 4.68s 528: learn: 0.0382165 total: 5.24s remaining: 4.67s 529: learn: 0.0381763 total: 5.26s remaining: 4.66s 530: learn: 0.0381432 total: 5.27s remaining: 4.65s 531: learn: 0.0381034 total: 5.28s remaining: 4.64s 532: learn: 0.0380680 total: 5.28s remaining: 4.63s 533: learn: 0.0380336 total: 5.29s remaining: 4.62s 534: learn: 0.0379984 total: 5.3s remaining: 4.61s 535: learn: 0.0379655 total: 5.31s remaining: 4.6s 536: learn: 0.0379407 total: 5.32s remaining: 4.59s 537: learn: 0.0378966 total: 5.33s remaining: 4.58s 538: learn: 0.0378630 total: 5.34s remaining: 4.57s 539: learn: 0.0378260 total: 5.35s remaining: 4.56s 540: learn: 0.0378017 total: 5.36s remaining: 4.54s 541: learn: 0.0377661 total: 5.37s remaining: 4.54s 542: learn: 0.0377208 total: 5.38s remaining: 4.52s 543: learn: 0.0376623 total: 5.39s remaining: 4.52s 544: learn: 0.0376519 total: 5.4s remaining: 4.51s 545: learn: 0.0376170 total: 5.41s remaining: 4.5s 546: learn: 0.0376130 total: 5.42s remaining: 4.49s 547: learn: 0.0375823 total: 5.42s remaining: 4.47s 548: learn: 0.0375566 total: 5.43s remaining: 4.46s 549: learn: 0.0375293 total: 5.44s remaining: 4.45s 550: learn: 0.0374935 total: 5.46s remaining: 4.45s 551: learn: 0.0374689 total: 5.47s remaining: 4.44s 552: learn: 0.0374315 total: 5.48s remaining: 4.43s 553: learn: 0.0373819 total: 5.5s remaining: 4.43s 554: learn: 0.0373249 total: 5.51s remaining: 4.42s 555: learn: 0.0373020 total: 5.52s remaining: 4.41s 556: learn: 0.0372710 total: 5.53s remaining: 4.4s 557: learn: 0.0372431 total: 5.54s remaining: 4.38s 558: learn: 0.0372022 total: 5.55s remaining: 4.38s 559: learn: 0.0371811 total: 5.56s remaining: 4.37s 560: learn: 0.0371352 total: 5.57s remaining: 4.36s 561: learn: 0.0370937 total: 5.58s remaining: 4.34s 562: learn: 0.0370631 total: 5.58s remaining: 4.33s 563: learn: 0.0370199 total: 5.6s remaining: 4.33s 564: learn: 0.0369857 total: 5.61s remaining: 4.32s 565: learn: 0.0369565 total: 5.62s remaining: 4.3s 566: learn: 0.0369260 total: 5.62s remaining: 4.29s 567: learn: 0.0368864 total: 5.63s remaining: 4.28s 568: learn: 0.0368610 total: 5.64s remaining: 4.27s 569: learn: 0.0368384 total: 5.65s remaining: 4.26s 570: learn: 0.0368004 total: 5.67s remaining: 4.26s 571: learn: 0.0367596 total: 5.68s remaining: 4.25s 572: learn: 0.0367208 total: 5.69s remaining: 4.24s 573: learn: 0.0366773 total: 5.7s remaining: 4.23s 574: learn: 0.0366591 total: 5.7s remaining: 4.22s 575: learn: 0.0366085 total: 5.71s remaining: 4.21s 576: learn: 0.0365753 total: 5.72s remaining: 4.2s 577: learn: 0.0365420 total: 5.73s remaining: 4.18s 578: learn: 0.0365030 total: 5.74s remaining: 4.17s 579: learn: 0.0364657 total: 5.75s remaining: 4.16s 580: learn: 0.0364457 total: 5.76s remaining: 4.15s 581: learn: 0.0364123 total: 5.77s remaining: 4.14s 582: learn: 0.0363792 total: 5.78s remaining: 4.13s 583: learn: 0.0363609 total: 5.79s remaining: 4.13s 584: learn: 0.0363261 total: 5.8s remaining: 4.11s 585: learn: 0.0362898 total: 5.81s remaining: 4.1s 586: learn: 0.0362536 total: 5.82s remaining: 4.09s 587: learn: 0.0362271 total: 5.83s remaining: 4.08s 588: learn: 0.0361903 total: 5.84s remaining: 4.07s 589: learn: 0.0361509 total: 5.85s remaining: 4.06s 590: learn: 0.0361245 total: 5.86s remaining: 4.06s 591: learn: 0.0360835 total: 5.87s remaining: 4.05s 592: learn: 0.0360599 total: 5.88s remaining: 4.04s 593: learn: 0.0360411 total: 5.89s remaining: 4.03s 594: learn: 0.0360102 total: 5.9s remaining: 4.01s 595: learn: 0.0359643 total: 5.91s remaining: 4s 596: learn: 0.0359246 total: 5.92s remaining: 4s 597: learn: 0.0358902 total: 5.93s remaining: 3.98s 598: learn: 0.0358656 total: 5.94s remaining: 3.97s 599: learn: 0.0358190 total: 5.95s remaining: 3.96s 600: learn: 0.0357885 total: 5.96s remaining: 3.95s 601: learn: 0.0357376 total: 5.96s remaining: 3.94s 602: learn: 0.0357026 total: 5.97s remaining: 3.93s 603: learn: 0.0356875 total: 5.98s remaining: 3.92s 604: learn: 0.0356635 total: 5.99s remaining: 3.91s 605: learn: 0.0356175 total: 6s remaining: 3.9s 606: learn: 0.0355802 total: 6.01s remaining: 3.89s 607: learn: 0.0355610 total: 6.02s remaining: 3.88s 608: learn: 0.0355271 total: 6.03s remaining: 3.87s 609: learn: 0.0354947 total: 6.04s remaining: 3.86s 610: learn: 0.0354790 total: 6.05s remaining: 3.85s 611: learn: 0.0354592 total: 6.07s remaining: 3.85s 612: learn: 0.0354254 total: 6.08s remaining: 3.84s 613: learn: 0.0354008 total: 6.09s remaining: 3.83s 614: learn: 0.0353684 total: 6.1s remaining: 3.82s 615: learn: 0.0353354 total: 6.11s remaining: 3.81s 616: learn: 0.0353054 total: 6.12s remaining: 3.8s 617: learn: 0.0352733 total: 6.13s remaining: 3.79s 618: learn: 0.0352355 total: 6.14s remaining: 3.78s 619: learn: 0.0351981 total: 6.15s remaining: 3.77s 620: learn: 0.0351598 total: 6.16s remaining: 3.76s 621: learn: 0.0351377 total: 6.17s remaining: 3.75s 622: learn: 0.0351173 total: 6.18s remaining: 3.74s 623: learn: 0.0350989 total: 6.19s remaining: 3.73s 624: learn: 0.0350757 total: 6.2s remaining: 3.72s 625: learn: 0.0350461 total: 6.21s remaining: 3.71s 626: learn: 0.0350235 total: 6.21s remaining: 3.7s 627: learn: 0.0349995 total: 6.22s remaining: 3.69s 628: learn: 0.0349728 total: 6.23s remaining: 3.67s 629: learn: 0.0349510 total: 6.24s remaining: 3.67s 630: learn: 0.0349143 total: 6.25s remaining: 3.65s 631: learn: 0.0348745 total: 6.26s remaining: 3.65s 632: learn: 0.0348461 total: 6.28s remaining: 3.64s 633: learn: 0.0348171 total: 6.29s remaining: 3.63s 634: learn: 0.0347853 total: 6.3s remaining: 3.62s 635: learn: 0.0347680 total: 6.31s remaining: 3.61s 636: learn: 0.0347420 total: 6.32s remaining: 3.6s 637: learn: 0.0347178 total: 6.33s remaining: 3.59s 638: learn: 0.0346805 total: 6.33s remaining: 3.58s 639: learn: 0.0346458 total: 6.35s remaining: 3.57s 640: learn: 0.0346134 total: 6.36s remaining: 3.56s 641: learn: 0.0345743 total: 6.37s remaining: 3.55s 642: learn: 0.0345557 total: 6.38s remaining: 3.54s 643: learn: 0.0345277 total: 6.39s remaining: 3.53s 644: learn: 0.0344919 total: 6.39s remaining: 3.52s 645: learn: 0.0344678 total: 6.4s remaining: 3.51s 646: learn: 0.0344430 total: 6.41s remaining: 3.5s 647: learn: 0.0344187 total: 6.42s remaining: 3.49s 648: learn: 0.0344106 total: 6.43s remaining: 3.48s 649: learn: 0.0343606 total: 6.44s remaining: 3.47s 650: learn: 0.0343242 total: 6.45s remaining: 3.46s 651: learn: 0.0342903 total: 6.46s remaining: 3.45s 652: learn: 0.0342688 total: 6.49s remaining: 3.45s 653: learn: 0.0342412 total: 6.5s remaining: 3.44s 654: learn: 0.0342093 total: 6.51s remaining: 3.43s 655: learn: 0.0341839 total: 6.52s remaining: 3.42s 656: learn: 0.0341613 total: 6.53s remaining: 3.41s 657: learn: 0.0341276 total: 6.53s remaining: 3.4s 658: learn: 0.0340925 total: 6.54s remaining: 3.39s 659: learn: 0.0340697 total: 6.55s remaining: 3.38s 660: learn: 0.0340539 total: 6.56s remaining: 3.37s 661: learn: 0.0340232 total: 6.57s remaining: 3.36s 662: learn: 0.0339934 total: 6.58s remaining: 3.35s 663: learn: 0.0339755 total: 6.59s remaining: 3.33s 664: learn: 0.0339511 total: 6.6s remaining: 3.33s 665: learn: 0.0339317 total: 6.61s remaining: 3.31s 666: learn: 0.0339035 total: 6.62s remaining: 3.31s 667: learn: 0.0338875 total: 6.63s remaining: 3.29s 668: learn: 0.0338577 total: 6.64s remaining: 3.28s 669: learn: 0.0338365 total: 6.65s remaining: 3.27s 670: learn: 0.0338092 total: 6.66s remaining: 3.26s 671: learn: 0.0337722 total: 6.67s remaining: 3.26s 672: learn: 0.0337415 total: 6.68s remaining: 3.25s 673: learn: 0.0337164 total: 6.69s remaining: 3.23s 674: learn: 0.0336809 total: 6.7s remaining: 3.23s 675: learn: 0.0336599 total: 6.71s remaining: 3.21s 676: learn: 0.0336318 total: 6.72s remaining: 3.2s 677: learn: 0.0336104 total: 6.73s remaining: 3.19s 678: learn: 0.0335774 total: 6.74s remaining: 3.19s 679: learn: 0.0335502 total: 6.75s remaining: 3.17s 680: learn: 0.0335271 total: 6.76s remaining: 3.17s 681: learn: 0.0335038 total: 6.77s remaining: 3.15s 682: learn: 0.0334667 total: 6.78s remaining: 3.14s 683: learn: 0.0334533 total: 6.78s remaining: 3.13s 684: learn: 0.0334295 total: 6.79s remaining: 3.12s 685: learn: 0.0333997 total: 6.8s remaining: 3.11s 686: learn: 0.0333740 total: 6.81s remaining: 3.1s 687: learn: 0.0333458 total: 6.82s remaining: 3.09s 688: learn: 0.0333240 total: 6.83s remaining: 3.08s 689: learn: 0.0332996 total: 6.84s remaining: 3.07s 690: learn: 0.0332760 total: 6.85s remaining: 3.06s 691: learn: 0.0332358 total: 6.86s remaining: 3.05s 692: learn: 0.0332024 total: 6.88s remaining: 3.05s 693: learn: 0.0331719 total: 6.88s remaining: 3.04s 694: learn: 0.0331440 total: 6.89s remaining: 3.02s 695: learn: 0.0331169 total: 6.9s remaining: 3.02s 696: learn: 0.0331041 total: 6.91s remaining: 3s 697: learn: 0.0330700 total: 6.92s remaining: 2.99s 698: learn: 0.0330472 total: 6.93s remaining: 2.98s 699: learn: 0.0330141 total: 6.94s remaining: 2.97s 700: learn: 0.0329970 total: 6.95s remaining: 2.96s 701: learn: 0.0329659 total: 6.96s remaining: 2.95s 702: learn: 0.0329335 total: 6.97s remaining: 2.94s 703: learn: 0.0329109 total: 6.98s remaining: 2.93s 704: learn: 0.0328801 total: 6.99s remaining: 2.92s 705: learn: 0.0328510 total: 7s remaining: 2.91s 706: learn: 0.0328304 total: 7.01s remaining: 2.9s 707: learn: 0.0328180 total: 7.01s remaining: 2.89s 708: learn: 0.0327903 total: 7.02s remaining: 2.88s 709: learn: 0.0327688 total: 7.03s remaining: 2.87s 710: learn: 0.0327499 total: 7.04s remaining: 2.86s 711: learn: 0.0327251 total: 7.05s remaining: 2.85s 712: learn: 0.0327078 total: 7.06s remaining: 2.84s 713: learn: 0.0326884 total: 7.08s remaining: 2.83s 714: learn: 0.0326629 total: 7.09s remaining: 2.82s 715: learn: 0.0326367 total: 7.09s remaining: 2.81s 716: learn: 0.0326096 total: 7.1s remaining: 2.8s 717: learn: 0.0325968 total: 7.11s remaining: 2.79s 718: learn: 0.0325887 total: 7.12s remaining: 2.78s 719: learn: 0.0325578 total: 7.13s remaining: 2.77s 720: learn: 0.0325332 total: 7.14s remaining: 2.77s 721: learn: 0.0325019 total: 7.15s remaining: 2.75s 722: learn: 0.0324809 total: 7.16s remaining: 2.74s 723: learn: 0.0324600 total: 7.17s remaining: 2.73s 724: learn: 0.0324257 total: 7.18s remaining: 2.72s 725: learn: 0.0323976 total: 7.19s remaining: 2.71s 726: learn: 0.0323958 total: 7.2s remaining: 2.7s 727: learn: 0.0323690 total: 7.21s remaining: 2.69s 728: learn: 0.0323653 total: 7.22s remaining: 2.68s 729: learn: 0.0323290 total: 7.23s remaining: 2.67s 730: learn: 0.0323072 total: 7.24s remaining: 2.66s 731: learn: 0.0322831 total: 7.25s remaining: 2.65s 732: learn: 0.0322597 total: 7.26s remaining: 2.64s 733: learn: 0.0322448 total: 7.27s remaining: 2.63s 734: learn: 0.0322019 total: 7.28s remaining: 2.63s 735: learn: 0.0321840 total: 7.29s remaining: 2.62s 736: learn: 0.0321592 total: 7.3s remaining: 2.61s 737: learn: 0.0321320 total: 7.31s remaining: 2.6s 738: learn: 0.0321048 total: 7.32s remaining: 2.59s 739: learn: 0.0320996 total: 7.33s remaining: 2.58s 740: learn: 0.0320736 total: 7.34s remaining: 2.57s 741: learn: 0.0320510 total: 7.35s remaining: 2.56s 742: learn: 0.0320183 total: 7.36s remaining: 2.55s 743: learn: 0.0319834 total: 7.37s remaining: 2.54s 744: learn: 0.0319604 total: 7.38s remaining: 2.52s 745: learn: 0.0319384 total: 7.39s remaining: 2.52s 746: learn: 0.0319086 total: 7.4s remaining: 2.51s 747: learn: 0.0318897 total: 7.41s remaining: 2.5s 748: learn: 0.0318572 total: 7.42s remaining: 2.49s 749: learn: 0.0318385 total: 7.43s remaining: 2.48s 750: learn: 0.0318156 total: 7.44s remaining: 2.47s 751: learn: 0.0317912 total: 7.46s remaining: 2.46s 752: learn: 0.0317863 total: 7.47s remaining: 2.45s 753: learn: 0.0317499 total: 7.49s remaining: 2.44s 754: learn: 0.0317260 total: 7.5s remaining: 2.43s 755: learn: 0.0316951 total: 7.5s remaining: 2.42s 756: learn: 0.0316702 total: 7.51s remaining: 2.41s 757: learn: 0.0316613 total: 7.52s remaining: 2.4s 758: learn: 0.0316488 total: 7.53s remaining: 2.39s 759: learn: 0.0316215 total: 7.54s remaining: 2.38s 760: learn: 0.0315900 total: 7.55s remaining: 2.37s 761: learn: 0.0315681 total: 7.56s remaining: 2.36s 762: learn: 0.0315474 total: 7.57s remaining: 2.35s 763: learn: 0.0315189 total: 7.58s remaining: 2.34s 764: learn: 0.0315037 total: 7.58s remaining: 2.33s 765: learn: 0.0314779 total: 7.59s remaining: 2.32s 766: learn: 0.0314467 total: 7.6s remaining: 2.31s 767: learn: 0.0314196 total: 7.61s remaining: 2.3s 768: learn: 0.0314008 total: 7.62s remaining: 2.29s 769: learn: 0.0313805 total: 7.63s remaining: 2.28s 770: learn: 0.0313587 total: 7.64s remaining: 2.27s 771: learn: 0.0313368 total: 7.65s remaining: 2.26s 772: learn: 0.0313130 total: 7.66s remaining: 2.25s 773: learn: 0.0312890 total: 7.67s remaining: 2.24s 774: learn: 0.0312674 total: 7.68s remaining: 2.23s 775: learn: 0.0312508 total: 7.7s remaining: 2.22s 776: learn: 0.0312131 total: 7.71s remaining: 2.21s 777: learn: 0.0311881 total: 7.71s remaining: 2.2s 778: learn: 0.0311662 total: 7.72s remaining: 2.19s 779: learn: 0.0311645 total: 7.74s remaining: 2.18s 780: learn: 0.0311435 total: 7.75s remaining: 2.17s 781: learn: 0.0311172 total: 7.75s remaining: 2.16s 782: learn: 0.0310743 total: 7.76s remaining: 2.15s 783: learn: 0.0310542 total: 7.77s remaining: 2.14s 784: learn: 0.0310466 total: 7.78s remaining: 2.13s 785: learn: 0.0310334 total: 7.79s remaining: 2.12s 786: learn: 0.0310228 total: 7.8s remaining: 2.11s 787: learn: 0.0310080 total: 7.81s remaining: 2.1s 788: learn: 0.0309855 total: 7.82s remaining: 2.09s 789: learn: 0.0309599 total: 7.83s remaining: 2.08s 790: learn: 0.0309569 total: 7.84s remaining: 2.07s 791: learn: 0.0309351 total: 7.85s remaining: 2.06s 792: learn: 0.0309149 total: 7.86s remaining: 2.05s 793: learn: 0.0308887 total: 7.87s remaining: 2.04s 794: learn: 0.0308645 total: 7.88s remaining: 2.03s 795: learn: 0.0308451 total: 7.9s remaining: 2.02s 796: learn: 0.0308211 total: 7.91s remaining: 2.01s 797: learn: 0.0307942 total: 7.92s remaining: 2s 798: learn: 0.0307651 total: 7.93s remaining: 1.99s 799: learn: 0.0307438 total: 7.94s remaining: 1.98s 800: learn: 0.0307111 total: 7.95s remaining: 1.97s 801: learn: 0.0306873 total: 7.96s remaining: 1.96s 802: learn: 0.0306589 total: 7.97s remaining: 1.95s 803: learn: 0.0306334 total: 7.98s remaining: 1.94s 804: learn: 0.0306129 total: 7.99s remaining: 1.93s 805: learn: 0.0305915 total: 8s remaining: 1.92s 806: learn: 0.0305563 total: 8.01s remaining: 1.91s 807: learn: 0.0305463 total: 8.01s remaining: 1.9s 808: learn: 0.0305227 total: 8.02s remaining: 1.89s 809: learn: 0.0305034 total: 8.03s remaining: 1.88s 810: learn: 0.0304693 total: 8.04s remaining: 1.87s 811: learn: 0.0304406 total: 8.05s remaining: 1.86s 812: learn: 0.0304147 total: 8.06s remaining: 1.85s 813: learn: 0.0303979 total: 8.07s remaining: 1.84s 814: learn: 0.0303820 total: 8.08s remaining: 1.83s 815: learn: 0.0303596 total: 8.09s remaining: 1.82s 816: learn: 0.0303465 total: 8.1s remaining: 1.81s 817: learn: 0.0303144 total: 8.11s remaining: 1.8s 818: learn: 0.0302903 total: 8.12s remaining: 1.79s 819: learn: 0.0302655 total: 8.13s remaining: 1.78s 820: learn: 0.0302573 total: 8.14s remaining: 1.77s 821: learn: 0.0302338 total: 8.15s remaining: 1.76s 822: learn: 0.0302047 total: 8.16s remaining: 1.75s 823: learn: 0.0301641 total: 8.17s remaining: 1.74s 824: learn: 0.0301359 total: 8.18s remaining: 1.73s 825: learn: 0.0301208 total: 8.18s remaining: 1.72s 826: learn: 0.0301032 total: 8.2s remaining: 1.71s 827: learn: 0.0300918 total: 8.21s remaining: 1.7s 828: learn: 0.0300681 total: 8.21s remaining: 1.69s 829: learn: 0.0300506 total: 8.22s remaining: 1.68s 830: learn: 0.0300250 total: 8.23s remaining: 1.67s 831: learn: 0.0300002 total: 8.24s remaining: 1.66s 832: learn: 0.0299856 total: 8.25s remaining: 1.65s 833: learn: 0.0299702 total: 8.26s remaining: 1.64s 834: learn: 0.0299568 total: 8.27s remaining: 1.63s 835: learn: 0.0299462 total: 8.28s remaining: 1.62s 836: learn: 0.0299273 total: 8.29s remaining: 1.61s 837: learn: 0.0299201 total: 8.3s remaining: 1.6s 838: learn: 0.0298958 total: 8.32s remaining: 1.6s 839: learn: 0.0298808 total: 8.33s remaining: 1.59s 840: learn: 0.0298495 total: 8.34s remaining: 1.58s 841: learn: 0.0298276 total: 8.34s remaining: 1.56s 842: learn: 0.0298096 total: 8.36s remaining: 1.56s 843: learn: 0.0298050 total: 8.37s remaining: 1.55s 844: learn: 0.0297872 total: 8.38s remaining: 1.54s 845: learn: 0.0297656 total: 8.38s remaining: 1.53s 846: learn: 0.0297420 total: 8.39s remaining: 1.52s 847: learn: 0.0297233 total: 8.4s remaining: 1.51s 848: learn: 0.0296996 total: 8.41s remaining: 1.5s 849: learn: 0.0296721 total: 8.43s remaining: 1.49s 850: learn: 0.0296417 total: 8.44s remaining: 1.48s 851: learn: 0.0296251 total: 8.45s remaining: 1.47s 852: learn: 0.0295945 total: 8.46s remaining: 1.46s 853: learn: 0.0295738 total: 8.47s remaining: 1.45s 854: learn: 0.0295528 total: 8.48s remaining: 1.44s 855: learn: 0.0295383 total: 8.48s remaining: 1.43s 856: learn: 0.0295213 total: 8.49s remaining: 1.42s 857: learn: 0.0295071 total: 8.51s remaining: 1.41s 858: learn: 0.0294936 total: 8.52s remaining: 1.4s 859: learn: 0.0294919 total: 8.53s remaining: 1.39s 860: learn: 0.0294801 total: 8.54s remaining: 1.38s 861: learn: 0.0294666 total: 8.54s remaining: 1.37s 862: learn: 0.0294540 total: 8.55s remaining: 1.36s 863: learn: 0.0294222 total: 8.56s remaining: 1.35s 864: learn: 0.0294042 total: 8.57s remaining: 1.34s 865: learn: 0.0293858 total: 8.58s remaining: 1.33s 866: learn: 0.0293711 total: 8.59s remaining: 1.32s 867: learn: 0.0293521 total: 8.6s remaining: 1.31s 868: learn: 0.0293338 total: 8.61s remaining: 1.3s 869: learn: 0.0293149 total: 8.62s remaining: 1.29s 870: learn: 0.0292912 total: 8.63s remaining: 1.28s 871: learn: 0.0292775 total: 8.64s remaining: 1.27s 872: learn: 0.0292485 total: 8.65s remaining: 1.26s 873: learn: 0.0292400 total: 8.66s remaining: 1.25s 874: learn: 0.0292190 total: 8.67s remaining: 1.24s 875: learn: 0.0292027 total: 8.68s remaining: 1.23s 876: learn: 0.0291873 total: 8.69s remaining: 1.22s 877: learn: 0.0291676 total: 8.7s remaining: 1.21s 878: learn: 0.0291530 total: 8.71s remaining: 1.2s 879: learn: 0.0291365 total: 8.72s remaining: 1.19s 880: learn: 0.0291289 total: 8.73s remaining: 1.18s 881: learn: 0.0291085 total: 8.74s remaining: 1.17s 882: learn: 0.0290935 total: 8.75s remaining: 1.16s 883: learn: 0.0290821 total: 8.76s remaining: 1.15s 884: learn: 0.0290532 total: 8.77s remaining: 1.14s 885: learn: 0.0290520 total: 8.77s remaining: 1.13s 886: learn: 0.0290249 total: 8.78s remaining: 1.12s 887: learn: 0.0290051 total: 8.79s remaining: 1.11s 888: learn: 0.0289937 total: 8.8s remaining: 1.1s 889: learn: 0.0289757 total: 8.81s remaining: 1.09s 890: learn: 0.0289575 total: 8.82s remaining: 1.08s 891: learn: 0.0289561 total: 8.83s remaining: 1.07s 892: learn: 0.0289378 total: 8.84s remaining: 1.06s 893: learn: 0.0289300 total: 8.85s remaining: 1.05s 894: learn: 0.0289203 total: 8.86s remaining: 1.04s 895: learn: 0.0289084 total: 8.87s remaining: 1.03s 896: learn: 0.0288888 total: 8.88s remaining: 1.02s 897: learn: 0.0288621 total: 8.89s remaining: 1.01s 898: learn: 0.0288405 total: 8.9s remaining: 1s 899: learn: 0.0288117 total: 8.92s remaining: 991ms 900: learn: 0.0287887 total: 8.93s remaining: 981ms 901: learn: 0.0287605 total: 8.94s remaining: 971ms 902: learn: 0.0287429 total: 8.94s remaining: 961ms 903: learn: 0.0287328 total: 8.95s remaining: 951ms 904: learn: 0.0287196 total: 8.96s remaining: 941ms 905: learn: 0.0287037 total: 8.97s remaining: 931ms 906: learn: 0.0286818 total: 8.98s remaining: 921ms 907: learn: 0.0286636 total: 8.99s remaining: 911ms 908: learn: 0.0286487 total: 9s remaining: 901ms 909: learn: 0.0286374 total: 9.01s remaining: 891ms 910: learn: 0.0286172 total: 9.02s remaining: 881ms 911: learn: 0.0285937 total: 9.02s remaining: 871ms 912: learn: 0.0285798 total: 9.03s remaining: 861ms 913: learn: 0.0285572 total: 9.04s remaining: 851ms 914: learn: 0.0285298 total: 9.05s remaining: 841ms 915: learn: 0.0285012 total: 9.06s remaining: 831ms 916: learn: 0.0284797 total: 9.07s remaining: 821ms 917: learn: 0.0284627 total: 9.08s remaining: 811ms 918: learn: 0.0284442 total: 9.09s remaining: 801ms 919: learn: 0.0284248 total: 9.1s remaining: 791ms 920: learn: 0.0284000 total: 9.12s remaining: 782ms 921: learn: 0.0283762 total: 9.13s remaining: 772ms 922: learn: 0.0283506 total: 9.14s remaining: 762ms 923: learn: 0.0283291 total: 9.15s remaining: 752ms 924: learn: 0.0283084 total: 9.16s remaining: 743ms 925: learn: 0.0282853 total: 9.17s remaining: 733ms 926: learn: 0.0282745 total: 9.18s remaining: 723ms 927: learn: 0.0282543 total: 9.19s remaining: 713ms 928: learn: 0.0282329 total: 9.2s remaining: 703ms 929: learn: 0.0282215 total: 9.2s remaining: 693ms 930: learn: 0.0282043 total: 9.21s remaining: 683ms 931: learn: 0.0281847 total: 9.22s remaining: 673ms 932: learn: 0.0281636 total: 9.23s remaining: 663ms 933: learn: 0.0281445 total: 9.24s remaining: 653ms 934: learn: 0.0281247 total: 9.25s remaining: 643ms 935: learn: 0.0281006 total: 9.26s remaining: 633ms 936: learn: 0.0280832 total: 9.27s remaining: 623ms 937: learn: 0.0280582 total: 9.28s remaining: 613ms 938: learn: 0.0280436 total: 9.29s remaining: 603ms 939: learn: 0.0280333 total: 9.3s remaining: 593ms 940: learn: 0.0280236 total: 9.31s remaining: 584ms 941: learn: 0.0280094 total: 9.32s remaining: 574ms 942: learn: 0.0279864 total: 9.33s remaining: 564ms 943: learn: 0.0279749 total: 9.34s remaining: 554ms 944: learn: 0.0279554 total: 9.35s remaining: 544ms 945: learn: 0.0279330 total: 9.36s remaining: 535ms 946: learn: 0.0279105 total: 9.37s remaining: 525ms 947: learn: 0.0278877 total: 9.38s remaining: 515ms 948: learn: 0.0278736 total: 9.4s remaining: 505ms 949: learn: 0.0278583 total: 9.41s remaining: 495ms 950: learn: 0.0278402 total: 9.42s remaining: 485ms 951: learn: 0.0278253 total: 9.43s remaining: 475ms 952: learn: 0.0278067 total: 9.44s remaining: 466ms 953: learn: 0.0277908 total: 9.45s remaining: 456ms 954: learn: 0.0277737 total: 9.46s remaining: 446ms 955: learn: 0.0277445 total: 9.47s remaining: 436ms 956: learn: 0.0277264 total: 9.47s remaining: 426ms 957: learn: 0.0277166 total: 9.48s remaining: 416ms 958: learn: 0.0277007 total: 9.49s remaining: 406ms 959: learn: 0.0276811 total: 9.5s remaining: 396ms 960: learn: 0.0276610 total: 9.51s remaining: 386ms 961: learn: 0.0276368 total: 9.53s remaining: 376ms 962: learn: 0.0276096 total: 9.54s remaining: 366ms 963: learn: 0.0276002 total: 9.55s remaining: 357ms 964: learn: 0.0275777 total: 9.56s remaining: 347ms 965: learn: 0.0275632 total: 9.56s remaining: 337ms 966: learn: 0.0275502 total: 9.57s remaining: 327ms 967: learn: 0.0275249 total: 9.58s remaining: 317ms 968: learn: 0.0275078 total: 9.59s remaining: 307ms 969: learn: 0.0274836 total: 9.6s remaining: 297ms 970: learn: 0.0274651 total: 9.62s remaining: 287ms 971: learn: 0.0274558 total: 9.63s remaining: 277ms 972: learn: 0.0274333 total: 9.64s remaining: 267ms 973: learn: 0.0274056 total: 9.65s remaining: 258ms 974: learn: 0.0273821 total: 9.66s remaining: 248ms 975: learn: 0.0273617 total: 9.66s remaining: 238ms 976: learn: 0.0273413 total: 9.68s remaining: 228ms 977: learn: 0.0273278 total: 9.69s remaining: 218ms 978: learn: 0.0273127 total: 9.7s remaining: 208ms 979: learn: 0.0272971 total: 9.7s remaining: 198ms 980: learn: 0.0272924 total: 9.71s remaining: 188ms 981: learn: 0.0272668 total: 9.72s remaining: 178ms 982: learn: 0.0272465 total: 9.74s remaining: 168ms 983: learn: 0.0272320 total: 9.75s remaining: 159ms 984: learn: 0.0272146 total: 9.76s remaining: 149ms 985: learn: 0.0272014 total: 9.77s remaining: 139ms 986: learn: 0.0271896 total: 9.78s remaining: 129ms 987: learn: 0.0271832 total: 9.78s remaining: 119ms 988: learn: 0.0271624 total: 9.79s remaining: 109ms 989: learn: 0.0271400 total: 9.8s remaining: 99ms 990: learn: 0.0271214 total: 9.81s remaining: 89.1ms 991: learn: 0.0271109 total: 9.82s remaining: 79.2ms 992: learn: 0.0271004 total: 9.83s remaining: 69.3ms 993: learn: 0.0270726 total: 9.84s remaining: 59.4ms 994: learn: 0.0270644 total: 9.85s remaining: 49.5ms 995: learn: 0.0270512 total: 9.86s remaining: 39.6ms 996: learn: 0.0270236 total: 9.87s remaining: 29.7ms 997: learn: 0.0269987 total: 9.88s remaining: 19.8ms 998: learn: 0.0269667 total: 9.89s remaining: 9.9ms 999: learn: 0.0269519 total: 9.89s remaining: 0us . def blended_predictions(X): return ((0.85 * cat_f.predict(X)) + (0.05 * brr_model.predict(X)) + (0.05 * gbr_model.predict(X)) + (0.05 * stack_model.predict(np.array(X)))) . blend_pred = blended_predictions(X_val) blend_score = rmse(y_val, blend_pred) blend_score . 0.09451955533435485 . test_pred = cat_model_f.predict(test) submission = pd.DataFrame(test_id, columns = [&#39;Id&#39;]) test_pred = np.expm1(test_pred) submission[&#39;SalePrice&#39;] = test_pred submission.head() . Id SalePrice . 0 1461 | 119192.073723 | . 1 1462 | 162598.470642 | . 2 1463 | 183851.523276 | . 3 1464 | 195256.441612 | . 4 1465 | 187760.880810 | . submission.to_csv(&quot;result.csv&quot;, index = False, header = True) . feat_imp = cat_model_f.get_feature_importance(prettified=True) plt.figure(figsize = (12,8)) sns.barplot(feat_imp[&#39;Importances&#39;][:20],feat_imp[&#39;Feature Id&#39;][:20], orient = &#39;h&#39;) plt.show() . /usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variables as keyword args: x, y. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. FutureWarning . https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models/ . cat = CatBoostRegressor() cat_model = cat.fit(X_train,y_train, eval_set = (X_val,y_val), plot=True, verbose = 0) . cat_pred = cat_model.predict(X_val) cat_score = rmse(y_val, cat_pred) cat_score . 0.10526973322950023 . grid = {&#39;iterations&#39;: [1000,6000], &#39;learning_rate&#39;: [0.05, 0.005, 0.0005], &#39;depth&#39;: [4, 6, 10], &#39;l2_leaf_reg&#39;: [1, 3, 5, 9]} final_model = CatBoostRegressor() randomized_search_result = final_model.randomized_search(grid, X = X_train, y= y_train, verbose = False, plot=True) . 스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다. 1000: learn: 0.1734559 test: 0.1980725 best: 0.1980725 (1000) 1001: learn: 0.1731469 test: 0.1977736 best: 0.1977736 (1001) 1002: learn: 0.1728569 test: 0.1974994 best: 0.1974994 (1002) 1003: learn: 0.1725645 test: 0.1972204 best: 0.1972204 (1003) 1004: learn: 0.1722553 test: 0.1969387 best: 0.1969387 (1004) 1005: learn: 0.1719529 test: 0.1966548 best: 0.1966548 (1005) 1006: learn: 0.1716456 test: 0.1963540 best: 0.1963540 (1006) 1007: learn: 0.1713674 test: 0.1960844 best: 0.1960844 (1007) 1008: learn: 0.1710715 test: 0.1958089 best: 0.1958089 (1008) 1009: learn: 0.1707954 test: 0.1955282 best: 0.1955282 (1009) 1010: learn: 0.1705043 test: 0.1952786 best: 0.1952786 (1010) 1011: learn: 0.1702286 test: 0.1950246 best: 0.1950246 (1011) 1012: learn: 0.1699490 test: 0.1947536 best: 0.1947536 (1012) 1013: learn: 0.1696478 test: 0.1945010 best: 0.1945010 (1013) 1014: learn: 0.1693575 test: 0.1942202 best: 0.1942202 (1014) 1015: learn: 0.1690792 test: 0.1939662 best: 0.1939662 (1015) 1016: learn: 0.1687949 test: 0.1937086 best: 0.1937086 (1016) 1017: learn: 0.1685071 test: 0.1934333 best: 0.1934333 (1017) 1018: learn: 0.1682377 test: 0.1931894 best: 0.1931894 (1018) 1019: learn: 0.1679686 test: 0.1929146 best: 0.1929146 (1019) 1020: learn: 0.1677041 test: 0.1926742 best: 0.1926742 (1020) 1021: learn: 0.1674230 test: 0.1924380 best: 0.1924380 (1021) 1022: learn: 0.1671820 test: 0.1921975 best: 0.1921975 (1022) 1023: learn: 0.1668980 test: 0.1919340 best: 0.1919340 (1023) 1024: learn: 0.1666294 test: 0.1916703 best: 0.1916703 (1024) 1025: learn: 0.1663710 test: 0.1914220 best: 0.1914220 (1025) 1026: learn: 0.1661191 test: 0.1911865 best: 0.1911865 (1026) 1027: learn: 0.1658699 test: 0.1909415 best: 0.1909415 (1027) 1028: learn: 0.1656178 test: 0.1906889 best: 0.1906889 (1028) 1029: learn: 0.1653801 test: 0.1904580 best: 0.1904580 (1029) 1030: learn: 0.1651359 test: 0.1902352 best: 0.1902352 (1030) 1031: learn: 0.1648967 test: 0.1900224 best: 0.1900224 (1031) 1032: learn: 0.1646400 test: 0.1897871 best: 0.1897871 (1032) 1033: learn: 0.1643899 test: 0.1895526 best: 0.1895526 (1033) 1034: learn: 0.1641551 test: 0.1893352 best: 0.1893352 (1034) 1035: learn: 0.1639096 test: 0.1891283 best: 0.1891283 (1035) 1036: learn: 0.1636607 test: 0.1888819 best: 0.1888819 (1036) 1037: learn: 0.1634101 test: 0.1886493 best: 0.1886493 (1037) 1038: learn: 0.1631534 test: 0.1884351 best: 0.1884351 (1038) 1039: learn: 0.1629184 test: 0.1882019 best: 0.1882019 (1039) 1040: learn: 0.1626880 test: 0.1879919 best: 0.1879919 (1040) 1041: learn: 0.1624329 test: 0.1877684 best: 0.1877684 (1041) 1042: learn: 0.1621872 test: 0.1875415 best: 0.1875415 (1042) total: 11.6s remaining: 55.1s 1043: learn: 0.1619599 test: 0.1873355 best: 0.1873355 (1043) 1044: learn: 0.1617133 test: 0.1871186 best: 0.1871186 (1044) 1045: learn: 0.1614704 test: 0.1868907 best: 0.1868907 (1045) 1046: learn: 0.1612400 test: 0.1866788 best: 0.1866788 (1046) 1047: learn: 0.1610080 test: 0.1864668 best: 0.1864668 (1047) 1048: learn: 0.1607786 test: 0.1862547 best: 0.1862547 (1048) 1049: learn: 0.1605389 test: 0.1860402 best: 0.1860402 (1049) 1050: learn: 0.1603207 test: 0.1858267 best: 0.1858267 (1050) 1051: learn: 0.1601068 test: 0.1856343 best: 0.1856343 (1051) 1052: learn: 0.1598868 test: 0.1854317 best: 0.1854317 (1052) 1053: learn: 0.1596527 test: 0.1852451 best: 0.1852451 (1053) 1054: learn: 0.1594342 test: 0.1850426 best: 0.1850426 (1054) 1055: learn: 0.1592184 test: 0.1848661 best: 0.1848661 (1055) 1056: learn: 0.1589938 test: 0.1846557 best: 0.1846557 (1056) 1057: learn: 0.1587582 test: 0.1844385 best: 0.1844385 (1057) 1058: learn: 0.1585323 test: 0.1842284 best: 0.1842284 (1058) 1059: learn: 0.1583069 test: 0.1840224 best: 0.1840224 (1059) 1060: learn: 0.1580822 test: 0.1838265 best: 0.1838265 (1060) 1061: learn: 0.1578651 test: 0.1836413 best: 0.1836413 (1061) 1062: learn: 0.1576478 test: 0.1834425 best: 0.1834425 (1062) 1063: learn: 0.1574437 test: 0.1832569 best: 0.1832569 (1063) 1064: learn: 0.1572403 test: 0.1830643 best: 0.1830643 (1064) 1065: learn: 0.1570052 test: 0.1828432 best: 0.1828432 (1065) 1066: learn: 0.1567970 test: 0.1826496 best: 0.1826496 (1066) 1067: learn: 0.1566011 test: 0.1824612 best: 0.1824612 (1067) 1068: learn: 0.1563981 test: 0.1822713 best: 0.1822713 (1068) 1069: learn: 0.1562034 test: 0.1821026 best: 0.1821026 (1069) 1070: learn: 0.1559956 test: 0.1819093 best: 0.1819093 (1070) 1071: learn: 0.1557831 test: 0.1817651 best: 0.1817651 (1071) 1072: learn: 0.1555607 test: 0.1815737 best: 0.1815737 (1072) 1073: learn: 0.1553523 test: 0.1813908 best: 0.1813908 (1073) 1074: learn: 0.1551336 test: 0.1812040 best: 0.1812040 (1074) 1075: learn: 0.1549434 test: 0.1810298 best: 0.1810298 (1075) 1076: learn: 0.1547319 test: 0.1808508 best: 0.1808508 (1076) 1077: learn: 0.1545409 test: 0.1806738 best: 0.1806738 (1077) 1078: learn: 0.1543351 test: 0.1804861 best: 0.1804861 (1078) 1079: learn: 0.1541212 test: 0.1802899 best: 0.1802899 (1079) 1080: learn: 0.1539194 test: 0.1801033 best: 0.1801033 (1080) 1081: learn: 0.1537255 test: 0.1799343 best: 0.1799343 (1081) 1082: learn: 0.1535317 test: 0.1797678 best: 0.1797678 (1082) 1083: learn: 0.1533488 test: 0.1795927 best: 0.1795927 (1083) 1084: learn: 0.1531575 test: 0.1794339 best: 0.1794339 (1084) 1085: learn: 0.1529693 test: 0.1792678 best: 0.1792678 (1085) 1086: learn: 0.1527902 test: 0.1791145 best: 0.1791145 (1086) 1087: learn: 0.1526116 test: 0.1789564 best: 0.1789564 (1087) 1088: learn: 0.1524149 test: 0.1787919 best: 0.1787919 (1088) 1089: learn: 0.1522287 test: 0.1786217 best: 0.1786217 (1089) 1090: learn: 0.1520381 test: 0.1784439 best: 0.1784439 (1090) 1091: learn: 0.1518548 test: 0.1782743 best: 0.1782743 (1091) 1092: learn: 0.1516731 test: 0.1781223 best: 0.1781223 (1092) 1093: learn: 0.1514913 test: 0.1779545 best: 0.1779545 (1093) 1094: learn: 0.1513043 test: 0.1777952 best: 0.1777952 (1094) 1095: learn: 0.1511175 test: 0.1776368 best: 0.1776368 (1095) 1096: learn: 0.1509306 test: 0.1774761 best: 0.1774761 (1096) 1097: learn: 0.1507596 test: 0.1773257 best: 0.1773257 (1097) 1098: learn: 0.1505778 test: 0.1771805 best: 0.1771805 (1098) 1099: learn: 0.1504142 test: 0.1770327 best: 0.1770327 (1099) 1100: learn: 0.1502365 test: 0.1768652 best: 0.1768652 (1100) 1101: learn: 0.1500599 test: 0.1767085 best: 0.1767085 (1101) 1102: learn: 0.1498953 test: 0.1765603 best: 0.1765603 (1102) 1103: learn: 0.1497175 test: 0.1764034 best: 0.1764034 (1103) 1104: learn: 0.1495486 test: 0.1762505 best: 0.1762505 (1104) 1105: learn: 0.1493765 test: 0.1760887 best: 0.1760887 (1105) 1106: learn: 0.1492203 test: 0.1759416 best: 0.1759416 (1106) 1107: learn: 0.1490354 test: 0.1757766 best: 0.1757766 (1107) 1108: learn: 0.1488738 test: 0.1756386 best: 0.1756386 (1108) 1109: learn: 0.1487150 test: 0.1754953 best: 0.1754953 (1109) 1110: learn: 0.1485617 test: 0.1753639 best: 0.1753639 (1110) 1111: learn: 0.1483907 test: 0.1752205 best: 0.1752205 (1111) 1112: learn: 0.1482307 test: 0.1750845 best: 0.1750845 (1112) 1113: learn: 0.1480667 test: 0.1749277 best: 0.1749277 (1113) 1114: learn: 0.1478952 test: 0.1747803 best: 0.1747803 (1114) 1115: learn: 0.1477339 test: 0.1746538 best: 0.1746538 (1115) 1116: learn: 0.1475784 test: 0.1745213 best: 0.1745213 (1116) 1117: learn: 0.1474173 test: 0.1743926 best: 0.1743926 (1117) 1118: learn: 0.1472644 test: 0.1742416 best: 0.1742416 (1118) 1119: learn: 0.1471047 test: 0.1741040 best: 0.1741040 (1119) 1120: learn: 0.1469636 test: 0.1739900 best: 0.1739900 (1120) 1121: learn: 0.1468121 test: 0.1738674 best: 0.1738674 (1121) 1122: learn: 0.1466554 test: 0.1737270 best: 0.1737270 (1122) 1123: learn: 0.1465036 test: 0.1736057 best: 0.1736057 (1123) 1124: learn: 0.1463424 test: 0.1734843 best: 0.1734843 (1124) 1125: learn: 0.1461805 test: 0.1733453 best: 0.1733453 (1125) 1126: learn: 0.1460347 test: 0.1732256 best: 0.1732256 (1126) 1127: learn: 0.1458561 test: 0.1731019 best: 0.1731019 (1127) 1128: learn: 0.1456986 test: 0.1729668 best: 0.1729668 (1128) 1129: learn: 0.1455524 test: 0.1728373 best: 0.1728373 (1129) 1130: learn: 0.1453957 test: 0.1727112 best: 0.1727112 (1130) 1131: learn: 0.1452458 test: 0.1725717 best: 0.1725717 (1131) 1132: learn: 0.1450895 test: 0.1724485 best: 0.1724485 (1132) 1133: learn: 0.1449522 test: 0.1723201 best: 0.1723201 (1133) 1134: learn: 0.1448015 test: 0.1721849 best: 0.1721849 (1134) 1135: learn: 0.1446478 test: 0.1720680 best: 0.1720680 (1135) 1136: learn: 0.1445036 test: 0.1719430 best: 0.1719430 (1136) 1137: learn: 0.1443666 test: 0.1718113 best: 0.1718113 (1137) 1138: learn: 0.1442198 test: 0.1716796 best: 0.1716796 (1138) 1139: learn: 0.1440732 test: 0.1715466 best: 0.1715466 (1139) 1140: learn: 0.1439201 test: 0.1714148 best: 0.1714148 (1140) 1141: learn: 0.1437832 test: 0.1712801 best: 0.1712801 (1141) 1142: learn: 0.1436358 test: 0.1711801 best: 0.1711801 (1142) 1143: learn: 0.1435157 test: 0.1710815 best: 0.1710815 (1143) 1144: learn: 0.1433778 test: 0.1709485 best: 0.1709485 (1144) 1145: learn: 0.1432451 test: 0.1708353 best: 0.1708353 (1145) 1146: learn: 0.1431015 test: 0.1707214 best: 0.1707214 (1146) 1147: learn: 0.1429716 test: 0.1706010 best: 0.1706010 (1147) 1148: learn: 0.1428343 test: 0.1704861 best: 0.1704861 (1148) 1149: learn: 0.1426911 test: 0.1703581 best: 0.1703581 (1149) 1150: learn: 0.1425499 test: 0.1702347 best: 0.1702347 (1150) 1151: learn: 0.1424065 test: 0.1701227 best: 0.1701227 (1151) 1152: learn: 0.1422722 test: 0.1699917 best: 0.1699917 (1152) 1153: learn: 0.1421353 test: 0.1698723 best: 0.1698723 (1153) 1154: learn: 0.1420030 test: 0.1697707 best: 0.1697707 (1154) 1155: learn: 0.1418741 test: 0.1696459 best: 0.1696459 (1155) 1156: learn: 0.1417439 test: 0.1695378 best: 0.1695378 (1156) 1157: learn: 0.1416010 test: 0.1694201 best: 0.1694201 (1157) 1158: learn: 0.1414817 test: 0.1693205 best: 0.1693205 (1158) 1159: learn: 0.1413492 test: 0.1692036 best: 0.1692036 (1159) 1160: learn: 0.1412111 test: 0.1690861 best: 0.1690861 (1160) 1161: learn: 0.1410707 test: 0.1689748 best: 0.1689748 (1161) 1162: learn: 0.1409402 test: 0.1688630 best: 0.1688630 (1162) 1163: learn: 0.1408145 test: 0.1687679 best: 0.1687679 (1163) 1164: learn: 0.1406697 test: 0.1686517 best: 0.1686517 (1164) 1165: learn: 0.1405382 test: 0.1685460 best: 0.1685460 (1165) 1166: learn: 0.1404130 test: 0.1684541 best: 0.1684541 (1166) 1167: learn: 0.1402806 test: 0.1683383 best: 0.1683383 (1167) 1168: learn: 0.1401374 test: 0.1682418 best: 0.1682418 (1168) 1169: learn: 0.1400167 test: 0.1681372 best: 0.1681372 (1169) 1170: learn: 0.1399092 test: 0.1680426 best: 0.1680426 (1170) 1171: learn: 0.1397986 test: 0.1679400 best: 0.1679400 (1171) 1172: learn: 0.1396668 test: 0.1678242 best: 0.1678242 (1172) 1173: learn: 0.1395320 test: 0.1677122 best: 0.1677122 (1173) 1174: learn: 0.1393958 test: 0.1676027 best: 0.1676027 (1174) 1175: learn: 0.1392861 test: 0.1674916 best: 0.1674916 (1175) 1176: learn: 0.1391495 test: 0.1674015 best: 0.1674015 (1176) 1177: learn: 0.1390338 test: 0.1673036 best: 0.1673036 (1177) 1178: learn: 0.1389181 test: 0.1672242 best: 0.1672242 (1178) 1179: learn: 0.1387915 test: 0.1671325 best: 0.1671325 (1179) 1180: learn: 0.1386716 test: 0.1670333 best: 0.1670333 (1180) 1181: learn: 0.1385555 test: 0.1669309 best: 0.1669309 (1181) 1182: learn: 0.1384255 test: 0.1668347 best: 0.1668347 (1182) 1183: learn: 0.1382982 test: 0.1667557 best: 0.1667557 (1183) 1184: learn: 0.1381831 test: 0.1666549 best: 0.1666549 (1184) 1185: learn: 0.1380758 test: 0.1665727 best: 0.1665727 (1185) 1186: learn: 0.1379639 test: 0.1664836 best: 0.1664836 (1186) 1187: learn: 0.1378537 test: 0.1663906 best: 0.1663906 (1187) 1188: learn: 0.1377322 test: 0.1663091 best: 0.1663091 (1188) 1189: learn: 0.1376120 test: 0.1662122 best: 0.1662122 (1189) 1190: learn: 0.1374922 test: 0.1661178 best: 0.1661178 (1190) 1191: learn: 0.1373717 test: 0.1660259 best: 0.1660259 (1191) 1192: learn: 0.1372547 test: 0.1659234 best: 0.1659234 (1192) 1193: learn: 0.1371335 test: 0.1658450 best: 0.1658450 (1193) 1194: learn: 0.1370014 test: 0.1657577 best: 0.1657577 (1194) total: 13.3s remaining: 53.4s 1195: learn: 0.1368846 test: 0.1656658 best: 0.1656658 (1195) 1196: learn: 0.1367654 test: 0.1655886 best: 0.1655886 (1196) 1197: learn: 0.1366591 test: 0.1655109 best: 0.1655109 (1197) 1198: learn: 0.1365486 test: 0.1654161 best: 0.1654161 (1198) 1199: learn: 0.1364477 test: 0.1653267 best: 0.1653267 (1199) 1200: learn: 0.1363444 test: 0.1652561 best: 0.1652561 (1200) 1201: learn: 0.1362331 test: 0.1651671 best: 0.1651671 (1201) 1202: learn: 0.1361289 test: 0.1650985 best: 0.1650985 (1202) 1203: learn: 0.1360236 test: 0.1650129 best: 0.1650129 (1203) 1204: learn: 0.1359096 test: 0.1649259 best: 0.1649259 (1204) 1205: learn: 0.1357948 test: 0.1648375 best: 0.1648375 (1205) 1206: learn: 0.1356875 test: 0.1647483 best: 0.1647483 (1206) 1207: learn: 0.1355800 test: 0.1646688 best: 0.1646688 (1207) 1208: learn: 0.1354674 test: 0.1645910 best: 0.1645910 (1208) 1209: learn: 0.1353597 test: 0.1645028 best: 0.1645028 (1209) 1210: learn: 0.1352553 test: 0.1644274 best: 0.1644274 (1210) 1211: learn: 0.1351625 test: 0.1643511 best: 0.1643511 (1211) 1212: learn: 0.1350699 test: 0.1642616 best: 0.1642616 (1212) 1213: learn: 0.1349567 test: 0.1641929 best: 0.1641929 (1213) 1214: learn: 0.1348459 test: 0.1640917 best: 0.1640917 (1214) 1215: learn: 0.1347459 test: 0.1640110 best: 0.1640110 (1215) 1216: learn: 0.1346401 test: 0.1639307 best: 0.1639307 (1216) 1217: learn: 0.1345472 test: 0.1638701 best: 0.1638701 (1217) 1218: learn: 0.1344484 test: 0.1637944 best: 0.1637944 (1218) 1219: learn: 0.1343369 test: 0.1637162 best: 0.1637162 (1219) 1220: learn: 0.1342277 test: 0.1636434 best: 0.1636434 (1220) 1221: learn: 0.1341200 test: 0.1635713 best: 0.1635713 (1221) 1222: learn: 0.1340100 test: 0.1634851 best: 0.1634851 (1222) 1223: learn: 0.1339136 test: 0.1634022 best: 0.1634022 (1223) 1224: learn: 0.1338043 test: 0.1633109 best: 0.1633109 (1224) 1225: learn: 0.1337125 test: 0.1632449 best: 0.1632449 (1225) 1226: learn: 0.1336130 test: 0.1631692 best: 0.1631692 (1226) 1227: learn: 0.1335083 test: 0.1630808 best: 0.1630808 (1227) 1228: learn: 0.1334065 test: 0.1629968 best: 0.1629968 (1228) 1229: learn: 0.1333158 test: 0.1629198 best: 0.1629198 (1229) 1230: learn: 0.1332225 test: 0.1628499 best: 0.1628499 (1230) 1231: learn: 0.1331265 test: 0.1627708 best: 0.1627708 (1231) 1232: learn: 0.1330357 test: 0.1627143 best: 0.1627143 (1232) 1233: learn: 0.1329391 test: 0.1626433 best: 0.1626433 (1233) 1234: learn: 0.1328443 test: 0.1625641 best: 0.1625641 (1234) 1235: learn: 0.1327612 test: 0.1625154 best: 0.1625154 (1235) 1236: learn: 0.1326612 test: 0.1624350 best: 0.1624350 (1236) 1237: learn: 0.1325557 test: 0.1623587 best: 0.1623587 (1237) 1238: learn: 0.1324620 test: 0.1622788 best: 0.1622788 (1238) 1239: learn: 0.1323765 test: 0.1622034 best: 0.1622034 (1239) 1240: learn: 0.1322929 test: 0.1621431 best: 0.1621431 (1240) 1241: learn: 0.1321879 test: 0.1620751 best: 0.1620751 (1241) 1242: learn: 0.1321013 test: 0.1620082 best: 0.1620082 (1242) 1243: learn: 0.1320075 test: 0.1619442 best: 0.1619442 (1243) 1244: learn: 0.1319069 test: 0.1618783 best: 0.1618783 (1244) 1245: learn: 0.1318150 test: 0.1617984 best: 0.1617984 (1245) 1246: learn: 0.1317238 test: 0.1617273 best: 0.1617273 (1246) 1247: learn: 0.1316270 test: 0.1616763 best: 0.1616763 (1247) 1248: learn: 0.1315371 test: 0.1616098 best: 0.1616098 (1248) 1249: learn: 0.1314485 test: 0.1615410 best: 0.1615410 (1249) 1250: learn: 0.1313569 test: 0.1614652 best: 0.1614652 (1250) 1251: learn: 0.1312609 test: 0.1613963 best: 0.1613963 (1251) 1252: learn: 0.1311684 test: 0.1613191 best: 0.1613191 (1252) 1253: learn: 0.1310840 test: 0.1612708 best: 0.1612708 (1253) 1254: learn: 0.1310033 test: 0.1612102 best: 0.1612102 (1254) 1255: learn: 0.1309171 test: 0.1611359 best: 0.1611359 (1255) 1256: learn: 0.1308079 test: 0.1610552 best: 0.1610552 (1256) 1257: learn: 0.1307154 test: 0.1609862 best: 0.1609862 (1257) 1258: learn: 0.1306206 test: 0.1609189 best: 0.1609189 (1258) 1259: learn: 0.1305423 test: 0.1608678 best: 0.1608678 (1259) 1260: learn: 0.1304621 test: 0.1607944 best: 0.1607944 (1260) 1261: learn: 0.1303747 test: 0.1607131 best: 0.1607131 (1261) 1262: learn: 0.1302748 test: 0.1606555 best: 0.1606555 (1262) 1263: learn: 0.1301913 test: 0.1605980 best: 0.1605980 (1263) 1264: learn: 0.1301077 test: 0.1605354 best: 0.1605354 (1264) 1265: learn: 0.1300247 test: 0.1604760 best: 0.1604760 (1265) 1266: learn: 0.1299420 test: 0.1604178 best: 0.1604178 (1266) 1267: learn: 0.1298541 test: 0.1603577 best: 0.1603577 (1267) 1268: learn: 0.1297707 test: 0.1603069 best: 0.1603069 (1268) 1269: learn: 0.1296891 test: 0.1602494 best: 0.1602494 (1269) 1270: learn: 0.1296006 test: 0.1601865 best: 0.1601865 (1270) 1271: learn: 0.1295176 test: 0.1601247 best: 0.1601247 (1271) 1272: learn: 0.1294260 test: 0.1600695 best: 0.1600695 (1272) 1273: learn: 0.1293457 test: 0.1600121 best: 0.1600121 (1273) 1274: learn: 0.1292595 test: 0.1599579 best: 0.1599579 (1274) 1275: learn: 0.1291806 test: 0.1598944 best: 0.1598944 (1275) 1276: learn: 0.1291062 test: 0.1598399 best: 0.1598399 (1276) 1277: learn: 0.1290231 test: 0.1597753 best: 0.1597753 (1277) 1278: learn: 0.1289450 test: 0.1597230 best: 0.1597230 (1278) 1279: learn: 0.1288613 test: 0.1596677 best: 0.1596677 (1279) 1280: learn: 0.1287685 test: 0.1596160 best: 0.1596160 (1280) 1281: learn: 0.1286917 test: 0.1595684 best: 0.1595684 (1281) 1282: learn: 0.1286059 test: 0.1595014 best: 0.1595014 (1282) 1283: learn: 0.1285264 test: 0.1594418 best: 0.1594418 (1283) 1284: learn: 0.1284517 test: 0.1593724 best: 0.1593724 (1284) 1285: learn: 0.1283645 test: 0.1593107 best: 0.1593107 (1285) 1286: learn: 0.1282770 test: 0.1592710 best: 0.1592710 (1286) 1287: learn: 0.1281988 test: 0.1592106 best: 0.1592106 (1287) 1288: learn: 0.1281152 test: 0.1591685 best: 0.1591685 (1288) 1289: learn: 0.1280316 test: 0.1591166 best: 0.1591166 (1289) 1290: learn: 0.1279457 test: 0.1590533 best: 0.1590533 (1290) 1291: learn: 0.1278726 test: 0.1589969 best: 0.1589969 (1291) total: 14.4s remaining: 52.5s 1292: learn: 0.1277925 test: 0.1589458 best: 0.1589458 (1292) 1293: learn: 0.1277113 test: 0.1589108 best: 0.1589108 (1293) 1294: learn: 0.1276273 test: 0.1588565 best: 0.1588565 (1294) 1295: learn: 0.1275497 test: 0.1588142 best: 0.1588142 (1295) 1296: learn: 0.1274719 test: 0.1587647 best: 0.1587647 (1296) 1297: learn: 0.1273949 test: 0.1587059 best: 0.1587059 (1297) 1298: learn: 0.1273192 test: 0.1586425 best: 0.1586425 (1298) 1299: learn: 0.1272423 test: 0.1585944 best: 0.1585944 (1299) 1300: learn: 0.1271559 test: 0.1585442 best: 0.1585442 (1300) 1301: learn: 0.1270695 test: 0.1584936 best: 0.1584936 (1301) 1302: learn: 0.1269906 test: 0.1584405 best: 0.1584405 (1302) 1303: learn: 0.1269245 test: 0.1583986 best: 0.1583986 (1303) 1304: learn: 0.1268447 test: 0.1583577 best: 0.1583577 (1304) 1305: learn: 0.1267650 test: 0.1583095 best: 0.1583095 (1305) 1306: learn: 0.1266929 test: 0.1582567 best: 0.1582567 (1306) 1307: learn: 0.1266113 test: 0.1582064 best: 0.1582064 (1307) 1308: learn: 0.1265299 test: 0.1581339 best: 0.1581339 (1308) 1309: learn: 0.1264604 test: 0.1580727 best: 0.1580727 (1309) 1310: learn: 0.1263808 test: 0.1580187 best: 0.1580187 (1310) 1311: learn: 0.1263065 test: 0.1579895 best: 0.1579895 (1311) 1312: learn: 0.1262260 test: 0.1579350 best: 0.1579350 (1312) 1313: learn: 0.1261549 test: 0.1578822 best: 0.1578822 (1313) 1314: learn: 0.1260720 test: 0.1578392 best: 0.1578392 (1314) 1315: learn: 0.1260041 test: 0.1577941 best: 0.1577941 (1315) 1316: learn: 0.1259243 test: 0.1577502 best: 0.1577502 (1316) 1317: learn: 0.1258463 test: 0.1576966 best: 0.1576966 (1317) 1318: learn: 0.1257733 test: 0.1576647 best: 0.1576647 (1318) 1319: learn: 0.1256986 test: 0.1576032 best: 0.1576032 (1319) 1320: learn: 0.1256157 test: 0.1575530 best: 0.1575530 (1320) 1321: learn: 0.1255452 test: 0.1575145 best: 0.1575145 (1321) 1322: learn: 0.1254723 test: 0.1574794 best: 0.1574794 (1322) 1323: learn: 0.1253970 test: 0.1574266 best: 0.1574266 (1323) 1324: learn: 0.1253280 test: 0.1573749 best: 0.1573749 (1324) 1325: learn: 0.1252599 test: 0.1573310 best: 0.1573310 (1325) 1326: learn: 0.1251878 test: 0.1572713 best: 0.1572713 (1326) 1327: learn: 0.1251200 test: 0.1572233 best: 0.1572233 (1327) total: 14.9s remaining: 52.5s 1328: learn: 0.1250500 test: 0.1571702 best: 0.1571702 (1328) 1329: learn: 0.1249726 test: 0.1571369 best: 0.1571369 (1329) 1330: learn: 0.1249046 test: 0.1570849 best: 0.1570849 (1330) 1331: learn: 0.1248390 test: 0.1570366 best: 0.1570366 (1331) 1332: learn: 0.1247645 test: 0.1569912 best: 0.1569912 (1332) 1333: learn: 0.1246889 test: 0.1569390 best: 0.1569390 (1333) 1334: learn: 0.1246234 test: 0.1568879 best: 0.1568879 (1334) 1335: learn: 0.1245575 test: 0.1568336 best: 0.1568336 (1335) 1336: learn: 0.1244889 test: 0.1567929 best: 0.1567929 (1336) 1337: learn: 0.1244163 test: 0.1567445 best: 0.1567445 (1337) 1338: learn: 0.1243426 test: 0.1566899 best: 0.1566899 (1338) 1339: learn: 0.1242715 test: 0.1566536 best: 0.1566536 (1339) 1340: learn: 0.1241998 test: 0.1566167 best: 0.1566167 (1340) 1341: learn: 0.1241280 test: 0.1565756 best: 0.1565756 (1341) 1342: learn: 0.1240632 test: 0.1565269 best: 0.1565269 (1342) 1343: learn: 0.1239946 test: 0.1564769 best: 0.1564769 (1343) 1344: learn: 0.1239196 test: 0.1564203 best: 0.1564203 (1344) 1345: learn: 0.1238429 test: 0.1563802 best: 0.1563802 (1345) 1346: learn: 0.1237873 test: 0.1563405 best: 0.1563405 (1346) 1347: learn: 0.1237140 test: 0.1563064 best: 0.1563064 (1347) 1348: learn: 0.1236449 test: 0.1562611 best: 0.1562611 (1348) 1349: learn: 0.1235784 test: 0.1562153 best: 0.1562153 (1349) 1350: learn: 0.1235187 test: 0.1561680 best: 0.1561680 (1350) 1351: learn: 0.1234537 test: 0.1561264 best: 0.1561264 (1351) 1352: learn: 0.1233927 test: 0.1560806 best: 0.1560806 (1352) 1353: learn: 0.1233175 test: 0.1560346 best: 0.1560346 (1353) 1354: learn: 0.1232452 test: 0.1559997 best: 0.1559997 (1354) 1355: learn: 0.1231859 test: 0.1559508 best: 0.1559508 (1355) 1356: learn: 0.1231279 test: 0.1559153 best: 0.1559153 (1356) 1357: learn: 0.1230590 test: 0.1558750 best: 0.1558750 (1357) 1358: learn: 0.1229992 test: 0.1558450 best: 0.1558450 (1358) 1359: learn: 0.1229373 test: 0.1558153 best: 0.1558153 (1359) 1360: learn: 0.1228772 test: 0.1557864 best: 0.1557864 (1360) 1361: learn: 0.1228137 test: 0.1557504 best: 0.1557504 (1361) 1362: learn: 0.1227585 test: 0.1557094 best: 0.1557094 (1362) 1363: learn: 0.1226909 test: 0.1556788 best: 0.1556788 (1363) 1364: learn: 0.1226325 test: 0.1556465 best: 0.1556465 (1364) 1365: learn: 0.1225658 test: 0.1556128 best: 0.1556128 (1365) 1366: learn: 0.1225035 test: 0.1555723 best: 0.1555723 (1366) 1367: learn: 0.1224552 test: 0.1555264 best: 0.1555264 (1367) 1368: learn: 0.1223954 test: 0.1554929 best: 0.1554929 (1368) 1369: learn: 0.1223229 test: 0.1554560 best: 0.1554560 (1369) 1370: learn: 0.1222623 test: 0.1554090 best: 0.1554090 (1370) 1371: learn: 0.1222029 test: 0.1553767 best: 0.1553767 (1371) 1372: learn: 0.1221439 test: 0.1553315 best: 0.1553315 (1372) 1373: learn: 0.1220863 test: 0.1552974 best: 0.1552974 (1373) 1374: learn: 0.1220223 test: 0.1552490 best: 0.1552490 (1374) 1375: learn: 0.1219645 test: 0.1552079 best: 0.1552079 (1375) 1376: learn: 0.1219042 test: 0.1551602 best: 0.1551602 (1376) 1377: learn: 0.1218449 test: 0.1551098 best: 0.1551098 (1377) 1378: learn: 0.1217799 test: 0.1550726 best: 0.1550726 (1378) 1379: learn: 0.1217181 test: 0.1550325 best: 0.1550325 (1379) 1380: learn: 0.1216549 test: 0.1549973 best: 0.1549973 (1380) 1381: learn: 0.1215981 test: 0.1549600 best: 0.1549600 (1381) 1382: learn: 0.1215401 test: 0.1549189 best: 0.1549189 (1382) 1383: learn: 0.1214806 test: 0.1548797 best: 0.1548797 (1383) 1384: learn: 0.1214237 test: 0.1548508 best: 0.1548508 (1384) 1385: learn: 0.1213589 test: 0.1548214 best: 0.1548214 (1385) 1386: learn: 0.1213035 test: 0.1547832 best: 0.1547832 (1386) 1387: learn: 0.1212437 test: 0.1547453 best: 0.1547453 (1387) 1388: learn: 0.1211895 test: 0.1547050 best: 0.1547050 (1388) 1389: learn: 0.1211289 test: 0.1546725 best: 0.1546725 (1389) 1390: learn: 0.1210688 test: 0.1546297 best: 0.1546297 (1390) 1391: learn: 0.1210099 test: 0.1545913 best: 0.1545913 (1391) 1392: learn: 0.1209401 test: 0.1545536 best: 0.1545536 (1392) 1393: learn: 0.1208855 test: 0.1545074 best: 0.1545074 (1393) 1394: learn: 0.1208252 test: 0.1544682 best: 0.1544682 (1394) 1395: learn: 0.1207663 test: 0.1544322 best: 0.1544322 (1395) 1396: learn: 0.1207074 test: 0.1543979 best: 0.1543979 (1396) 1397: learn: 0.1206533 test: 0.1543654 best: 0.1543654 (1397) 1398: learn: 0.1205976 test: 0.1543354 best: 0.1543354 (1398) 1399: learn: 0.1205378 test: 0.1542945 best: 0.1542945 (1399) 1400: learn: 0.1204767 test: 0.1542552 best: 0.1542552 (1400) 1401: learn: 0.1204219 test: 0.1542261 best: 0.1542261 (1401) 1402: learn: 0.1203687 test: 0.1541878 best: 0.1541878 (1402) 1403: learn: 0.1203128 test: 0.1541557 best: 0.1541557 (1403) 1404: learn: 0.1202542 test: 0.1541144 best: 0.1541144 (1404) 1405: learn: 0.1202009 test: 0.1540906 best: 0.1540906 (1405) 1406: learn: 0.1201491 test: 0.1540546 best: 0.1540546 (1406) 1407: learn: 0.1200981 test: 0.1540125 best: 0.1540125 (1407) 1408: learn: 0.1200510 test: 0.1539745 best: 0.1539745 (1408) 1409: learn: 0.1199966 test: 0.1539299 best: 0.1539299 (1409) 1410: learn: 0.1199348 test: 0.1539022 best: 0.1539022 (1410) 1411: learn: 0.1198805 test: 0.1538676 best: 0.1538676 (1411) 1412: learn: 0.1198336 test: 0.1538381 best: 0.1538381 (1412) 1413: learn: 0.1197761 test: 0.1538038 best: 0.1538038 (1413) 1414: learn: 0.1197234 test: 0.1537856 best: 0.1537856 (1414) 1415: learn: 0.1196700 test: 0.1537478 best: 0.1537478 (1415) 1416: learn: 0.1196123 test: 0.1537266 best: 0.1537266 (1416) 1417: learn: 0.1195573 test: 0.1537029 best: 0.1537029 (1417) 1418: learn: 0.1195102 test: 0.1536644 best: 0.1536644 (1418) 1419: learn: 0.1194559 test: 0.1536323 best: 0.1536323 (1419) 1420: learn: 0.1193972 test: 0.1536095 best: 0.1536095 (1420) 1421: learn: 0.1193460 test: 0.1535696 best: 0.1535696 (1421) 1422: learn: 0.1192990 test: 0.1535326 best: 0.1535326 (1422) 1423: learn: 0.1192375 test: 0.1535158 best: 0.1535158 (1423) 1424: learn: 0.1191778 test: 0.1534830 best: 0.1534830 (1424) 1425: learn: 0.1191241 test: 0.1534562 best: 0.1534562 (1425) 1426: learn: 0.1190629 test: 0.1534201 best: 0.1534201 (1426) 1427: learn: 0.1190118 test: 0.1533968 best: 0.1533968 (1427) 1428: learn: 0.1189576 test: 0.1533745 best: 0.1533745 (1428) 1429: learn: 0.1189065 test: 0.1533515 best: 0.1533515 (1429) 1430: learn: 0.1188573 test: 0.1533289 best: 0.1533289 (1430) 1431: learn: 0.1188032 test: 0.1532915 best: 0.1532915 (1431) 1432: learn: 0.1187531 test: 0.1532592 best: 0.1532592 (1432) 1433: learn: 0.1187064 test: 0.1532414 best: 0.1532414 (1433) 1434: learn: 0.1186569 test: 0.1532064 best: 0.1532064 (1434) 1435: learn: 0.1185966 test: 0.1531848 best: 0.1531848 (1435) 1436: learn: 0.1185400 test: 0.1531348 best: 0.1531348 (1436) 1437: learn: 0.1184834 test: 0.1530991 best: 0.1530991 (1437) 1438: learn: 0.1184297 test: 0.1530589 best: 0.1530589 (1438) 1439: learn: 0.1183788 test: 0.1530225 best: 0.1530225 (1439) 1440: learn: 0.1183258 test: 0.1530000 best: 0.1530000 (1440) 1441: learn: 0.1182754 test: 0.1529835 best: 0.1529835 (1441) 1442: learn: 0.1182268 test: 0.1529533 best: 0.1529533 (1442) 1443: learn: 0.1181778 test: 0.1529277 best: 0.1529277 (1443) 1444: learn: 0.1181255 test: 0.1528951 best: 0.1528951 (1444) 1445: learn: 0.1180700 test: 0.1528541 best: 0.1528541 (1445) 1446: learn: 0.1180156 test: 0.1528218 best: 0.1528218 (1446) 1447: learn: 0.1179535 test: 0.1527826 best: 0.1527826 (1447) 1448: learn: 0.1179002 test: 0.1527537 best: 0.1527537 (1448) 1449: learn: 0.1178479 test: 0.1527215 best: 0.1527215 (1449) 1450: learn: 0.1177990 test: 0.1526918 best: 0.1526918 (1450) 1451: learn: 0.1177522 test: 0.1526628 best: 0.1526628 (1451) 1452: learn: 0.1177009 test: 0.1526116 best: 0.1526116 (1452) 1453: learn: 0.1176440 test: 0.1525751 best: 0.1525751 (1453) 1454: learn: 0.1175894 test: 0.1525452 best: 0.1525452 (1454) 1455: learn: 0.1175383 test: 0.1525191 best: 0.1525191 (1455) 1456: learn: 0.1174787 test: 0.1524936 best: 0.1524936 (1456) 1457: learn: 0.1174231 test: 0.1524614 best: 0.1524614 (1457) 1458: learn: 0.1173735 test: 0.1524314 best: 0.1524314 (1458) 1459: learn: 0.1173215 test: 0.1524105 best: 0.1524105 (1459) 1460: learn: 0.1172716 test: 0.1523883 best: 0.1523883 (1460) 1461: learn: 0.1172183 test: 0.1523650 best: 0.1523650 (1461) 1462: learn: 0.1171654 test: 0.1523393 best: 0.1523393 (1462) 1463: learn: 0.1171165 test: 0.1523099 best: 0.1523099 (1463) 1464: learn: 0.1170705 test: 0.1522860 best: 0.1522860 (1464) 1465: learn: 0.1170242 test: 0.1522618 best: 0.1522618 (1465) 1466: learn: 0.1169743 test: 0.1522386 best: 0.1522386 (1466) 1467: learn: 0.1169278 test: 0.1522151 best: 0.1522151 (1467) 1468: learn: 0.1168762 test: 0.1521871 best: 0.1521871 (1468) 1469: learn: 0.1168267 test: 0.1521614 best: 0.1521614 (1469) 1470: learn: 0.1167889 test: 0.1521440 best: 0.1521440 (1470) 1471: learn: 0.1167423 test: 0.1521200 best: 0.1521200 (1471) 1472: learn: 0.1166937 test: 0.1520903 best: 0.1520903 (1472) 1473: learn: 0.1166455 test: 0.1520607 best: 0.1520607 (1473) 1474: learn: 0.1165954 test: 0.1520328 best: 0.1520328 (1474) 1475: learn: 0.1165495 test: 0.1520008 best: 0.1520008 (1475) 1476: learn: 0.1165045 test: 0.1519751 best: 0.1519751 (1476) 1477: learn: 0.1164534 test: 0.1519518 best: 0.1519518 (1477) 1478: learn: 0.1164026 test: 0.1519355 best: 0.1519355 (1478) 1479: learn: 0.1163549 test: 0.1519135 best: 0.1519135 (1479) total: 16.6s remaining: 50.7s 1480: learn: 0.1163148 test: 0.1518869 best: 0.1518869 (1480) 1481: learn: 0.1162677 test: 0.1518555 best: 0.1518555 (1481) 1482: learn: 0.1162141 test: 0.1518278 best: 0.1518278 (1482) 1483: learn: 0.1161691 test: 0.1518031 best: 0.1518031 (1483) 1484: learn: 0.1161187 test: 0.1517703 best: 0.1517703 (1484) 1485: learn: 0.1160665 test: 0.1517503 best: 0.1517503 (1485) 1486: learn: 0.1160185 test: 0.1517162 best: 0.1517162 (1486) 1487: learn: 0.1159770 test: 0.1516930 best: 0.1516930 (1487) 1488: learn: 0.1159288 test: 0.1516652 best: 0.1516652 (1488) 1489: learn: 0.1158840 test: 0.1516321 best: 0.1516321 (1489) 1490: learn: 0.1158368 test: 0.1516102 best: 0.1516102 (1490) 1491: learn: 0.1157923 test: 0.1516013 best: 0.1516013 (1491) 1492: learn: 0.1157393 test: 0.1515750 best: 0.1515750 (1492) 1493: learn: 0.1156935 test: 0.1515697 best: 0.1515697 (1493) 1494: learn: 0.1156436 test: 0.1515573 best: 0.1515573 (1494) 1495: learn: 0.1155998 test: 0.1515467 best: 0.1515467 (1495) 1496: learn: 0.1155494 test: 0.1515125 best: 0.1515125 (1496) 1497: learn: 0.1154986 test: 0.1514938 best: 0.1514938 (1497) 1498: learn: 0.1154597 test: 0.1514695 best: 0.1514695 (1498) 1499: learn: 0.1154143 test: 0.1514525 best: 0.1514525 (1499) 1500: learn: 0.1153635 test: 0.1514330 best: 0.1514330 (1500) 1501: learn: 0.1153169 test: 0.1514094 best: 0.1514094 (1501) 1502: learn: 0.1152748 test: 0.1513961 best: 0.1513961 (1502) 1503: learn: 0.1152262 test: 0.1513852 best: 0.1513852 (1503) 1504: learn: 0.1151782 test: 0.1513564 best: 0.1513564 (1504) 1505: learn: 0.1151372 test: 0.1513281 best: 0.1513281 (1505) 1506: learn: 0.1150904 test: 0.1512965 best: 0.1512965 (1506) 1507: learn: 0.1150480 test: 0.1512611 best: 0.1512611 (1507) 1508: learn: 0.1150054 test: 0.1512439 best: 0.1512439 (1508) 1509: learn: 0.1149594 test: 0.1512155 best: 0.1512155 (1509) 1510: learn: 0.1149152 test: 0.1511926 best: 0.1511926 (1510) 1511: learn: 0.1148739 test: 0.1511680 best: 0.1511680 (1511) 1512: learn: 0.1148313 test: 0.1511350 best: 0.1511350 (1512) 1513: learn: 0.1147905 test: 0.1511200 best: 0.1511200 (1513) 1514: learn: 0.1147443 test: 0.1510962 best: 0.1510962 (1514) 1515: learn: 0.1147011 test: 0.1510770 best: 0.1510770 (1515) 1516: learn: 0.1146516 test: 0.1510522 best: 0.1510522 (1516) 1517: learn: 0.1146086 test: 0.1510240 best: 0.1510240 (1517) 1518: learn: 0.1145703 test: 0.1509945 best: 0.1509945 (1518) 1519: learn: 0.1145270 test: 0.1509703 best: 0.1509703 (1519) 1520: learn: 0.1144807 test: 0.1509412 best: 0.1509412 (1520) 1521: learn: 0.1144316 test: 0.1509222 best: 0.1509222 (1521) 1522: learn: 0.1143923 test: 0.1508973 best: 0.1508973 (1522) 1523: learn: 0.1143500 test: 0.1508708 best: 0.1508708 (1523) 1524: learn: 0.1143082 test: 0.1508465 best: 0.1508465 (1524) 1525: learn: 0.1142688 test: 0.1508316 best: 0.1508316 (1525) 1526: learn: 0.1142296 test: 0.1508169 best: 0.1508169 (1526) 1527: learn: 0.1141892 test: 0.1507902 best: 0.1507902 (1527) 1528: learn: 0.1141532 test: 0.1507832 best: 0.1507832 (1528) 1529: learn: 0.1141131 test: 0.1507543 best: 0.1507543 (1529) 1530: learn: 0.1140739 test: 0.1507396 best: 0.1507396 (1530) 1531: learn: 0.1140330 test: 0.1507253 best: 0.1507253 (1531) 1532: learn: 0.1139914 test: 0.1506923 best: 0.1506923 (1532) 1533: learn: 0.1139454 test: 0.1506623 best: 0.1506623 (1533) 1534: learn: 0.1139046 test: 0.1506409 best: 0.1506409 (1534) 1535: learn: 0.1138637 test: 0.1506342 best: 0.1506342 (1535) 1536: learn: 0.1138249 test: 0.1506309 best: 0.1506309 (1536) 1537: learn: 0.1137825 test: 0.1506153 best: 0.1506153 (1537) 1538: learn: 0.1137404 test: 0.1505989 best: 0.1505989 (1538) 1539: learn: 0.1136959 test: 0.1505766 best: 0.1505766 (1539) 1540: learn: 0.1136560 test: 0.1505531 best: 0.1505531 (1540) 1541: learn: 0.1136195 test: 0.1505349 best: 0.1505349 (1541) 1542: learn: 0.1135752 test: 0.1505167 best: 0.1505167 (1542) 1543: learn: 0.1135291 test: 0.1504916 best: 0.1504916 (1543) 1544: learn: 0.1134889 test: 0.1504733 best: 0.1504733 (1544) 1545: learn: 0.1134472 test: 0.1504495 best: 0.1504495 (1545) 1546: learn: 0.1134051 test: 0.1504217 best: 0.1504217 (1546) 1547: learn: 0.1133661 test: 0.1504064 best: 0.1504064 (1547) 1548: learn: 0.1133293 test: 0.1503841 best: 0.1503841 (1548) 1549: learn: 0.1132855 test: 0.1503661 best: 0.1503661 (1549) 1550: learn: 0.1132477 test: 0.1503396 best: 0.1503396 (1550) 1551: learn: 0.1132143 test: 0.1503306 best: 0.1503306 (1551) 1552: learn: 0.1131744 test: 0.1503199 best: 0.1503199 (1552) 1553: learn: 0.1131403 test: 0.1502970 best: 0.1502970 (1553) 1554: learn: 0.1130975 test: 0.1502774 best: 0.1502774 (1554) 1555: learn: 0.1130621 test: 0.1502566 best: 0.1502566 (1555) 1556: learn: 0.1130273 test: 0.1502431 best: 0.1502431 (1556) 1557: learn: 0.1129898 test: 0.1502159 best: 0.1502159 (1557) 1558: learn: 0.1129431 test: 0.1501942 best: 0.1501942 (1558) 1559: learn: 0.1129000 test: 0.1501743 best: 0.1501743 (1559) 1560: learn: 0.1128605 test: 0.1501577 best: 0.1501577 (1560) 1561: learn: 0.1128202 test: 0.1501381 best: 0.1501381 (1561) 1562: learn: 0.1127715 test: 0.1501078 best: 0.1501078 (1562) 1563: learn: 0.1127272 test: 0.1500831 best: 0.1500831 (1563) 1564: learn: 0.1126914 test: 0.1500744 best: 0.1500744 (1564) 1565: learn: 0.1126500 test: 0.1500622 best: 0.1500622 (1565) 1566: learn: 0.1126117 test: 0.1500374 best: 0.1500374 (1566) 1567: learn: 0.1125756 test: 0.1500115 best: 0.1500115 (1567) 1568: learn: 0.1125363 test: 0.1499850 best: 0.1499850 (1568) 1569: learn: 0.1124916 test: 0.1499651 best: 0.1499651 (1569) 1570: learn: 0.1124571 test: 0.1499484 best: 0.1499484 (1570) 1571: learn: 0.1124177 test: 0.1499241 best: 0.1499241 (1571) 1572: learn: 0.1123846 test: 0.1499099 best: 0.1499099 (1572) 1573: learn: 0.1123502 test: 0.1498864 best: 0.1498864 (1573) 1574: learn: 0.1123217 test: 0.1498809 best: 0.1498809 (1574) 1575: learn: 0.1122845 test: 0.1498600 best: 0.1498600 (1575) 1576: learn: 0.1122465 test: 0.1498423 best: 0.1498423 (1576) 1577: learn: 0.1122072 test: 0.1498321 best: 0.1498321 (1577) 1578: learn: 0.1121652 test: 0.1498192 best: 0.1498192 (1578) 1579: learn: 0.1121272 test: 0.1497942 best: 0.1497942 (1579) 1580: learn: 0.1120884 test: 0.1497735 best: 0.1497735 (1580) 1581: learn: 0.1120446 test: 0.1497481 best: 0.1497481 (1581) 1582: learn: 0.1120046 test: 0.1497269 best: 0.1497269 (1582) 1583: learn: 0.1119712 test: 0.1497140 best: 0.1497140 (1583) 1584: learn: 0.1119338 test: 0.1497071 best: 0.1497071 (1584) 1585: learn: 0.1118991 test: 0.1496913 best: 0.1496913 (1585) 1586: learn: 0.1118585 test: 0.1496677 best: 0.1496677 (1586) 1587: learn: 0.1118229 test: 0.1496469 best: 0.1496469 (1587) 1588: learn: 0.1117885 test: 0.1496329 best: 0.1496329 (1588) 1589: learn: 0.1117563 test: 0.1496182 best: 0.1496182 (1589) 1590: learn: 0.1117121 test: 0.1495963 best: 0.1495963 (1590) 1591: learn: 0.1116706 test: 0.1495816 best: 0.1495816 (1591) 1592: learn: 0.1116348 test: 0.1495621 best: 0.1495621 (1592) 1593: learn: 0.1115950 test: 0.1495558 best: 0.1495558 (1593) 1594: learn: 0.1115587 test: 0.1495349 best: 0.1495349 (1594) 1595: learn: 0.1115224 test: 0.1495124 best: 0.1495124 (1595) 1596: learn: 0.1114847 test: 0.1494941 best: 0.1494941 (1596) 1597: learn: 0.1114488 test: 0.1494759 best: 0.1494759 (1597) 1598: learn: 0.1114083 test: 0.1494582 best: 0.1494582 (1598) 1599: learn: 0.1113693 test: 0.1494430 best: 0.1494430 (1599) 1600: learn: 0.1113306 test: 0.1494187 best: 0.1494187 (1600) 1601: learn: 0.1112949 test: 0.1494018 best: 0.1494018 (1601) 1602: learn: 0.1112558 test: 0.1493753 best: 0.1493753 (1602) 1603: learn: 0.1112195 test: 0.1493598 best: 0.1493598 (1603) 1604: learn: 0.1111833 test: 0.1493350 best: 0.1493350 (1604) 1605: learn: 0.1111447 test: 0.1493154 best: 0.1493154 (1605) 1606: learn: 0.1111086 test: 0.1492953 best: 0.1492953 (1606) 1607: learn: 0.1110688 test: 0.1492904 best: 0.1492904 (1607) 1608: learn: 0.1110335 test: 0.1492628 best: 0.1492628 (1608) 1609: learn: 0.1109955 test: 0.1492377 best: 0.1492377 (1609) 1610: learn: 0.1109554 test: 0.1492158 best: 0.1492158 (1610) total: 18.1s remaining: 49.3s 1611: learn: 0.1109151 test: 0.1491978 best: 0.1491978 (1611) 1612: learn: 0.1108743 test: 0.1491748 best: 0.1491748 (1612) 1613: learn: 0.1108373 test: 0.1491597 best: 0.1491597 (1613) 1614: learn: 0.1108019 test: 0.1491502 best: 0.1491502 (1614) 1615: learn: 0.1107679 test: 0.1491308 best: 0.1491308 (1615) 1616: learn: 0.1107309 test: 0.1491220 best: 0.1491220 (1616) 1617: learn: 0.1106926 test: 0.1490990 best: 0.1490990 (1617) 1618: learn: 0.1106555 test: 0.1490844 best: 0.1490844 (1618) 1619: learn: 0.1106203 test: 0.1490732 best: 0.1490732 (1619) 1620: learn: 0.1105843 test: 0.1490534 best: 0.1490534 (1620) 1621: learn: 0.1105456 test: 0.1490334 best: 0.1490334 (1621) 1622: learn: 0.1105096 test: 0.1490163 best: 0.1490163 (1622) 1623: learn: 0.1104702 test: 0.1490112 best: 0.1490112 (1623) 1624: learn: 0.1104378 test: 0.1489856 best: 0.1489856 (1624) 1625: learn: 0.1104006 test: 0.1489611 best: 0.1489611 (1625) 1626: learn: 0.1103644 test: 0.1489529 best: 0.1489529 (1626) 1627: learn: 0.1103276 test: 0.1489294 best: 0.1489294 (1627) 1628: learn: 0.1102846 test: 0.1489001 best: 0.1489001 (1628) 1629: learn: 0.1102462 test: 0.1488906 best: 0.1488906 (1629) 1630: learn: 0.1102121 test: 0.1488687 best: 0.1488687 (1630) 1631: learn: 0.1101813 test: 0.1488536 best: 0.1488536 (1631) 1632: learn: 0.1101465 test: 0.1488273 best: 0.1488273 (1632) 1633: learn: 0.1101091 test: 0.1488145 best: 0.1488145 (1633) 1634: learn: 0.1100704 test: 0.1488032 best: 0.1488032 (1634) 1635: learn: 0.1100385 test: 0.1487835 best: 0.1487835 (1635) 1636: learn: 0.1100013 test: 0.1487692 best: 0.1487692 (1636) 1637: learn: 0.1099693 test: 0.1487468 best: 0.1487468 (1637) 1638: learn: 0.1099313 test: 0.1487368 best: 0.1487368 (1638) 1639: learn: 0.1098927 test: 0.1487248 best: 0.1487248 (1639) 1640: learn: 0.1098568 test: 0.1487092 best: 0.1487092 (1640) 1641: learn: 0.1098238 test: 0.1486803 best: 0.1486803 (1641) total: 18.5s remaining: 49.2s 1642: learn: 0.1097916 test: 0.1486632 best: 0.1486632 (1642) 1643: learn: 0.1097605 test: 0.1486473 best: 0.1486473 (1643) 1644: learn: 0.1097236 test: 0.1486408 best: 0.1486408 (1644) 1645: learn: 0.1096948 test: 0.1486359 best: 0.1486359 (1645) 1646: learn: 0.1096555 test: 0.1486268 best: 0.1486268 (1646) 1647: learn: 0.1096249 test: 0.1486104 best: 0.1486104 (1647) 1648: learn: 0.1095903 test: 0.1485892 best: 0.1485892 (1648) 1649: learn: 0.1095597 test: 0.1485753 best: 0.1485753 (1649) 1650: learn: 0.1095286 test: 0.1485597 best: 0.1485597 (1650) 1651: learn: 0.1094969 test: 0.1485391 best: 0.1485391 (1651) 1652: learn: 0.1094610 test: 0.1485220 best: 0.1485220 (1652) 1653: learn: 0.1094310 test: 0.1485098 best: 0.1485098 (1653) 1654: learn: 0.1094016 test: 0.1484983 best: 0.1484983 (1654) 1655: learn: 0.1093691 test: 0.1484779 best: 0.1484779 (1655) 1656: learn: 0.1093330 test: 0.1484526 best: 0.1484526 (1656) 1657: learn: 0.1092997 test: 0.1484272 best: 0.1484272 (1657) 1658: learn: 0.1092657 test: 0.1484035 best: 0.1484035 (1658) 1659: learn: 0.1092329 test: 0.1483928 best: 0.1483928 (1659) 1660: learn: 0.1091952 test: 0.1483764 best: 0.1483764 (1660) 1661: learn: 0.1091575 test: 0.1483553 best: 0.1483553 (1661) 1662: learn: 0.1091225 test: 0.1483410 best: 0.1483410 (1662) 1663: learn: 0.1090901 test: 0.1483328 best: 0.1483328 (1663) 1664: learn: 0.1090601 test: 0.1483121 best: 0.1483121 (1664) 1665: learn: 0.1090275 test: 0.1483047 best: 0.1483047 (1665) 1666: learn: 0.1089971 test: 0.1482968 best: 0.1482968 (1666) 1667: learn: 0.1089639 test: 0.1482818 best: 0.1482818 (1667) 1668: learn: 0.1089324 test: 0.1482733 best: 0.1482733 (1668) 1669: learn: 0.1088968 test: 0.1482531 best: 0.1482531 (1669) 1670: learn: 0.1088661 test: 0.1482280 best: 0.1482280 (1670) 1671: learn: 0.1088311 test: 0.1482126 best: 0.1482126 (1671) 1672: learn: 0.1087979 test: 0.1481982 best: 0.1481982 (1672) 1673: learn: 0.1087611 test: 0.1481791 best: 0.1481791 (1673) 1674: learn: 0.1087316 test: 0.1481639 best: 0.1481639 (1674) 1675: learn: 0.1087001 test: 0.1481485 best: 0.1481485 (1675) 1676: learn: 0.1086671 test: 0.1481435 best: 0.1481435 (1676) 1677: learn: 0.1086411 test: 0.1481346 best: 0.1481346 (1677) 1678: learn: 0.1086107 test: 0.1481186 best: 0.1481186 (1678) 1679: learn: 0.1085815 test: 0.1481126 best: 0.1481126 (1679) 1680: learn: 0.1085459 test: 0.1481031 best: 0.1481031 (1680) 1681: learn: 0.1085178 test: 0.1480920 best: 0.1480920 (1681) 1682: learn: 0.1084834 test: 0.1480666 best: 0.1480666 (1682) 1683: learn: 0.1084458 test: 0.1480468 best: 0.1480468 (1683) 1684: learn: 0.1084124 test: 0.1480261 best: 0.1480261 (1684) 1685: learn: 0.1083804 test: 0.1480077 best: 0.1480077 (1685) 1686: learn: 0.1083494 test: 0.1479921 best: 0.1479921 (1686) 1687: learn: 0.1083156 test: 0.1479722 best: 0.1479722 (1687) 1688: learn: 0.1082849 test: 0.1479522 best: 0.1479522 (1688) 1689: learn: 0.1082558 test: 0.1479321 best: 0.1479321 (1689) 1690: learn: 0.1082150 test: 0.1479093 best: 0.1479093 (1690) 1691: learn: 0.1081809 test: 0.1478870 best: 0.1478870 (1691) 1692: learn: 0.1081508 test: 0.1478714 best: 0.1478714 (1692) 1693: learn: 0.1081194 test: 0.1478660 best: 0.1478660 (1693) 1694: learn: 0.1080860 test: 0.1478509 best: 0.1478509 (1694) 1695: learn: 0.1080514 test: 0.1478401 best: 0.1478401 (1695) 1696: learn: 0.1080169 test: 0.1478327 best: 0.1478327 (1696) 1697: learn: 0.1079864 test: 0.1478278 best: 0.1478278 (1697) 1698: learn: 0.1079541 test: 0.1478085 best: 0.1478085 (1698) 1699: learn: 0.1079234 test: 0.1477979 best: 0.1477979 (1699) 1700: learn: 0.1078844 test: 0.1477786 best: 0.1477786 (1700) 1701: learn: 0.1078526 test: 0.1477566 best: 0.1477566 (1701) 1702: learn: 0.1078245 test: 0.1477362 best: 0.1477362 (1702) 1703: learn: 0.1077900 test: 0.1477203 best: 0.1477203 (1703) 1704: learn: 0.1077590 test: 0.1477107 best: 0.1477107 (1704) 1705: learn: 0.1077244 test: 0.1476986 best: 0.1476986 (1705) 1706: learn: 0.1076995 test: 0.1476854 best: 0.1476854 (1706) 1707: learn: 0.1076726 test: 0.1476702 best: 0.1476702 (1707) 1708: learn: 0.1076445 test: 0.1476623 best: 0.1476623 (1708) 1709: learn: 0.1076148 test: 0.1476474 best: 0.1476474 (1709) 1710: learn: 0.1075862 test: 0.1476322 best: 0.1476322 (1710) 1711: learn: 0.1075516 test: 0.1476148 best: 0.1476148 (1711) 1712: learn: 0.1075194 test: 0.1476014 best: 0.1476014 (1712) 1713: learn: 0.1074816 test: 0.1475930 best: 0.1475930 (1713) 1714: learn: 0.1074480 test: 0.1475730 best: 0.1475730 (1714) 1715: learn: 0.1074127 test: 0.1475609 best: 0.1475609 (1715) 1716: learn: 0.1073872 test: 0.1475460 best: 0.1475460 (1716) 1717: learn: 0.1073557 test: 0.1475351 best: 0.1475351 (1717) 1718: learn: 0.1073266 test: 0.1475231 best: 0.1475231 (1718) 1719: learn: 0.1072936 test: 0.1475090 best: 0.1475090 (1719) 1720: learn: 0.1072622 test: 0.1474871 best: 0.1474871 (1720) 1721: learn: 0.1072260 test: 0.1474811 best: 0.1474811 (1721) 1722: learn: 0.1071988 test: 0.1474676 best: 0.1474676 (1722) 1723: learn: 0.1071729 test: 0.1474610 best: 0.1474610 (1723) 1724: learn: 0.1071420 test: 0.1474503 best: 0.1474503 (1724) 1725: learn: 0.1071125 test: 0.1474366 best: 0.1474366 (1725) 1726: learn: 0.1070838 test: 0.1474154 best: 0.1474154 (1726) 1727: learn: 0.1070539 test: 0.1474091 best: 0.1474091 (1727) 1728: learn: 0.1070197 test: 0.1473900 best: 0.1473900 (1728) 1729: learn: 0.1069880 test: 0.1473784 best: 0.1473784 (1729) 1730: learn: 0.1069580 test: 0.1473624 best: 0.1473624 (1730) 1731: learn: 0.1069287 test: 0.1473498 best: 0.1473498 (1731) 1732: learn: 0.1068976 test: 0.1473472 best: 0.1473472 (1732) 1733: learn: 0.1068658 test: 0.1473263 best: 0.1473263 (1733) 1734: learn: 0.1068332 test: 0.1473122 best: 0.1473122 (1734) 1735: learn: 0.1068031 test: 0.1472967 best: 0.1472967 (1735) 1736: learn: 0.1067661 test: 0.1472770 best: 0.1472770 (1736) 1737: learn: 0.1067313 test: 0.1472645 best: 0.1472645 (1737) 1738: learn: 0.1067041 test: 0.1472582 best: 0.1472582 (1738) 1739: learn: 0.1066741 test: 0.1472436 best: 0.1472436 (1739) 1740: learn: 0.1066431 test: 0.1472248 best: 0.1472248 (1740) 1741: learn: 0.1066063 test: 0.1472059 best: 0.1472059 (1741) 1742: learn: 0.1065801 test: 0.1471893 best: 0.1471893 (1742) 1743: learn: 0.1065567 test: 0.1471738 best: 0.1471738 (1743) 1744: learn: 0.1065266 test: 0.1471502 best: 0.1471502 (1744) 1745: learn: 0.1064883 test: 0.1471320 best: 0.1471320 (1745) 1746: learn: 0.1064576 test: 0.1471147 best: 0.1471147 (1746) 1747: learn: 0.1064265 test: 0.1471030 best: 0.1471030 (1747) 1748: learn: 0.1064011 test: 0.1470840 best: 0.1470840 (1748) 1749: learn: 0.1063666 test: 0.1470657 best: 0.1470657 (1749) 1750: learn: 0.1063315 test: 0.1470542 best: 0.1470542 (1750) 1751: learn: 0.1063076 test: 0.1470429 best: 0.1470429 (1751) 1752: learn: 0.1062783 test: 0.1470334 best: 0.1470334 (1752) 1753: learn: 0.1062464 test: 0.1470268 best: 0.1470268 (1753) 1754: learn: 0.1062128 test: 0.1470163 best: 0.1470163 (1754) 1755: learn: 0.1061803 test: 0.1470057 best: 0.1470057 (1755) 1756: learn: 0.1061495 test: 0.1469964 best: 0.1469964 (1756) 1757: learn: 0.1061191 test: 0.1469822 best: 0.1469822 (1757) 1758: learn: 0.1060844 test: 0.1469750 best: 0.1469750 (1758) 1759: learn: 0.1060540 test: 0.1469709 best: 0.1469709 (1759) 1760: learn: 0.1060211 test: 0.1469541 best: 0.1469541 (1760) 1761: learn: 0.1059905 test: 0.1469416 best: 0.1469416 (1761) 1762: learn: 0.1059615 test: 0.1469244 best: 0.1469244 (1762) 1763: learn: 0.1059348 test: 0.1469122 best: 0.1469122 (1763) 1764: learn: 0.1059100 test: 0.1468980 best: 0.1468980 (1764) 1765: learn: 0.1058807 test: 0.1468799 best: 0.1468799 (1765) 1766: learn: 0.1058538 test: 0.1468660 best: 0.1468660 (1766) 1767: learn: 0.1058244 test: 0.1468553 best: 0.1468553 (1767) 1768: learn: 0.1057971 test: 0.1468428 best: 0.1468428 (1768) 1769: learn: 0.1057640 test: 0.1468294 best: 0.1468294 (1769) 1770: learn: 0.1057351 test: 0.1468122 best: 0.1468122 (1770) 1771: learn: 0.1057127 test: 0.1468058 best: 0.1468058 (1771) 1772: learn: 0.1056843 test: 0.1467815 best: 0.1467815 (1772) 1773: learn: 0.1056599 test: 0.1467709 best: 0.1467709 (1773) 1774: learn: 0.1056285 test: 0.1467591 best: 0.1467591 (1774) 1775: learn: 0.1055977 test: 0.1467471 best: 0.1467471 (1775) 1776: learn: 0.1055713 test: 0.1467334 best: 0.1467334 (1776) 1777: learn: 0.1055391 test: 0.1467279 best: 0.1467279 (1777) 1778: learn: 0.1055129 test: 0.1467122 best: 0.1467122 (1778) 1779: learn: 0.1054851 test: 0.1467001 best: 0.1467001 (1779) 1780: learn: 0.1054558 test: 0.1466811 best: 0.1466811 (1780) 1781: learn: 0.1054278 test: 0.1466641 best: 0.1466641 (1781) 1782: learn: 0.1054024 test: 0.1466493 best: 0.1466493 (1782) 1783: learn: 0.1053717 test: 0.1466401 best: 0.1466401 (1783) 1784: learn: 0.1053425 test: 0.1466255 best: 0.1466255 (1784) 1785: learn: 0.1053174 test: 0.1466124 best: 0.1466124 (1785) 1786: learn: 0.1052884 test: 0.1465878 best: 0.1465878 (1786) 1787: learn: 0.1052543 test: 0.1465590 best: 0.1465590 (1787) 1788: learn: 0.1052283 test: 0.1465543 best: 0.1465543 (1788) 1789: learn: 0.1051977 test: 0.1465475 best: 0.1465475 (1789) 1790: learn: 0.1051746 test: 0.1465381 best: 0.1465381 (1790) total: 20.2s remaining: 47.4s 1791: learn: 0.1051409 test: 0.1465146 best: 0.1465146 (1791) 1792: learn: 0.1051102 test: 0.1464969 best: 0.1464969 (1792) 1793: learn: 0.1050821 test: 0.1464835 best: 0.1464835 (1793) 1794: learn: 0.1050578 test: 0.1464710 best: 0.1464710 (1794) 1795: learn: 0.1050285 test: 0.1464605 best: 0.1464605 (1795) 1796: learn: 0.1049994 test: 0.1464546 best: 0.1464546 (1796) 1797: learn: 0.1049693 test: 0.1464419 best: 0.1464419 (1797) 1798: learn: 0.1049408 test: 0.1464258 best: 0.1464258 (1798) 1799: learn: 0.1049114 test: 0.1464092 best: 0.1464092 (1799) 1800: learn: 0.1048837 test: 0.1464018 best: 0.1464018 (1800) 1801: learn: 0.1048578 test: 0.1463934 best: 0.1463934 (1801) 1802: learn: 0.1048320 test: 0.1463714 best: 0.1463714 (1802) 1803: learn: 0.1048030 test: 0.1463579 best: 0.1463579 (1803) 1804: learn: 0.1047735 test: 0.1463541 best: 0.1463541 (1804) 1805: learn: 0.1047445 test: 0.1463414 best: 0.1463414 (1805) 1806: learn: 0.1047217 test: 0.1463206 best: 0.1463206 (1806) 1807: learn: 0.1047021 test: 0.1463137 best: 0.1463137 (1807) 1808: learn: 0.1046763 test: 0.1462923 best: 0.1462923 (1808) 1809: learn: 0.1046498 test: 0.1462853 best: 0.1462853 (1809) 1810: learn: 0.1046198 test: 0.1462782 best: 0.1462782 (1810) 1811: learn: 0.1045948 test: 0.1462647 best: 0.1462647 (1811) 1812: learn: 0.1045659 test: 0.1462565 best: 0.1462565 (1812) 1813: learn: 0.1045376 test: 0.1462436 best: 0.1462436 (1813) 1814: learn: 0.1045141 test: 0.1462371 best: 0.1462371 (1814) 1815: learn: 0.1044876 test: 0.1462312 best: 0.1462312 (1815) 1816: learn: 0.1044602 test: 0.1462238 best: 0.1462238 (1816) 1817: learn: 0.1044253 test: 0.1462118 best: 0.1462118 (1817) 1818: learn: 0.1043998 test: 0.1462009 best: 0.1462009 (1818) 1819: learn: 0.1043672 test: 0.1461856 best: 0.1461856 (1819) 1820: learn: 0.1043424 test: 0.1461689 best: 0.1461689 (1820) 1821: learn: 0.1043163 test: 0.1461586 best: 0.1461586 (1821) 1822: learn: 0.1042927 test: 0.1461499 best: 0.1461499 (1822) 1823: learn: 0.1042618 test: 0.1461395 best: 0.1461395 (1823) 1824: learn: 0.1042334 test: 0.1461298 best: 0.1461298 (1824) 1825: learn: 0.1042068 test: 0.1461194 best: 0.1461194 (1825) 1826: learn: 0.1041792 test: 0.1460992 best: 0.1460992 (1826) 1827: learn: 0.1041529 test: 0.1460884 best: 0.1460884 (1827) 1828: learn: 0.1041266 test: 0.1460714 best: 0.1460714 (1828) 1829: learn: 0.1041017 test: 0.1460533 best: 0.1460533 (1829) 1830: learn: 0.1040740 test: 0.1460443 best: 0.1460443 (1830) 1831: learn: 0.1040476 test: 0.1460278 best: 0.1460278 (1831) 1832: learn: 0.1040184 test: 0.1460226 best: 0.1460226 (1832) 1833: learn: 0.1039948 test: 0.1460069 best: 0.1460069 (1833) 1834: learn: 0.1039711 test: 0.1460004 best: 0.1460004 (1834) 1835: learn: 0.1039466 test: 0.1459909 best: 0.1459909 (1835) 1836: learn: 0.1039157 test: 0.1459845 best: 0.1459845 (1836) 1837: learn: 0.1038869 test: 0.1459728 best: 0.1459728 (1837) 1838: learn: 0.1038581 test: 0.1459623 best: 0.1459623 (1838) 1839: learn: 0.1038265 test: 0.1459441 best: 0.1459441 (1839) 1840: learn: 0.1038011 test: 0.1459460 best: 0.1459441 (1839) 1841: learn: 0.1037722 test: 0.1459367 best: 0.1459367 (1841) 1842: learn: 0.1037425 test: 0.1459304 best: 0.1459304 (1842) 1843: learn: 0.1037196 test: 0.1459233 best: 0.1459233 (1843) 1844: learn: 0.1036889 test: 0.1459090 best: 0.1459090 (1844) 1845: learn: 0.1036608 test: 0.1458990 best: 0.1458990 (1845) 1846: learn: 0.1036310 test: 0.1458811 best: 0.1458811 (1846) 1847: learn: 0.1035996 test: 0.1458648 best: 0.1458648 (1847) 1848: learn: 0.1035752 test: 0.1458469 best: 0.1458469 (1848) 1849: learn: 0.1035455 test: 0.1458311 best: 0.1458311 (1849) 1850: learn: 0.1035160 test: 0.1458175 best: 0.1458175 (1850) 1851: learn: 0.1034893 test: 0.1458015 best: 0.1458015 (1851) 1852: learn: 0.1034604 test: 0.1457899 best: 0.1457899 (1852) 1853: learn: 0.1034395 test: 0.1457675 best: 0.1457675 (1853) 1854: learn: 0.1034067 test: 0.1457586 best: 0.1457586 (1854) 1855: learn: 0.1033796 test: 0.1457472 best: 0.1457472 (1855) 1856: learn: 0.1033495 test: 0.1457399 best: 0.1457399 (1856) 1857: learn: 0.1033248 test: 0.1457329 best: 0.1457329 (1857) 1858: learn: 0.1033024 test: 0.1457257 best: 0.1457257 (1858) 1859: learn: 0.1032791 test: 0.1457154 best: 0.1457154 (1859) 1860: learn: 0.1032565 test: 0.1457136 best: 0.1457136 (1860) 1861: learn: 0.1032287 test: 0.1457013 best: 0.1457013 (1861) 1862: learn: 0.1031980 test: 0.1456926 best: 0.1456926 (1862) 1863: learn: 0.1031751 test: 0.1456863 best: 0.1456863 (1863) 1864: learn: 0.1031420 test: 0.1456772 best: 0.1456772 (1864) 1865: learn: 0.1031161 test: 0.1456698 best: 0.1456698 (1865) 1866: learn: 0.1030966 test: 0.1456605 best: 0.1456605 (1866) 1867: learn: 0.1030723 test: 0.1456415 best: 0.1456415 (1867) 1868: learn: 0.1030466 test: 0.1456233 best: 0.1456233 (1868) 1869: learn: 0.1030196 test: 0.1456135 best: 0.1456135 (1869) 1870: learn: 0.1029922 test: 0.1456035 best: 0.1456035 (1870) 1871: learn: 0.1029661 test: 0.1455880 best: 0.1455880 (1871) 1872: learn: 0.1029376 test: 0.1455824 best: 0.1455824 (1872) 1873: learn: 0.1029062 test: 0.1455721 best: 0.1455721 (1873) 1874: learn: 0.1028765 test: 0.1455564 best: 0.1455564 (1874) 1875: learn: 0.1028503 test: 0.1455415 best: 0.1455415 (1875) 1876: learn: 0.1028275 test: 0.1455266 best: 0.1455266 (1876) 1877: learn: 0.1028001 test: 0.1455144 best: 0.1455144 (1877) 1878: learn: 0.1027776 test: 0.1455030 best: 0.1455030 (1878) 1879: learn: 0.1027531 test: 0.1455059 best: 0.1455030 (1878) 1880: learn: 0.1027307 test: 0.1454962 best: 0.1454962 (1880) 1881: learn: 0.1027036 test: 0.1454857 best: 0.1454857 (1881) 1882: learn: 0.1026786 test: 0.1454724 best: 0.1454724 (1882) 1883: learn: 0.1026474 test: 0.1454620 best: 0.1454620 (1883) 1884: learn: 0.1026246 test: 0.1454440 best: 0.1454440 (1884) 1885: learn: 0.1025967 test: 0.1454301 best: 0.1454301 (1885) 1886: learn: 0.1025720 test: 0.1454154 best: 0.1454154 (1886) 1887: learn: 0.1025492 test: 0.1454024 best: 0.1454024 (1887) 1888: learn: 0.1025264 test: 0.1453883 best: 0.1453883 (1888) 1889: learn: 0.1025006 test: 0.1453739 best: 0.1453739 (1889) 1890: learn: 0.1024741 test: 0.1453599 best: 0.1453599 (1890) 1891: learn: 0.1024433 test: 0.1453497 best: 0.1453497 (1891) 1892: learn: 0.1024186 test: 0.1453336 best: 0.1453336 (1892) 1893: learn: 0.1023886 test: 0.1453208 best: 0.1453208 (1893) 1894: learn: 0.1023609 test: 0.1453026 best: 0.1453026 (1894) 1895: learn: 0.1023375 test: 0.1452851 best: 0.1452851 (1895) 1896: learn: 0.1023128 test: 0.1452860 best: 0.1452851 (1895) 1897: learn: 0.1022901 test: 0.1452792 best: 0.1452792 (1897) 1898: learn: 0.1022629 test: 0.1452636 best: 0.1452636 (1898) 1899: learn: 0.1022344 test: 0.1452438 best: 0.1452438 (1899) 1900: learn: 0.1022037 test: 0.1452340 best: 0.1452340 (1900) 1901: learn: 0.1021762 test: 0.1452125 best: 0.1452125 (1901) 1902: learn: 0.1021540 test: 0.1452016 best: 0.1452016 (1902) 1903: learn: 0.1021277 test: 0.1451880 best: 0.1451880 (1903) 1904: learn: 0.1021040 test: 0.1451790 best: 0.1451790 (1904) 1905: learn: 0.1020778 test: 0.1451745 best: 0.1451745 (1905) 1906: learn: 0.1020519 test: 0.1451648 best: 0.1451648 (1906) 1907: learn: 0.1020254 test: 0.1451474 best: 0.1451474 (1907) 1908: learn: 0.1020023 test: 0.1451335 best: 0.1451335 (1908) 1909: learn: 0.1019742 test: 0.1451061 best: 0.1451061 (1909) 1910: learn: 0.1019477 test: 0.1450948 best: 0.1450948 (1910) 1911: learn: 0.1019205 test: 0.1450862 best: 0.1450862 (1911) 1912: learn: 0.1018953 test: 0.1450840 best: 0.1450840 (1912) 1913: learn: 0.1018683 test: 0.1450683 best: 0.1450683 (1913) 1914: learn: 0.1018444 test: 0.1450587 best: 0.1450587 (1914) 1915: learn: 0.1018182 test: 0.1450487 best: 0.1450487 (1915) 1916: learn: 0.1017950 test: 0.1450345 best: 0.1450345 (1916) 1917: learn: 0.1017722 test: 0.1450215 best: 0.1450215 (1917) 1918: learn: 0.1017458 test: 0.1450100 best: 0.1450100 (1918) 1919: learn: 0.1017214 test: 0.1450020 best: 0.1450020 (1919) 1920: learn: 0.1016991 test: 0.1449857 best: 0.1449857 (1920) 1921: learn: 0.1016733 test: 0.1449672 best: 0.1449672 (1921) 1922: learn: 0.1016488 test: 0.1449564 best: 0.1449564 (1922) 1923: learn: 0.1016190 test: 0.1449443 best: 0.1449443 (1923) 1924: learn: 0.1015948 test: 0.1449408 best: 0.1449408 (1924) 1925: learn: 0.1015754 test: 0.1449387 best: 0.1449387 (1925) 1926: learn: 0.1015513 test: 0.1449332 best: 0.1449332 (1926) 1927: learn: 0.1015272 test: 0.1449217 best: 0.1449217 (1927) 1928: learn: 0.1015005 test: 0.1449099 best: 0.1449099 (1928) 1929: learn: 0.1014733 test: 0.1449063 best: 0.1449063 (1929) 1930: learn: 0.1014522 test: 0.1448994 best: 0.1448994 (1930) 1931: learn: 0.1014266 test: 0.1448924 best: 0.1448924 (1931) 1932: learn: 0.1014054 test: 0.1448781 best: 0.1448781 (1932) 1933: learn: 0.1013822 test: 0.1448685 best: 0.1448685 (1933) 1934: learn: 0.1013544 test: 0.1448624 best: 0.1448624 (1934) 1935: learn: 0.1013339 test: 0.1448553 best: 0.1448553 (1935) 1936: learn: 0.1013101 test: 0.1448399 best: 0.1448399 (1936) 1937: learn: 0.1012822 test: 0.1448273 best: 0.1448273 (1937) 1938: learn: 0.1012565 test: 0.1448197 best: 0.1448197 (1938) 1939: learn: 0.1012295 test: 0.1448083 best: 0.1448083 (1939) 1940: learn: 0.1012063 test: 0.1448060 best: 0.1448060 (1940) 1941: learn: 0.1011829 test: 0.1447983 best: 0.1447983 (1941) 1942: learn: 0.1011604 test: 0.1447883 best: 0.1447883 (1942) total: 21.9s remaining: 45.7s 1943: learn: 0.1011365 test: 0.1447828 best: 0.1447828 (1943) 1944: learn: 0.1011141 test: 0.1447681 best: 0.1447681 (1944) 1945: learn: 0.1010840 test: 0.1447599 best: 0.1447599 (1945) 1946: learn: 0.1010596 test: 0.1447526 best: 0.1447526 (1946) 1947: learn: 0.1010322 test: 0.1447278 best: 0.1447278 (1947) 1948: learn: 0.1010083 test: 0.1447177 best: 0.1447177 (1948) 1949: learn: 0.1009840 test: 0.1447054 best: 0.1447054 (1949) 1950: learn: 0.1009539 test: 0.1446858 best: 0.1446858 (1950) 1951: learn: 0.1009338 test: 0.1446677 best: 0.1446677 (1951) 1952: learn: 0.1009164 test: 0.1446624 best: 0.1446624 (1952) 1953: learn: 0.1008908 test: 0.1446563 best: 0.1446563 (1953) 1954: learn: 0.1008628 test: 0.1446432 best: 0.1446432 (1954) 1955: learn: 0.1008373 test: 0.1446354 best: 0.1446354 (1955) 1956: learn: 0.1008142 test: 0.1446275 best: 0.1446275 (1956) 1957: learn: 0.1007894 test: 0.1446201 best: 0.1446201 (1957) 1958: learn: 0.1007618 test: 0.1446044 best: 0.1446044 (1958) 1959: learn: 0.1007394 test: 0.1445975 best: 0.1445975 (1959) 1960: learn: 0.1007153 test: 0.1445915 best: 0.1445915 (1960) 1961: learn: 0.1006898 test: 0.1445866 best: 0.1445866 (1961) 1962: learn: 0.1006619 test: 0.1445766 best: 0.1445766 (1962) 1963: learn: 0.1006357 test: 0.1445703 best: 0.1445703 (1963) 1964: learn: 0.1006151 test: 0.1445613 best: 0.1445613 (1964) 1965: learn: 0.1005868 test: 0.1445555 best: 0.1445555 (1965) 1966: learn: 0.1005607 test: 0.1445388 best: 0.1445388 (1966) 1967: learn: 0.1005278 test: 0.1445318 best: 0.1445318 (1967) 1968: learn: 0.1004994 test: 0.1445222 best: 0.1445222 (1968) 1969: learn: 0.1004771 test: 0.1445107 best: 0.1445107 (1969) 1970: learn: 0.1004539 test: 0.1445010 best: 0.1445010 (1970) 1971: learn: 0.1004280 test: 0.1444849 best: 0.1444849 (1971) 1972: learn: 0.1004061 test: 0.1444760 best: 0.1444760 (1972) 1973: learn: 0.1003842 test: 0.1444649 best: 0.1444649 (1973) 1974: learn: 0.1003597 test: 0.1444579 best: 0.1444579 (1974) 1975: learn: 0.1003340 test: 0.1444490 best: 0.1444490 (1975) 1976: learn: 0.1003095 test: 0.1444364 best: 0.1444364 (1976) 1977: learn: 0.1002841 test: 0.1444253 best: 0.1444253 (1977) 1978: learn: 0.1002589 test: 0.1444088 best: 0.1444088 (1978) 1979: learn: 0.1002372 test: 0.1443986 best: 0.1443986 (1979) 1980: learn: 0.1002170 test: 0.1443866 best: 0.1443866 (1980) 1981: learn: 0.1001958 test: 0.1443788 best: 0.1443788 (1981) 1982: learn: 0.1001736 test: 0.1443722 best: 0.1443722 (1982) 1983: learn: 0.1001498 test: 0.1443654 best: 0.1443654 (1983) 1984: learn: 0.1001246 test: 0.1443523 best: 0.1443523 (1984) 1985: learn: 0.1001023 test: 0.1443399 best: 0.1443399 (1985) 1986: learn: 0.1000742 test: 0.1443256 best: 0.1443256 (1986) 1987: learn: 0.1000507 test: 0.1443234 best: 0.1443234 (1987) 1988: learn: 0.1000248 test: 0.1443161 best: 0.1443161 (1988) 1989: learn: 0.1000021 test: 0.1443094 best: 0.1443094 (1989) 1990: learn: 0.0999749 test: 0.1443080 best: 0.1443080 (1990) 1991: learn: 0.0999521 test: 0.1442982 best: 0.1442982 (1991) 1992: learn: 0.0999288 test: 0.1442895 best: 0.1442895 (1992) 1993: learn: 0.0999053 test: 0.1442729 best: 0.1442729 (1993) 1994: learn: 0.0998795 test: 0.1442610 best: 0.1442610 (1994) 1995: learn: 0.0998542 test: 0.1442482 best: 0.1442482 (1995) 1996: learn: 0.0998299 test: 0.1442424 best: 0.1442424 (1996) 1997: learn: 0.0998080 test: 0.1442357 best: 0.1442357 (1997) 1998: learn: 0.0997845 test: 0.1442242 best: 0.1442242 (1998) 1999: learn: 0.0997595 test: 0.1442229 best: 0.1442229 (1999) 2000: learn: 0.0997374 test: 0.1442152 best: 0.1442152 (2000) 2001: learn: 0.0997156 test: 0.1442060 best: 0.1442060 (2001) 2002: learn: 0.0996922 test: 0.1441946 best: 0.1441946 (2002) 2003: learn: 0.0996711 test: 0.1441866 best: 0.1441866 (2003) 2004: learn: 0.0996479 test: 0.1441820 best: 0.1441820 (2004) 2005: learn: 0.0996273 test: 0.1441743 best: 0.1441743 (2005) 2006: learn: 0.0996027 test: 0.1441702 best: 0.1441702 (2006) 2007: learn: 0.0995776 test: 0.1441634 best: 0.1441634 (2007) 2008: learn: 0.0995533 test: 0.1441545 best: 0.1441545 (2008) 2009: learn: 0.0995311 test: 0.1441440 best: 0.1441440 (2009) 2010: learn: 0.0995124 test: 0.1441389 best: 0.1441389 (2010) 2011: learn: 0.0994910 test: 0.1441297 best: 0.1441297 (2011) 2012: learn: 0.0994727 test: 0.1441183 best: 0.1441183 (2012) 2013: learn: 0.0994556 test: 0.1441173 best: 0.1441173 (2013) 2014: learn: 0.0994329 test: 0.1441129 best: 0.1441129 (2014) 2015: learn: 0.0994087 test: 0.1441012 best: 0.1441012 (2015) 2016: learn: 0.0993881 test: 0.1440898 best: 0.1440898 (2016) 2017: learn: 0.0993667 test: 0.1440835 best: 0.1440835 (2017) 2018: learn: 0.0993440 test: 0.1440723 best: 0.1440723 (2018) 2019: learn: 0.0993284 test: 0.1440679 best: 0.1440679 (2019) 2020: learn: 0.0993046 test: 0.1440620 best: 0.1440620 (2020) 2021: learn: 0.0992845 test: 0.1440554 best: 0.1440554 (2021) 2022: learn: 0.0992628 test: 0.1440519 best: 0.1440519 (2022) 2023: learn: 0.0992391 test: 0.1440491 best: 0.1440491 (2023) 2024: learn: 0.0992174 test: 0.1440445 best: 0.1440445 (2024) 2025: learn: 0.0991950 test: 0.1440341 best: 0.1440341 (2025) 2026: learn: 0.0991759 test: 0.1440256 best: 0.1440256 (2026) 2027: learn: 0.0991500 test: 0.1440122 best: 0.1440122 (2027) 2028: learn: 0.0991275 test: 0.1439948 best: 0.1439948 (2028) 2029: learn: 0.0991053 test: 0.1439933 best: 0.1439933 (2029) 2030: learn: 0.0990818 test: 0.1439917 best: 0.1439917 (2030) 2031: learn: 0.0990609 test: 0.1439836 best: 0.1439836 (2031) 2032: learn: 0.0990401 test: 0.1439787 best: 0.1439787 (2032) 2033: learn: 0.0990144 test: 0.1439656 best: 0.1439656 (2033) 2034: learn: 0.0989955 test: 0.1439584 best: 0.1439584 (2034) 2035: learn: 0.0989729 test: 0.1439497 best: 0.1439497 (2035) 2036: learn: 0.0989537 test: 0.1439359 best: 0.1439359 (2036) 2037: learn: 0.0989288 test: 0.1439308 best: 0.1439308 (2037) 2038: learn: 0.0989080 test: 0.1439226 best: 0.1439226 (2038) 2039: learn: 0.0988876 test: 0.1439142 best: 0.1439142 (2039) 2040: learn: 0.0988638 test: 0.1439096 best: 0.1439096 (2040) 2041: learn: 0.0988367 test: 0.1439070 best: 0.1439070 (2041) 2042: learn: 0.0988149 test: 0.1438982 best: 0.1438982 (2042) 2043: learn: 0.0987920 test: 0.1438900 best: 0.1438900 (2043) 2044: learn: 0.0987703 test: 0.1438804 best: 0.1438804 (2044) 2045: learn: 0.0987491 test: 0.1438729 best: 0.1438729 (2045) 2046: learn: 0.0987242 test: 0.1438646 best: 0.1438646 (2046) 2047: learn: 0.0987015 test: 0.1438552 best: 0.1438552 (2047) 2048: learn: 0.0986794 test: 0.1438533 best: 0.1438533 (2048) 2049: learn: 0.0986619 test: 0.1438428 best: 0.1438428 (2049) 2050: learn: 0.0986404 test: 0.1438275 best: 0.1438275 (2050) 2051: learn: 0.0986186 test: 0.1438208 best: 0.1438208 (2051) 2052: learn: 0.0985995 test: 0.1438219 best: 0.1438208 (2051) 2053: learn: 0.0985767 test: 0.1438154 best: 0.1438154 (2053) 2054: learn: 0.0985557 test: 0.1438085 best: 0.1438085 (2054) 2055: learn: 0.0985350 test: 0.1438076 best: 0.1438076 (2055) 2056: learn: 0.0985137 test: 0.1437997 best: 0.1437997 (2056) 2057: learn: 0.0984950 test: 0.1437972 best: 0.1437972 (2057) 2058: learn: 0.0984689 test: 0.1437864 best: 0.1437864 (2058) 2059: learn: 0.0984439 test: 0.1437801 best: 0.1437801 (2059) 2060: learn: 0.0984174 test: 0.1437671 best: 0.1437671 (2060) 2061: learn: 0.0983971 test: 0.1437583 best: 0.1437583 (2061) 2062: learn: 0.0983748 test: 0.1437419 best: 0.1437419 (2062) 2063: learn: 0.0983554 test: 0.1437344 best: 0.1437344 (2063) 2064: learn: 0.0983350 test: 0.1437295 best: 0.1437295 (2064) 2065: learn: 0.0983143 test: 0.1437236 best: 0.1437236 (2065) 2066: learn: 0.0982897 test: 0.1437202 best: 0.1437202 (2066) 2067: learn: 0.0982649 test: 0.1437219 best: 0.1437202 (2066) 2068: learn: 0.0982448 test: 0.1437146 best: 0.1437146 (2068) 2069: learn: 0.0982255 test: 0.1437108 best: 0.1437108 (2069) 2070: learn: 0.0982025 test: 0.1436953 best: 0.1436953 (2070) 2071: learn: 0.0981801 test: 0.1436847 best: 0.1436847 (2071) 2072: learn: 0.0981607 test: 0.1436792 best: 0.1436792 (2072) 2073: learn: 0.0981365 test: 0.1436740 best: 0.1436740 (2073) 2074: learn: 0.0981147 test: 0.1436634 best: 0.1436634 (2074) 2075: learn: 0.0980944 test: 0.1436466 best: 0.1436466 (2075) 2076: learn: 0.0980686 test: 0.1436392 best: 0.1436392 (2076) 2077: learn: 0.0980430 test: 0.1436222 best: 0.1436222 (2077) 2078: learn: 0.0980186 test: 0.1436166 best: 0.1436166 (2078) 2079: learn: 0.0980010 test: 0.1436090 best: 0.1436090 (2079) 2080: learn: 0.0979813 test: 0.1436034 best: 0.1436034 (2080) 2081: learn: 0.0979629 test: 0.1435980 best: 0.1435980 (2081) 2082: learn: 0.0979386 test: 0.1435789 best: 0.1435789 (2082) 2083: learn: 0.0979144 test: 0.1435584 best: 0.1435584 (2083) 2084: learn: 0.0978911 test: 0.1435492 best: 0.1435492 (2084) 2085: learn: 0.0978693 test: 0.1435401 best: 0.1435401 (2085) 2086: learn: 0.0978472 test: 0.1435347 best: 0.1435347 (2086) 2087: learn: 0.0978286 test: 0.1435313 best: 0.1435313 (2087) 2088: learn: 0.0978075 test: 0.1435187 best: 0.1435187 (2088) 2089: learn: 0.0977856 test: 0.1435082 best: 0.1435082 (2089) total: 23.5s remaining: 43.9s 2090: learn: 0.0977677 test: 0.1434971 best: 0.1434971 (2090) 2091: learn: 0.0977479 test: 0.1434882 best: 0.1434882 (2091) 2092: learn: 0.0977284 test: 0.1434817 best: 0.1434817 (2092) 2093: learn: 0.0977068 test: 0.1434815 best: 0.1434815 (2093) 2094: learn: 0.0976861 test: 0.1434735 best: 0.1434735 (2094) 2095: learn: 0.0976611 test: 0.1434640 best: 0.1434640 (2095) 2096: learn: 0.0976389 test: 0.1434577 best: 0.1434577 (2096) 2097: learn: 0.0976186 test: 0.1434535 best: 0.1434535 (2097) 2098: learn: 0.0976007 test: 0.1434487 best: 0.1434487 (2098) 2099: learn: 0.0975795 test: 0.1434426 best: 0.1434426 (2099) 2100: learn: 0.0975551 test: 0.1434348 best: 0.1434348 (2100) 2101: learn: 0.0975362 test: 0.1434235 best: 0.1434235 (2101) 2102: learn: 0.0975172 test: 0.1434184 best: 0.1434184 (2102) 2103: learn: 0.0974931 test: 0.1434147 best: 0.1434147 (2103) 2104: learn: 0.0974678 test: 0.1434107 best: 0.1434107 (2104) 2105: learn: 0.0974498 test: 0.1434017 best: 0.1434017 (2105) 2106: learn: 0.0974299 test: 0.1433945 best: 0.1433945 (2106) 2107: learn: 0.0974087 test: 0.1433847 best: 0.1433847 (2107) 2108: learn: 0.0973883 test: 0.1433748 best: 0.1433748 (2108) 2109: learn: 0.0973658 test: 0.1433648 best: 0.1433648 (2109) 2110: learn: 0.0973458 test: 0.1433483 best: 0.1433483 (2110) 2111: learn: 0.0973244 test: 0.1433426 best: 0.1433426 (2111) 2112: learn: 0.0973028 test: 0.1433340 best: 0.1433340 (2112) 2113: learn: 0.0972835 test: 0.1433238 best: 0.1433238 (2113) 2114: learn: 0.0972623 test: 0.1433164 best: 0.1433164 (2114) 2115: learn: 0.0972382 test: 0.1433016 best: 0.1433016 (2115) 2116: learn: 0.0972185 test: 0.1432946 best: 0.1432946 (2116) 2117: learn: 0.0971925 test: 0.1432887 best: 0.1432887 (2117) 2118: learn: 0.0971700 test: 0.1432812 best: 0.1432812 (2118) 2119: learn: 0.0971488 test: 0.1432773 best: 0.1432773 (2119) 2120: learn: 0.0971253 test: 0.1432772 best: 0.1432772 (2120) 2121: learn: 0.0971040 test: 0.1432697 best: 0.1432697 (2121) 2122: learn: 0.0970839 test: 0.1432573 best: 0.1432573 (2122) 2123: learn: 0.0970616 test: 0.1432430 best: 0.1432430 (2123) 2124: learn: 0.0970404 test: 0.1432303 best: 0.1432303 (2124) 2125: learn: 0.0970194 test: 0.1432280 best: 0.1432280 (2125) 2126: learn: 0.0970030 test: 0.1432292 best: 0.1432280 (2125) 2127: learn: 0.0969847 test: 0.1432258 best: 0.1432258 (2127) 2128: learn: 0.0969654 test: 0.1432221 best: 0.1432221 (2128) 2129: learn: 0.0969417 test: 0.1432175 best: 0.1432175 (2129) 2130: learn: 0.0969176 test: 0.1432109 best: 0.1432109 (2130) 2131: learn: 0.0968948 test: 0.1432022 best: 0.1432022 (2131) 2132: learn: 0.0968774 test: 0.1431985 best: 0.1431985 (2132) 2133: learn: 0.0968575 test: 0.1431910 best: 0.1431910 (2133) 2134: learn: 0.0968381 test: 0.1431827 best: 0.1431827 (2134) 2135: learn: 0.0968160 test: 0.1431757 best: 0.1431757 (2135) 2136: learn: 0.0967944 test: 0.1431712 best: 0.1431712 (2136) 2137: learn: 0.0967740 test: 0.1431595 best: 0.1431595 (2137) 2138: learn: 0.0967527 test: 0.1431467 best: 0.1431467 (2138) 2139: learn: 0.0967324 test: 0.1431382 best: 0.1431382 (2139) 2140: learn: 0.0967149 test: 0.1431286 best: 0.1431286 (2140) 2141: learn: 0.0966896 test: 0.1431210 best: 0.1431210 (2141) 2142: learn: 0.0966700 test: 0.1431140 best: 0.1431140 (2142) 2143: learn: 0.0966475 test: 0.1431082 best: 0.1431082 (2143) 2144: learn: 0.0966283 test: 0.1431018 best: 0.1431018 (2144) 2145: learn: 0.0966094 test: 0.1430998 best: 0.1430998 (2145) 2146: learn: 0.0965925 test: 0.1430981 best: 0.1430981 (2146) 2147: learn: 0.0965707 test: 0.1430914 best: 0.1430914 (2147) 2148: learn: 0.0965497 test: 0.1430771 best: 0.1430771 (2148) 2149: learn: 0.0965258 test: 0.1430694 best: 0.1430694 (2149) 2150: learn: 0.0965032 test: 0.1430683 best: 0.1430683 (2150) 2151: learn: 0.0964826 test: 0.1430633 best: 0.1430633 (2151) 2152: learn: 0.0964611 test: 0.1430542 best: 0.1430542 (2152) 2153: learn: 0.0964412 test: 0.1430486 best: 0.1430486 (2153) 2154: learn: 0.0964193 test: 0.1430351 best: 0.1430351 (2154) 2155: learn: 0.0964030 test: 0.1430290 best: 0.1430290 (2155) 2156: learn: 0.0963845 test: 0.1430221 best: 0.1430221 (2156) 2157: learn: 0.0963654 test: 0.1430123 best: 0.1430123 (2157) 2158: learn: 0.0963476 test: 0.1429988 best: 0.1429988 (2158) 2159: learn: 0.0963255 test: 0.1429988 best: 0.1429988 (2158) 2160: learn: 0.0963047 test: 0.1429925 best: 0.1429925 (2160) 2161: learn: 0.0962867 test: 0.1429844 best: 0.1429844 (2161) 2162: learn: 0.0962689 test: 0.1429749 best: 0.1429749 (2162) 2163: learn: 0.0962448 test: 0.1429716 best: 0.1429716 (2163) 2164: learn: 0.0962266 test: 0.1429715 best: 0.1429715 (2164) 2165: learn: 0.0962062 test: 0.1429577 best: 0.1429577 (2165) 2166: learn: 0.0961889 test: 0.1429483 best: 0.1429483 (2166) 2167: learn: 0.0961684 test: 0.1429404 best: 0.1429404 (2167) 2168: learn: 0.0961488 test: 0.1429353 best: 0.1429353 (2168) 2169: learn: 0.0961273 test: 0.1429300 best: 0.1429300 (2169) 2170: learn: 0.0961081 test: 0.1429244 best: 0.1429244 (2170) 2171: learn: 0.0960897 test: 0.1429239 best: 0.1429239 (2171) 2172: learn: 0.0960709 test: 0.1429142 best: 0.1429142 (2172) 2173: learn: 0.0960537 test: 0.1429048 best: 0.1429048 (2173) 2174: learn: 0.0960348 test: 0.1428955 best: 0.1428955 (2174) 2175: learn: 0.0960167 test: 0.1428933 best: 0.1428933 (2175) 2176: learn: 0.0959986 test: 0.1428896 best: 0.1428896 (2176) 2177: learn: 0.0959800 test: 0.1428793 best: 0.1428793 (2177) 2178: learn: 0.0959569 test: 0.1428738 best: 0.1428738 (2178) 2179: learn: 0.0959390 test: 0.1428730 best: 0.1428730 (2179) 2180: learn: 0.0959204 test: 0.1428717 best: 0.1428717 (2180) 2181: learn: 0.0958999 test: 0.1428600 best: 0.1428600 (2181) 2182: learn: 0.0958820 test: 0.1428566 best: 0.1428566 (2182) 2183: learn: 0.0958613 test: 0.1428497 best: 0.1428497 (2183) 2184: learn: 0.0958452 test: 0.1428335 best: 0.1428335 (2184) 2185: learn: 0.0958216 test: 0.1428267 best: 0.1428267 (2185) 2186: learn: 0.0957997 test: 0.1428198 best: 0.1428198 (2186) 2187: learn: 0.0957794 test: 0.1428163 best: 0.1428163 (2187) 2188: learn: 0.0957604 test: 0.1428095 best: 0.1428095 (2188) 2189: learn: 0.0957430 test: 0.1428024 best: 0.1428024 (2189) 2190: learn: 0.0957228 test: 0.1427974 best: 0.1427974 (2190) 2191: learn: 0.0957022 test: 0.1427839 best: 0.1427839 (2191) 2192: learn: 0.0956793 test: 0.1427732 best: 0.1427732 (2192) 2193: learn: 0.0956602 test: 0.1427731 best: 0.1427731 (2193) 2194: learn: 0.0956387 test: 0.1427674 best: 0.1427674 (2194) 2195: learn: 0.0956193 test: 0.1427535 best: 0.1427535 (2195) 2196: learn: 0.0955985 test: 0.1427487 best: 0.1427487 (2196) 2197: learn: 0.0955764 test: 0.1427340 best: 0.1427340 (2197) 2198: learn: 0.0955572 test: 0.1427310 best: 0.1427310 (2198) 2199: learn: 0.0955346 test: 0.1427178 best: 0.1427178 (2199) 2200: learn: 0.0955134 test: 0.1427139 best: 0.1427139 (2200) 2201: learn: 0.0954910 test: 0.1426988 best: 0.1426988 (2201) 2202: learn: 0.0954695 test: 0.1426893 best: 0.1426893 (2202) 2203: learn: 0.0954481 test: 0.1426844 best: 0.1426844 (2203) 2204: learn: 0.0954270 test: 0.1426795 best: 0.1426795 (2204) 2205: learn: 0.0954037 test: 0.1426777 best: 0.1426777 (2205) 2206: learn: 0.0953858 test: 0.1426741 best: 0.1426741 (2206) 2207: learn: 0.0953655 test: 0.1426599 best: 0.1426599 (2207) 2208: learn: 0.0953460 test: 0.1426556 best: 0.1426556 (2208) total: 24.8s remaining: 42.6s 2209: learn: 0.0953267 test: 0.1426453 best: 0.1426453 (2209) 2210: learn: 0.0953089 test: 0.1426385 best: 0.1426385 (2210) 2211: learn: 0.0952851 test: 0.1426251 best: 0.1426251 (2211) 2212: learn: 0.0952620 test: 0.1426143 best: 0.1426143 (2212) 2213: learn: 0.0952417 test: 0.1426026 best: 0.1426026 (2213) 2214: learn: 0.0952218 test: 0.1425961 best: 0.1425961 (2214) 2215: learn: 0.0952015 test: 0.1425865 best: 0.1425865 (2215) 2216: learn: 0.0951810 test: 0.1425853 best: 0.1425853 (2216) 2217: learn: 0.0951611 test: 0.1425727 best: 0.1425727 (2217) 2218: learn: 0.0951392 test: 0.1425691 best: 0.1425691 (2218) 2219: learn: 0.0951225 test: 0.1425631 best: 0.1425631 (2219) 2220: learn: 0.0951022 test: 0.1425551 best: 0.1425551 (2220) 2221: learn: 0.0950845 test: 0.1425449 best: 0.1425449 (2221) 2222: learn: 0.0950638 test: 0.1425319 best: 0.1425319 (2222) 2223: learn: 0.0950477 test: 0.1425221 best: 0.1425221 (2223) 2224: learn: 0.0950309 test: 0.1425192 best: 0.1425192 (2224) 2225: learn: 0.0950124 test: 0.1425123 best: 0.1425123 (2225) 2226: learn: 0.0949975 test: 0.1425001 best: 0.1425001 (2226) 2227: learn: 0.0949735 test: 0.1424929 best: 0.1424929 (2227) 2228: learn: 0.0949531 test: 0.1424861 best: 0.1424861 (2228) 2229: learn: 0.0949328 test: 0.1424788 best: 0.1424788 (2229) 2230: learn: 0.0949109 test: 0.1424795 best: 0.1424788 (2229) 2231: learn: 0.0948940 test: 0.1424788 best: 0.1424788 (2231) 2232: learn: 0.0948766 test: 0.1424736 best: 0.1424736 (2232) 2233: learn: 0.0948558 test: 0.1424648 best: 0.1424648 (2233) 2234: learn: 0.0948365 test: 0.1424577 best: 0.1424577 (2234) 2235: learn: 0.0948199 test: 0.1424529 best: 0.1424529 (2235) 2236: learn: 0.0948017 test: 0.1424511 best: 0.1424511 (2236) 2237: learn: 0.0947837 test: 0.1424451 best: 0.1424451 (2237) 2238: learn: 0.0947610 test: 0.1424308 best: 0.1424308 (2238) 2239: learn: 0.0947436 test: 0.1424219 best: 0.1424219 (2239) 2240: learn: 0.0947206 test: 0.1424195 best: 0.1424195 (2240) 2241: learn: 0.0946990 test: 0.1424109 best: 0.1424109 (2241) 2242: learn: 0.0946782 test: 0.1424081 best: 0.1424081 (2242) 2243: learn: 0.0946608 test: 0.1424046 best: 0.1424046 (2243) 2244: learn: 0.0946422 test: 0.1423968 best: 0.1423968 (2244) 2245: learn: 0.0946209 test: 0.1424011 best: 0.1423968 (2244) 2246: learn: 0.0945980 test: 0.1423989 best: 0.1423968 (2244) 2247: learn: 0.0945836 test: 0.1423987 best: 0.1423968 (2244) 2248: learn: 0.0945636 test: 0.1423925 best: 0.1423925 (2248) 2249: learn: 0.0945425 test: 0.1423858 best: 0.1423858 (2249) 2250: learn: 0.0945201 test: 0.1423749 best: 0.1423749 (2250) total: 25.4s remaining: 42.3s 2251: learn: 0.0944999 test: 0.1423713 best: 0.1423713 (2251) 2252: learn: 0.0944762 test: 0.1423592 best: 0.1423592 (2252) 2253: learn: 0.0944558 test: 0.1423472 best: 0.1423472 (2253) 2254: learn: 0.0944374 test: 0.1423424 best: 0.1423424 (2254) 2255: learn: 0.0944193 test: 0.1423335 best: 0.1423335 (2255) 2256: learn: 0.0944028 test: 0.1423288 best: 0.1423288 (2256) 2257: learn: 0.0943785 test: 0.1423294 best: 0.1423288 (2256) 2258: learn: 0.0943577 test: 0.1423214 best: 0.1423214 (2258) 2259: learn: 0.0943364 test: 0.1423180 best: 0.1423180 (2259) 2260: learn: 0.0943165 test: 0.1423036 best: 0.1423036 (2260) 2261: learn: 0.0942973 test: 0.1422952 best: 0.1422952 (2261) 2262: learn: 0.0942735 test: 0.1422888 best: 0.1422888 (2262) 2263: learn: 0.0942490 test: 0.1422854 best: 0.1422854 (2263) 2264: learn: 0.0942257 test: 0.1422786 best: 0.1422786 (2264) 2265: learn: 0.0942056 test: 0.1422621 best: 0.1422621 (2265) 2266: learn: 0.0941877 test: 0.1422570 best: 0.1422570 (2266) 2267: learn: 0.0941684 test: 0.1422562 best: 0.1422562 (2267) 2268: learn: 0.0941494 test: 0.1422448 best: 0.1422448 (2268) 2269: learn: 0.0941283 test: 0.1422373 best: 0.1422373 (2269) 2270: learn: 0.0941045 test: 0.1422264 best: 0.1422264 (2270) 2271: learn: 0.0940843 test: 0.1422116 best: 0.1422116 (2271) 2272: learn: 0.0940643 test: 0.1422036 best: 0.1422036 (2272) 2273: learn: 0.0940456 test: 0.1421987 best: 0.1421987 (2273) 2274: learn: 0.0940302 test: 0.1421826 best: 0.1421826 (2274) 2275: learn: 0.0940106 test: 0.1421763 best: 0.1421763 (2275) 2276: learn: 0.0939955 test: 0.1421689 best: 0.1421689 (2276) 2277: learn: 0.0939806 test: 0.1421640 best: 0.1421640 (2277) 2278: learn: 0.0939633 test: 0.1421548 best: 0.1421548 (2278) 2279: learn: 0.0939436 test: 0.1421534 best: 0.1421534 (2279) 2280: learn: 0.0939248 test: 0.1421422 best: 0.1421422 (2280) 2281: learn: 0.0939071 test: 0.1421352 best: 0.1421352 (2281) 2282: learn: 0.0938852 test: 0.1421314 best: 0.1421314 (2282) 2283: learn: 0.0938668 test: 0.1421268 best: 0.1421268 (2283) 2284: learn: 0.0938458 test: 0.1421179 best: 0.1421179 (2284) 2285: learn: 0.0938270 test: 0.1421097 best: 0.1421097 (2285) 2286: learn: 0.0938059 test: 0.1421104 best: 0.1421097 (2285) 2287: learn: 0.0937840 test: 0.1421057 best: 0.1421057 (2287) 2288: learn: 0.0937654 test: 0.1421048 best: 0.1421048 (2288) 2289: learn: 0.0937467 test: 0.1421027 best: 0.1421027 (2289) 2290: learn: 0.0937241 test: 0.1421015 best: 0.1421015 (2290) 2291: learn: 0.0937053 test: 0.1420997 best: 0.1420997 (2291) 2292: learn: 0.0936865 test: 0.1420896 best: 0.1420896 (2292) 2293: learn: 0.0936686 test: 0.1420872 best: 0.1420872 (2293) 2294: learn: 0.0936501 test: 0.1420852 best: 0.1420852 (2294) 2295: learn: 0.0936323 test: 0.1420777 best: 0.1420777 (2295) 2296: learn: 0.0936195 test: 0.1420744 best: 0.1420744 (2296) 2297: learn: 0.0936012 test: 0.1420670 best: 0.1420670 (2297) 2298: learn: 0.0935842 test: 0.1420640 best: 0.1420640 (2298) 2299: learn: 0.0935682 test: 0.1420639 best: 0.1420639 (2299) 2300: learn: 0.0935522 test: 0.1420578 best: 0.1420578 (2300) 2301: learn: 0.0935355 test: 0.1420569 best: 0.1420569 (2301) 2302: learn: 0.0935160 test: 0.1420568 best: 0.1420568 (2302) 2303: learn: 0.0934956 test: 0.1420503 best: 0.1420503 (2303) 2304: learn: 0.0934790 test: 0.1420458 best: 0.1420458 (2304) 2305: learn: 0.0934597 test: 0.1420421 best: 0.1420421 (2305) 2306: learn: 0.0934442 test: 0.1420314 best: 0.1420314 (2306) 2307: learn: 0.0934245 test: 0.1420265 best: 0.1420265 (2307) 2308: learn: 0.0934066 test: 0.1420221 best: 0.1420221 (2308) 2309: learn: 0.0933874 test: 0.1420146 best: 0.1420146 (2309) 2310: learn: 0.0933708 test: 0.1420096 best: 0.1420096 (2310) 2311: learn: 0.0933508 test: 0.1420030 best: 0.1420030 (2311) 2312: learn: 0.0933347 test: 0.1419949 best: 0.1419949 (2312) 2313: learn: 0.0933147 test: 0.1419882 best: 0.1419882 (2313) 2314: learn: 0.0932992 test: 0.1419791 best: 0.1419791 (2314) 2315: learn: 0.0932832 test: 0.1419752 best: 0.1419752 (2315) 2316: learn: 0.0932631 test: 0.1419653 best: 0.1419653 (2316) 2317: learn: 0.0932459 test: 0.1419572 best: 0.1419572 (2317) 2318: learn: 0.0932241 test: 0.1419541 best: 0.1419541 (2318) 2319: learn: 0.0932067 test: 0.1419527 best: 0.1419527 (2319) 2320: learn: 0.0931870 test: 0.1419518 best: 0.1419518 (2320) 2321: learn: 0.0931642 test: 0.1419454 best: 0.1419454 (2321) 2322: learn: 0.0931475 test: 0.1419367 best: 0.1419367 (2322) 2323: learn: 0.0931305 test: 0.1419286 best: 0.1419286 (2323) 2324: learn: 0.0931136 test: 0.1419276 best: 0.1419276 (2324) 2325: learn: 0.0930961 test: 0.1419321 best: 0.1419276 (2324) 2326: learn: 0.0930827 test: 0.1419266 best: 0.1419266 (2326) 2327: learn: 0.0930667 test: 0.1419143 best: 0.1419143 (2327) 2328: learn: 0.0930505 test: 0.1419107 best: 0.1419107 (2328) 2329: learn: 0.0930295 test: 0.1419115 best: 0.1419107 (2328) 2330: learn: 0.0930092 test: 0.1419098 best: 0.1419098 (2330) 2331: learn: 0.0929932 test: 0.1419067 best: 0.1419067 (2331) 2332: learn: 0.0929721 test: 0.1418939 best: 0.1418939 (2332) 2333: learn: 0.0929562 test: 0.1418847 best: 0.1418847 (2333) 2334: learn: 0.0929354 test: 0.1418798 best: 0.1418798 (2334) 2335: learn: 0.0929186 test: 0.1418733 best: 0.1418733 (2335) 2336: learn: 0.0928988 test: 0.1418719 best: 0.1418719 (2336) 2337: learn: 0.0928844 test: 0.1418722 best: 0.1418719 (2336) 2338: learn: 0.0928679 test: 0.1418696 best: 0.1418696 (2338) 2339: learn: 0.0928529 test: 0.1418651 best: 0.1418651 (2339) 2340: learn: 0.0928345 test: 0.1418530 best: 0.1418530 (2340) 2341: learn: 0.0928175 test: 0.1418539 best: 0.1418530 (2340) 2342: learn: 0.0927997 test: 0.1418469 best: 0.1418469 (2342) 2343: learn: 0.0927804 test: 0.1418389 best: 0.1418389 (2343) 2344: learn: 0.0927630 test: 0.1418360 best: 0.1418360 (2344) 2345: learn: 0.0927431 test: 0.1418260 best: 0.1418260 (2345) 2346: learn: 0.0927240 test: 0.1418198 best: 0.1418198 (2346) 2347: learn: 0.0927082 test: 0.1418144 best: 0.1418144 (2347) 2348: learn: 0.0926899 test: 0.1418071 best: 0.1418071 (2348) 2349: learn: 0.0926711 test: 0.1417969 best: 0.1417969 (2349) 2350: learn: 0.0926562 test: 0.1417931 best: 0.1417931 (2350) 2351: learn: 0.0926393 test: 0.1417901 best: 0.1417901 (2351) 2352: learn: 0.0926194 test: 0.1417855 best: 0.1417855 (2352) 2353: learn: 0.0926030 test: 0.1417798 best: 0.1417798 (2353) 2354: learn: 0.0925810 test: 0.1417726 best: 0.1417726 (2354) 2355: learn: 0.0925639 test: 0.1417698 best: 0.1417698 (2355) 2356: learn: 0.0925458 test: 0.1417622 best: 0.1417622 (2356) 2357: learn: 0.0925296 test: 0.1417540 best: 0.1417540 (2357) 2358: learn: 0.0925113 test: 0.1417398 best: 0.1417398 (2358) 2359: learn: 0.0924900 test: 0.1417306 best: 0.1417306 (2359) 2360: learn: 0.0924706 test: 0.1417262 best: 0.1417262 (2360) 2361: learn: 0.0924509 test: 0.1417140 best: 0.1417140 (2361) 2362: learn: 0.0924309 test: 0.1417114 best: 0.1417114 (2362) 2363: learn: 0.0924114 test: 0.1417066 best: 0.1417066 (2363) 2364: learn: 0.0923863 test: 0.1416961 best: 0.1416961 (2364) 2365: learn: 0.0923677 test: 0.1416881 best: 0.1416881 (2365) 2366: learn: 0.0923517 test: 0.1416833 best: 0.1416833 (2366) 2367: learn: 0.0923354 test: 0.1416739 best: 0.1416739 (2367) 2368: learn: 0.0923173 test: 0.1416690 best: 0.1416690 (2368) 2369: learn: 0.0922989 test: 0.1416685 best: 0.1416685 (2369) 2370: learn: 0.0922793 test: 0.1416700 best: 0.1416685 (2369) 2371: learn: 0.0922597 test: 0.1416643 best: 0.1416643 (2371) 2372: learn: 0.0922441 test: 0.1416675 best: 0.1416643 (2371) 2373: learn: 0.0922227 test: 0.1416728 best: 0.1416643 (2371) 2374: learn: 0.0922053 test: 0.1416631 best: 0.1416631 (2374) 2375: learn: 0.0921849 test: 0.1416515 best: 0.1416515 (2375) 2376: learn: 0.0921659 test: 0.1416536 best: 0.1416515 (2375) 2377: learn: 0.0921490 test: 0.1416498 best: 0.1416498 (2377) 2378: learn: 0.0921279 test: 0.1416466 best: 0.1416466 (2378) 2379: learn: 0.0921107 test: 0.1416451 best: 0.1416451 (2379) 2380: learn: 0.0920917 test: 0.1416434 best: 0.1416434 (2380) 2381: learn: 0.0920739 test: 0.1416367 best: 0.1416367 (2381) 2382: learn: 0.0920536 test: 0.1416333 best: 0.1416333 (2382) 2383: learn: 0.0920339 test: 0.1416256 best: 0.1416256 (2383) 2384: learn: 0.0920167 test: 0.1416181 best: 0.1416181 (2384) 2385: learn: 0.0919980 test: 0.1416123 best: 0.1416123 (2385) 2386: learn: 0.0919830 test: 0.1416096 best: 0.1416096 (2386) 2387: learn: 0.0919669 test: 0.1416014 best: 0.1416014 (2387) 2388: learn: 0.0919499 test: 0.1415975 best: 0.1415975 (2388) 2389: learn: 0.0919317 test: 0.1415955 best: 0.1415955 (2389) 2390: learn: 0.0919163 test: 0.1415977 best: 0.1415955 (2389) 2391: learn: 0.0918959 test: 0.1415908 best: 0.1415908 (2391) 2392: learn: 0.0918754 test: 0.1415876 best: 0.1415876 (2392) 2393: learn: 0.0918580 test: 0.1415765 best: 0.1415765 (2393) 2394: learn: 0.0918376 test: 0.1415650 best: 0.1415650 (2394) 2395: learn: 0.0918205 test: 0.1415631 best: 0.1415631 (2395) 2396: learn: 0.0918012 test: 0.1415506 best: 0.1415506 (2396) 2397: learn: 0.0917833 test: 0.1415472 best: 0.1415472 (2397) 2398: learn: 0.0917651 test: 0.1415485 best: 0.1415472 (2397) 2399: learn: 0.0917464 test: 0.1415382 best: 0.1415382 (2399) 2400: learn: 0.0917262 test: 0.1415437 best: 0.1415382 (2399) 2401: learn: 0.0917063 test: 0.1415391 best: 0.1415382 (2399) total: 27s remaining: 40.5s 2402: learn: 0.0916910 test: 0.1415313 best: 0.1415313 (2402) 2403: learn: 0.0916735 test: 0.1415236 best: 0.1415236 (2403) 2404: learn: 0.0916564 test: 0.1415222 best: 0.1415222 (2404) 2405: learn: 0.0916394 test: 0.1415193 best: 0.1415193 (2405) 2406: learn: 0.0916184 test: 0.1415149 best: 0.1415149 (2406) 2407: learn: 0.0916005 test: 0.1415062 best: 0.1415062 (2407) 2408: learn: 0.0915874 test: 0.1415068 best: 0.1415062 (2407) 2409: learn: 0.0915675 test: 0.1415003 best: 0.1415003 (2409) 2410: learn: 0.0915500 test: 0.1414978 best: 0.1414978 (2410) 2411: learn: 0.0915334 test: 0.1414907 best: 0.1414907 (2411) 2412: learn: 0.0915154 test: 0.1414903 best: 0.1414903 (2412) 2413: learn: 0.0914941 test: 0.1414862 best: 0.1414862 (2413) 2414: learn: 0.0914763 test: 0.1414836 best: 0.1414836 (2414) 2415: learn: 0.0914570 test: 0.1414807 best: 0.1414807 (2415) 2416: learn: 0.0914395 test: 0.1414778 best: 0.1414778 (2416) 2417: learn: 0.0914217 test: 0.1414733 best: 0.1414733 (2417) 2418: learn: 0.0914009 test: 0.1414660 best: 0.1414660 (2418) 2419: learn: 0.0913837 test: 0.1414612 best: 0.1414612 (2419) 2420: learn: 0.0913649 test: 0.1414580 best: 0.1414580 (2420) 2421: learn: 0.0913458 test: 0.1414521 best: 0.1414521 (2421) 2422: learn: 0.0913301 test: 0.1414473 best: 0.1414473 (2422) 2423: learn: 0.0913178 test: 0.1414363 best: 0.1414363 (2423) 2424: learn: 0.0913047 test: 0.1414272 best: 0.1414272 (2424) 2425: learn: 0.0912893 test: 0.1414266 best: 0.1414266 (2425) 2426: learn: 0.0912703 test: 0.1414123 best: 0.1414123 (2426) 2427: learn: 0.0912535 test: 0.1414126 best: 0.1414123 (2426) 2428: learn: 0.0912337 test: 0.1414120 best: 0.1414120 (2428) 2429: learn: 0.0912160 test: 0.1414050 best: 0.1414050 (2429) 2430: learn: 0.0912003 test: 0.1414010 best: 0.1414010 (2430) 2431: learn: 0.0911862 test: 0.1413880 best: 0.1413880 (2431) 2432: learn: 0.0911662 test: 0.1413896 best: 0.1413880 (2431) 2433: learn: 0.0911467 test: 0.1413801 best: 0.1413801 (2433) 2434: learn: 0.0911303 test: 0.1413741 best: 0.1413741 (2434) 2435: learn: 0.0911105 test: 0.1413644 best: 0.1413644 (2435) 2436: learn: 0.0910901 test: 0.1413555 best: 0.1413555 (2436) 2437: learn: 0.0910713 test: 0.1413476 best: 0.1413476 (2437) 2438: learn: 0.0910529 test: 0.1413397 best: 0.1413397 (2438) 2439: learn: 0.0910351 test: 0.1413401 best: 0.1413397 (2438) 2440: learn: 0.0910195 test: 0.1413373 best: 0.1413373 (2440) 2441: learn: 0.0910030 test: 0.1413328 best: 0.1413328 (2441) 2442: learn: 0.0909847 test: 0.1413239 best: 0.1413239 (2442) 2443: learn: 0.0909692 test: 0.1413150 best: 0.1413150 (2443) 2444: learn: 0.0909472 test: 0.1413121 best: 0.1413121 (2444) 2445: learn: 0.0909313 test: 0.1413078 best: 0.1413078 (2445) 2446: learn: 0.0909130 test: 0.1413012 best: 0.1413012 (2446) 2447: learn: 0.0908993 test: 0.1412893 best: 0.1412893 (2447) 2448: learn: 0.0908824 test: 0.1412836 best: 0.1412836 (2448) 2449: learn: 0.0908655 test: 0.1412803 best: 0.1412803 (2449) 2450: learn: 0.0908471 test: 0.1412734 best: 0.1412734 (2450) 2451: learn: 0.0908268 test: 0.1412747 best: 0.1412734 (2450) 2452: learn: 0.0908065 test: 0.1412635 best: 0.1412635 (2452) 2453: learn: 0.0907926 test: 0.1412552 best: 0.1412552 (2453) 2454: learn: 0.0907753 test: 0.1412479 best: 0.1412479 (2454) 2455: learn: 0.0907608 test: 0.1412391 best: 0.1412391 (2455) 2456: learn: 0.0907456 test: 0.1412305 best: 0.1412305 (2456) 2457: learn: 0.0907272 test: 0.1412198 best: 0.1412198 (2457) 2458: learn: 0.0907072 test: 0.1412113 best: 0.1412113 (2458) 2459: learn: 0.0906940 test: 0.1412032 best: 0.1412032 (2459) 2460: learn: 0.0906714 test: 0.1412086 best: 0.1412032 (2459) 2461: learn: 0.0906540 test: 0.1412022 best: 0.1412022 (2461) 2462: learn: 0.0906330 test: 0.1411996 best: 0.1411996 (2462) 2463: learn: 0.0906183 test: 0.1411955 best: 0.1411955 (2463) 2464: learn: 0.0906007 test: 0.1411899 best: 0.1411899 (2464) 2465: learn: 0.0905833 test: 0.1411928 best: 0.1411899 (2464) 2466: learn: 0.0905680 test: 0.1411844 best: 0.1411844 (2466) 2467: learn: 0.0905499 test: 0.1411868 best: 0.1411844 (2466) 2468: learn: 0.0905344 test: 0.1411794 best: 0.1411794 (2468) 2469: learn: 0.0905192 test: 0.1411825 best: 0.1411794 (2468) 2470: learn: 0.0905009 test: 0.1411800 best: 0.1411794 (2468) 2471: learn: 0.0904856 test: 0.1411756 best: 0.1411756 (2471) 2472: learn: 0.0904647 test: 0.1411764 best: 0.1411756 (2471) 2473: learn: 0.0904425 test: 0.1411732 best: 0.1411732 (2473) 2474: learn: 0.0904231 test: 0.1411698 best: 0.1411698 (2474) 2475: learn: 0.0903965 test: 0.1411671 best: 0.1411671 (2475) 2476: learn: 0.0903819 test: 0.1411605 best: 0.1411605 (2476) 2477: learn: 0.0903706 test: 0.1411551 best: 0.1411551 (2477) 2478: learn: 0.0903514 test: 0.1411448 best: 0.1411448 (2478) 2479: learn: 0.0903340 test: 0.1411427 best: 0.1411427 (2479) 2480: learn: 0.0903179 test: 0.1411335 best: 0.1411335 (2480) 2481: learn: 0.0902999 test: 0.1411298 best: 0.1411298 (2481) 2482: learn: 0.0902817 test: 0.1411279 best: 0.1411279 (2482) 2483: learn: 0.0902601 test: 0.1411209 best: 0.1411209 (2483) 2484: learn: 0.0902406 test: 0.1411103 best: 0.1411103 (2484) 2485: learn: 0.0902260 test: 0.1410937 best: 0.1410937 (2485) 2486: learn: 0.0902082 test: 0.1410873 best: 0.1410873 (2486) 2487: learn: 0.0901899 test: 0.1410818 best: 0.1410818 (2487) 2488: learn: 0.0901726 test: 0.1410749 best: 0.1410749 (2488) 2489: learn: 0.0901573 test: 0.1410757 best: 0.1410749 (2488) 2490: learn: 0.0901389 test: 0.1410705 best: 0.1410705 (2490) 2491: learn: 0.0901166 test: 0.1410659 best: 0.1410659 (2491) 2492: learn: 0.0900978 test: 0.1410679 best: 0.1410659 (2491) 2493: learn: 0.0900793 test: 0.1410662 best: 0.1410659 (2491) 2494: learn: 0.0900597 test: 0.1410660 best: 0.1410659 (2491) 2495: learn: 0.0900377 test: 0.1410611 best: 0.1410611 (2495) 2496: learn: 0.0900236 test: 0.1410594 best: 0.1410594 (2496) 2497: learn: 0.0900075 test: 0.1410538 best: 0.1410538 (2497) 2498: learn: 0.0899924 test: 0.1410514 best: 0.1410514 (2498) 2499: learn: 0.0899741 test: 0.1410460 best: 0.1410460 (2499) 2500: learn: 0.0899536 test: 0.1410395 best: 0.1410395 (2500) 2501: learn: 0.0899373 test: 0.1410356 best: 0.1410356 (2501) 2502: learn: 0.0899191 test: 0.1410310 best: 0.1410310 (2502) 2503: learn: 0.0899051 test: 0.1410284 best: 0.1410284 (2503) 2504: learn: 0.0898888 test: 0.1410268 best: 0.1410268 (2504) 2505: learn: 0.0898724 test: 0.1410203 best: 0.1410203 (2505) 2506: learn: 0.0898547 test: 0.1410165 best: 0.1410165 (2506) 2507: learn: 0.0898406 test: 0.1410104 best: 0.1410104 (2507) 2508: learn: 0.0898228 test: 0.1410047 best: 0.1410047 (2508) 2509: learn: 0.0898069 test: 0.1410039 best: 0.1410039 (2509) 2510: learn: 0.0897886 test: 0.1410023 best: 0.1410023 (2510) 2511: learn: 0.0897715 test: 0.1409975 best: 0.1409975 (2511) 2512: learn: 0.0897539 test: 0.1409943 best: 0.1409943 (2512) 2513: learn: 0.0897323 test: 0.1409948 best: 0.1409943 (2512) 2514: learn: 0.0897084 test: 0.1409891 best: 0.1409891 (2514) 2515: learn: 0.0896933 test: 0.1409870 best: 0.1409870 (2515) 2516: learn: 0.0896801 test: 0.1409785 best: 0.1409785 (2516) 2517: learn: 0.0896658 test: 0.1409747 best: 0.1409747 (2517) 2518: learn: 0.0896495 test: 0.1409716 best: 0.1409716 (2518) 2519: learn: 0.0896317 test: 0.1409736 best: 0.1409716 (2518) 2520: learn: 0.0896135 test: 0.1409691 best: 0.1409691 (2520) 2521: learn: 0.0895932 test: 0.1409630 best: 0.1409630 (2521) 2522: learn: 0.0895757 test: 0.1409569 best: 0.1409569 (2522) 2523: learn: 0.0895572 test: 0.1409586 best: 0.1409569 (2522) 2524: learn: 0.0895419 test: 0.1409508 best: 0.1409508 (2524) 2525: learn: 0.0895266 test: 0.1409489 best: 0.1409489 (2525) 2526: learn: 0.0895078 test: 0.1409429 best: 0.1409429 (2526) 2527: learn: 0.0894885 test: 0.1409389 best: 0.1409389 (2527) 2528: learn: 0.0894726 test: 0.1409370 best: 0.1409370 (2528) 2529: learn: 0.0894577 test: 0.1409363 best: 0.1409363 (2529) 2530: learn: 0.0894327 test: 0.1409293 best: 0.1409293 (2530) 2531: learn: 0.0894160 test: 0.1409230 best: 0.1409230 (2531) 2532: learn: 0.0893961 test: 0.1409185 best: 0.1409185 (2532) 2533: learn: 0.0893818 test: 0.1409112 best: 0.1409112 (2533) 2534: learn: 0.0893682 test: 0.1409076 best: 0.1409076 (2534) 2535: learn: 0.0893483 test: 0.1409020 best: 0.1409020 (2535) 2536: learn: 0.0893319 test: 0.1408945 best: 0.1408945 (2536) 2537: learn: 0.0893182 test: 0.1408857 best: 0.1408857 (2537) 2538: learn: 0.0893031 test: 0.1408840 best: 0.1408840 (2538) 2539: learn: 0.0892843 test: 0.1408725 best: 0.1408725 (2539) 2540: learn: 0.0892678 test: 0.1408658 best: 0.1408658 (2540) 2541: learn: 0.0892512 test: 0.1408681 best: 0.1408658 (2540) 2542: learn: 0.0892332 test: 0.1408701 best: 0.1408658 (2540) 2543: learn: 0.0892134 test: 0.1408630 best: 0.1408630 (2543) 2544: learn: 0.0892008 test: 0.1408592 best: 0.1408592 (2544) 2545: learn: 0.0891822 test: 0.1408581 best: 0.1408581 (2545) 2546: learn: 0.0891642 test: 0.1408487 best: 0.1408487 (2546) 2547: learn: 0.0891435 test: 0.1408460 best: 0.1408460 (2547) 2548: learn: 0.0891252 test: 0.1408453 best: 0.1408453 (2548) total: 28.6s remaining: 38.8s 2549: learn: 0.0891071 test: 0.1408427 best: 0.1408427 (2549) 2550: learn: 0.0890844 test: 0.1408451 best: 0.1408427 (2549) 2551: learn: 0.0890682 test: 0.1408460 best: 0.1408427 (2549) 2552: learn: 0.0890519 test: 0.1408421 best: 0.1408421 (2552) 2553: learn: 0.0890347 test: 0.1408407 best: 0.1408407 (2553) 2554: learn: 0.0890182 test: 0.1408395 best: 0.1408395 (2554) 2555: learn: 0.0890006 test: 0.1408332 best: 0.1408332 (2555) 2556: learn: 0.0889867 test: 0.1408306 best: 0.1408306 (2556) 2557: learn: 0.0889706 test: 0.1408229 best: 0.1408229 (2557) 2558: learn: 0.0889574 test: 0.1408133 best: 0.1408133 (2558) 2559: learn: 0.0889382 test: 0.1408117 best: 0.1408117 (2559) 2560: learn: 0.0889219 test: 0.1408122 best: 0.1408117 (2559) 2561: learn: 0.0889042 test: 0.1408077 best: 0.1408077 (2561) 2562: learn: 0.0888872 test: 0.1408026 best: 0.1408026 (2562) 2563: learn: 0.0888730 test: 0.1407896 best: 0.1407896 (2563) 2564: learn: 0.0888595 test: 0.1407841 best: 0.1407841 (2564) 2565: learn: 0.0888441 test: 0.1407750 best: 0.1407750 (2565) 2566: learn: 0.0888291 test: 0.1407704 best: 0.1407704 (2566) 2567: learn: 0.0888077 test: 0.1407649 best: 0.1407649 (2567) 2568: learn: 0.0887903 test: 0.1407632 best: 0.1407632 (2568) 2569: learn: 0.0887727 test: 0.1407649 best: 0.1407632 (2568) 2570: learn: 0.0887518 test: 0.1407650 best: 0.1407632 (2568) 2571: learn: 0.0887381 test: 0.1407670 best: 0.1407632 (2568) 2572: learn: 0.0887227 test: 0.1407671 best: 0.1407632 (2568) 2573: learn: 0.0887071 test: 0.1407656 best: 0.1407632 (2568) 2574: learn: 0.0886931 test: 0.1407622 best: 0.1407622 (2574) 2575: learn: 0.0886807 test: 0.1407644 best: 0.1407622 (2574) total: 29s remaining: 38.6s 2576: learn: 0.0886640 test: 0.1407623 best: 0.1407622 (2574) 2577: learn: 0.0886482 test: 0.1407557 best: 0.1407557 (2577) 2578: learn: 0.0886343 test: 0.1407546 best: 0.1407546 (2578) 2579: learn: 0.0886149 test: 0.1407464 best: 0.1407464 (2579) 2580: learn: 0.0885960 test: 0.1407441 best: 0.1407441 (2580) 2581: learn: 0.0885809 test: 0.1407410 best: 0.1407410 (2581) 2582: learn: 0.0885662 test: 0.1407372 best: 0.1407372 (2582) 2583: learn: 0.0885508 test: 0.1407297 best: 0.1407297 (2583) 2584: learn: 0.0885356 test: 0.1407297 best: 0.1407297 (2583) 2585: learn: 0.0885219 test: 0.1407261 best: 0.1407261 (2585) 2586: learn: 0.0885047 test: 0.1407301 best: 0.1407261 (2585) 2587: learn: 0.0884909 test: 0.1407257 best: 0.1407257 (2587) 2588: learn: 0.0884715 test: 0.1407213 best: 0.1407213 (2588) 2589: learn: 0.0884519 test: 0.1407193 best: 0.1407193 (2589) 2590: learn: 0.0884362 test: 0.1407208 best: 0.1407193 (2589) 2591: learn: 0.0884181 test: 0.1407105 best: 0.1407105 (2591) 2592: learn: 0.0884052 test: 0.1407064 best: 0.1407064 (2592) 2593: learn: 0.0883882 test: 0.1406974 best: 0.1406974 (2593) 2594: learn: 0.0883724 test: 0.1406916 best: 0.1406916 (2594) 2595: learn: 0.0883575 test: 0.1406887 best: 0.1406887 (2595) 2596: learn: 0.0883393 test: 0.1406845 best: 0.1406845 (2596) 2597: learn: 0.0883237 test: 0.1406804 best: 0.1406804 (2597) 2598: learn: 0.0883058 test: 0.1406827 best: 0.1406804 (2597) 2599: learn: 0.0882849 test: 0.1406717 best: 0.1406717 (2599) 2600: learn: 0.0882657 test: 0.1406651 best: 0.1406651 (2600) 2601: learn: 0.0882437 test: 0.1406688 best: 0.1406651 (2600) 2602: learn: 0.0882295 test: 0.1406618 best: 0.1406618 (2602) 2603: learn: 0.0882167 test: 0.1406593 best: 0.1406593 (2603) 2604: learn: 0.0882013 test: 0.1406541 best: 0.1406541 (2604) 2605: learn: 0.0881867 test: 0.1406440 best: 0.1406440 (2605) 2606: learn: 0.0881657 test: 0.1406435 best: 0.1406435 (2606) 2607: learn: 0.0881475 test: 0.1406397 best: 0.1406397 (2607) 2608: learn: 0.0881309 test: 0.1406405 best: 0.1406397 (2607) 2609: learn: 0.0881100 test: 0.1406346 best: 0.1406346 (2609) 2610: learn: 0.0880936 test: 0.1406319 best: 0.1406319 (2610) 2611: learn: 0.0880798 test: 0.1406235 best: 0.1406235 (2611) 2612: learn: 0.0880655 test: 0.1406183 best: 0.1406183 (2612) 2613: learn: 0.0880505 test: 0.1406144 best: 0.1406144 (2613) 2614: learn: 0.0880323 test: 0.1406114 best: 0.1406114 (2614) 2615: learn: 0.0880124 test: 0.1406090 best: 0.1406090 (2615) 2616: learn: 0.0879982 test: 0.1406085 best: 0.1406085 (2616) 2617: learn: 0.0879809 test: 0.1406004 best: 0.1406004 (2617) 2618: learn: 0.0879630 test: 0.1406027 best: 0.1406004 (2617) 2619: learn: 0.0879520 test: 0.1406005 best: 0.1406004 (2617) 2620: learn: 0.0879415 test: 0.1405995 best: 0.1405995 (2620) 2621: learn: 0.0879241 test: 0.1405957 best: 0.1405957 (2621) 2622: learn: 0.0879078 test: 0.1406015 best: 0.1405957 (2621) 2623: learn: 0.0878912 test: 0.1405998 best: 0.1405957 (2621) 2624: learn: 0.0878717 test: 0.1405982 best: 0.1405957 (2621) 2625: learn: 0.0878537 test: 0.1405957 best: 0.1405957 (2621) 2626: learn: 0.0878356 test: 0.1405931 best: 0.1405931 (2626) 2627: learn: 0.0878203 test: 0.1405896 best: 0.1405896 (2627) 2628: learn: 0.0878039 test: 0.1405832 best: 0.1405832 (2628) 2629: learn: 0.0877869 test: 0.1405796 best: 0.1405796 (2629) 2630: learn: 0.0877719 test: 0.1405813 best: 0.1405796 (2629) 2631: learn: 0.0877539 test: 0.1405753 best: 0.1405753 (2631) 2632: learn: 0.0877370 test: 0.1405719 best: 0.1405719 (2632) 2633: learn: 0.0877205 test: 0.1405725 best: 0.1405719 (2632) 2634: learn: 0.0876997 test: 0.1405787 best: 0.1405719 (2632) 2635: learn: 0.0876826 test: 0.1405785 best: 0.1405719 (2632) 2636: learn: 0.0876652 test: 0.1405747 best: 0.1405719 (2632) 2637: learn: 0.0876498 test: 0.1405692 best: 0.1405692 (2637) 2638: learn: 0.0876358 test: 0.1405604 best: 0.1405604 (2638) 2639: learn: 0.0876205 test: 0.1405554 best: 0.1405554 (2639) 2640: learn: 0.0876100 test: 0.1405516 best: 0.1405516 (2640) 2641: learn: 0.0875929 test: 0.1405469 best: 0.1405469 (2641) 2642: learn: 0.0875763 test: 0.1405425 best: 0.1405425 (2642) 2643: learn: 0.0875619 test: 0.1405363 best: 0.1405363 (2643) 2644: learn: 0.0875453 test: 0.1405311 best: 0.1405311 (2644) 2645: learn: 0.0875288 test: 0.1405281 best: 0.1405281 (2645) 2646: learn: 0.0875139 test: 0.1405187 best: 0.1405187 (2646) 2647: learn: 0.0874993 test: 0.1405133 best: 0.1405133 (2647) 2648: learn: 0.0874819 test: 0.1405079 best: 0.1405079 (2648) 2649: learn: 0.0874716 test: 0.1405017 best: 0.1405017 (2649) 2650: learn: 0.0874579 test: 0.1405004 best: 0.1405004 (2650) 2651: learn: 0.0874422 test: 0.1405031 best: 0.1405004 (2650) 2652: learn: 0.0874294 test: 0.1404987 best: 0.1404987 (2652) 2653: learn: 0.0874134 test: 0.1404950 best: 0.1404950 (2653) 2654: learn: 0.0873991 test: 0.1404923 best: 0.1404923 (2654) 2655: learn: 0.0873817 test: 0.1404872 best: 0.1404872 (2655) 2656: learn: 0.0873613 test: 0.1404924 best: 0.1404872 (2655) 2657: learn: 0.0873416 test: 0.1404884 best: 0.1404872 (2655) 2658: learn: 0.0873210 test: 0.1404830 best: 0.1404830 (2658) 2659: learn: 0.0873061 test: 0.1404809 best: 0.1404809 (2659) 2660: learn: 0.0872921 test: 0.1404779 best: 0.1404779 (2660) 2661: learn: 0.0872738 test: 0.1404717 best: 0.1404717 (2661) 2662: learn: 0.0872589 test: 0.1404693 best: 0.1404693 (2662) 2663: learn: 0.0872439 test: 0.1404587 best: 0.1404587 (2663) 2664: learn: 0.0872248 test: 0.1404573 best: 0.1404573 (2664) 2665: learn: 0.0872080 test: 0.1404554 best: 0.1404554 (2665) 2666: learn: 0.0871920 test: 0.1404494 best: 0.1404494 (2666) 2667: learn: 0.0871757 test: 0.1404448 best: 0.1404448 (2667) 2668: learn: 0.0871617 test: 0.1404371 best: 0.1404371 (2668) 2669: learn: 0.0871498 test: 0.1404342 best: 0.1404342 (2669) 2670: learn: 0.0871364 test: 0.1404332 best: 0.1404332 (2670) 2671: learn: 0.0871212 test: 0.1404234 best: 0.1404234 (2671) 2672: learn: 0.0871035 test: 0.1404210 best: 0.1404210 (2672) 2673: learn: 0.0870891 test: 0.1404128 best: 0.1404128 (2673) 2674: learn: 0.0870643 test: 0.1404076 best: 0.1404076 (2674) 2675: learn: 0.0870486 test: 0.1404056 best: 0.1404056 (2675) 2676: learn: 0.0870355 test: 0.1404029 best: 0.1404029 (2676) 2677: learn: 0.0870211 test: 0.1404043 best: 0.1404029 (2676) 2678: learn: 0.0870026 test: 0.1403971 best: 0.1403971 (2678) 2679: learn: 0.0869855 test: 0.1403951 best: 0.1403951 (2679) 2680: learn: 0.0869709 test: 0.1403953 best: 0.1403951 (2679) 2681: learn: 0.0869537 test: 0.1403856 best: 0.1403856 (2681) 2682: learn: 0.0869409 test: 0.1403777 best: 0.1403777 (2682) 2683: learn: 0.0869280 test: 0.1403745 best: 0.1403745 (2683) 2684: learn: 0.0869114 test: 0.1403725 best: 0.1403725 (2684) 2685: learn: 0.0868958 test: 0.1403678 best: 0.1403678 (2685) 2686: learn: 0.0868808 test: 0.1403602 best: 0.1403602 (2686) 2687: learn: 0.0868658 test: 0.1403530 best: 0.1403530 (2687) 2688: learn: 0.0868486 test: 0.1403503 best: 0.1403503 (2688) 2689: learn: 0.0868299 test: 0.1403514 best: 0.1403503 (2688) 2690: learn: 0.0868179 test: 0.1403481 best: 0.1403481 (2690) 2691: learn: 0.0868033 test: 0.1403375 best: 0.1403375 (2691) 2692: learn: 0.0867891 test: 0.1403324 best: 0.1403324 (2692) 2693: learn: 0.0867751 test: 0.1403282 best: 0.1403282 (2693) 2694: learn: 0.0867611 test: 0.1403258 best: 0.1403258 (2694) 2695: learn: 0.0867477 test: 0.1403230 best: 0.1403230 (2695) 2696: learn: 0.0867346 test: 0.1403140 best: 0.1403140 (2696) 2697: learn: 0.0867195 test: 0.1403156 best: 0.1403140 (2696) 2698: learn: 0.0867037 test: 0.1403154 best: 0.1403140 (2696) 2699: learn: 0.0866889 test: 0.1403189 best: 0.1403140 (2696) 2700: learn: 0.0866706 test: 0.1403162 best: 0.1403140 (2696) 2701: learn: 0.0866549 test: 0.1403189 best: 0.1403140 (2696) 2702: learn: 0.0866375 test: 0.1403180 best: 0.1403140 (2696) 2703: learn: 0.0866230 test: 0.1403173 best: 0.1403140 (2696) 2704: learn: 0.0866053 test: 0.1403189 best: 0.1403140 (2696) 2705: learn: 0.0865873 test: 0.1403185 best: 0.1403140 (2696) 2706: learn: 0.0865712 test: 0.1403162 best: 0.1403140 (2696) 2707: learn: 0.0865530 test: 0.1403177 best: 0.1403140 (2696) 2708: learn: 0.0865398 test: 0.1403123 best: 0.1403123 (2708) 2709: learn: 0.0865266 test: 0.1403087 best: 0.1403087 (2709) 2710: learn: 0.0865096 test: 0.1403085 best: 0.1403085 (2710) 2711: learn: 0.0864892 test: 0.1403030 best: 0.1403030 (2711) 2712: learn: 0.0864740 test: 0.1403061 best: 0.1403030 (2711) 2713: learn: 0.0864604 test: 0.1402997 best: 0.1402997 (2713) 2714: learn: 0.0864463 test: 0.1402960 best: 0.1402960 (2714) 2715: learn: 0.0864330 test: 0.1402980 best: 0.1402960 (2714) 2716: learn: 0.0864171 test: 0.1402980 best: 0.1402960 (2714) total: 30.6s remaining: 37s 2717: learn: 0.0864009 test: 0.1402937 best: 0.1402937 (2717) 2718: learn: 0.0863877 test: 0.1402908 best: 0.1402908 (2718) 2719: learn: 0.0863762 test: 0.1402906 best: 0.1402906 (2719) 2720: learn: 0.0863619 test: 0.1402849 best: 0.1402849 (2720) 2721: learn: 0.0863500 test: 0.1402770 best: 0.1402770 (2721) 2722: learn: 0.0863356 test: 0.1402746 best: 0.1402746 (2722) 2723: learn: 0.0863215 test: 0.1402727 best: 0.1402727 (2723) 2724: learn: 0.0863020 test: 0.1402735 best: 0.1402727 (2723) 2725: learn: 0.0862871 test: 0.1402731 best: 0.1402727 (2723) 2726: learn: 0.0862706 test: 0.1402692 best: 0.1402692 (2726) 2727: learn: 0.0862552 test: 0.1402647 best: 0.1402647 (2727) 2728: learn: 0.0862394 test: 0.1402574 best: 0.1402574 (2728) 2729: learn: 0.0862224 test: 0.1402564 best: 0.1402564 (2729) 2730: learn: 0.0862011 test: 0.1402535 best: 0.1402535 (2730) 2731: learn: 0.0861856 test: 0.1402510 best: 0.1402510 (2731) 2732: learn: 0.0861715 test: 0.1402504 best: 0.1402504 (2732) 2733: learn: 0.0861552 test: 0.1402461 best: 0.1402461 (2733) 2734: learn: 0.0861401 test: 0.1402425 best: 0.1402425 (2734) 2735: learn: 0.0861277 test: 0.1402360 best: 0.1402360 (2735) 2736: learn: 0.0861097 test: 0.1402374 best: 0.1402360 (2735) 2737: learn: 0.0860978 test: 0.1402319 best: 0.1402319 (2737) 2738: learn: 0.0860800 test: 0.1402284 best: 0.1402284 (2738) 2739: learn: 0.0860608 test: 0.1402247 best: 0.1402247 (2739) 2740: learn: 0.0860440 test: 0.1402257 best: 0.1402247 (2739) 2741: learn: 0.0860299 test: 0.1402265 best: 0.1402247 (2739) 2742: learn: 0.0860192 test: 0.1402271 best: 0.1402247 (2739) 2743: learn: 0.0860044 test: 0.1402290 best: 0.1402247 (2739) 2744: learn: 0.0859902 test: 0.1402208 best: 0.1402208 (2744) 2745: learn: 0.0859732 test: 0.1402208 best: 0.1402208 (2744) 2746: learn: 0.0859587 test: 0.1402170 best: 0.1402170 (2746) 2747: learn: 0.0859451 test: 0.1402128 best: 0.1402128 (2747) 2748: learn: 0.0859255 test: 0.1402086 best: 0.1402086 (2748) total: 31.1s remaining: 36.7s 2749: learn: 0.0859136 test: 0.1402013 best: 0.1402013 (2749) 2750: learn: 0.0858971 test: 0.1402021 best: 0.1402013 (2749) 2751: learn: 0.0858860 test: 0.1401946 best: 0.1401946 (2751) 2752: learn: 0.0858718 test: 0.1401890 best: 0.1401890 (2752) 2753: learn: 0.0858565 test: 0.1401891 best: 0.1401890 (2752) 2754: learn: 0.0858405 test: 0.1401836 best: 0.1401836 (2754) 2755: learn: 0.0858274 test: 0.1401833 best: 0.1401833 (2755) 2756: learn: 0.0858113 test: 0.1401838 best: 0.1401833 (2755) 2757: learn: 0.0857947 test: 0.1401776 best: 0.1401776 (2757) 2758: learn: 0.0857758 test: 0.1401813 best: 0.1401776 (2757) 2759: learn: 0.0857589 test: 0.1401802 best: 0.1401776 (2757) 2760: learn: 0.0857423 test: 0.1401779 best: 0.1401776 (2757) 2761: learn: 0.0857272 test: 0.1401755 best: 0.1401755 (2761) 2762: learn: 0.0857098 test: 0.1401722 best: 0.1401722 (2762) 2763: learn: 0.0856927 test: 0.1401677 best: 0.1401677 (2763) 2764: learn: 0.0856792 test: 0.1401608 best: 0.1401608 (2764) 2765: learn: 0.0856614 test: 0.1401573 best: 0.1401573 (2765) 2766: learn: 0.0856489 test: 0.1401538 best: 0.1401538 (2766) 2767: learn: 0.0856306 test: 0.1401541 best: 0.1401538 (2766) 2768: learn: 0.0856110 test: 0.1401530 best: 0.1401530 (2768) 2769: learn: 0.0855972 test: 0.1401451 best: 0.1401451 (2769) 2770: learn: 0.0855874 test: 0.1401435 best: 0.1401435 (2770) 2771: learn: 0.0855733 test: 0.1401376 best: 0.1401376 (2771) 2772: learn: 0.0855551 test: 0.1401333 best: 0.1401333 (2772) 2773: learn: 0.0855413 test: 0.1401296 best: 0.1401296 (2773) 2774: learn: 0.0855218 test: 0.1401301 best: 0.1401296 (2773) 2775: learn: 0.0855027 test: 0.1401284 best: 0.1401284 (2775) 2776: learn: 0.0854889 test: 0.1401263 best: 0.1401263 (2776) 2777: learn: 0.0854741 test: 0.1401232 best: 0.1401232 (2777) 2778: learn: 0.0854549 test: 0.1401226 best: 0.1401226 (2778) 2779: learn: 0.0854413 test: 0.1401189 best: 0.1401189 (2779) 2780: learn: 0.0854269 test: 0.1401132 best: 0.1401132 (2780) 2781: learn: 0.0854163 test: 0.1401100 best: 0.1401100 (2781) 2782: learn: 0.0854036 test: 0.1401024 best: 0.1401024 (2782) 2783: learn: 0.0853843 test: 0.1400999 best: 0.1400999 (2783) 2784: learn: 0.0853700 test: 0.1400903 best: 0.1400903 (2784) 2785: learn: 0.0853533 test: 0.1400899 best: 0.1400899 (2785) 2786: learn: 0.0853377 test: 0.1400907 best: 0.1400899 (2785) 2787: learn: 0.0853241 test: 0.1400919 best: 0.1400899 (2785) 2788: learn: 0.0853115 test: 0.1400883 best: 0.1400883 (2788) 2789: learn: 0.0852990 test: 0.1400828 best: 0.1400828 (2789) 2790: learn: 0.0852848 test: 0.1400749 best: 0.1400749 (2790) 2791: learn: 0.0852690 test: 0.1400707 best: 0.1400707 (2791) 2792: learn: 0.0852586 test: 0.1400677 best: 0.1400677 (2792) 2793: learn: 0.0852461 test: 0.1400630 best: 0.1400630 (2793) 2794: learn: 0.0852281 test: 0.1400697 best: 0.1400630 (2793) 2795: learn: 0.0852118 test: 0.1400683 best: 0.1400630 (2793) 2796: learn: 0.0851959 test: 0.1400719 best: 0.1400630 (2793) 2797: learn: 0.0851825 test: 0.1400694 best: 0.1400630 (2793) 2798: learn: 0.0851722 test: 0.1400687 best: 0.1400630 (2793) 2799: learn: 0.0851614 test: 0.1400679 best: 0.1400630 (2793) 2800: learn: 0.0851459 test: 0.1400637 best: 0.1400630 (2793) 2801: learn: 0.0851297 test: 0.1400610 best: 0.1400610 (2801) 2802: learn: 0.0851175 test: 0.1400611 best: 0.1400610 (2801) 2803: learn: 0.0851055 test: 0.1400576 best: 0.1400576 (2803) 2804: learn: 0.0850921 test: 0.1400552 best: 0.1400552 (2804) 2805: learn: 0.0850772 test: 0.1400543 best: 0.1400543 (2805) 2806: learn: 0.0850595 test: 0.1400507 best: 0.1400507 (2806) 2807: learn: 0.0850470 test: 0.1400487 best: 0.1400487 (2807) 2808: learn: 0.0850328 test: 0.1400365 best: 0.1400365 (2808) 2809: learn: 0.0850160 test: 0.1400303 best: 0.1400303 (2809) 2810: learn: 0.0850031 test: 0.1400259 best: 0.1400259 (2810) 2811: learn: 0.0849876 test: 0.1400238 best: 0.1400238 (2811) 2812: learn: 0.0849710 test: 0.1400212 best: 0.1400212 (2812) 2813: learn: 0.0849583 test: 0.1400105 best: 0.1400105 (2813) 2814: learn: 0.0849478 test: 0.1400084 best: 0.1400084 (2814) 2815: learn: 0.0849331 test: 0.1400005 best: 0.1400005 (2815) 2816: learn: 0.0849160 test: 0.1400004 best: 0.1400004 (2816) 2817: learn: 0.0848993 test: 0.1399977 best: 0.1399977 (2817) 2818: learn: 0.0848885 test: 0.1400011 best: 0.1399977 (2817) 2819: learn: 0.0848744 test: 0.1399975 best: 0.1399975 (2819) 2820: learn: 0.0848637 test: 0.1399915 best: 0.1399915 (2820) 2821: learn: 0.0848516 test: 0.1399895 best: 0.1399895 (2821) 2822: learn: 0.0848330 test: 0.1399835 best: 0.1399835 (2822) 2823: learn: 0.0848176 test: 0.1399782 best: 0.1399782 (2823) 2824: learn: 0.0848058 test: 0.1399732 best: 0.1399732 (2824) 2825: learn: 0.0847934 test: 0.1399719 best: 0.1399719 (2825) 2826: learn: 0.0847780 test: 0.1399666 best: 0.1399666 (2826) 2827: learn: 0.0847655 test: 0.1399613 best: 0.1399613 (2827) 2828: learn: 0.0847507 test: 0.1399579 best: 0.1399579 (2828) 2829: learn: 0.0847338 test: 0.1399578 best: 0.1399578 (2829) 2830: learn: 0.0847190 test: 0.1399594 best: 0.1399578 (2829) 2831: learn: 0.0847026 test: 0.1399579 best: 0.1399578 (2829) 2832: learn: 0.0846863 test: 0.1399522 best: 0.1399522 (2832) 2833: learn: 0.0846709 test: 0.1399457 best: 0.1399457 (2833) 2834: learn: 0.0846594 test: 0.1399417 best: 0.1399417 (2834) 2835: learn: 0.0846446 test: 0.1399348 best: 0.1399348 (2835) 2836: learn: 0.0846311 test: 0.1399222 best: 0.1399222 (2836) 2837: learn: 0.0846146 test: 0.1399224 best: 0.1399222 (2836) 2838: learn: 0.0846008 test: 0.1399243 best: 0.1399222 (2836) 2839: learn: 0.0845846 test: 0.1399169 best: 0.1399169 (2839) 2840: learn: 0.0845671 test: 0.1399205 best: 0.1399169 (2839) 2841: learn: 0.0845495 test: 0.1399176 best: 0.1399169 (2839) 2842: learn: 0.0845356 test: 0.1399147 best: 0.1399147 (2842) 2843: learn: 0.0845211 test: 0.1399133 best: 0.1399133 (2843) 2844: learn: 0.0845070 test: 0.1399197 best: 0.1399133 (2843) 2845: learn: 0.0844962 test: 0.1399223 best: 0.1399133 (2843) 2846: learn: 0.0844781 test: 0.1399191 best: 0.1399133 (2843) 2847: learn: 0.0844629 test: 0.1399172 best: 0.1399133 (2843) 2848: learn: 0.0844494 test: 0.1399143 best: 0.1399133 (2843) 2849: learn: 0.0844323 test: 0.1399133 best: 0.1399133 (2849) 2850: learn: 0.0844166 test: 0.1399118 best: 0.1399118 (2850) 2851: learn: 0.0844062 test: 0.1399079 best: 0.1399079 (2851) 2852: learn: 0.0843907 test: 0.1399038 best: 0.1399038 (2852) 2853: learn: 0.0843754 test: 0.1399007 best: 0.1399007 (2853) 2854: learn: 0.0843631 test: 0.1399009 best: 0.1399007 (2853) 2855: learn: 0.0843495 test: 0.1399013 best: 0.1399007 (2853) 2856: learn: 0.0843375 test: 0.1398985 best: 0.1398985 (2856) 2857: learn: 0.0843239 test: 0.1398927 best: 0.1398927 (2857) 2858: learn: 0.0843100 test: 0.1398897 best: 0.1398897 (2858) 2859: learn: 0.0842953 test: 0.1398835 best: 0.1398835 (2859) 2860: learn: 0.0842771 test: 0.1398847 best: 0.1398835 (2859) 2861: learn: 0.0842590 test: 0.1398838 best: 0.1398835 (2859) 2862: learn: 0.0842465 test: 0.1398802 best: 0.1398802 (2862) 2863: learn: 0.0842326 test: 0.1398771 best: 0.1398771 (2863) 2864: learn: 0.0842204 test: 0.1398704 best: 0.1398704 (2864) 2865: learn: 0.0842061 test: 0.1398726 best: 0.1398704 (2864) 2866: learn: 0.0841931 test: 0.1398703 best: 0.1398703 (2866) 2867: learn: 0.0841828 test: 0.1398657 best: 0.1398657 (2867) 2868: learn: 0.0841725 test: 0.1398676 best: 0.1398657 (2867) 2869: learn: 0.0841585 test: 0.1398594 best: 0.1398594 (2869) 2870: learn: 0.0841454 test: 0.1398550 best: 0.1398550 (2870) 2871: learn: 0.0841293 test: 0.1398521 best: 0.1398521 (2871) 2872: learn: 0.0841125 test: 0.1398548 best: 0.1398521 (2871) 2873: learn: 0.0840985 test: 0.1398503 best: 0.1398503 (2873) 2874: learn: 0.0840844 test: 0.1398535 best: 0.1398503 (2873) 2875: learn: 0.0840688 test: 0.1398470 best: 0.1398470 (2875) 2876: learn: 0.0840548 test: 0.1398438 best: 0.1398438 (2876) 2877: learn: 0.0840444 test: 0.1398466 best: 0.1398438 (2876) 2878: learn: 0.0840285 test: 0.1398444 best: 0.1398438 (2876) 2879: learn: 0.0840164 test: 0.1398433 best: 0.1398433 (2879) 2880: learn: 0.0840044 test: 0.1398385 best: 0.1398385 (2880) total: 32.6s remaining: 35.2s 2881: learn: 0.0839879 test: 0.1398350 best: 0.1398350 (2881) 2882: learn: 0.0839706 test: 0.1398268 best: 0.1398268 (2882) 2883: learn: 0.0839585 test: 0.1398248 best: 0.1398248 (2883) 2884: learn: 0.0839458 test: 0.1398190 best: 0.1398190 (2884) 2885: learn: 0.0839293 test: 0.1398194 best: 0.1398190 (2884) 2886: learn: 0.0839146 test: 0.1398218 best: 0.1398190 (2884) 2887: learn: 0.0838990 test: 0.1398153 best: 0.1398153 (2887) 2888: learn: 0.0838834 test: 0.1398126 best: 0.1398126 (2888) 2889: learn: 0.0838671 test: 0.1398082 best: 0.1398082 (2889) 2890: learn: 0.0838549 test: 0.1398117 best: 0.1398082 (2889) 2891: learn: 0.0838414 test: 0.1398095 best: 0.1398082 (2889) 2892: learn: 0.0838295 test: 0.1398016 best: 0.1398016 (2892) 2893: learn: 0.0838151 test: 0.1397959 best: 0.1397959 (2893) 2894: learn: 0.0837967 test: 0.1397910 best: 0.1397910 (2894) 2895: learn: 0.0837811 test: 0.1397905 best: 0.1397905 (2895) 2896: learn: 0.0837617 test: 0.1397874 best: 0.1397874 (2896) 2897: learn: 0.0837432 test: 0.1397834 best: 0.1397834 (2897) 2898: learn: 0.0837209 test: 0.1397791 best: 0.1397791 (2898) 2899: learn: 0.0837107 test: 0.1397789 best: 0.1397789 (2899) 2900: learn: 0.0836925 test: 0.1397776 best: 0.1397776 (2900) 2901: learn: 0.0836755 test: 0.1397767 best: 0.1397767 (2901) 2902: learn: 0.0836592 test: 0.1397745 best: 0.1397745 (2902) 2903: learn: 0.0836458 test: 0.1397720 best: 0.1397720 (2903) 2904: learn: 0.0836309 test: 0.1397737 best: 0.1397720 (2903) 2905: learn: 0.0836119 test: 0.1397705 best: 0.1397705 (2905) 2906: learn: 0.0835957 test: 0.1397682 best: 0.1397682 (2906) 2907: learn: 0.0835819 test: 0.1397655 best: 0.1397655 (2907) 2908: learn: 0.0835675 test: 0.1397659 best: 0.1397655 (2907) 2909: learn: 0.0835508 test: 0.1397717 best: 0.1397655 (2907) 2910: learn: 0.0835393 test: 0.1397702 best: 0.1397655 (2907) 2911: learn: 0.0835270 test: 0.1397707 best: 0.1397655 (2907) 2912: learn: 0.0835144 test: 0.1397638 best: 0.1397638 (2912) 2913: learn: 0.0835040 test: 0.1397589 best: 0.1397589 (2913) 2914: learn: 0.0834908 test: 0.1397551 best: 0.1397551 (2914) 2915: learn: 0.0834789 test: 0.1397556 best: 0.1397551 (2914) 2916: learn: 0.0834632 test: 0.1397579 best: 0.1397551 (2914) 2917: learn: 0.0834466 test: 0.1397595 best: 0.1397551 (2914) 2918: learn: 0.0834241 test: 0.1397669 best: 0.1397551 (2914) 2919: learn: 0.0834120 test: 0.1397619 best: 0.1397551 (2914) 2920: learn: 0.0833981 test: 0.1397633 best: 0.1397551 (2914) 2921: learn: 0.0833832 test: 0.1397613 best: 0.1397551 (2914) 2922: learn: 0.0833706 test: 0.1397617 best: 0.1397551 (2914) 2923: learn: 0.0833576 test: 0.1397615 best: 0.1397551 (2914) 2924: learn: 0.0833401 test: 0.1397610 best: 0.1397551 (2914) 2925: learn: 0.0833239 test: 0.1397576 best: 0.1397551 (2914) 2926: learn: 0.0833094 test: 0.1397573 best: 0.1397551 (2914) 2927: learn: 0.0832961 test: 0.1397538 best: 0.1397538 (2927) 2928: learn: 0.0832856 test: 0.1397539 best: 0.1397538 (2927) 2929: learn: 0.0832669 test: 0.1397559 best: 0.1397538 (2927) 2930: learn: 0.0832504 test: 0.1397478 best: 0.1397478 (2930) 2931: learn: 0.0832340 test: 0.1397402 best: 0.1397402 (2931) 2932: learn: 0.0832135 test: 0.1397413 best: 0.1397402 (2931) 2933: learn: 0.0832015 test: 0.1397395 best: 0.1397395 (2933) 2934: learn: 0.0831897 test: 0.1397371 best: 0.1397371 (2934) 2935: learn: 0.0831773 test: 0.1397337 best: 0.1397337 (2935) 2936: learn: 0.0831643 test: 0.1397338 best: 0.1397337 (2935) 2937: learn: 0.0831509 test: 0.1397329 best: 0.1397329 (2937) 2938: learn: 0.0831364 test: 0.1397297 best: 0.1397297 (2938) 2939: learn: 0.0831195 test: 0.1397307 best: 0.1397297 (2938) 2940: learn: 0.0831045 test: 0.1397345 best: 0.1397297 (2938) 2941: learn: 0.0830920 test: 0.1397356 best: 0.1397297 (2938) 2942: learn: 0.0830825 test: 0.1397312 best: 0.1397297 (2938) 2943: learn: 0.0830657 test: 0.1397273 best: 0.1397273 (2943) 2944: learn: 0.0830547 test: 0.1397257 best: 0.1397257 (2944) 2945: learn: 0.0830406 test: 0.1397279 best: 0.1397257 (2944) 2946: learn: 0.0830320 test: 0.1397240 best: 0.1397240 (2946) 2947: learn: 0.0830170 test: 0.1397200 best: 0.1397200 (2947) 2948: learn: 0.0830026 test: 0.1397174 best: 0.1397174 (2948) 2949: learn: 0.0829920 test: 0.1397163 best: 0.1397163 (2949) 2950: learn: 0.0829767 test: 0.1397140 best: 0.1397140 (2950) 2951: learn: 0.0829633 test: 0.1397135 best: 0.1397135 (2951) 2952: learn: 0.0829475 test: 0.1397114 best: 0.1397114 (2952) 2953: learn: 0.0829332 test: 0.1397092 best: 0.1397092 (2953) 2954: learn: 0.0829242 test: 0.1397061 best: 0.1397061 (2954) 2955: learn: 0.0829138 test: 0.1397034 best: 0.1397034 (2955) 2956: learn: 0.0829008 test: 0.1396989 best: 0.1396989 (2956) 2957: learn: 0.0828855 test: 0.1397018 best: 0.1396989 (2956) 2958: learn: 0.0828730 test: 0.1397049 best: 0.1396989 (2956) 2959: learn: 0.0828555 test: 0.1396988 best: 0.1396988 (2959) 2960: learn: 0.0828391 test: 0.1396987 best: 0.1396987 (2960) 2961: learn: 0.0828246 test: 0.1396973 best: 0.1396973 (2961) 2962: learn: 0.0828146 test: 0.1396918 best: 0.1396918 (2962) 2963: learn: 0.0828024 test: 0.1396904 best: 0.1396904 (2963) 2964: learn: 0.0827845 test: 0.1396915 best: 0.1396904 (2963) 2965: learn: 0.0827727 test: 0.1396893 best: 0.1396893 (2965) 2966: learn: 0.0827632 test: 0.1396896 best: 0.1396893 (2965) 2967: learn: 0.0827515 test: 0.1396893 best: 0.1396893 (2965) 2968: learn: 0.0827406 test: 0.1396842 best: 0.1396842 (2968) 2969: learn: 0.0827259 test: 0.1396826 best: 0.1396826 (2969) 2970: learn: 0.0827122 test: 0.1396847 best: 0.1396826 (2969) 2971: learn: 0.0826945 test: 0.1396851 best: 0.1396826 (2969) 2972: learn: 0.0826828 test: 0.1396867 best: 0.1396826 (2969) 2973: learn: 0.0826690 test: 0.1396857 best: 0.1396826 (2969) 2974: learn: 0.0826571 test: 0.1396859 best: 0.1396826 (2969) 2975: learn: 0.0826424 test: 0.1396858 best: 0.1396826 (2969) 2976: learn: 0.0826298 test: 0.1396906 best: 0.1396826 (2969) 2977: learn: 0.0826155 test: 0.1396860 best: 0.1396826 (2969) 2978: learn: 0.0825962 test: 0.1396845 best: 0.1396826 (2969) 2979: learn: 0.0825821 test: 0.1396867 best: 0.1396826 (2969) 2980: learn: 0.0825688 test: 0.1396801 best: 0.1396801 (2980) 2981: learn: 0.0825533 test: 0.1396779 best: 0.1396779 (2981) 2982: learn: 0.0825379 test: 0.1396774 best: 0.1396774 (2982) 2983: learn: 0.0825253 test: 0.1396733 best: 0.1396733 (2983) 2984: learn: 0.0825135 test: 0.1396717 best: 0.1396717 (2984) 2985: learn: 0.0824976 test: 0.1396668 best: 0.1396668 (2985) 2986: learn: 0.0824889 test: 0.1396603 best: 0.1396603 (2986) 2987: learn: 0.0824721 test: 0.1396644 best: 0.1396603 (2986) 2988: learn: 0.0824562 test: 0.1396622 best: 0.1396603 (2986) 2989: learn: 0.0824449 test: 0.1396548 best: 0.1396548 (2989) 2990: learn: 0.0824275 test: 0.1396579 best: 0.1396548 (2989) 2991: learn: 0.0824118 test: 0.1396579 best: 0.1396548 (2989) 2992: learn: 0.0823975 test: 0.1396559 best: 0.1396548 (2989) 2993: learn: 0.0823856 test: 0.1396544 best: 0.1396544 (2993) 2994: learn: 0.0823719 test: 0.1396524 best: 0.1396524 (2994) 2995: learn: 0.0823594 test: 0.1396472 best: 0.1396472 (2995) 2996: learn: 0.0823449 test: 0.1396473 best: 0.1396472 (2995) 2997: learn: 0.0823331 test: 0.1396444 best: 0.1396444 (2997) 2998: learn: 0.0823196 test: 0.1396380 best: 0.1396380 (2998) 2999: learn: 0.0823068 test: 0.1396355 best: 0.1396355 (2999) 3000: learn: 0.0822880 test: 0.1396306 best: 0.1396306 (3000) 3001: learn: 0.0822727 test: 0.1396297 best: 0.1396297 (3001) 3002: learn: 0.0822569 test: 0.1396360 best: 0.1396297 (3001) 3003: learn: 0.0822423 test: 0.1396346 best: 0.1396297 (3001) 3004: learn: 0.0822273 test: 0.1396322 best: 0.1396297 (3001) 3005: learn: 0.0822158 test: 0.1396256 best: 0.1396256 (3005) 3006: learn: 0.0822022 test: 0.1396276 best: 0.1396256 (3005) 3007: learn: 0.0821905 test: 0.1396294 best: 0.1396256 (3005) 3008: learn: 0.0821779 test: 0.1396254 best: 0.1396254 (3008) 3009: learn: 0.0821640 test: 0.1396237 best: 0.1396237 (3009) 3010: learn: 0.0821509 test: 0.1396215 best: 0.1396215 (3010) 3011: learn: 0.0821358 test: 0.1396225 best: 0.1396215 (3010) 3012: learn: 0.0821221 test: 0.1396232 best: 0.1396215 (3010) 3013: learn: 0.0821118 test: 0.1396196 best: 0.1396196 (3013) 3014: learn: 0.0820980 test: 0.1396193 best: 0.1396193 (3014) 3015: learn: 0.0820779 test: 0.1396146 best: 0.1396146 (3015) 3016: learn: 0.0820663 test: 0.1396143 best: 0.1396143 (3016) 3017: learn: 0.0820534 test: 0.1396137 best: 0.1396137 (3017) 3018: learn: 0.0820373 test: 0.1396122 best: 0.1396122 (3018) 3019: learn: 0.0820220 test: 0.1396139 best: 0.1396122 (3018) 3020: learn: 0.0820080 test: 0.1396133 best: 0.1396122 (3018) 3021: learn: 0.0819950 test: 0.1396103 best: 0.1396103 (3021) total: 34.2s remaining: 33.7s 3022: learn: 0.0819812 test: 0.1396094 best: 0.1396094 (3022) 3023: learn: 0.0819662 test: 0.1396074 best: 0.1396074 (3023) 3024: learn: 0.0819500 test: 0.1396078 best: 0.1396074 (3023) 3025: learn: 0.0819343 test: 0.1396063 best: 0.1396063 (3025) 3026: learn: 0.0819183 test: 0.1396073 best: 0.1396063 (3025) 3027: learn: 0.0818999 test: 0.1396050 best: 0.1396050 (3027) 3028: learn: 0.0818925 test: 0.1396050 best: 0.1396050 (3027) 3029: learn: 0.0818779 test: 0.1395981 best: 0.1395981 (3029) 3030: learn: 0.0818657 test: 0.1395972 best: 0.1395972 (3030) 3031: learn: 0.0818489 test: 0.1395972 best: 0.1395972 (3030) 3032: learn: 0.0818371 test: 0.1395952 best: 0.1395952 (3032) 3033: learn: 0.0818220 test: 0.1395968 best: 0.1395952 (3032) 3034: learn: 0.0818062 test: 0.1395970 best: 0.1395952 (3032) 3035: learn: 0.0817908 test: 0.1395931 best: 0.1395931 (3035) 3036: learn: 0.0817747 test: 0.1395922 best: 0.1395922 (3036) 3037: learn: 0.0817629 test: 0.1395914 best: 0.1395914 (3037) 3038: learn: 0.0817464 test: 0.1395898 best: 0.1395898 (3038) 3039: learn: 0.0817278 test: 0.1395862 best: 0.1395862 (3039) 3040: learn: 0.0817118 test: 0.1395823 best: 0.1395823 (3040) 3041: learn: 0.0816968 test: 0.1395777 best: 0.1395777 (3041) 3042: learn: 0.0816837 test: 0.1395827 best: 0.1395777 (3041) 3043: learn: 0.0816678 test: 0.1395825 best: 0.1395777 (3041) 3044: learn: 0.0816567 test: 0.1395879 best: 0.1395777 (3041) 3045: learn: 0.0816470 test: 0.1395827 best: 0.1395777 (3041) 3046: learn: 0.0816391 test: 0.1395795 best: 0.1395777 (3041) 3047: learn: 0.0816190 test: 0.1395759 best: 0.1395759 (3047) 3048: learn: 0.0816060 test: 0.1395746 best: 0.1395746 (3048) 3049: learn: 0.0815901 test: 0.1395752 best: 0.1395746 (3048) 3050: learn: 0.0815742 test: 0.1395708 best: 0.1395708 (3050) 3051: learn: 0.0815620 test: 0.1395672 best: 0.1395672 (3051) 3052: learn: 0.0815490 test: 0.1395660 best: 0.1395660 (3052) 3053: learn: 0.0815342 test: 0.1395629 best: 0.1395629 (3053) 3054: learn: 0.0815224 test: 0.1395634 best: 0.1395629 (3053) 3055: learn: 0.0815066 test: 0.1395631 best: 0.1395629 (3053) 3056: learn: 0.0814924 test: 0.1395590 best: 0.1395590 (3056) 3057: learn: 0.0814775 test: 0.1395599 best: 0.1395590 (3056) 3058: learn: 0.0814659 test: 0.1395573 best: 0.1395573 (3058) 3059: learn: 0.0814499 test: 0.1395582 best: 0.1395573 (3058) 3060: learn: 0.0814363 test: 0.1395543 best: 0.1395543 (3060) 3061: learn: 0.0814249 test: 0.1395513 best: 0.1395513 (3061) 3062: learn: 0.0814112 test: 0.1395553 best: 0.1395513 (3061) 3063: learn: 0.0813980 test: 0.1395553 best: 0.1395513 (3061) 3064: learn: 0.0813833 test: 0.1395584 best: 0.1395513 (3061) 3065: learn: 0.0813689 test: 0.1395572 best: 0.1395513 (3061) 3066: learn: 0.0813566 test: 0.1395580 best: 0.1395513 (3061) 3067: learn: 0.0813437 test: 0.1395503 best: 0.1395503 (3067) 3068: learn: 0.0813295 test: 0.1395437 best: 0.1395437 (3068) 3069: learn: 0.0813185 test: 0.1395479 best: 0.1395437 (3068) 3070: learn: 0.0812965 test: 0.1395514 best: 0.1395437 (3068) 3071: learn: 0.0812836 test: 0.1395551 best: 0.1395437 (3068) 3072: learn: 0.0812706 test: 0.1395492 best: 0.1395437 (3068) 3073: learn: 0.0812604 test: 0.1395467 best: 0.1395437 (3068) 3074: learn: 0.0812489 test: 0.1395446 best: 0.1395437 (3068) 3075: learn: 0.0812356 test: 0.1395416 best: 0.1395416 (3075) 3076: learn: 0.0812189 test: 0.1395382 best: 0.1395382 (3076) 3077: learn: 0.0812094 test: 0.1395385 best: 0.1395382 (3076) 3078: learn: 0.0811978 test: 0.1395372 best: 0.1395372 (3078) 3079: learn: 0.0811879 test: 0.1395332 best: 0.1395332 (3079) 3080: learn: 0.0811733 test: 0.1395416 best: 0.1395332 (3079) 3081: learn: 0.0811608 test: 0.1395386 best: 0.1395332 (3079) 3082: learn: 0.0811441 test: 0.1395400 best: 0.1395332 (3079) 3083: learn: 0.0811268 test: 0.1395405 best: 0.1395332 (3079) 3084: learn: 0.0811145 test: 0.1395418 best: 0.1395332 (3079) 3085: learn: 0.0810997 test: 0.1395388 best: 0.1395332 (3079) 3086: learn: 0.0810876 test: 0.1395369 best: 0.1395332 (3079) 3087: learn: 0.0810752 test: 0.1395380 best: 0.1395332 (3079) 3088: learn: 0.0810649 test: 0.1395363 best: 0.1395332 (3079) 3089: learn: 0.0810539 test: 0.1395368 best: 0.1395332 (3079) 3090: learn: 0.0810424 test: 0.1395321 best: 0.1395321 (3090) 3091: learn: 0.0810250 test: 0.1395285 best: 0.1395285 (3091) 3092: learn: 0.0810078 test: 0.1395291 best: 0.1395285 (3091) 3093: learn: 0.0809971 test: 0.1395244 best: 0.1395244 (3093) 3094: learn: 0.0809835 test: 0.1395142 best: 0.1395142 (3094) 3095: learn: 0.0809705 test: 0.1395151 best: 0.1395142 (3094) 3096: learn: 0.0809580 test: 0.1395177 best: 0.1395142 (3094) 3097: learn: 0.0809448 test: 0.1395182 best: 0.1395142 (3094) 3098: learn: 0.0809274 test: 0.1395150 best: 0.1395142 (3094) 3099: learn: 0.0809117 test: 0.1395089 best: 0.1395089 (3099) 3100: learn: 0.0808986 test: 0.1395064 best: 0.1395064 (3100) 3101: learn: 0.0808815 test: 0.1395024 best: 0.1395024 (3101) 3102: learn: 0.0808712 test: 0.1395006 best: 0.1395006 (3102) 3103: learn: 0.0808537 test: 0.1394935 best: 0.1394935 (3103) 3104: learn: 0.0808441 test: 0.1394919 best: 0.1394919 (3104) 3105: learn: 0.0808321 test: 0.1394914 best: 0.1394914 (3105) 3106: learn: 0.0808198 test: 0.1394879 best: 0.1394879 (3106) 3107: learn: 0.0808035 test: 0.1394856 best: 0.1394856 (3107) 3108: learn: 0.0807901 test: 0.1394823 best: 0.1394823 (3108) 3109: learn: 0.0807759 test: 0.1394806 best: 0.1394806 (3109) 3110: learn: 0.0807601 test: 0.1394763 best: 0.1394763 (3110) 3111: learn: 0.0807494 test: 0.1394729 best: 0.1394729 (3111) 3112: learn: 0.0807402 test: 0.1394688 best: 0.1394688 (3112) 3113: learn: 0.0807258 test: 0.1394621 best: 0.1394621 (3113) 3114: learn: 0.0807136 test: 0.1394622 best: 0.1394621 (3113) 3115: learn: 0.0806998 test: 0.1394632 best: 0.1394621 (3113) 3116: learn: 0.0806918 test: 0.1394634 best: 0.1394621 (3113) 3117: learn: 0.0806763 test: 0.1394655 best: 0.1394621 (3113) 3118: learn: 0.0806664 test: 0.1394638 best: 0.1394621 (3113) 3119: learn: 0.0806538 test: 0.1394629 best: 0.1394621 (3113) 3120: learn: 0.0806452 test: 0.1394595 best: 0.1394595 (3120) 3121: learn: 0.0806370 test: 0.1394598 best: 0.1394595 (3120) 3122: learn: 0.0806226 test: 0.1394597 best: 0.1394595 (3120) 3123: learn: 0.0806102 test: 0.1394571 best: 0.1394571 (3123) 3124: learn: 0.0805958 test: 0.1394555 best: 0.1394555 (3124) total: 35.5s remaining: 32.6s 3125: learn: 0.0805855 test: 0.1394552 best: 0.1394552 (3125) 3126: learn: 0.0805742 test: 0.1394485 best: 0.1394485 (3126) 3127: learn: 0.0805637 test: 0.1394426 best: 0.1394426 (3127) 3128: learn: 0.0805545 test: 0.1394388 best: 0.1394388 (3128) 3129: learn: 0.0805409 test: 0.1394348 best: 0.1394348 (3129) 3130: learn: 0.0805271 test: 0.1394301 best: 0.1394301 (3130) 3131: learn: 0.0805163 test: 0.1394319 best: 0.1394301 (3130) 3132: learn: 0.0805064 test: 0.1394274 best: 0.1394274 (3132) 3133: learn: 0.0804910 test: 0.1394242 best: 0.1394242 (3133) 3134: learn: 0.0804766 test: 0.1394193 best: 0.1394193 (3134) 3135: learn: 0.0804562 test: 0.1394128 best: 0.1394128 (3135) 3136: learn: 0.0804413 test: 0.1394111 best: 0.1394111 (3136) 3137: learn: 0.0804262 test: 0.1394017 best: 0.1394017 (3137) 3138: learn: 0.0804099 test: 0.1394008 best: 0.1394008 (3138) 3139: learn: 0.0803982 test: 0.1393986 best: 0.1393986 (3139) 3140: learn: 0.0803850 test: 0.1393958 best: 0.1393958 (3140) 3141: learn: 0.0803683 test: 0.1393952 best: 0.1393952 (3141) 3142: learn: 0.0803586 test: 0.1393937 best: 0.1393937 (3142) 3143: learn: 0.0803464 test: 0.1393904 best: 0.1393904 (3143) 3144: learn: 0.0803345 test: 0.1393904 best: 0.1393904 (3144) 3145: learn: 0.0803172 test: 0.1393866 best: 0.1393866 (3145) 3146: learn: 0.0803030 test: 0.1393809 best: 0.1393809 (3146) 3147: learn: 0.0802845 test: 0.1393811 best: 0.1393809 (3146) 3148: learn: 0.0802732 test: 0.1393799 best: 0.1393799 (3148) 3149: learn: 0.0802602 test: 0.1393776 best: 0.1393776 (3149) 3150: learn: 0.0802460 test: 0.1393778 best: 0.1393776 (3149) 3151: learn: 0.0802315 test: 0.1393813 best: 0.1393776 (3149) 3152: learn: 0.0802129 test: 0.1393816 best: 0.1393776 (3149) 3153: learn: 0.0801975 test: 0.1393798 best: 0.1393776 (3149) 3154: learn: 0.0801877 test: 0.1393741 best: 0.1393741 (3154) 3155: learn: 0.0801763 test: 0.1393758 best: 0.1393741 (3154) 3156: learn: 0.0801657 test: 0.1393801 best: 0.1393741 (3154) 3157: learn: 0.0801557 test: 0.1393787 best: 0.1393741 (3154) 3158: learn: 0.0801429 test: 0.1393799 best: 0.1393741 (3154) 3159: learn: 0.0801311 test: 0.1393794 best: 0.1393741 (3154) 3160: learn: 0.0801204 test: 0.1393744 best: 0.1393741 (3154) 3161: learn: 0.0801085 test: 0.1393683 best: 0.1393683 (3161) 3162: learn: 0.0800995 test: 0.1393681 best: 0.1393681 (3162) 3163: learn: 0.0800876 test: 0.1393619 best: 0.1393619 (3163) 3164: learn: 0.0800749 test: 0.1393595 best: 0.1393595 (3164) 3165: learn: 0.0800594 test: 0.1393588 best: 0.1393588 (3165) 3166: learn: 0.0800484 test: 0.1393574 best: 0.1393574 (3166) 3167: learn: 0.0800408 test: 0.1393501 best: 0.1393501 (3167) 3168: learn: 0.0800255 test: 0.1393466 best: 0.1393466 (3168) 3169: learn: 0.0800130 test: 0.1393432 best: 0.1393432 (3169) 3170: learn: 0.0799976 test: 0.1393424 best: 0.1393424 (3170) 3171: learn: 0.0799838 test: 0.1393402 best: 0.1393402 (3171) 3172: learn: 0.0799704 test: 0.1393435 best: 0.1393402 (3171) 3173: learn: 0.0799589 test: 0.1393435 best: 0.1393402 (3171) 3174: learn: 0.0799500 test: 0.1393399 best: 0.1393399 (3174) 3175: learn: 0.0799389 test: 0.1393388 best: 0.1393388 (3175) 3176: learn: 0.0799255 test: 0.1393388 best: 0.1393388 (3176) 3177: learn: 0.0799115 test: 0.1393360 best: 0.1393360 (3177) 3178: learn: 0.0799007 test: 0.1393339 best: 0.1393339 (3178) 3179: learn: 0.0798886 test: 0.1393362 best: 0.1393339 (3178) 3180: learn: 0.0798805 test: 0.1393340 best: 0.1393339 (3178) 3181: learn: 0.0798743 test: 0.1393305 best: 0.1393305 (3181) 3182: learn: 0.0798585 test: 0.1393354 best: 0.1393305 (3181) 3183: learn: 0.0798461 test: 0.1393322 best: 0.1393305 (3181) 3184: learn: 0.0798319 test: 0.1393295 best: 0.1393295 (3184) 3185: learn: 0.0798177 test: 0.1393303 best: 0.1393295 (3184) 3186: learn: 0.0798052 test: 0.1393272 best: 0.1393272 (3186) 3187: learn: 0.0797901 test: 0.1393253 best: 0.1393253 (3187) 3188: learn: 0.0797746 test: 0.1393201 best: 0.1393201 (3188) 3189: learn: 0.0797594 test: 0.1393221 best: 0.1393201 (3188) 3190: learn: 0.0797427 test: 0.1393229 best: 0.1393201 (3188) 3191: learn: 0.0797271 test: 0.1393243 best: 0.1393201 (3188) 3192: learn: 0.0797155 test: 0.1393245 best: 0.1393201 (3188) 3193: learn: 0.0797014 test: 0.1393158 best: 0.1393158 (3193) 3194: learn: 0.0796896 test: 0.1393102 best: 0.1393102 (3194) 3195: learn: 0.0796770 test: 0.1393134 best: 0.1393102 (3194) 3196: learn: 0.0796638 test: 0.1393128 best: 0.1393102 (3194) 3197: learn: 0.0796493 test: 0.1393131 best: 0.1393102 (3194) 3198: learn: 0.0796391 test: 0.1393065 best: 0.1393065 (3198) 3199: learn: 0.0796248 test: 0.1393086 best: 0.1393065 (3198) 3200: learn: 0.0796139 test: 0.1393035 best: 0.1393035 (3200) 3201: learn: 0.0796019 test: 0.1393043 best: 0.1393035 (3200) 3202: learn: 0.0795902 test: 0.1392978 best: 0.1392978 (3202) 3203: learn: 0.0795739 test: 0.1392939 best: 0.1392939 (3203) 3204: learn: 0.0795572 test: 0.1392932 best: 0.1392932 (3204) 3205: learn: 0.0795454 test: 0.1392944 best: 0.1392932 (3204) 3206: learn: 0.0795302 test: 0.1392929 best: 0.1392929 (3206) 3207: learn: 0.0795163 test: 0.1392907 best: 0.1392907 (3207) 3208: learn: 0.0794994 test: 0.1392925 best: 0.1392907 (3207) 3209: learn: 0.0794888 test: 0.1392876 best: 0.1392876 (3209) 3210: learn: 0.0794735 test: 0.1392861 best: 0.1392861 (3210) 3211: learn: 0.0794636 test: 0.1392805 best: 0.1392805 (3211) 3212: learn: 0.0794498 test: 0.1392759 best: 0.1392759 (3212) 3213: learn: 0.0794365 test: 0.1392803 best: 0.1392759 (3212) 3214: learn: 0.0794218 test: 0.1392850 best: 0.1392759 (3212) 3215: learn: 0.0794038 test: 0.1392850 best: 0.1392759 (3212) 3216: learn: 0.0793880 test: 0.1392837 best: 0.1392759 (3212) 3217: learn: 0.0793740 test: 0.1392795 best: 0.1392759 (3212) 3218: learn: 0.0793652 test: 0.1392796 best: 0.1392759 (3212) 3219: learn: 0.0793508 test: 0.1392823 best: 0.1392759 (3212) 3220: learn: 0.0793405 test: 0.1392806 best: 0.1392759 (3212) 3221: learn: 0.0793272 test: 0.1392758 best: 0.1392758 (3221) 3222: learn: 0.0793118 test: 0.1392733 best: 0.1392733 (3222) 3223: learn: 0.0792992 test: 0.1392732 best: 0.1392732 (3223) 3224: learn: 0.0792858 test: 0.1392734 best: 0.1392732 (3223) 3225: learn: 0.0792744 test: 0.1392725 best: 0.1392725 (3225) 3226: learn: 0.0792588 test: 0.1392710 best: 0.1392710 (3226) 3227: learn: 0.0792440 test: 0.1392737 best: 0.1392710 (3226) 3228: learn: 0.0792327 test: 0.1392682 best: 0.1392682 (3228) 3229: learn: 0.0792185 test: 0.1392716 best: 0.1392682 (3228) 3230: learn: 0.0792035 test: 0.1392699 best: 0.1392682 (3228) 3231: learn: 0.0791913 test: 0.1392677 best: 0.1392677 (3231) 3232: learn: 0.0791793 test: 0.1392608 best: 0.1392608 (3232) 3233: learn: 0.0791689 test: 0.1392617 best: 0.1392608 (3232) 3234: learn: 0.0791573 test: 0.1392581 best: 0.1392581 (3234) 3235: learn: 0.0791395 test: 0.1392511 best: 0.1392511 (3235) 3236: learn: 0.0791317 test: 0.1392500 best: 0.1392500 (3236) 3237: learn: 0.0791157 test: 0.1392479 best: 0.1392479 (3237) 3238: learn: 0.0791044 test: 0.1392500 best: 0.1392479 (3237) 3239: learn: 0.0790944 test: 0.1392497 best: 0.1392479 (3237) 3240: learn: 0.0790804 test: 0.1392435 best: 0.1392435 (3240) 3241: learn: 0.0790709 test: 0.1392383 best: 0.1392383 (3241) 3242: learn: 0.0790569 test: 0.1392385 best: 0.1392383 (3241) 3243: learn: 0.0790414 test: 0.1392341 best: 0.1392341 (3243) 3244: learn: 0.0790278 test: 0.1392342 best: 0.1392341 (3243) 3245: learn: 0.0790161 test: 0.1392353 best: 0.1392341 (3243) 3246: learn: 0.0790034 test: 0.1392317 best: 0.1392317 (3246) 3247: learn: 0.0789883 test: 0.1392306 best: 0.1392306 (3247) 3248: learn: 0.0789755 test: 0.1392312 best: 0.1392306 (3247) 3249: learn: 0.0789636 test: 0.1392334 best: 0.1392306 (3247) 3250: learn: 0.0789530 test: 0.1392368 best: 0.1392306 (3247) 3251: learn: 0.0789428 test: 0.1392360 best: 0.1392306 (3247) 3252: learn: 0.0789304 test: 0.1392366 best: 0.1392306 (3247) 3253: learn: 0.0789169 test: 0.1392301 best: 0.1392301 (3253) 3254: learn: 0.0789024 test: 0.1392278 best: 0.1392278 (3254) 3255: learn: 0.0788902 test: 0.1392235 best: 0.1392235 (3255) total: 37s remaining: 31.2s 3256: learn: 0.0788818 test: 0.1392227 best: 0.1392227 (3256) 3257: learn: 0.0788669 test: 0.1392261 best: 0.1392227 (3256) 3258: learn: 0.0788530 test: 0.1392288 best: 0.1392227 (3256) 3259: learn: 0.0788434 test: 0.1392259 best: 0.1392227 (3256) 3260: learn: 0.0788280 test: 0.1392227 best: 0.1392227 (3260) 3261: learn: 0.0788151 test: 0.1392173 best: 0.1392173 (3261) 3262: learn: 0.0788042 test: 0.1392170 best: 0.1392170 (3262) 3263: learn: 0.0787902 test: 0.1392153 best: 0.1392153 (3263) 3264: learn: 0.0787768 test: 0.1392128 best: 0.1392128 (3264) 3265: learn: 0.0787613 test: 0.1392091 best: 0.1392091 (3265) 3266: learn: 0.0787526 test: 0.1392108 best: 0.1392091 (3265) 3267: learn: 0.0787360 test: 0.1392086 best: 0.1392086 (3267) 3268: learn: 0.0787224 test: 0.1392102 best: 0.1392086 (3267) 3269: learn: 0.0787077 test: 0.1392034 best: 0.1392034 (3269) 3270: learn: 0.0786946 test: 0.1392012 best: 0.1392012 (3270) 3271: learn: 0.0786820 test: 0.1391945 best: 0.1391945 (3271) 3272: learn: 0.0786654 test: 0.1391929 best: 0.1391929 (3272) 3273: learn: 0.0786514 test: 0.1391937 best: 0.1391929 (3272) 3274: learn: 0.0786377 test: 0.1391922 best: 0.1391922 (3274) 3275: learn: 0.0786249 test: 0.1391892 best: 0.1391892 (3275) 3276: learn: 0.0786133 test: 0.1391890 best: 0.1391890 (3276) 3277: learn: 0.0786022 test: 0.1391839 best: 0.1391839 (3277) 3278: learn: 0.0785918 test: 0.1391775 best: 0.1391775 (3278) 3279: learn: 0.0785806 test: 0.1391783 best: 0.1391775 (3278) 3280: learn: 0.0785685 test: 0.1391750 best: 0.1391750 (3280) 3281: learn: 0.0785531 test: 0.1391809 best: 0.1391750 (3280) 3282: learn: 0.0785398 test: 0.1391807 best: 0.1391750 (3280) 3283: learn: 0.0785285 test: 0.1391790 best: 0.1391750 (3280) 3284: learn: 0.0785174 test: 0.1391784 best: 0.1391750 (3280) 3285: learn: 0.0785070 test: 0.1391764 best: 0.1391750 (3280) 3286: learn: 0.0784933 test: 0.1391734 best: 0.1391734 (3286) 3287: learn: 0.0784796 test: 0.1391700 best: 0.1391700 (3287) 3288: learn: 0.0784643 test: 0.1391747 best: 0.1391700 (3287) 3289: learn: 0.0784516 test: 0.1391769 best: 0.1391700 (3287) 3290: learn: 0.0784421 test: 0.1391774 best: 0.1391700 (3287) 3291: learn: 0.0784298 test: 0.1391758 best: 0.1391700 (3287) 3292: learn: 0.0784161 test: 0.1391753 best: 0.1391700 (3287) 3293: learn: 0.0783996 test: 0.1391689 best: 0.1391689 (3293) 3294: learn: 0.0783863 test: 0.1391676 best: 0.1391676 (3294) 3295: learn: 0.0783732 test: 0.1391680 best: 0.1391676 (3294) 3296: learn: 0.0783617 test: 0.1391667 best: 0.1391667 (3296) 3297: learn: 0.0783483 test: 0.1391633 best: 0.1391633 (3297) 3298: learn: 0.0783332 test: 0.1391592 best: 0.1391592 (3298) 3299: learn: 0.0783174 test: 0.1391638 best: 0.1391592 (3298) 3300: learn: 0.0783018 test: 0.1391598 best: 0.1391592 (3298) 3301: learn: 0.0782882 test: 0.1391599 best: 0.1391592 (3298) 3302: learn: 0.0782786 test: 0.1391553 best: 0.1391553 (3302) 3303: learn: 0.0782670 test: 0.1391555 best: 0.1391553 (3302) 3304: learn: 0.0782529 test: 0.1391467 best: 0.1391467 (3304) 3305: learn: 0.0782402 test: 0.1391452 best: 0.1391452 (3305) 3306: learn: 0.0782236 test: 0.1391484 best: 0.1391452 (3305) 3307: learn: 0.0782118 test: 0.1391494 best: 0.1391452 (3305) 3308: learn: 0.0782000 test: 0.1391464 best: 0.1391452 (3305) 3309: learn: 0.0781867 test: 0.1391436 best: 0.1391436 (3309) 3310: learn: 0.0781735 test: 0.1391441 best: 0.1391436 (3309) 3311: learn: 0.0781611 test: 0.1391440 best: 0.1391436 (3309) 3312: learn: 0.0781546 test: 0.1391392 best: 0.1391392 (3312) 3313: learn: 0.0781451 test: 0.1391362 best: 0.1391362 (3313) 3314: learn: 0.0781305 test: 0.1391350 best: 0.1391350 (3314) 3315: learn: 0.0781216 test: 0.1391365 best: 0.1391350 (3314) 3316: learn: 0.0781105 test: 0.1391378 best: 0.1391350 (3314) 3317: learn: 0.0780937 test: 0.1391402 best: 0.1391350 (3314) 3318: learn: 0.0780812 test: 0.1391411 best: 0.1391350 (3314) 3319: learn: 0.0780730 test: 0.1391377 best: 0.1391350 (3314) 3320: learn: 0.0780616 test: 0.1391361 best: 0.1391350 (3314) 3321: learn: 0.0780451 test: 0.1391327 best: 0.1391327 (3321) 3322: learn: 0.0780337 test: 0.1391280 best: 0.1391280 (3322) 3323: learn: 0.0780213 test: 0.1391276 best: 0.1391276 (3323) 3324: learn: 0.0780096 test: 0.1391269 best: 0.1391269 (3324) 3325: learn: 0.0779994 test: 0.1391261 best: 0.1391261 (3325) 3326: learn: 0.0779891 test: 0.1391230 best: 0.1391230 (3326) 3327: learn: 0.0779769 test: 0.1391220 best: 0.1391220 (3327) 3328: learn: 0.0779650 test: 0.1391174 best: 0.1391174 (3328) 3329: learn: 0.0779506 test: 0.1391145 best: 0.1391145 (3329) 3330: learn: 0.0779388 test: 0.1391136 best: 0.1391136 (3330) 3331: learn: 0.0779288 test: 0.1391175 best: 0.1391136 (3330) 3332: learn: 0.0779146 test: 0.1391159 best: 0.1391136 (3330) 3333: learn: 0.0779055 test: 0.1391157 best: 0.1391136 (3330) 3334: learn: 0.0778981 test: 0.1391136 best: 0.1391136 (3330) 3335: learn: 0.0778862 test: 0.1391123 best: 0.1391123 (3335) 3336: learn: 0.0778741 test: 0.1391109 best: 0.1391109 (3336) 3337: learn: 0.0778625 test: 0.1391088 best: 0.1391088 (3337) 3338: learn: 0.0778494 test: 0.1390991 best: 0.1390991 (3338) 3339: learn: 0.0778359 test: 0.1390982 best: 0.1390982 (3339) 3340: learn: 0.0778219 test: 0.1390955 best: 0.1390955 (3340) 3341: learn: 0.0778119 test: 0.1390937 best: 0.1390937 (3341) 3342: learn: 0.0777969 test: 0.1390976 best: 0.1390937 (3341) 3343: learn: 0.0777875 test: 0.1391023 best: 0.1390937 (3341) 3344: learn: 0.0777788 test: 0.1391017 best: 0.1390937 (3341) 3345: learn: 0.0777674 test: 0.1390999 best: 0.1390937 (3341) 3346: learn: 0.0777575 test: 0.1390985 best: 0.1390937 (3341) 3347: learn: 0.0777401 test: 0.1390966 best: 0.1390937 (3341) 3348: learn: 0.0777259 test: 0.1391004 best: 0.1390937 (3341) 3349: learn: 0.0777138 test: 0.1390971 best: 0.1390937 (3341) 3350: learn: 0.0776986 test: 0.1390986 best: 0.1390937 (3341) 3351: learn: 0.0776899 test: 0.1390952 best: 0.1390937 (3341) 3352: learn: 0.0776741 test: 0.1390945 best: 0.1390937 (3341) 3353: learn: 0.0776619 test: 0.1390932 best: 0.1390932 (3353) 3354: learn: 0.0776486 test: 0.1390930 best: 0.1390930 (3354) 3355: learn: 0.0776386 test: 0.1390924 best: 0.1390924 (3355) 3356: learn: 0.0776298 test: 0.1390953 best: 0.1390924 (3355) 3357: learn: 0.0776165 test: 0.1390936 best: 0.1390924 (3355) 3358: learn: 0.0776040 test: 0.1390900 best: 0.1390900 (3358) 3359: learn: 0.0775913 test: 0.1390925 best: 0.1390900 (3358) 3360: learn: 0.0775793 test: 0.1390893 best: 0.1390893 (3360) 3361: learn: 0.0775700 test: 0.1390884 best: 0.1390884 (3361) 3362: learn: 0.0775516 test: 0.1390800 best: 0.1390800 (3362) 3363: learn: 0.0775372 test: 0.1390810 best: 0.1390800 (3362) 3364: learn: 0.0775301 test: 0.1390791 best: 0.1390791 (3364) 3365: learn: 0.0775171 test: 0.1390739 best: 0.1390739 (3365) 3366: learn: 0.0774995 test: 0.1390735 best: 0.1390735 (3366) 3367: learn: 0.0774870 test: 0.1390752 best: 0.1390735 (3366) 3368: learn: 0.0774779 test: 0.1390767 best: 0.1390735 (3366) 3369: learn: 0.0774619 test: 0.1390728 best: 0.1390728 (3369) 3370: learn: 0.0774475 test: 0.1390690 best: 0.1390690 (3370) 3371: learn: 0.0774324 test: 0.1390643 best: 0.1390643 (3371) 3372: learn: 0.0774181 test: 0.1390611 best: 0.1390611 (3372) 3373: learn: 0.0774034 test: 0.1390645 best: 0.1390611 (3372) 3374: learn: 0.0773927 test: 0.1390622 best: 0.1390611 (3372) 3375: learn: 0.0773801 test: 0.1390615 best: 0.1390611 (3372) 3376: learn: 0.0773654 test: 0.1390603 best: 0.1390603 (3376) 3377: learn: 0.0773482 test: 0.1390586 best: 0.1390586 (3377) 3378: learn: 0.0773383 test: 0.1390558 best: 0.1390558 (3378) 3379: learn: 0.0773222 test: 0.1390559 best: 0.1390558 (3378) 3380: learn: 0.0773107 test: 0.1390597 best: 0.1390558 (3378) 3381: learn: 0.0772969 test: 0.1390624 best: 0.1390558 (3378) 3382: learn: 0.0772810 test: 0.1390617 best: 0.1390558 (3378) 3383: learn: 0.0772727 test: 0.1390604 best: 0.1390558 (3378) 3384: learn: 0.0772651 test: 0.1390590 best: 0.1390558 (3378) 3385: learn: 0.0772511 test: 0.1390583 best: 0.1390558 (3378) 3386: learn: 0.0772348 test: 0.1390560 best: 0.1390558 (3378) 3387: learn: 0.0772238 test: 0.1390563 best: 0.1390558 (3378) 3388: learn: 0.0772116 test: 0.1390546 best: 0.1390546 (3388) 3389: learn: 0.0771993 test: 0.1390565 best: 0.1390546 (3388) 3390: learn: 0.0771827 test: 0.1390566 best: 0.1390546 (3388) 3391: learn: 0.0771693 test: 0.1390530 best: 0.1390530 (3391) 3392: learn: 0.0771593 test: 0.1390518 best: 0.1390518 (3392) 3393: learn: 0.0771468 test: 0.1390477 best: 0.1390477 (3393) 3394: learn: 0.0771329 test: 0.1390449 best: 0.1390449 (3394) 3395: learn: 0.0771168 test: 0.1390393 best: 0.1390393 (3395) 3396: learn: 0.0771042 test: 0.1390353 best: 0.1390353 (3396) 3397: learn: 0.0770890 test: 0.1390362 best: 0.1390353 (3396) 3398: learn: 0.0770766 test: 0.1390366 best: 0.1390353 (3396) 3399: learn: 0.0770628 test: 0.1390314 best: 0.1390314 (3399) 3400: learn: 0.0770512 test: 0.1390300 best: 0.1390300 (3400) 3401: learn: 0.0770420 test: 0.1390294 best: 0.1390294 (3401) 3402: learn: 0.0770303 test: 0.1390264 best: 0.1390264 (3402) 3403: learn: 0.0770158 test: 0.1390242 best: 0.1390242 (3403) 3404: learn: 0.0770015 test: 0.1390251 best: 0.1390242 (3403) 3405: learn: 0.0769900 test: 0.1390203 best: 0.1390203 (3405) total: 38.7s remaining: 29.5s 3406: learn: 0.0769803 test: 0.1390158 best: 0.1390158 (3406) 3407: learn: 0.0769702 test: 0.1390175 best: 0.1390158 (3406) 3408: learn: 0.0769578 test: 0.1390150 best: 0.1390150 (3408) 3409: learn: 0.0769491 test: 0.1390174 best: 0.1390150 (3408) 3410: learn: 0.0769360 test: 0.1390204 best: 0.1390150 (3408) 3411: learn: 0.0769242 test: 0.1390175 best: 0.1390150 (3408) 3412: learn: 0.0769100 test: 0.1390140 best: 0.1390140 (3412) 3413: learn: 0.0768965 test: 0.1390064 best: 0.1390064 (3413) 3414: learn: 0.0768808 test: 0.1390108 best: 0.1390064 (3413) 3415: learn: 0.0768698 test: 0.1390102 best: 0.1390064 (3413) 3416: learn: 0.0768589 test: 0.1390093 best: 0.1390064 (3413) 3417: learn: 0.0768483 test: 0.1390052 best: 0.1390052 (3417) 3418: learn: 0.0768346 test: 0.1390048 best: 0.1390048 (3418) 3419: learn: 0.0768255 test: 0.1390034 best: 0.1390034 (3419) 3420: learn: 0.0768153 test: 0.1389979 best: 0.1389979 (3420) 3421: learn: 0.0768068 test: 0.1389972 best: 0.1389972 (3421) 3422: learn: 0.0767902 test: 0.1389955 best: 0.1389955 (3422) 3423: learn: 0.0767777 test: 0.1389971 best: 0.1389955 (3422) 3424: learn: 0.0767643 test: 0.1389971 best: 0.1389955 (3422) 3425: learn: 0.0767511 test: 0.1390006 best: 0.1389955 (3422) 3426: learn: 0.0767430 test: 0.1389971 best: 0.1389955 (3422) 3427: learn: 0.0767270 test: 0.1389959 best: 0.1389955 (3422) 3428: learn: 0.0767163 test: 0.1389935 best: 0.1389935 (3428) 3429: learn: 0.0767053 test: 0.1389885 best: 0.1389885 (3429) 3430: learn: 0.0766938 test: 0.1389873 best: 0.1389873 (3430) total: 39.2s remaining: 29.3s 3431: learn: 0.0766817 test: 0.1389833 best: 0.1389833 (3431) 3432: learn: 0.0766716 test: 0.1389834 best: 0.1389833 (3431) 3433: learn: 0.0766601 test: 0.1389786 best: 0.1389786 (3433) 3434: learn: 0.0766495 test: 0.1389752 best: 0.1389752 (3434) 3435: learn: 0.0766381 test: 0.1389742 best: 0.1389742 (3435) 3436: learn: 0.0766255 test: 0.1389674 best: 0.1389674 (3436) 3437: learn: 0.0766144 test: 0.1389735 best: 0.1389674 (3436) 3438: learn: 0.0766024 test: 0.1389762 best: 0.1389674 (3436) 3439: learn: 0.0765894 test: 0.1389811 best: 0.1389674 (3436) 3440: learn: 0.0765798 test: 0.1389801 best: 0.1389674 (3436) 3441: learn: 0.0765660 test: 0.1389820 best: 0.1389674 (3436) 3442: learn: 0.0765555 test: 0.1389793 best: 0.1389674 (3436) 3443: learn: 0.0765389 test: 0.1389784 best: 0.1389674 (3436) 3444: learn: 0.0765310 test: 0.1389754 best: 0.1389674 (3436) 3445: learn: 0.0765209 test: 0.1389654 best: 0.1389654 (3445) 3446: learn: 0.0765088 test: 0.1389616 best: 0.1389616 (3446) 3447: learn: 0.0764967 test: 0.1389552 best: 0.1389552 (3447) 3448: learn: 0.0764826 test: 0.1389515 best: 0.1389515 (3448) 3449: learn: 0.0764644 test: 0.1389518 best: 0.1389515 (3448) 3450: learn: 0.0764536 test: 0.1389496 best: 0.1389496 (3450) 3451: learn: 0.0764442 test: 0.1389504 best: 0.1389496 (3450) 3452: learn: 0.0764273 test: 0.1389507 best: 0.1389496 (3450) 3453: learn: 0.0764176 test: 0.1389494 best: 0.1389494 (3453) 3454: learn: 0.0764062 test: 0.1389419 best: 0.1389419 (3454) 3455: learn: 0.0763902 test: 0.1389374 best: 0.1389374 (3455) 3456: learn: 0.0763761 test: 0.1389384 best: 0.1389374 (3455) 3457: learn: 0.0763612 test: 0.1389426 best: 0.1389374 (3455) 3458: learn: 0.0763500 test: 0.1389460 best: 0.1389374 (3455) 3459: learn: 0.0763343 test: 0.1389429 best: 0.1389374 (3455) 3460: learn: 0.0763217 test: 0.1389403 best: 0.1389374 (3455) 3461: learn: 0.0763098 test: 0.1389396 best: 0.1389374 (3455) 3462: learn: 0.0763013 test: 0.1389363 best: 0.1389363 (3462) 3463: learn: 0.0762921 test: 0.1389324 best: 0.1389324 (3463) 3464: learn: 0.0762802 test: 0.1389302 best: 0.1389302 (3464) 3465: learn: 0.0762713 test: 0.1389274 best: 0.1389274 (3465) 3466: learn: 0.0762576 test: 0.1389252 best: 0.1389252 (3466) 3467: learn: 0.0762439 test: 0.1389224 best: 0.1389224 (3467) 3468: learn: 0.0762342 test: 0.1389189 best: 0.1389189 (3468) 3469: learn: 0.0762239 test: 0.1389162 best: 0.1389162 (3469) 3470: learn: 0.0762125 test: 0.1389143 best: 0.1389143 (3470) 3471: learn: 0.0761984 test: 0.1389145 best: 0.1389143 (3470) 3472: learn: 0.0761854 test: 0.1389133 best: 0.1389133 (3472) 3473: learn: 0.0761713 test: 0.1389128 best: 0.1389128 (3473) 3474: learn: 0.0761629 test: 0.1389160 best: 0.1389128 (3473) 3475: learn: 0.0761520 test: 0.1389143 best: 0.1389128 (3473) 3476: learn: 0.0761387 test: 0.1389094 best: 0.1389094 (3476) 3477: learn: 0.0761257 test: 0.1389123 best: 0.1389094 (3476) 3478: learn: 0.0761118 test: 0.1389115 best: 0.1389094 (3476) 3479: learn: 0.0760963 test: 0.1389145 best: 0.1389094 (3476) 3480: learn: 0.0760829 test: 0.1389131 best: 0.1389094 (3476) 3481: learn: 0.0760711 test: 0.1389105 best: 0.1389094 (3476) 3482: learn: 0.0760573 test: 0.1389085 best: 0.1389085 (3482) 3483: learn: 0.0760449 test: 0.1389045 best: 0.1389045 (3483) 3484: learn: 0.0760316 test: 0.1389025 best: 0.1389025 (3484) 3485: learn: 0.0760149 test: 0.1389053 best: 0.1389025 (3484) 3486: learn: 0.0760000 test: 0.1389048 best: 0.1389025 (3484) 3487: learn: 0.0759882 test: 0.1388975 best: 0.1388975 (3487) 3488: learn: 0.0759774 test: 0.1388989 best: 0.1388975 (3487) 3489: learn: 0.0759669 test: 0.1389001 best: 0.1388975 (3487) 3490: learn: 0.0759552 test: 0.1388988 best: 0.1388975 (3487) 3491: learn: 0.0759416 test: 0.1388996 best: 0.1388975 (3487) 3492: learn: 0.0759293 test: 0.1389005 best: 0.1388975 (3487) 3493: learn: 0.0759137 test: 0.1388989 best: 0.1388975 (3487) 3494: learn: 0.0758991 test: 0.1388958 best: 0.1388958 (3494) 3495: learn: 0.0758914 test: 0.1388974 best: 0.1388958 (3494) 3496: learn: 0.0758832 test: 0.1388975 best: 0.1388958 (3494) 3497: learn: 0.0758729 test: 0.1389015 best: 0.1388958 (3494) 3498: learn: 0.0758601 test: 0.1388991 best: 0.1388958 (3494) 3499: learn: 0.0758475 test: 0.1388944 best: 0.1388944 (3499) 3500: learn: 0.0758344 test: 0.1388931 best: 0.1388931 (3500) 3501: learn: 0.0758250 test: 0.1388906 best: 0.1388906 (3501) 3502: learn: 0.0758075 test: 0.1388913 best: 0.1388906 (3501) 3503: learn: 0.0757903 test: 0.1388909 best: 0.1388906 (3501) 3504: learn: 0.0757757 test: 0.1388884 best: 0.1388884 (3504) 3505: learn: 0.0757671 test: 0.1388838 best: 0.1388838 (3505) 3506: learn: 0.0757576 test: 0.1388798 best: 0.1388798 (3506) 3507: learn: 0.0757420 test: 0.1388779 best: 0.1388779 (3507) 3508: learn: 0.0757356 test: 0.1388761 best: 0.1388761 (3508) 3509: learn: 0.0757254 test: 0.1388769 best: 0.1388761 (3508) 3510: learn: 0.0757128 test: 0.1388739 best: 0.1388739 (3510) 3511: learn: 0.0757053 test: 0.1388737 best: 0.1388737 (3511) 3512: learn: 0.0756928 test: 0.1388704 best: 0.1388704 (3512) 3513: learn: 0.0756800 test: 0.1388661 best: 0.1388661 (3513) 3514: learn: 0.0756702 test: 0.1388647 best: 0.1388647 (3514) 3515: learn: 0.0756563 test: 0.1388654 best: 0.1388647 (3514) 3516: learn: 0.0756422 test: 0.1388652 best: 0.1388647 (3514) 3517: learn: 0.0756261 test: 0.1388662 best: 0.1388647 (3514) 3518: learn: 0.0756145 test: 0.1388620 best: 0.1388620 (3518) 3519: learn: 0.0756025 test: 0.1388622 best: 0.1388620 (3518) 3520: learn: 0.0755913 test: 0.1388594 best: 0.1388594 (3520) 3521: learn: 0.0755783 test: 0.1388571 best: 0.1388571 (3521) 3522: learn: 0.0755647 test: 0.1388578 best: 0.1388571 (3521) 3523: learn: 0.0755523 test: 0.1388512 best: 0.1388512 (3523) 3524: learn: 0.0755405 test: 0.1388497 best: 0.1388497 (3524) 3525: learn: 0.0755289 test: 0.1388501 best: 0.1388497 (3524) 3526: learn: 0.0755151 test: 0.1388498 best: 0.1388497 (3524) 3527: learn: 0.0754998 test: 0.1388505 best: 0.1388497 (3524) 3528: learn: 0.0754899 test: 0.1388476 best: 0.1388476 (3528) 3529: learn: 0.0754813 test: 0.1388468 best: 0.1388468 (3529) 3530: learn: 0.0754708 test: 0.1388492 best: 0.1388468 (3529) 3531: learn: 0.0754591 test: 0.1388505 best: 0.1388468 (3529) 3532: learn: 0.0754472 test: 0.1388511 best: 0.1388468 (3529) 3533: learn: 0.0754385 test: 0.1388507 best: 0.1388468 (3529) 3534: learn: 0.0754232 test: 0.1388477 best: 0.1388468 (3529) 3535: learn: 0.0754102 test: 0.1388461 best: 0.1388461 (3535) 3536: learn: 0.0753961 test: 0.1388511 best: 0.1388461 (3535) 3537: learn: 0.0753877 test: 0.1388506 best: 0.1388461 (3535) 3538: learn: 0.0753815 test: 0.1388500 best: 0.1388461 (3535) 3539: learn: 0.0753681 test: 0.1388513 best: 0.1388461 (3535) 3540: learn: 0.0753584 test: 0.1388486 best: 0.1388461 (3535) 3541: learn: 0.0753427 test: 0.1388508 best: 0.1388461 (3535) 3542: learn: 0.0753337 test: 0.1388515 best: 0.1388461 (3535) 3543: learn: 0.0753218 test: 0.1388505 best: 0.1388461 (3535) 3544: learn: 0.0753103 test: 0.1388504 best: 0.1388461 (3535) 3545: learn: 0.0752989 test: 0.1388497 best: 0.1388461 (3535) 3546: learn: 0.0752870 test: 0.1388522 best: 0.1388461 (3535) 3547: learn: 0.0752761 test: 0.1388488 best: 0.1388461 (3535) 3548: learn: 0.0752608 test: 0.1388471 best: 0.1388461 (3535) 3549: learn: 0.0752476 test: 0.1388481 best: 0.1388461 (3535) 3550: learn: 0.0752351 test: 0.1388505 best: 0.1388461 (3535) 3551: learn: 0.0752228 test: 0.1388444 best: 0.1388444 (3551) 3552: learn: 0.0752078 test: 0.1388430 best: 0.1388430 (3552) 3553: learn: 0.0751988 test: 0.1388407 best: 0.1388407 (3553) 3554: learn: 0.0751899 test: 0.1388442 best: 0.1388407 (3553) 3555: learn: 0.0751795 test: 0.1388420 best: 0.1388407 (3553) 3556: learn: 0.0751706 test: 0.1388435 best: 0.1388407 (3553) 3557: learn: 0.0751581 test: 0.1388394 best: 0.1388394 (3557) total: 40.7s remaining: 27.9s 3558: learn: 0.0751486 test: 0.1388370 best: 0.1388370 (3558) 3559: learn: 0.0751314 test: 0.1388421 best: 0.1388370 (3558) 3560: learn: 0.0751220 test: 0.1388433 best: 0.1388370 (3558) 3561: learn: 0.0751106 test: 0.1388443 best: 0.1388370 (3558) 3562: learn: 0.0750967 test: 0.1388421 best: 0.1388370 (3558) 3563: learn: 0.0750873 test: 0.1388359 best: 0.1388359 (3563) 3564: learn: 0.0750774 test: 0.1388321 best: 0.1388321 (3564) 3565: learn: 0.0750659 test: 0.1388334 best: 0.1388321 (3564) 3566: learn: 0.0750566 test: 0.1388311 best: 0.1388311 (3566) 3567: learn: 0.0750476 test: 0.1388289 best: 0.1388289 (3567) 3568: learn: 0.0750334 test: 0.1388291 best: 0.1388289 (3567) 3569: learn: 0.0750229 test: 0.1388250 best: 0.1388250 (3569) 3570: learn: 0.0750111 test: 0.1388226 best: 0.1388226 (3570) 3571: learn: 0.0749997 test: 0.1388225 best: 0.1388225 (3571) 3572: learn: 0.0749902 test: 0.1388218 best: 0.1388218 (3572) 3573: learn: 0.0749797 test: 0.1388207 best: 0.1388207 (3573) 3574: learn: 0.0749689 test: 0.1388193 best: 0.1388193 (3574) 3575: learn: 0.0749537 test: 0.1388143 best: 0.1388143 (3575) 3576: learn: 0.0749448 test: 0.1388119 best: 0.1388119 (3576) 3577: learn: 0.0749317 test: 0.1388093 best: 0.1388093 (3577) 3578: learn: 0.0749175 test: 0.1388088 best: 0.1388088 (3578) 3579: learn: 0.0749033 test: 0.1388033 best: 0.1388033 (3579) 3580: learn: 0.0748948 test: 0.1388019 best: 0.1388019 (3580) 3581: learn: 0.0748831 test: 0.1388042 best: 0.1388019 (3580) 3582: learn: 0.0748724 test: 0.1388018 best: 0.1388018 (3582) 3583: learn: 0.0748584 test: 0.1388031 best: 0.1388018 (3582) 3584: learn: 0.0748453 test: 0.1388071 best: 0.1388018 (3582) 3585: learn: 0.0748323 test: 0.1388018 best: 0.1388018 (3582) 3586: learn: 0.0748196 test: 0.1387980 best: 0.1387980 (3586) 3587: learn: 0.0748084 test: 0.1387983 best: 0.1387980 (3586) 3588: learn: 0.0747940 test: 0.1387989 best: 0.1387980 (3586) 3589: learn: 0.0747800 test: 0.1388001 best: 0.1387980 (3586) 3590: learn: 0.0747702 test: 0.1387990 best: 0.1387980 (3586) 3591: learn: 0.0747567 test: 0.1387975 best: 0.1387975 (3591) 3592: learn: 0.0747448 test: 0.1387927 best: 0.1387927 (3592) 3593: learn: 0.0747348 test: 0.1387926 best: 0.1387926 (3593) 3594: learn: 0.0747215 test: 0.1387942 best: 0.1387926 (3593) 3595: learn: 0.0747095 test: 0.1387912 best: 0.1387912 (3595) 3596: learn: 0.0747008 test: 0.1387889 best: 0.1387889 (3596) 3597: learn: 0.0746898 test: 0.1387850 best: 0.1387850 (3597) 3598: learn: 0.0746800 test: 0.1387842 best: 0.1387842 (3598) 3599: learn: 0.0746730 test: 0.1387851 best: 0.1387842 (3598) 3600: learn: 0.0746634 test: 0.1387841 best: 0.1387841 (3600) 3601: learn: 0.0746518 test: 0.1387859 best: 0.1387841 (3600) 3602: learn: 0.0746405 test: 0.1387880 best: 0.1387841 (3600) 3603: learn: 0.0746287 test: 0.1387870 best: 0.1387841 (3600) 3604: learn: 0.0746172 test: 0.1387901 best: 0.1387841 (3600) 3605: learn: 0.0746051 test: 0.1387909 best: 0.1387841 (3600) 3606: learn: 0.0745945 test: 0.1387886 best: 0.1387841 (3600) 3607: learn: 0.0745827 test: 0.1387882 best: 0.1387841 (3600) 3608: learn: 0.0745717 test: 0.1387880 best: 0.1387841 (3600) 3609: learn: 0.0745599 test: 0.1387903 best: 0.1387841 (3600) 3610: learn: 0.0745519 test: 0.1387819 best: 0.1387819 (3610) 3611: learn: 0.0745463 test: 0.1387802 best: 0.1387802 (3611) 3612: learn: 0.0745365 test: 0.1387779 best: 0.1387779 (3612) 3613: learn: 0.0745192 test: 0.1387845 best: 0.1387779 (3612) 3614: learn: 0.0745073 test: 0.1387866 best: 0.1387779 (3612) 3615: learn: 0.0744958 test: 0.1387858 best: 0.1387779 (3612) 3616: learn: 0.0744853 test: 0.1387856 best: 0.1387779 (3612) 3617: learn: 0.0744786 test: 0.1387870 best: 0.1387779 (3612) 3618: learn: 0.0744667 test: 0.1387862 best: 0.1387779 (3612) 3619: learn: 0.0744592 test: 0.1387904 best: 0.1387779 (3612) 3620: learn: 0.0744472 test: 0.1387905 best: 0.1387779 (3612) 3621: learn: 0.0744408 test: 0.1387911 best: 0.1387779 (3612) 3622: learn: 0.0744283 test: 0.1387893 best: 0.1387779 (3612) 3623: learn: 0.0744197 test: 0.1387886 best: 0.1387779 (3612) 3624: learn: 0.0744033 test: 0.1387868 best: 0.1387779 (3612) 3625: learn: 0.0743890 test: 0.1387797 best: 0.1387779 (3612) 3626: learn: 0.0743763 test: 0.1387816 best: 0.1387779 (3612) 3627: learn: 0.0743645 test: 0.1387802 best: 0.1387779 (3612) 3628: learn: 0.0743520 test: 0.1387823 best: 0.1387779 (3612) 3629: learn: 0.0743389 test: 0.1387815 best: 0.1387779 (3612) 3630: learn: 0.0743241 test: 0.1387795 best: 0.1387779 (3612) 3631: learn: 0.0743100 test: 0.1387803 best: 0.1387779 (3612) 3632: learn: 0.0742983 test: 0.1387842 best: 0.1387779 (3612) 3633: learn: 0.0742863 test: 0.1387867 best: 0.1387779 (3612) 3634: learn: 0.0742747 test: 0.1387838 best: 0.1387779 (3612) 3635: learn: 0.0742644 test: 0.1387920 best: 0.1387779 (3612) 3636: learn: 0.0742561 test: 0.1387924 best: 0.1387779 (3612) 3637: learn: 0.0742458 test: 0.1387891 best: 0.1387779 (3612) 3638: learn: 0.0742352 test: 0.1387870 best: 0.1387779 (3612) 3639: learn: 0.0742172 test: 0.1387899 best: 0.1387779 (3612) 3640: learn: 0.0742071 test: 0.1387908 best: 0.1387779 (3612) 3641: learn: 0.0741997 test: 0.1387869 best: 0.1387779 (3612) 3642: learn: 0.0741902 test: 0.1387833 best: 0.1387779 (3612) 3643: learn: 0.0741821 test: 0.1387860 best: 0.1387779 (3612) 3644: learn: 0.0741739 test: 0.1387888 best: 0.1387779 (3612) 3645: learn: 0.0741648 test: 0.1387884 best: 0.1387779 (3612) 3646: learn: 0.0741543 test: 0.1387899 best: 0.1387779 (3612) 3647: learn: 0.0741396 test: 0.1387861 best: 0.1387779 (3612) 3648: learn: 0.0741259 test: 0.1387877 best: 0.1387779 (3612) 3649: learn: 0.0741172 test: 0.1387850 best: 0.1387779 (3612) 3650: learn: 0.0741084 test: 0.1387773 best: 0.1387773 (3650) 3651: learn: 0.0741008 test: 0.1387753 best: 0.1387753 (3651) 3652: learn: 0.0740919 test: 0.1387792 best: 0.1387753 (3651) 3653: learn: 0.0740797 test: 0.1387781 best: 0.1387753 (3651) 3654: learn: 0.0740702 test: 0.1387789 best: 0.1387753 (3651) 3655: learn: 0.0740591 test: 0.1387802 best: 0.1387753 (3651) 3656: learn: 0.0740465 test: 0.1387835 best: 0.1387753 (3651) 3657: learn: 0.0740346 test: 0.1387802 best: 0.1387753 (3651) 3658: learn: 0.0740227 test: 0.1387800 best: 0.1387753 (3651) 3659: learn: 0.0740118 test: 0.1387742 best: 0.1387742 (3659) 3660: learn: 0.0740004 test: 0.1387690 best: 0.1387690 (3660) 3661: learn: 0.0739875 test: 0.1387693 best: 0.1387690 (3660) 3662: learn: 0.0739802 test: 0.1387656 best: 0.1387656 (3662) 3663: learn: 0.0739734 test: 0.1387640 best: 0.1387640 (3663) 3664: learn: 0.0739618 test: 0.1387653 best: 0.1387640 (3663) 3665: learn: 0.0739516 test: 0.1387665 best: 0.1387640 (3663) 3666: learn: 0.0739377 test: 0.1387656 best: 0.1387640 (3663) 3667: learn: 0.0739232 test: 0.1387624 best: 0.1387624 (3667) 3668: learn: 0.0739125 test: 0.1387653 best: 0.1387624 (3667) 3669: learn: 0.0738962 test: 0.1387622 best: 0.1387622 (3669) 3670: learn: 0.0738889 test: 0.1387615 best: 0.1387615 (3670) 3671: learn: 0.0738725 test: 0.1387602 best: 0.1387602 (3671) 3672: learn: 0.0738636 test: 0.1387581 best: 0.1387581 (3672) total: 42s remaining: 26.6s 3673: learn: 0.0738584 test: 0.1387570 best: 0.1387570 (3673) 3674: learn: 0.0738449 test: 0.1387573 best: 0.1387570 (3673) 3675: learn: 0.0738346 test: 0.1387595 best: 0.1387570 (3673) 3676: learn: 0.0738257 test: 0.1387583 best: 0.1387570 (3673) 3677: learn: 0.0738132 test: 0.1387565 best: 0.1387565 (3677) 3678: learn: 0.0738022 test: 0.1387557 best: 0.1387557 (3678) 3679: learn: 0.0737940 test: 0.1387552 best: 0.1387552 (3679) 3680: learn: 0.0737812 test: 0.1387543 best: 0.1387543 (3680) 3681: learn: 0.0737670 test: 0.1387590 best: 0.1387543 (3680) 3682: learn: 0.0737574 test: 0.1387558 best: 0.1387543 (3680) 3683: learn: 0.0737531 test: 0.1387553 best: 0.1387543 (3680) 3684: learn: 0.0737398 test: 0.1387555 best: 0.1387543 (3680) 3685: learn: 0.0737310 test: 0.1387528 best: 0.1387528 (3685) 3686: learn: 0.0737173 test: 0.1387525 best: 0.1387525 (3686) 3687: learn: 0.0737081 test: 0.1387499 best: 0.1387499 (3687) 3688: learn: 0.0736954 test: 0.1387483 best: 0.1387483 (3688) 3689: learn: 0.0736864 test: 0.1387482 best: 0.1387482 (3689) 3690: learn: 0.0736757 test: 0.1387460 best: 0.1387460 (3690) 3691: learn: 0.0736608 test: 0.1387437 best: 0.1387437 (3691) 3692: learn: 0.0736489 test: 0.1387443 best: 0.1387437 (3691) 3693: learn: 0.0736360 test: 0.1387414 best: 0.1387414 (3693) 3694: learn: 0.0736226 test: 0.1387406 best: 0.1387406 (3694) 3695: learn: 0.0736107 test: 0.1387375 best: 0.1387375 (3695) 3696: learn: 0.0735970 test: 0.1387385 best: 0.1387375 (3695) 3697: learn: 0.0735845 test: 0.1387386 best: 0.1387375 (3695) 3698: learn: 0.0735729 test: 0.1387366 best: 0.1387366 (3698) 3699: learn: 0.0735625 test: 0.1387361 best: 0.1387361 (3699) 3700: learn: 0.0735539 test: 0.1387334 best: 0.1387334 (3700) 3701: learn: 0.0735464 test: 0.1387339 best: 0.1387334 (3700) 3702: learn: 0.0735327 test: 0.1387319 best: 0.1387319 (3702) 3703: learn: 0.0735223 test: 0.1387310 best: 0.1387310 (3703) 3704: learn: 0.0735098 test: 0.1387325 best: 0.1387310 (3703) 3705: learn: 0.0734994 test: 0.1387282 best: 0.1387282 (3705) 3706: learn: 0.0734899 test: 0.1387247 best: 0.1387247 (3706) 3707: learn: 0.0734797 test: 0.1387230 best: 0.1387230 (3707) 3708: learn: 0.0734720 test: 0.1387210 best: 0.1387210 (3708) 3709: learn: 0.0734586 test: 0.1387200 best: 0.1387200 (3709) 3710: learn: 0.0734462 test: 0.1387208 best: 0.1387200 (3709) 3711: learn: 0.0734337 test: 0.1387207 best: 0.1387200 (3709) 3712: learn: 0.0734228 test: 0.1387167 best: 0.1387167 (3712) 3713: learn: 0.0734115 test: 0.1387190 best: 0.1387167 (3712) 3714: learn: 0.0733970 test: 0.1387180 best: 0.1387167 (3712) 3715: learn: 0.0733821 test: 0.1387161 best: 0.1387161 (3715) 3716: learn: 0.0733725 test: 0.1387139 best: 0.1387139 (3716) 3717: learn: 0.0733603 test: 0.1387152 best: 0.1387139 (3716) 3718: learn: 0.0733481 test: 0.1387164 best: 0.1387139 (3716) 3719: learn: 0.0733400 test: 0.1387134 best: 0.1387134 (3719) 3720: learn: 0.0733297 test: 0.1387099 best: 0.1387099 (3720) 3721: learn: 0.0733162 test: 0.1387067 best: 0.1387067 (3721) 3722: learn: 0.0733080 test: 0.1387087 best: 0.1387067 (3721) 3723: learn: 0.0732942 test: 0.1387048 best: 0.1387048 (3723) 3724: learn: 0.0732810 test: 0.1387001 best: 0.1387001 (3724) 3725: learn: 0.0732684 test: 0.1386943 best: 0.1386943 (3725) 3726: learn: 0.0732570 test: 0.1386975 best: 0.1386943 (3725) 3727: learn: 0.0732444 test: 0.1386939 best: 0.1386939 (3727) 3728: learn: 0.0732361 test: 0.1386933 best: 0.1386933 (3728) 3729: learn: 0.0732267 test: 0.1386970 best: 0.1386933 (3728) 3730: learn: 0.0732180 test: 0.1386898 best: 0.1386898 (3730) 3731: learn: 0.0732070 test: 0.1386891 best: 0.1386891 (3731) 3732: learn: 0.0731966 test: 0.1386905 best: 0.1386891 (3731) 3733: learn: 0.0731857 test: 0.1386889 best: 0.1386889 (3733) 3734: learn: 0.0731768 test: 0.1386890 best: 0.1386889 (3733) 3735: learn: 0.0731655 test: 0.1386845 best: 0.1386845 (3735) 3736: learn: 0.0731552 test: 0.1386805 best: 0.1386805 (3736) 3737: learn: 0.0731453 test: 0.1386794 best: 0.1386794 (3737) 3738: learn: 0.0731347 test: 0.1386810 best: 0.1386794 (3737) 3739: learn: 0.0731254 test: 0.1386781 best: 0.1386781 (3739) 3740: learn: 0.0731144 test: 0.1386797 best: 0.1386781 (3739) 3741: learn: 0.0731051 test: 0.1386736 best: 0.1386736 (3741) 3742: learn: 0.0730952 test: 0.1386718 best: 0.1386718 (3742) 3743: learn: 0.0730853 test: 0.1386670 best: 0.1386670 (3743) 3744: learn: 0.0730754 test: 0.1386666 best: 0.1386666 (3744) 3745: learn: 0.0730648 test: 0.1386654 best: 0.1386654 (3745) 3746: learn: 0.0730551 test: 0.1386658 best: 0.1386654 (3745) 3747: learn: 0.0730454 test: 0.1386657 best: 0.1386654 (3745) 3748: learn: 0.0730317 test: 0.1386617 best: 0.1386617 (3748) 3749: learn: 0.0730210 test: 0.1386563 best: 0.1386563 (3749) 3750: learn: 0.0730099 test: 0.1386538 best: 0.1386538 (3750) 3751: learn: 0.0729978 test: 0.1386530 best: 0.1386530 (3751) 3752: learn: 0.0729886 test: 0.1386477 best: 0.1386477 (3752) 3753: learn: 0.0729776 test: 0.1386476 best: 0.1386476 (3753) 3754: learn: 0.0729705 test: 0.1386474 best: 0.1386474 (3754) 3755: learn: 0.0729626 test: 0.1386421 best: 0.1386421 (3755) 3756: learn: 0.0729499 test: 0.1386417 best: 0.1386417 (3756) 3757: learn: 0.0729399 test: 0.1386363 best: 0.1386363 (3757) 3758: learn: 0.0729285 test: 0.1386350 best: 0.1386350 (3758) 3759: learn: 0.0729188 test: 0.1386300 best: 0.1386300 (3759) 3760: learn: 0.0729041 test: 0.1386280 best: 0.1386280 (3760) 3761: learn: 0.0728921 test: 0.1386291 best: 0.1386280 (3760) 3762: learn: 0.0728813 test: 0.1386290 best: 0.1386280 (3760) 3763: learn: 0.0728706 test: 0.1386298 best: 0.1386280 (3760) 3764: learn: 0.0728553 test: 0.1386306 best: 0.1386280 (3760) 3765: learn: 0.0728393 test: 0.1386294 best: 0.1386280 (3760) 3766: learn: 0.0728265 test: 0.1386233 best: 0.1386233 (3766) 3767: learn: 0.0728178 test: 0.1386235 best: 0.1386233 (3766) 3768: learn: 0.0728088 test: 0.1386263 best: 0.1386233 (3766) 3769: learn: 0.0727972 test: 0.1386259 best: 0.1386233 (3766) 3770: learn: 0.0727846 test: 0.1386264 best: 0.1386233 (3766) 3771: learn: 0.0727737 test: 0.1386275 best: 0.1386233 (3766) 3772: learn: 0.0727619 test: 0.1386247 best: 0.1386233 (3766) 3773: learn: 0.0727522 test: 0.1386256 best: 0.1386233 (3766) 3774: learn: 0.0727415 test: 0.1386230 best: 0.1386230 (3774) 3775: learn: 0.0727299 test: 0.1386272 best: 0.1386230 (3774) 3776: learn: 0.0727150 test: 0.1386305 best: 0.1386230 (3774) 3777: learn: 0.0727075 test: 0.1386284 best: 0.1386230 (3774) 3778: learn: 0.0726949 test: 0.1386243 best: 0.1386230 (3774) 3779: learn: 0.0726835 test: 0.1386210 best: 0.1386210 (3779) 3780: learn: 0.0726728 test: 0.1386196 best: 0.1386196 (3780) 3781: learn: 0.0726606 test: 0.1386192 best: 0.1386192 (3781) 3782: learn: 0.0726520 test: 0.1386189 best: 0.1386189 (3782) 3783: learn: 0.0726434 test: 0.1386198 best: 0.1386189 (3782) 3784: learn: 0.0726357 test: 0.1386213 best: 0.1386189 (3782) 3785: learn: 0.0726249 test: 0.1386186 best: 0.1386186 (3785) 3786: learn: 0.0726139 test: 0.1386179 best: 0.1386179 (3786) 3787: learn: 0.0726003 test: 0.1386201 best: 0.1386179 (3786) 3788: learn: 0.0725904 test: 0.1386192 best: 0.1386179 (3786) 3789: learn: 0.0725741 test: 0.1386246 best: 0.1386179 (3786) 3790: learn: 0.0725547 test: 0.1386259 best: 0.1386179 (3786) total: 43.4s remaining: 25.3s 3791: learn: 0.0725456 test: 0.1386266 best: 0.1386179 (3786) 3792: learn: 0.0725306 test: 0.1386254 best: 0.1386179 (3786) 3793: learn: 0.0725193 test: 0.1386209 best: 0.1386179 (3786) 3794: learn: 0.0725087 test: 0.1386197 best: 0.1386179 (3786) 3795: learn: 0.0724979 test: 0.1386212 best: 0.1386179 (3786) 3796: learn: 0.0724911 test: 0.1386220 best: 0.1386179 (3786) 3797: learn: 0.0724766 test: 0.1386239 best: 0.1386179 (3786) 3798: learn: 0.0724610 test: 0.1386220 best: 0.1386179 (3786) 3799: learn: 0.0724529 test: 0.1386208 best: 0.1386179 (3786) 3800: learn: 0.0724418 test: 0.1386213 best: 0.1386179 (3786) 3801: learn: 0.0724326 test: 0.1386187 best: 0.1386179 (3786) 3802: learn: 0.0724233 test: 0.1386179 best: 0.1386179 (3802) 3803: learn: 0.0724166 test: 0.1386150 best: 0.1386150 (3803) 3804: learn: 0.0724024 test: 0.1386116 best: 0.1386116 (3804) 3805: learn: 0.0723877 test: 0.1386126 best: 0.1386116 (3804) 3806: learn: 0.0723784 test: 0.1386084 best: 0.1386084 (3806) 3807: learn: 0.0723712 test: 0.1386068 best: 0.1386068 (3807) 3808: learn: 0.0723578 test: 0.1386061 best: 0.1386061 (3808) 3809: learn: 0.0723466 test: 0.1386044 best: 0.1386044 (3809) 3810: learn: 0.0723310 test: 0.1386029 best: 0.1386029 (3810) 3811: learn: 0.0723190 test: 0.1386046 best: 0.1386029 (3810) 3812: learn: 0.0723104 test: 0.1386053 best: 0.1386029 (3810) 3813: learn: 0.0722976 test: 0.1385998 best: 0.1385998 (3813) 3814: learn: 0.0722863 test: 0.1385977 best: 0.1385977 (3814) 3815: learn: 0.0722759 test: 0.1386009 best: 0.1385977 (3814) 3816: learn: 0.0722679 test: 0.1385998 best: 0.1385977 (3814) 3817: learn: 0.0722566 test: 0.1385956 best: 0.1385956 (3817) 3818: learn: 0.0722474 test: 0.1385927 best: 0.1385927 (3818) 3819: learn: 0.0722388 test: 0.1385962 best: 0.1385927 (3818) 3820: learn: 0.0722268 test: 0.1385971 best: 0.1385927 (3818) 3821: learn: 0.0722160 test: 0.1385961 best: 0.1385927 (3818) 3822: learn: 0.0722029 test: 0.1385909 best: 0.1385909 (3822) 3823: learn: 0.0721935 test: 0.1385890 best: 0.1385890 (3823) 3824: learn: 0.0721836 test: 0.1385903 best: 0.1385890 (3823) 3825: learn: 0.0721671 test: 0.1385915 best: 0.1385890 (3823) 3826: learn: 0.0721580 test: 0.1385922 best: 0.1385890 (3823) 3827: learn: 0.0721466 test: 0.1385888 best: 0.1385888 (3827) 3828: learn: 0.0721375 test: 0.1385899 best: 0.1385888 (3827) 3829: learn: 0.0721245 test: 0.1385917 best: 0.1385888 (3827) 3830: learn: 0.0721140 test: 0.1385927 best: 0.1385888 (3827) 3831: learn: 0.0721023 test: 0.1385930 best: 0.1385888 (3827) 3832: learn: 0.0720913 test: 0.1385956 best: 0.1385888 (3827) 3833: learn: 0.0720805 test: 0.1385985 best: 0.1385888 (3827) 3834: learn: 0.0720680 test: 0.1385962 best: 0.1385888 (3827) 3835: learn: 0.0720550 test: 0.1385944 best: 0.1385888 (3827) 3836: learn: 0.0720467 test: 0.1385913 best: 0.1385888 (3827) 3837: learn: 0.0720340 test: 0.1385872 best: 0.1385872 (3837) 3838: learn: 0.0720209 test: 0.1385839 best: 0.1385839 (3838) 3839: learn: 0.0720091 test: 0.1385861 best: 0.1385839 (3838) 3840: learn: 0.0720036 test: 0.1385890 best: 0.1385839 (3838) 3841: learn: 0.0719915 test: 0.1385923 best: 0.1385839 (3838) 3842: learn: 0.0719802 test: 0.1385930 best: 0.1385839 (3838) 3843: learn: 0.0719705 test: 0.1385918 best: 0.1385839 (3838) 3844: learn: 0.0719576 test: 0.1385880 best: 0.1385839 (3838) 3845: learn: 0.0719481 test: 0.1385891 best: 0.1385839 (3838) 3846: learn: 0.0719392 test: 0.1385865 best: 0.1385839 (3838) 3847: learn: 0.0719246 test: 0.1385863 best: 0.1385839 (3838) 3848: learn: 0.0719101 test: 0.1385835 best: 0.1385835 (3848) 3849: learn: 0.0718984 test: 0.1385833 best: 0.1385833 (3849) 3850: learn: 0.0718856 test: 0.1385826 best: 0.1385826 (3850) 3851: learn: 0.0718740 test: 0.1385816 best: 0.1385816 (3851) 3852: learn: 0.0718626 test: 0.1385832 best: 0.1385816 (3851) 3853: learn: 0.0718471 test: 0.1385821 best: 0.1385816 (3851) 3854: learn: 0.0718377 test: 0.1385844 best: 0.1385816 (3851) 3855: learn: 0.0718277 test: 0.1385846 best: 0.1385816 (3851) 3856: learn: 0.0718150 test: 0.1385856 best: 0.1385816 (3851) 3857: learn: 0.0718038 test: 0.1385834 best: 0.1385816 (3851) 3858: learn: 0.0717947 test: 0.1385832 best: 0.1385816 (3851) 3859: learn: 0.0717851 test: 0.1385800 best: 0.1385800 (3859) 3860: learn: 0.0717740 test: 0.1385785 best: 0.1385785 (3860) 3861: learn: 0.0717659 test: 0.1385791 best: 0.1385785 (3860) 3862: learn: 0.0717525 test: 0.1385757 best: 0.1385757 (3862) 3863: learn: 0.0717458 test: 0.1385760 best: 0.1385757 (3862) 3864: learn: 0.0717377 test: 0.1385743 best: 0.1385743 (3864) 3865: learn: 0.0717249 test: 0.1385700 best: 0.1385700 (3865) 3866: learn: 0.0717131 test: 0.1385664 best: 0.1385664 (3866) 3867: learn: 0.0717031 test: 0.1385619 best: 0.1385619 (3867) 3868: learn: 0.0716952 test: 0.1385613 best: 0.1385613 (3868) 3869: learn: 0.0716866 test: 0.1385596 best: 0.1385596 (3869) 3870: learn: 0.0716767 test: 0.1385562 best: 0.1385562 (3870) 3871: learn: 0.0716641 test: 0.1385561 best: 0.1385561 (3871) 3872: learn: 0.0716494 test: 0.1385514 best: 0.1385514 (3872) 3873: learn: 0.0716405 test: 0.1385479 best: 0.1385479 (3873) 3874: learn: 0.0716280 test: 0.1385483 best: 0.1385479 (3873) 3875: learn: 0.0716175 test: 0.1385470 best: 0.1385470 (3875) 3876: learn: 0.0716075 test: 0.1385481 best: 0.1385470 (3875) 3877: learn: 0.0715930 test: 0.1385454 best: 0.1385454 (3877) 3878: learn: 0.0715832 test: 0.1385422 best: 0.1385422 (3878) 3879: learn: 0.0715740 test: 0.1385438 best: 0.1385422 (3878) 3880: learn: 0.0715638 test: 0.1385454 best: 0.1385422 (3878) 3881: learn: 0.0715506 test: 0.1385408 best: 0.1385408 (3881) 3882: learn: 0.0715385 test: 0.1385411 best: 0.1385408 (3881) 3883: learn: 0.0715278 test: 0.1385411 best: 0.1385408 (3881) 3884: learn: 0.0715134 test: 0.1385372 best: 0.1385372 (3884) 3885: learn: 0.0715003 test: 0.1385339 best: 0.1385339 (3885) 3886: learn: 0.0714928 test: 0.1385328 best: 0.1385328 (3886) 3887: learn: 0.0714816 test: 0.1385323 best: 0.1385323 (3887) 3888: learn: 0.0714711 test: 0.1385325 best: 0.1385323 (3887) 3889: learn: 0.0714651 test: 0.1385321 best: 0.1385321 (3889) 3890: learn: 0.0714527 test: 0.1385315 best: 0.1385315 (3890) 3891: learn: 0.0714436 test: 0.1385335 best: 0.1385315 (3890) 3892: learn: 0.0714325 test: 0.1385338 best: 0.1385315 (3890) 3893: learn: 0.0714204 test: 0.1385335 best: 0.1385315 (3890) 3894: learn: 0.0714104 test: 0.1385335 best: 0.1385315 (3890) 3895: learn: 0.0714013 test: 0.1385337 best: 0.1385315 (3890) 3896: learn: 0.0713900 test: 0.1385273 best: 0.1385273 (3896) 3897: learn: 0.0713752 test: 0.1385297 best: 0.1385273 (3896) 3898: learn: 0.0713668 test: 0.1385288 best: 0.1385273 (3896) 3899: learn: 0.0713557 test: 0.1385260 best: 0.1385260 (3899) 3900: learn: 0.0713417 test: 0.1385292 best: 0.1385260 (3899) 3901: learn: 0.0713319 test: 0.1385277 best: 0.1385260 (3899) 3902: learn: 0.0713196 test: 0.1385211 best: 0.1385211 (3902) 3903: learn: 0.0713116 test: 0.1385201 best: 0.1385201 (3903) 3904: learn: 0.0713004 test: 0.1385188 best: 0.1385188 (3904) 3905: learn: 0.0712881 test: 0.1385159 best: 0.1385159 (3905) 3906: learn: 0.0712798 test: 0.1385146 best: 0.1385146 (3906) 3907: learn: 0.0712666 test: 0.1385174 best: 0.1385146 (3906) 3908: learn: 0.0712537 test: 0.1385142 best: 0.1385142 (3908) 3909: learn: 0.0712411 test: 0.1385113 best: 0.1385113 (3909) 3910: learn: 0.0712268 test: 0.1385137 best: 0.1385113 (3909) 3911: learn: 0.0712151 test: 0.1385105 best: 0.1385105 (3911) 3912: learn: 0.0712035 test: 0.1385080 best: 0.1385080 (3912) 3913: learn: 0.0711925 test: 0.1385084 best: 0.1385080 (3912) 3914: learn: 0.0711840 test: 0.1385046 best: 0.1385046 (3914) 3915: learn: 0.0711719 test: 0.1385025 best: 0.1385025 (3915) 3916: learn: 0.0711604 test: 0.1385001 best: 0.1385001 (3916) 3917: learn: 0.0711531 test: 0.1384990 best: 0.1384990 (3917) 3918: learn: 0.0711419 test: 0.1384971 best: 0.1384971 (3918) 3919: learn: 0.0711288 test: 0.1384928 best: 0.1384928 (3919) 3920: learn: 0.0711182 test: 0.1384947 best: 0.1384928 (3919) 3921: learn: 0.0711072 test: 0.1384995 best: 0.1384928 (3919) 3922: learn: 0.0710967 test: 0.1384967 best: 0.1384928 (3919) 3923: learn: 0.0710858 test: 0.1384978 best: 0.1384928 (3919) 3924: learn: 0.0710800 test: 0.1384935 best: 0.1384928 (3919) 3925: learn: 0.0710736 test: 0.1384937 best: 0.1384928 (3919) 3926: learn: 0.0710658 test: 0.1384936 best: 0.1384928 (3919) total: 44.9s remaining: 23.7s 3927: learn: 0.0710537 test: 0.1384924 best: 0.1384924 (3927) 3928: learn: 0.0710424 test: 0.1384899 best: 0.1384899 (3928) 3929: learn: 0.0710299 test: 0.1384885 best: 0.1384885 (3929) 3930: learn: 0.0710212 test: 0.1384852 best: 0.1384852 (3930) 3931: learn: 0.0710094 test: 0.1384832 best: 0.1384832 (3931) 3932: learn: 0.0709994 test: 0.1384874 best: 0.1384832 (3931) 3933: learn: 0.0709855 test: 0.1384870 best: 0.1384832 (3931) 3934: learn: 0.0709774 test: 0.1384877 best: 0.1384832 (3931) 3935: learn: 0.0709630 test: 0.1384844 best: 0.1384832 (3931) 3936: learn: 0.0709542 test: 0.1384860 best: 0.1384832 (3931) 3937: learn: 0.0709412 test: 0.1384875 best: 0.1384832 (3931) 3938: learn: 0.0709309 test: 0.1384890 best: 0.1384832 (3931) 3939: learn: 0.0709194 test: 0.1384877 best: 0.1384832 (3931) 3940: learn: 0.0709088 test: 0.1384859 best: 0.1384832 (3931) 3941: learn: 0.0708997 test: 0.1384851 best: 0.1384832 (3931) 3942: learn: 0.0708855 test: 0.1384865 best: 0.1384832 (3931) 3943: learn: 0.0708776 test: 0.1384863 best: 0.1384832 (3931) 3944: learn: 0.0708661 test: 0.1384867 best: 0.1384832 (3931) 3945: learn: 0.0708557 test: 0.1384884 best: 0.1384832 (3931) 3946: learn: 0.0708452 test: 0.1384915 best: 0.1384832 (3931) 3947: learn: 0.0708356 test: 0.1384883 best: 0.1384832 (3931) 3948: learn: 0.0708232 test: 0.1384885 best: 0.1384832 (3931) 3949: learn: 0.0708092 test: 0.1384859 best: 0.1384832 (3931) 3950: learn: 0.0707999 test: 0.1384903 best: 0.1384832 (3931) 3951: learn: 0.0707876 test: 0.1384933 best: 0.1384832 (3931) 3952: learn: 0.0707757 test: 0.1384901 best: 0.1384832 (3931) 3953: learn: 0.0707650 test: 0.1384886 best: 0.1384832 (3931) 3954: learn: 0.0707508 test: 0.1384864 best: 0.1384832 (3931) 3955: learn: 0.0707400 test: 0.1384845 best: 0.1384832 (3931) 3956: learn: 0.0707289 test: 0.1384855 best: 0.1384832 (3931) 3957: learn: 0.0707184 test: 0.1384788 best: 0.1384788 (3957) 3958: learn: 0.0707089 test: 0.1384752 best: 0.1384752 (3958) 3959: learn: 0.0706997 test: 0.1384753 best: 0.1384752 (3958) 3960: learn: 0.0706872 test: 0.1384721 best: 0.1384721 (3960) 3961: learn: 0.0706776 test: 0.1384732 best: 0.1384721 (3960) 3962: learn: 0.0706704 test: 0.1384693 best: 0.1384693 (3962) 3963: learn: 0.0706582 test: 0.1384683 best: 0.1384683 (3963) 3964: learn: 0.0706461 test: 0.1384648 best: 0.1384648 (3964) 3965: learn: 0.0706354 test: 0.1384644 best: 0.1384644 (3965) 3966: learn: 0.0706269 test: 0.1384629 best: 0.1384629 (3966) 3967: learn: 0.0706145 test: 0.1384619 best: 0.1384619 (3967) 3968: learn: 0.0706013 test: 0.1384621 best: 0.1384619 (3967) 3969: learn: 0.0705901 test: 0.1384626 best: 0.1384619 (3967) 3970: learn: 0.0705796 test: 0.1384641 best: 0.1384619 (3967) 3971: learn: 0.0705692 test: 0.1384665 best: 0.1384619 (3967) 3972: learn: 0.0705615 test: 0.1384669 best: 0.1384619 (3967) 3973: learn: 0.0705550 test: 0.1384674 best: 0.1384619 (3967) 3974: learn: 0.0705443 test: 0.1384696 best: 0.1384619 (3967) 3975: learn: 0.0705324 test: 0.1384721 best: 0.1384619 (3967) 3976: learn: 0.0705185 test: 0.1384719 best: 0.1384619 (3967) 3977: learn: 0.0705075 test: 0.1384680 best: 0.1384619 (3967) 3978: learn: 0.0704963 test: 0.1384675 best: 0.1384619 (3967) 3979: learn: 0.0704870 test: 0.1384664 best: 0.1384619 (3967) 3980: learn: 0.0704781 test: 0.1384652 best: 0.1384619 (3967) 3981: learn: 0.0704706 test: 0.1384633 best: 0.1384619 (3967) 3982: learn: 0.0704608 test: 0.1384606 best: 0.1384606 (3982) 3983: learn: 0.0704527 test: 0.1384544 best: 0.1384544 (3983) 3984: learn: 0.0704368 test: 0.1384546 best: 0.1384544 (3983) 3985: learn: 0.0704249 test: 0.1384573 best: 0.1384544 (3983) 3986: learn: 0.0704145 test: 0.1384545 best: 0.1384544 (3983) 3987: learn: 0.0704017 test: 0.1384551 best: 0.1384544 (3983) 3988: learn: 0.0703928 test: 0.1384523 best: 0.1384523 (3988) 3989: learn: 0.0703821 test: 0.1384519 best: 0.1384519 (3989) 3990: learn: 0.0703687 test: 0.1384488 best: 0.1384488 (3990) 3991: learn: 0.0703594 test: 0.1384494 best: 0.1384488 (3990) 3992: learn: 0.0703490 test: 0.1384513 best: 0.1384488 (3990) 3993: learn: 0.0703399 test: 0.1384477 best: 0.1384477 (3993) 3994: learn: 0.0703298 test: 0.1384442 best: 0.1384442 (3994) 3995: learn: 0.0703182 test: 0.1384417 best: 0.1384417 (3995) 3996: learn: 0.0703066 test: 0.1384420 best: 0.1384417 (3995) 3997: learn: 0.0703029 test: 0.1384399 best: 0.1384399 (3997) 3998: learn: 0.0702915 test: 0.1384388 best: 0.1384388 (3998) 3999: learn: 0.0702756 test: 0.1384331 best: 0.1384331 (3999) 4000: learn: 0.0702643 test: 0.1384284 best: 0.1384284 (4000) 4001: learn: 0.0702544 test: 0.1384273 best: 0.1384273 (4001) 4002: learn: 0.0702451 test: 0.1384288 best: 0.1384273 (4001) 4003: learn: 0.0702334 test: 0.1384301 best: 0.1384273 (4001) 4004: learn: 0.0702267 test: 0.1384283 best: 0.1384273 (4001) 4005: learn: 0.0702176 test: 0.1384278 best: 0.1384273 (4001) 4006: learn: 0.0702073 test: 0.1384256 best: 0.1384256 (4006) 4007: learn: 0.0701964 test: 0.1384228 best: 0.1384228 (4007) 4008: learn: 0.0701861 test: 0.1384222 best: 0.1384222 (4008) 4009: learn: 0.0701745 test: 0.1384209 best: 0.1384209 (4009) 4010: learn: 0.0701670 test: 0.1384189 best: 0.1384189 (4010) 4011: learn: 0.0701595 test: 0.1384217 best: 0.1384189 (4010) 4012: learn: 0.0701464 test: 0.1384157 best: 0.1384157 (4012) 4013: learn: 0.0701342 test: 0.1384117 best: 0.1384117 (4013) 4014: learn: 0.0701255 test: 0.1384133 best: 0.1384117 (4013) 4015: learn: 0.0701145 test: 0.1384128 best: 0.1384117 (4013) 4016: learn: 0.0701050 test: 0.1384141 best: 0.1384117 (4013) 4017: learn: 0.0700927 test: 0.1384138 best: 0.1384117 (4013) 4018: learn: 0.0700827 test: 0.1384107 best: 0.1384107 (4018) 4019: learn: 0.0700685 test: 0.1384092 best: 0.1384092 (4019) 4020: learn: 0.0700586 test: 0.1384098 best: 0.1384092 (4019) 4021: learn: 0.0700479 test: 0.1384061 best: 0.1384061 (4021) 4022: learn: 0.0700435 test: 0.1384048 best: 0.1384048 (4022) 4023: learn: 0.0700307 test: 0.1384064 best: 0.1384048 (4022) 4024: learn: 0.0700213 test: 0.1384060 best: 0.1384048 (4022) 4025: learn: 0.0700078 test: 0.1384060 best: 0.1384048 (4022) 4026: learn: 0.0699979 test: 0.1384082 best: 0.1384048 (4022) 4027: learn: 0.0699880 test: 0.1384106 best: 0.1384048 (4022) 4028: learn: 0.0699810 test: 0.1384127 best: 0.1384048 (4022) 4029: learn: 0.0699682 test: 0.1384142 best: 0.1384048 (4022) 4030: learn: 0.0699589 test: 0.1384141 best: 0.1384048 (4022) 4031: learn: 0.0699498 test: 0.1384133 best: 0.1384048 (4022) 4032: learn: 0.0699365 test: 0.1384065 best: 0.1384048 (4022) 4033: learn: 0.0699300 test: 0.1384020 best: 0.1384020 (4033) 4034: learn: 0.0699200 test: 0.1384036 best: 0.1384020 (4033) 4035: learn: 0.0699115 test: 0.1384059 best: 0.1384020 (4033) 4036: learn: 0.0698990 test: 0.1384069 best: 0.1384020 (4033) 4037: learn: 0.0698896 test: 0.1384048 best: 0.1384020 (4033) 4038: learn: 0.0698781 test: 0.1384051 best: 0.1384020 (4033) 4039: learn: 0.0698706 test: 0.1384052 best: 0.1384020 (4033) 4040: learn: 0.0698633 test: 0.1384065 best: 0.1384020 (4033) 4041: learn: 0.0698552 test: 0.1384015 best: 0.1384015 (4041) 4042: learn: 0.0698445 test: 0.1383986 best: 0.1383986 (4042) 4043: learn: 0.0698354 test: 0.1383988 best: 0.1383986 (4042) 4044: learn: 0.0698251 test: 0.1383929 best: 0.1383929 (4044) 4045: learn: 0.0698158 test: 0.1383942 best: 0.1383929 (4044) 4046: learn: 0.0698042 test: 0.1383889 best: 0.1383889 (4046) 4047: learn: 0.0697959 test: 0.1383872 best: 0.1383872 (4047) 4048: learn: 0.0697846 test: 0.1383907 best: 0.1383872 (4047) 4049: learn: 0.0697809 test: 0.1383876 best: 0.1383872 (4047) 4050: learn: 0.0697730 test: 0.1383883 best: 0.1383872 (4047) 4051: learn: 0.0697597 test: 0.1383843 best: 0.1383843 (4051) 4052: learn: 0.0697520 test: 0.1383824 best: 0.1383824 (4052) 4053: learn: 0.0697408 test: 0.1383800 best: 0.1383800 (4053) 4054: learn: 0.0697287 test: 0.1383803 best: 0.1383800 (4053) 4055: learn: 0.0697179 test: 0.1383789 best: 0.1383789 (4055) 4056: learn: 0.0697096 test: 0.1383763 best: 0.1383763 (4056) 4057: learn: 0.0696976 test: 0.1383811 best: 0.1383763 (4056) 4058: learn: 0.0696863 test: 0.1383825 best: 0.1383763 (4056) 4059: learn: 0.0696763 test: 0.1383854 best: 0.1383763 (4056) 4060: learn: 0.0696637 test: 0.1383840 best: 0.1383763 (4056) 4061: learn: 0.0696544 test: 0.1383835 best: 0.1383763 (4056) 4062: learn: 0.0696463 test: 0.1383829 best: 0.1383763 (4056) 4063: learn: 0.0696379 test: 0.1383845 best: 0.1383763 (4056) 4064: learn: 0.0696301 test: 0.1383814 best: 0.1383763 (4056) 4065: learn: 0.0696188 test: 0.1383786 best: 0.1383763 (4056) 4066: learn: 0.0696076 test: 0.1383792 best: 0.1383763 (4056) 4067: learn: 0.0695959 test: 0.1383766 best: 0.1383763 (4056) 4068: learn: 0.0695838 test: 0.1383751 best: 0.1383751 (4068) 4069: learn: 0.0695712 test: 0.1383745 best: 0.1383745 (4069) 4070: learn: 0.0695613 test: 0.1383739 best: 0.1383739 (4070) 4071: learn: 0.0695518 test: 0.1383712 best: 0.1383712 (4071) 4072: learn: 0.0695389 test: 0.1383706 best: 0.1383706 (4072) total: 46.6s remaining: 22s 4073: learn: 0.0695347 test: 0.1383703 best: 0.1383703 (4073) 4074: learn: 0.0695242 test: 0.1383669 best: 0.1383669 (4074) 4075: learn: 0.0695150 test: 0.1383664 best: 0.1383664 (4075) 4076: learn: 0.0695037 test: 0.1383652 best: 0.1383652 (4076) 4077: learn: 0.0694943 test: 0.1383623 best: 0.1383623 (4077) 4078: learn: 0.0694839 test: 0.1383640 best: 0.1383623 (4077) 4079: learn: 0.0694734 test: 0.1383609 best: 0.1383609 (4079) 4080: learn: 0.0694642 test: 0.1383618 best: 0.1383609 (4079) 4081: learn: 0.0694531 test: 0.1383634 best: 0.1383609 (4079) 4082: learn: 0.0694406 test: 0.1383686 best: 0.1383609 (4079) 4083: learn: 0.0694316 test: 0.1383673 best: 0.1383609 (4079) 4084: learn: 0.0694194 test: 0.1383651 best: 0.1383609 (4079) 4085: learn: 0.0694091 test: 0.1383664 best: 0.1383609 (4079) 4086: learn: 0.0694005 test: 0.1383641 best: 0.1383609 (4079) 4087: learn: 0.0693915 test: 0.1383648 best: 0.1383609 (4079) 4088: learn: 0.0693807 test: 0.1383621 best: 0.1383609 (4079) 4089: learn: 0.0693722 test: 0.1383593 best: 0.1383593 (4089) 4090: learn: 0.0693632 test: 0.1383582 best: 0.1383582 (4090) 4091: learn: 0.0693541 test: 0.1383589 best: 0.1383582 (4090) 4092: learn: 0.0693429 test: 0.1383573 best: 0.1383573 (4092) 4093: learn: 0.0693349 test: 0.1383554 best: 0.1383554 (4093) 4094: learn: 0.0693254 test: 0.1383579 best: 0.1383554 (4093) 4095: learn: 0.0693123 test: 0.1383573 best: 0.1383554 (4093) 4096: learn: 0.0693027 test: 0.1383570 best: 0.1383554 (4093) 4097: learn: 0.0692935 test: 0.1383570 best: 0.1383554 (4093) 4098: learn: 0.0692832 test: 0.1383545 best: 0.1383545 (4098) 4099: learn: 0.0692732 test: 0.1383529 best: 0.1383529 (4099) 4100: learn: 0.0692629 test: 0.1383478 best: 0.1383478 (4100) 4101: learn: 0.0692543 test: 0.1383470 best: 0.1383470 (4101) 4102: learn: 0.0692474 test: 0.1383449 best: 0.1383449 (4102) 4103: learn: 0.0692365 test: 0.1383453 best: 0.1383449 (4102) 4104: learn: 0.0692280 test: 0.1383452 best: 0.1383449 (4102) 4105: learn: 0.0692160 test: 0.1383458 best: 0.1383449 (4102) 4106: learn: 0.0692075 test: 0.1383431 best: 0.1383431 (4106) 4107: learn: 0.0691986 test: 0.1383420 best: 0.1383420 (4107) 4108: learn: 0.0691852 test: 0.1383448 best: 0.1383420 (4107) 4109: learn: 0.0691756 test: 0.1383409 best: 0.1383409 (4109) 4110: learn: 0.0691662 test: 0.1383383 best: 0.1383383 (4110) 4111: learn: 0.0691553 test: 0.1383375 best: 0.1383375 (4111) 4112: learn: 0.0691478 test: 0.1383382 best: 0.1383375 (4111) 4113: learn: 0.0691372 test: 0.1383371 best: 0.1383371 (4113) 4114: learn: 0.0691273 test: 0.1383348 best: 0.1383348 (4114) 4115: learn: 0.0691206 test: 0.1383330 best: 0.1383330 (4115) 4116: learn: 0.0691112 test: 0.1383353 best: 0.1383330 (4115) 4117: learn: 0.0691032 test: 0.1383363 best: 0.1383330 (4115) 4118: learn: 0.0690951 test: 0.1383363 best: 0.1383330 (4115) 4119: learn: 0.0690844 test: 0.1383351 best: 0.1383330 (4115) 4120: learn: 0.0690765 test: 0.1383343 best: 0.1383330 (4115) 4121: learn: 0.0690693 test: 0.1383318 best: 0.1383318 (4121) 4122: learn: 0.0690632 test: 0.1383302 best: 0.1383302 (4122) 4123: learn: 0.0690521 test: 0.1383278 best: 0.1383278 (4123) 4124: learn: 0.0690416 test: 0.1383299 best: 0.1383278 (4123) 4125: learn: 0.0690321 test: 0.1383288 best: 0.1383278 (4123) 4126: learn: 0.0690236 test: 0.1383314 best: 0.1383278 (4123) 4127: learn: 0.0690127 test: 0.1383320 best: 0.1383278 (4123) 4128: learn: 0.0690035 test: 0.1383285 best: 0.1383278 (4123) 4129: learn: 0.0689948 test: 0.1383258 best: 0.1383258 (4129) 4130: learn: 0.0689845 test: 0.1383272 best: 0.1383258 (4129) 4131: learn: 0.0689733 test: 0.1383245 best: 0.1383245 (4131) 4132: learn: 0.0689597 test: 0.1383198 best: 0.1383198 (4132) 4133: learn: 0.0689482 test: 0.1383169 best: 0.1383169 (4133) 4134: learn: 0.0689355 test: 0.1383176 best: 0.1383169 (4133) 4135: learn: 0.0689234 test: 0.1383185 best: 0.1383169 (4133) 4136: learn: 0.0689137 test: 0.1383164 best: 0.1383164 (4136) 4137: learn: 0.0689036 test: 0.1383171 best: 0.1383164 (4136) 4138: learn: 0.0688935 test: 0.1383160 best: 0.1383160 (4138) 4139: learn: 0.0688846 test: 0.1383127 best: 0.1383127 (4139) 4140: learn: 0.0688696 test: 0.1383148 best: 0.1383127 (4139) 4141: learn: 0.0688571 test: 0.1383151 best: 0.1383127 (4139) 4142: learn: 0.0688506 test: 0.1383135 best: 0.1383127 (4139) 4143: learn: 0.0688381 test: 0.1383163 best: 0.1383127 (4139) 4144: learn: 0.0688280 test: 0.1383080 best: 0.1383080 (4144) 4145: learn: 0.0688172 test: 0.1383057 best: 0.1383057 (4145) 4146: learn: 0.0688056 test: 0.1383065 best: 0.1383057 (4145) 4147: learn: 0.0687963 test: 0.1383062 best: 0.1383057 (4145) 4148: learn: 0.0687880 test: 0.1383018 best: 0.1383018 (4148) 4149: learn: 0.0687797 test: 0.1383035 best: 0.1383018 (4148) 4150: learn: 0.0687707 test: 0.1383024 best: 0.1383018 (4148) 4151: learn: 0.0687622 test: 0.1382999 best: 0.1382999 (4151) 4152: learn: 0.0687533 test: 0.1382992 best: 0.1382992 (4152) 4153: learn: 0.0687425 test: 0.1382957 best: 0.1382957 (4153) 4154: learn: 0.0687308 test: 0.1383004 best: 0.1382957 (4153) 4155: learn: 0.0687164 test: 0.1383062 best: 0.1382957 (4153) 4156: learn: 0.0687073 test: 0.1383112 best: 0.1382957 (4153) 4157: learn: 0.0686982 test: 0.1383107 best: 0.1382957 (4153) 4158: learn: 0.0686904 test: 0.1383106 best: 0.1382957 (4153) 4159: learn: 0.0686801 test: 0.1383104 best: 0.1382957 (4153) 4160: learn: 0.0686690 test: 0.1383083 best: 0.1382957 (4153) 4161: learn: 0.0686589 test: 0.1383043 best: 0.1382957 (4153) 4162: learn: 0.0686524 test: 0.1383031 best: 0.1382957 (4153) 4163: learn: 0.0686451 test: 0.1383026 best: 0.1382957 (4153) 4164: learn: 0.0686363 test: 0.1382992 best: 0.1382957 (4153) 4165: learn: 0.0686269 test: 0.1383016 best: 0.1382957 (4153) 4166: learn: 0.0686156 test: 0.1383038 best: 0.1382957 (4153) 4167: learn: 0.0686036 test: 0.1383011 best: 0.1382957 (4153) 4168: learn: 0.0685921 test: 0.1382999 best: 0.1382957 (4153) 4169: learn: 0.0685797 test: 0.1382974 best: 0.1382957 (4153) 4170: learn: 0.0685670 test: 0.1382993 best: 0.1382957 (4153) 4171: learn: 0.0685578 test: 0.1382976 best: 0.1382957 (4153) 4172: learn: 0.0685506 test: 0.1382983 best: 0.1382957 (4153) 4173: learn: 0.0685388 test: 0.1382969 best: 0.1382957 (4153) 4174: learn: 0.0685292 test: 0.1382936 best: 0.1382936 (4174) 4175: learn: 0.0685215 test: 0.1382928 best: 0.1382928 (4175) 4176: learn: 0.0685097 test: 0.1382905 best: 0.1382905 (4176) 4177: learn: 0.0684981 test: 0.1382865 best: 0.1382865 (4177) 4178: learn: 0.0684875 test: 0.1382836 best: 0.1382836 (4178) 4179: learn: 0.0684766 test: 0.1382846 best: 0.1382836 (4178) 4180: learn: 0.0684641 test: 0.1382855 best: 0.1382836 (4178) 4181: learn: 0.0684562 test: 0.1382848 best: 0.1382836 (4178) 4182: learn: 0.0684465 test: 0.1382846 best: 0.1382836 (4178) 4183: learn: 0.0684353 test: 0.1382832 best: 0.1382832 (4183) 4184: learn: 0.0684285 test: 0.1382827 best: 0.1382827 (4184) 4185: learn: 0.0684170 test: 0.1382802 best: 0.1382802 (4185) 4186: learn: 0.0684105 test: 0.1382773 best: 0.1382773 (4186) 4187: learn: 0.0683997 test: 0.1382750 best: 0.1382750 (4187) 4188: learn: 0.0683881 test: 0.1382705 best: 0.1382705 (4188) 4189: learn: 0.0683763 test: 0.1382692 best: 0.1382692 (4189) 4190: learn: 0.0683650 test: 0.1382684 best: 0.1382684 (4190) 4191: learn: 0.0683584 test: 0.1382674 best: 0.1382674 (4191) 4192: learn: 0.0683473 test: 0.1382654 best: 0.1382654 (4192) 4193: learn: 0.0683333 test: 0.1382666 best: 0.1382654 (4192) 4194: learn: 0.0683202 test: 0.1382648 best: 0.1382648 (4194) 4195: learn: 0.0683108 test: 0.1382683 best: 0.1382648 (4194) 4196: learn: 0.0682999 test: 0.1382625 best: 0.1382625 (4196) 4197: learn: 0.0682876 test: 0.1382638 best: 0.1382625 (4196) 4198: learn: 0.0682773 test: 0.1382664 best: 0.1382625 (4196) 4199: learn: 0.0682679 test: 0.1382670 best: 0.1382625 (4196) 4200: learn: 0.0682555 test: 0.1382633 best: 0.1382625 (4196) 4201: learn: 0.0682464 test: 0.1382637 best: 0.1382625 (4196) 4202: learn: 0.0682354 test: 0.1382590 best: 0.1382590 (4202) 4203: learn: 0.0682245 test: 0.1382567 best: 0.1382567 (4203) 4204: learn: 0.0682146 test: 0.1382534 best: 0.1382534 (4204) 4205: learn: 0.0682043 test: 0.1382522 best: 0.1382522 (4205) 4206: learn: 0.0681954 test: 0.1382532 best: 0.1382522 (4205) 4207: learn: 0.0681857 test: 0.1382496 best: 0.1382496 (4207) 4208: learn: 0.0681762 test: 0.1382527 best: 0.1382496 (4207) 4209: learn: 0.0681655 test: 0.1382500 best: 0.1382496 (4207) 4210: learn: 0.0681538 test: 0.1382473 best: 0.1382473 (4210) 4211: learn: 0.0681415 test: 0.1382481 best: 0.1382473 (4210) 4212: learn: 0.0681307 test: 0.1382481 best: 0.1382473 (4210) 4213: learn: 0.0681244 test: 0.1382425 best: 0.1382425 (4213) 4214: learn: 0.0681160 test: 0.1382430 best: 0.1382425 (4213) 4215: learn: 0.0681066 test: 0.1382440 best: 0.1382425 (4213) 4216: learn: 0.0680960 test: 0.1382398 best: 0.1382398 (4216) 4217: learn: 0.0680838 test: 0.1382413 best: 0.1382398 (4216) 4218: learn: 0.0680738 test: 0.1382383 best: 0.1382383 (4218) 4219: learn: 0.0680651 test: 0.1382362 best: 0.1382362 (4219) 4220: learn: 0.0680583 test: 0.1382358 best: 0.1382358 (4220) 4221: learn: 0.0680531 test: 0.1382366 best: 0.1382358 (4220) 4222: learn: 0.0680426 test: 0.1382306 best: 0.1382306 (4222) 4223: learn: 0.0680311 test: 0.1382281 best: 0.1382281 (4223) 4224: learn: 0.0680218 test: 0.1382258 best: 0.1382258 (4224) 4225: learn: 0.0680139 test: 0.1382266 best: 0.1382258 (4224) 4226: learn: 0.0680052 test: 0.1382253 best: 0.1382253 (4226) total: 48.3s remaining: 20.3s 4227: learn: 0.0679952 test: 0.1382217 best: 0.1382217 (4227) 4228: learn: 0.0679911 test: 0.1382194 best: 0.1382194 (4228) 4229: learn: 0.0679784 test: 0.1382134 best: 0.1382134 (4229) 4230: learn: 0.0679656 test: 0.1382164 best: 0.1382134 (4229) 4231: learn: 0.0679524 test: 0.1382121 best: 0.1382121 (4231) 4232: learn: 0.0679411 test: 0.1382095 best: 0.1382095 (4232) 4233: learn: 0.0679287 test: 0.1382095 best: 0.1382095 (4233) 4234: learn: 0.0679209 test: 0.1382060 best: 0.1382060 (4234) 4235: learn: 0.0679116 test: 0.1382040 best: 0.1382040 (4235) 4236: learn: 0.0679019 test: 0.1382046 best: 0.1382040 (4235) 4237: learn: 0.0678924 test: 0.1382062 best: 0.1382040 (4235) 4238: learn: 0.0678848 test: 0.1382041 best: 0.1382040 (4235) 4239: learn: 0.0678769 test: 0.1382049 best: 0.1382040 (4235) 4240: learn: 0.0678687 test: 0.1382069 best: 0.1382040 (4235) 4241: learn: 0.0678581 test: 0.1382047 best: 0.1382040 (4235) 4242: learn: 0.0678468 test: 0.1382039 best: 0.1382039 (4242) 4243: learn: 0.0678361 test: 0.1382025 best: 0.1382025 (4243) 4244: learn: 0.0678268 test: 0.1381983 best: 0.1381983 (4244) 4245: learn: 0.0678157 test: 0.1381973 best: 0.1381973 (4245) 4246: learn: 0.0678052 test: 0.1382012 best: 0.1381973 (4245) 4247: learn: 0.0677967 test: 0.1382036 best: 0.1381973 (4245) 4248: learn: 0.0677863 test: 0.1381989 best: 0.1381973 (4245) 4249: learn: 0.0677793 test: 0.1382008 best: 0.1381973 (4245) 4250: learn: 0.0677708 test: 0.1382026 best: 0.1381973 (4245) 4251: learn: 0.0677616 test: 0.1382028 best: 0.1381973 (4245) 4252: learn: 0.0677540 test: 0.1382032 best: 0.1381973 (4245) 4253: learn: 0.0677445 test: 0.1382037 best: 0.1381973 (4245) 4254: learn: 0.0677361 test: 0.1382034 best: 0.1381973 (4245) 4255: learn: 0.0677276 test: 0.1382012 best: 0.1381973 (4245) 4256: learn: 0.0677191 test: 0.1382003 best: 0.1381973 (4245) 4257: learn: 0.0677076 test: 0.1381983 best: 0.1381973 (4245) 4258: learn: 0.0676991 test: 0.1381943 best: 0.1381943 (4258) 4259: learn: 0.0676892 test: 0.1381909 best: 0.1381909 (4259) 4260: learn: 0.0676836 test: 0.1381910 best: 0.1381909 (4259) 4261: learn: 0.0676709 test: 0.1381860 best: 0.1381860 (4261) 4262: learn: 0.0676652 test: 0.1381849 best: 0.1381849 (4262) 4263: learn: 0.0676550 test: 0.1381837 best: 0.1381837 (4263) 4264: learn: 0.0676480 test: 0.1381829 best: 0.1381829 (4264) 4265: learn: 0.0676385 test: 0.1381811 best: 0.1381811 (4265) 4266: learn: 0.0676262 test: 0.1381840 best: 0.1381811 (4265) 4267: learn: 0.0676165 test: 0.1381837 best: 0.1381811 (4265) 4268: learn: 0.0676047 test: 0.1381858 best: 0.1381811 (4265) 4269: learn: 0.0675913 test: 0.1381842 best: 0.1381811 (4265) 4270: learn: 0.0675814 test: 0.1381852 best: 0.1381811 (4265) 4271: learn: 0.0675717 test: 0.1381874 best: 0.1381811 (4265) 4272: learn: 0.0675631 test: 0.1381861 best: 0.1381811 (4265) 4273: learn: 0.0675537 test: 0.1381832 best: 0.1381811 (4265) 4274: learn: 0.0675427 test: 0.1381814 best: 0.1381811 (4265) 4275: learn: 0.0675342 test: 0.1381832 best: 0.1381811 (4265) 4276: learn: 0.0675222 test: 0.1381817 best: 0.1381811 (4265) 4277: learn: 0.0675137 test: 0.1381805 best: 0.1381805 (4277) 4278: learn: 0.0675038 test: 0.1381778 best: 0.1381778 (4278) 4279: learn: 0.0674946 test: 0.1381765 best: 0.1381765 (4279) 4280: learn: 0.0674868 test: 0.1381777 best: 0.1381765 (4279) 4281: learn: 0.0674762 test: 0.1381806 best: 0.1381765 (4279) 4282: learn: 0.0674660 test: 0.1381816 best: 0.1381765 (4279) 4283: learn: 0.0674567 test: 0.1381817 best: 0.1381765 (4279) 4284: learn: 0.0674448 test: 0.1381786 best: 0.1381765 (4279) 4285: learn: 0.0674355 test: 0.1381759 best: 0.1381759 (4285) 4286: learn: 0.0674277 test: 0.1381741 best: 0.1381741 (4286) 4287: learn: 0.0674208 test: 0.1381740 best: 0.1381740 (4287) 4288: learn: 0.0674110 test: 0.1381738 best: 0.1381738 (4288) 4289: learn: 0.0674034 test: 0.1381741 best: 0.1381738 (4288) 4290: learn: 0.0673944 test: 0.1381731 best: 0.1381731 (4290) 4291: learn: 0.0673847 test: 0.1381727 best: 0.1381727 (4291) 4292: learn: 0.0673784 test: 0.1381715 best: 0.1381715 (4292) 4293: learn: 0.0673667 test: 0.1381717 best: 0.1381715 (4292) 4294: learn: 0.0673607 test: 0.1381755 best: 0.1381715 (4292) 4295: learn: 0.0673523 test: 0.1381785 best: 0.1381715 (4292) 4296: learn: 0.0673403 test: 0.1381792 best: 0.1381715 (4292) 4297: learn: 0.0673316 test: 0.1381777 best: 0.1381715 (4292) 4298: learn: 0.0673203 test: 0.1381762 best: 0.1381715 (4292) 4299: learn: 0.0673084 test: 0.1381750 best: 0.1381715 (4292) 4300: learn: 0.0672977 test: 0.1381744 best: 0.1381715 (4292) 4301: learn: 0.0672860 test: 0.1381738 best: 0.1381715 (4292) 4302: learn: 0.0672734 test: 0.1381703 best: 0.1381703 (4302) 4303: learn: 0.0672628 test: 0.1381714 best: 0.1381703 (4302) 4304: learn: 0.0672517 test: 0.1381685 best: 0.1381685 (4304) 4305: learn: 0.0672393 test: 0.1381659 best: 0.1381659 (4305) 4306: learn: 0.0672313 test: 0.1381650 best: 0.1381650 (4306) 4307: learn: 0.0672218 test: 0.1381648 best: 0.1381648 (4307) 4308: learn: 0.0672134 test: 0.1381618 best: 0.1381618 (4308) 4309: learn: 0.0672066 test: 0.1381621 best: 0.1381618 (4308) 4310: learn: 0.0671934 test: 0.1381595 best: 0.1381595 (4310) 4311: learn: 0.0671805 test: 0.1381588 best: 0.1381588 (4311) 4312: learn: 0.0671728 test: 0.1381565 best: 0.1381565 (4312) 4313: learn: 0.0671649 test: 0.1381572 best: 0.1381565 (4312) 4314: learn: 0.0671542 test: 0.1381546 best: 0.1381546 (4314) 4315: learn: 0.0671415 test: 0.1381529 best: 0.1381529 (4315) 4316: learn: 0.0671307 test: 0.1381489 best: 0.1381489 (4316) 4317: learn: 0.0671238 test: 0.1381489 best: 0.1381489 (4316) 4318: learn: 0.0671141 test: 0.1381488 best: 0.1381488 (4318) 4319: learn: 0.0671041 test: 0.1381463 best: 0.1381463 (4319) 4320: learn: 0.0670959 test: 0.1381457 best: 0.1381457 (4320) 4321: learn: 0.0670879 test: 0.1381410 best: 0.1381410 (4321) 4322: learn: 0.0670795 test: 0.1381405 best: 0.1381405 (4322) 4323: learn: 0.0670670 test: 0.1381422 best: 0.1381405 (4322) 4324: learn: 0.0670583 test: 0.1381390 best: 0.1381390 (4324) 4325: learn: 0.0670503 test: 0.1381388 best: 0.1381388 (4325) 4326: learn: 0.0670415 test: 0.1381414 best: 0.1381388 (4325) 4327: learn: 0.0670349 test: 0.1381382 best: 0.1381382 (4327) 4328: learn: 0.0670258 test: 0.1381389 best: 0.1381382 (4327) 4329: learn: 0.0670188 test: 0.1381397 best: 0.1381382 (4327) 4330: learn: 0.0670082 test: 0.1381418 best: 0.1381382 (4327) 4331: learn: 0.0669989 test: 0.1381388 best: 0.1381382 (4327) 4332: learn: 0.0669934 test: 0.1381370 best: 0.1381370 (4332) 4333: learn: 0.0669888 test: 0.1381335 best: 0.1381335 (4333) 4334: learn: 0.0669827 test: 0.1381333 best: 0.1381333 (4334) 4335: learn: 0.0669729 test: 0.1381301 best: 0.1381301 (4335) 4336: learn: 0.0669685 test: 0.1381287 best: 0.1381287 (4336) 4337: learn: 0.0669579 test: 0.1381293 best: 0.1381287 (4336) 4338: learn: 0.0669488 test: 0.1381296 best: 0.1381287 (4336) 4339: learn: 0.0669389 test: 0.1381257 best: 0.1381257 (4339) 4340: learn: 0.0669277 test: 0.1381303 best: 0.1381257 (4339) 4341: learn: 0.0669178 test: 0.1381275 best: 0.1381257 (4339) 4342: learn: 0.0669083 test: 0.1381245 best: 0.1381245 (4342) 4343: learn: 0.0668990 test: 0.1381216 best: 0.1381216 (4343) 4344: learn: 0.0668912 test: 0.1381203 best: 0.1381203 (4344) 4345: learn: 0.0668841 test: 0.1381163 best: 0.1381163 (4345) 4346: learn: 0.0668771 test: 0.1381161 best: 0.1381161 (4346) 4347: learn: 0.0668668 test: 0.1381162 best: 0.1381161 (4346) 4348: learn: 0.0668597 test: 0.1381142 best: 0.1381142 (4348) 4349: learn: 0.0668513 test: 0.1381118 best: 0.1381118 (4349) 4350: learn: 0.0668406 test: 0.1381126 best: 0.1381118 (4349) 4351: learn: 0.0668318 test: 0.1381111 best: 0.1381111 (4351) 4352: learn: 0.0668243 test: 0.1381063 best: 0.1381063 (4352) 4353: learn: 0.0668164 test: 0.1381051 best: 0.1381051 (4353) 4354: learn: 0.0668052 test: 0.1381016 best: 0.1381016 (4354) 4355: learn: 0.0667974 test: 0.1380985 best: 0.1380985 (4355) 4356: learn: 0.0667871 test: 0.1380963 best: 0.1380963 (4356) 4357: learn: 0.0667775 test: 0.1380943 best: 0.1380943 (4357) 4358: learn: 0.0667670 test: 0.1380952 best: 0.1380943 (4357) 4359: learn: 0.0667538 test: 0.1380925 best: 0.1380925 (4359) 4360: learn: 0.0667463 test: 0.1380930 best: 0.1380925 (4359) 4361: learn: 0.0667340 test: 0.1380908 best: 0.1380908 (4361) 4362: learn: 0.0667241 test: 0.1380894 best: 0.1380894 (4362) 4363: learn: 0.0667144 test: 0.1380921 best: 0.1380894 (4362) total: 49.8s remaining: 18.7s 4364: learn: 0.0667061 test: 0.1380894 best: 0.1380894 (4362) 4365: learn: 0.0666992 test: 0.1380845 best: 0.1380845 (4365) 4366: learn: 0.0666906 test: 0.1380860 best: 0.1380845 (4365) 4367: learn: 0.0666858 test: 0.1380860 best: 0.1380845 (4365) 4368: learn: 0.0666780 test: 0.1380881 best: 0.1380845 (4365) 4369: learn: 0.0666668 test: 0.1380902 best: 0.1380845 (4365) 4370: learn: 0.0666600 test: 0.1380883 best: 0.1380845 (4365) 4371: learn: 0.0666477 test: 0.1380887 best: 0.1380845 (4365) 4372: learn: 0.0666386 test: 0.1380887 best: 0.1380845 (4365) 4373: learn: 0.0666281 test: 0.1380865 best: 0.1380845 (4365) 4374: learn: 0.0666172 test: 0.1380847 best: 0.1380845 (4365) 4375: learn: 0.0666042 test: 0.1380823 best: 0.1380823 (4375) 4376: learn: 0.0665957 test: 0.1380806 best: 0.1380806 (4376) 4377: learn: 0.0665858 test: 0.1380778 best: 0.1380778 (4377) 4378: learn: 0.0665769 test: 0.1380768 best: 0.1380768 (4378) 4379: learn: 0.0665685 test: 0.1380751 best: 0.1380751 (4379) 4380: learn: 0.0665584 test: 0.1380777 best: 0.1380751 (4379) 4381: learn: 0.0665485 test: 0.1380799 best: 0.1380751 (4379) 4382: learn: 0.0665422 test: 0.1380799 best: 0.1380751 (4379) 4383: learn: 0.0665368 test: 0.1380785 best: 0.1380751 (4379) 4384: learn: 0.0665265 test: 0.1380787 best: 0.1380751 (4379) 4385: learn: 0.0665140 test: 0.1380746 best: 0.1380746 (4385) 4386: learn: 0.0665036 test: 0.1380772 best: 0.1380746 (4385) 4387: learn: 0.0664960 test: 0.1380765 best: 0.1380746 (4385) 4388: learn: 0.0664845 test: 0.1380796 best: 0.1380746 (4385) 4389: learn: 0.0664755 test: 0.1380811 best: 0.1380746 (4385) 4390: learn: 0.0664669 test: 0.1380791 best: 0.1380746 (4385) 4391: learn: 0.0664578 test: 0.1380776 best: 0.1380746 (4385) 4392: learn: 0.0664510 test: 0.1380767 best: 0.1380746 (4385) 4393: learn: 0.0664394 test: 0.1380806 best: 0.1380746 (4385) 4394: learn: 0.0664311 test: 0.1380814 best: 0.1380746 (4385) 4395: learn: 0.0664198 test: 0.1380773 best: 0.1380746 (4385) 4396: learn: 0.0664060 test: 0.1380775 best: 0.1380746 (4385) 4397: learn: 0.0663990 test: 0.1380769 best: 0.1380746 (4385) 4398: learn: 0.0663891 test: 0.1380764 best: 0.1380746 (4385) 4399: learn: 0.0663792 test: 0.1380770 best: 0.1380746 (4385) 4400: learn: 0.0663700 test: 0.1380757 best: 0.1380746 (4385) 4401: learn: 0.0663608 test: 0.1380761 best: 0.1380746 (4385) 4402: learn: 0.0663530 test: 0.1380769 best: 0.1380746 (4385) 4403: learn: 0.0663425 test: 0.1380752 best: 0.1380746 (4385) 4404: learn: 0.0663339 test: 0.1380724 best: 0.1380724 (4404) 4405: learn: 0.0663243 test: 0.1380722 best: 0.1380722 (4405) 4406: learn: 0.0663167 test: 0.1380734 best: 0.1380722 (4405) 4407: learn: 0.0663036 test: 0.1380740 best: 0.1380722 (4405) 4408: learn: 0.0662960 test: 0.1380729 best: 0.1380722 (4405) 4409: learn: 0.0662905 test: 0.1380692 best: 0.1380692 (4409) 4410: learn: 0.0662819 test: 0.1380687 best: 0.1380687 (4410) 4411: learn: 0.0662748 test: 0.1380682 best: 0.1380682 (4411) 4412: learn: 0.0662660 test: 0.1380688 best: 0.1380682 (4411) 4413: learn: 0.0662551 test: 0.1380680 best: 0.1380680 (4413) 4414: learn: 0.0662465 test: 0.1380668 best: 0.1380668 (4414) 4415: learn: 0.0662399 test: 0.1380653 best: 0.1380653 (4415) 4416: learn: 0.0662288 test: 0.1380698 best: 0.1380653 (4415) 4417: learn: 0.0662222 test: 0.1380706 best: 0.1380653 (4415) 4418: learn: 0.0662132 test: 0.1380715 best: 0.1380653 (4415) 4419: learn: 0.0662030 test: 0.1380723 best: 0.1380653 (4415) 4420: learn: 0.0661948 test: 0.1380715 best: 0.1380653 (4415) 4421: learn: 0.0661854 test: 0.1380728 best: 0.1380653 (4415) 4422: learn: 0.0661755 test: 0.1380711 best: 0.1380653 (4415) 4423: learn: 0.0661654 test: 0.1380726 best: 0.1380653 (4415) 4424: learn: 0.0661580 test: 0.1380726 best: 0.1380653 (4415) 4425: learn: 0.0661458 test: 0.1380736 best: 0.1380653 (4415) 4426: learn: 0.0661371 test: 0.1380737 best: 0.1380653 (4415) 4427: learn: 0.0661282 test: 0.1380742 best: 0.1380653 (4415) 4428: learn: 0.0661127 test: 0.1380697 best: 0.1380653 (4415) 4429: learn: 0.0661020 test: 0.1380694 best: 0.1380653 (4415) 4430: learn: 0.0660939 test: 0.1380696 best: 0.1380653 (4415) 4431: learn: 0.0660838 test: 0.1380686 best: 0.1380653 (4415) 4432: learn: 0.0660718 test: 0.1380635 best: 0.1380635 (4432) 4433: learn: 0.0660588 test: 0.1380638 best: 0.1380635 (4432) 4434: learn: 0.0660496 test: 0.1380623 best: 0.1380623 (4434) 4435: learn: 0.0660406 test: 0.1380620 best: 0.1380620 (4435) 4436: learn: 0.0660305 test: 0.1380583 best: 0.1380583 (4436) 4437: learn: 0.0660202 test: 0.1380553 best: 0.1380553 (4437) 4438: learn: 0.0660138 test: 0.1380529 best: 0.1380529 (4438) 4439: learn: 0.0660003 test: 0.1380539 best: 0.1380529 (4438) 4440: learn: 0.0659901 test: 0.1380535 best: 0.1380529 (4438) 4441: learn: 0.0659768 test: 0.1380508 best: 0.1380508 (4441) 4442: learn: 0.0659690 test: 0.1380477 best: 0.1380477 (4442) 4443: learn: 0.0659608 test: 0.1380462 best: 0.1380462 (4443) 4444: learn: 0.0659536 test: 0.1380448 best: 0.1380448 (4444) 4445: learn: 0.0659421 test: 0.1380422 best: 0.1380422 (4445) 4446: learn: 0.0659322 test: 0.1380420 best: 0.1380420 (4446) 4447: learn: 0.0659269 test: 0.1380415 best: 0.1380415 (4447) 4448: learn: 0.0659163 test: 0.1380411 best: 0.1380411 (4448) 4449: learn: 0.0659080 test: 0.1380417 best: 0.1380411 (4448) 4450: learn: 0.0658991 test: 0.1380394 best: 0.1380394 (4450) 4451: learn: 0.0658895 test: 0.1380368 best: 0.1380368 (4451) 4452: learn: 0.0658843 test: 0.1380370 best: 0.1380368 (4451) 4453: learn: 0.0658762 test: 0.1380367 best: 0.1380367 (4453) 4454: learn: 0.0658666 test: 0.1380360 best: 0.1380360 (4454) 4455: learn: 0.0658577 test: 0.1380368 best: 0.1380360 (4454) 4456: learn: 0.0658483 test: 0.1380388 best: 0.1380360 (4454) 4457: learn: 0.0658381 test: 0.1380361 best: 0.1380360 (4454) 4458: learn: 0.0658276 test: 0.1380333 best: 0.1380333 (4458) 4459: learn: 0.0658156 test: 0.1380318 best: 0.1380318 (4459) 4460: learn: 0.0658050 test: 0.1380304 best: 0.1380304 (4460) 4461: learn: 0.0657965 test: 0.1380296 best: 0.1380296 (4461) 4462: learn: 0.0657867 test: 0.1380290 best: 0.1380290 (4462) 4463: learn: 0.0657781 test: 0.1380288 best: 0.1380288 (4463) 4464: learn: 0.0657735 test: 0.1380272 best: 0.1380272 (4464) 4465: learn: 0.0657676 test: 0.1380254 best: 0.1380254 (4465) 4466: learn: 0.0657592 test: 0.1380238 best: 0.1380238 (4466) 4467: learn: 0.0657490 test: 0.1380217 best: 0.1380217 (4467) 4468: learn: 0.0657404 test: 0.1380206 best: 0.1380206 (4468) 4469: learn: 0.0657301 test: 0.1380230 best: 0.1380206 (4468) 4470: learn: 0.0657200 test: 0.1380223 best: 0.1380206 (4468) 4471: learn: 0.0657125 test: 0.1380201 best: 0.1380201 (4471) 4472: learn: 0.0657031 test: 0.1380151 best: 0.1380151 (4472) 4473: learn: 0.0656942 test: 0.1380120 best: 0.1380120 (4473) 4474: learn: 0.0656835 test: 0.1380059 best: 0.1380059 (4474) 4475: learn: 0.0656722 test: 0.1380038 best: 0.1380038 (4475) 4476: learn: 0.0656610 test: 0.1380010 best: 0.1380010 (4476) 4477: learn: 0.0656546 test: 0.1379985 best: 0.1379985 (4477) 4478: learn: 0.0656420 test: 0.1379977 best: 0.1379977 (4478) 4479: learn: 0.0656327 test: 0.1379975 best: 0.1379975 (4479) 4480: learn: 0.0656220 test: 0.1379939 best: 0.1379939 (4480) 4481: learn: 0.0656135 test: 0.1379932 best: 0.1379932 (4481) 4482: learn: 0.0656022 test: 0.1379925 best: 0.1379925 (4482) 4483: learn: 0.0655916 test: 0.1379932 best: 0.1379925 (4482) 4484: learn: 0.0655815 test: 0.1379943 best: 0.1379925 (4482) 4485: learn: 0.0655750 test: 0.1379934 best: 0.1379925 (4482) 4486: learn: 0.0655677 test: 0.1379903 best: 0.1379903 (4486) 4487: learn: 0.0655598 test: 0.1379886 best: 0.1379886 (4487) 4488: learn: 0.0655514 test: 0.1379889 best: 0.1379886 (4487) 4489: learn: 0.0655419 test: 0.1379857 best: 0.1379857 (4489) 4490: learn: 0.0655357 test: 0.1379841 best: 0.1379841 (4490) 4491: learn: 0.0655265 test: 0.1379822 best: 0.1379822 (4491) 4492: learn: 0.0655151 test: 0.1379822 best: 0.1379822 (4492) 4493: learn: 0.0655087 test: 0.1379784 best: 0.1379784 (4493) 4494: learn: 0.0655023 test: 0.1379782 best: 0.1379782 (4494) 4495: learn: 0.0654932 test: 0.1379750 best: 0.1379750 (4495) 4496: learn: 0.0654828 test: 0.1379745 best: 0.1379745 (4496) 4497: learn: 0.0654735 test: 0.1379707 best: 0.1379707 (4497) 4498: learn: 0.0654638 test: 0.1379715 best: 0.1379707 (4497) 4499: learn: 0.0654542 test: 0.1379654 best: 0.1379654 (4499) 4500: learn: 0.0654417 test: 0.1379654 best: 0.1379654 (4500) 4501: learn: 0.0654321 test: 0.1379654 best: 0.1379654 (4500) 4502: learn: 0.0654229 test: 0.1379662 best: 0.1379654 (4500) 4503: learn: 0.0654107 test: 0.1379628 best: 0.1379628 (4503) 4504: learn: 0.0653984 test: 0.1379595 best: 0.1379595 (4504) 4505: learn: 0.0653899 test: 0.1379582 best: 0.1379582 (4505) 4506: learn: 0.0653819 test: 0.1379562 best: 0.1379562 (4506) 4507: learn: 0.0653723 test: 0.1379535 best: 0.1379535 (4507) 4508: learn: 0.0653638 test: 0.1379537 best: 0.1379535 (4507) 4509: learn: 0.0653555 test: 0.1379499 best: 0.1379499 (4509) 4510: learn: 0.0653475 test: 0.1379477 best: 0.1379477 (4510) 4511: learn: 0.0653383 test: 0.1379518 best: 0.1379477 (4510) 4512: learn: 0.0653320 test: 0.1379488 best: 0.1379477 (4510) 4513: learn: 0.0653241 test: 0.1379472 best: 0.1379472 (4513) 4514: learn: 0.0653136 test: 0.1379460 best: 0.1379460 (4514) 4515: learn: 0.0653039 test: 0.1379462 best: 0.1379460 (4514) total: 51.5s remaining: 16.9s 4516: learn: 0.0652937 test: 0.1379472 best: 0.1379460 (4514) 4517: learn: 0.0652819 test: 0.1379502 best: 0.1379460 (4514) 4518: learn: 0.0652724 test: 0.1379485 best: 0.1379460 (4514) 4519: learn: 0.0652634 test: 0.1379497 best: 0.1379460 (4514) 4520: learn: 0.0652529 test: 0.1379501 best: 0.1379460 (4514) 4521: learn: 0.0652422 test: 0.1379509 best: 0.1379460 (4514) 4522: learn: 0.0652342 test: 0.1379521 best: 0.1379460 (4514) 4523: learn: 0.0652244 test: 0.1379484 best: 0.1379460 (4514) 4524: learn: 0.0652162 test: 0.1379444 best: 0.1379444 (4524) 4525: learn: 0.0652079 test: 0.1379451 best: 0.1379444 (4524) 4526: learn: 0.0651975 test: 0.1379463 best: 0.1379444 (4524) 4527: learn: 0.0651884 test: 0.1379428 best: 0.1379428 (4527) 4528: learn: 0.0651822 test: 0.1379409 best: 0.1379409 (4528) 4529: learn: 0.0651733 test: 0.1379396 best: 0.1379396 (4529) 4530: learn: 0.0651649 test: 0.1379401 best: 0.1379396 (4529) 4531: learn: 0.0651579 test: 0.1379413 best: 0.1379396 (4529) 4532: learn: 0.0651492 test: 0.1379401 best: 0.1379396 (4529) 4533: learn: 0.0651394 test: 0.1379392 best: 0.1379392 (4533) 4534: learn: 0.0651303 test: 0.1379400 best: 0.1379392 (4533) 4535: learn: 0.0651197 test: 0.1379370 best: 0.1379370 (4535) 4536: learn: 0.0651129 test: 0.1379307 best: 0.1379307 (4536) 4537: learn: 0.0651077 test: 0.1379329 best: 0.1379307 (4536) 4538: learn: 0.0651003 test: 0.1379340 best: 0.1379307 (4536) 4539: learn: 0.0650902 test: 0.1379333 best: 0.1379307 (4536) 4540: learn: 0.0650818 test: 0.1379354 best: 0.1379307 (4536) 4541: learn: 0.0650755 test: 0.1379365 best: 0.1379307 (4536) 4542: learn: 0.0650691 test: 0.1379370 best: 0.1379307 (4536) 4543: learn: 0.0650597 test: 0.1379387 best: 0.1379307 (4536) 4544: learn: 0.0650504 test: 0.1379424 best: 0.1379307 (4536) 4545: learn: 0.0650441 test: 0.1379421 best: 0.1379307 (4536) 4546: learn: 0.0650335 test: 0.1379410 best: 0.1379307 (4536) 4547: learn: 0.0650246 test: 0.1379410 best: 0.1379307 (4536) 4548: learn: 0.0650147 test: 0.1379389 best: 0.1379307 (4536) 4549: learn: 0.0650036 test: 0.1379381 best: 0.1379307 (4536) 4550: learn: 0.0649950 test: 0.1379384 best: 0.1379307 (4536) 4551: learn: 0.0649862 test: 0.1379406 best: 0.1379307 (4536) 4552: learn: 0.0649775 test: 0.1379399 best: 0.1379307 (4536) 4553: learn: 0.0649688 test: 0.1379371 best: 0.1379307 (4536) 4554: learn: 0.0649584 test: 0.1379374 best: 0.1379307 (4536) 4555: learn: 0.0649479 test: 0.1379403 best: 0.1379307 (4536) 4556: learn: 0.0649398 test: 0.1379411 best: 0.1379307 (4536) 4557: learn: 0.0649313 test: 0.1379427 best: 0.1379307 (4536) 4558: learn: 0.0649220 test: 0.1379431 best: 0.1379307 (4536) 4559: learn: 0.0649107 test: 0.1379414 best: 0.1379307 (4536) 4560: learn: 0.0649004 test: 0.1379378 best: 0.1379307 (4536) 4561: learn: 0.0648893 test: 0.1379405 best: 0.1379307 (4536) 4562: learn: 0.0648829 test: 0.1379400 best: 0.1379307 (4536) 4563: learn: 0.0648752 test: 0.1379393 best: 0.1379307 (4536) 4564: learn: 0.0648694 test: 0.1379403 best: 0.1379307 (4536) 4565: learn: 0.0648620 test: 0.1379394 best: 0.1379307 (4536) 4566: learn: 0.0648508 test: 0.1379393 best: 0.1379307 (4536) 4567: learn: 0.0648421 test: 0.1379430 best: 0.1379307 (4536) 4568: learn: 0.0648337 test: 0.1379413 best: 0.1379307 (4536) 4569: learn: 0.0648246 test: 0.1379399 best: 0.1379307 (4536) 4570: learn: 0.0648145 test: 0.1379378 best: 0.1379307 (4536) 4571: learn: 0.0648031 test: 0.1379349 best: 0.1379307 (4536) 4572: learn: 0.0647947 test: 0.1379392 best: 0.1379307 (4536) 4573: learn: 0.0647867 test: 0.1379378 best: 0.1379307 (4536) 4574: learn: 0.0647729 test: 0.1379399 best: 0.1379307 (4536) 4575: learn: 0.0647621 test: 0.1379400 best: 0.1379307 (4536) 4576: learn: 0.0647554 test: 0.1379384 best: 0.1379307 (4536) 4577: learn: 0.0647462 test: 0.1379349 best: 0.1379307 (4536) 4578: learn: 0.0647382 test: 0.1379322 best: 0.1379307 (4536) 4579: learn: 0.0647315 test: 0.1379317 best: 0.1379307 (4536) 4580: learn: 0.0647241 test: 0.1379278 best: 0.1379278 (4580) 4581: learn: 0.0647152 test: 0.1379250 best: 0.1379250 (4581) 4582: learn: 0.0647081 test: 0.1379255 best: 0.1379250 (4581) 4583: learn: 0.0646992 test: 0.1379267 best: 0.1379250 (4581) 4584: learn: 0.0646949 test: 0.1379277 best: 0.1379250 (4581) 4585: learn: 0.0646849 test: 0.1379261 best: 0.1379250 (4581) 4586: learn: 0.0646758 test: 0.1379263 best: 0.1379250 (4581) 4587: learn: 0.0646667 test: 0.1379258 best: 0.1379250 (4581) 4588: learn: 0.0646560 test: 0.1379234 best: 0.1379234 (4588) 4589: learn: 0.0646450 test: 0.1379238 best: 0.1379234 (4588) 4590: learn: 0.0646337 test: 0.1379243 best: 0.1379234 (4588) 4591: learn: 0.0646268 test: 0.1379262 best: 0.1379234 (4588) 4592: learn: 0.0646182 test: 0.1379267 best: 0.1379234 (4588) 4593: learn: 0.0646088 test: 0.1379219 best: 0.1379219 (4593) 4594: learn: 0.0646011 test: 0.1379215 best: 0.1379215 (4594) 4595: learn: 0.0645912 test: 0.1379223 best: 0.1379215 (4594) 4596: learn: 0.0645820 test: 0.1379208 best: 0.1379208 (4596) 4597: learn: 0.0645703 test: 0.1379222 best: 0.1379208 (4596) 4598: learn: 0.0645623 test: 0.1379224 best: 0.1379208 (4596) 4599: learn: 0.0645564 test: 0.1379233 best: 0.1379208 (4596) 4600: learn: 0.0645482 test: 0.1379222 best: 0.1379208 (4596) 4601: learn: 0.0645412 test: 0.1379225 best: 0.1379208 (4596) 4602: learn: 0.0645333 test: 0.1379180 best: 0.1379180 (4602) 4603: learn: 0.0645265 test: 0.1379172 best: 0.1379172 (4603) 4604: learn: 0.0645156 test: 0.1379162 best: 0.1379162 (4604) 4605: learn: 0.0645071 test: 0.1379187 best: 0.1379162 (4604) 4606: learn: 0.0644961 test: 0.1379198 best: 0.1379162 (4604) 4607: learn: 0.0644875 test: 0.1379210 best: 0.1379162 (4604) 4608: learn: 0.0644783 test: 0.1379199 best: 0.1379162 (4604) 4609: learn: 0.0644714 test: 0.1379227 best: 0.1379162 (4604) 4610: learn: 0.0644615 test: 0.1379195 best: 0.1379162 (4604) 4611: learn: 0.0644492 test: 0.1379165 best: 0.1379162 (4604) 4612: learn: 0.0644388 test: 0.1379176 best: 0.1379162 (4604) 4613: learn: 0.0644319 test: 0.1379173 best: 0.1379162 (4604) 4614: learn: 0.0644225 test: 0.1379176 best: 0.1379162 (4604) 4615: learn: 0.0644157 test: 0.1379169 best: 0.1379162 (4604) 4616: learn: 0.0644083 test: 0.1379133 best: 0.1379133 (4616) 4617: learn: 0.0643993 test: 0.1379120 best: 0.1379120 (4617) 4618: learn: 0.0643947 test: 0.1379109 best: 0.1379109 (4618) 4619: learn: 0.0643826 test: 0.1379123 best: 0.1379109 (4618) 4620: learn: 0.0643709 test: 0.1379150 best: 0.1379109 (4618) 4621: learn: 0.0643642 test: 0.1379136 best: 0.1379109 (4618) 4622: learn: 0.0643582 test: 0.1379128 best: 0.1379109 (4618) 4623: learn: 0.0643501 test: 0.1379114 best: 0.1379109 (4618) 4624: learn: 0.0643398 test: 0.1379095 best: 0.1379095 (4624) 4625: learn: 0.0643349 test: 0.1379081 best: 0.1379081 (4625) 4626: learn: 0.0643254 test: 0.1379089 best: 0.1379081 (4625) 4627: learn: 0.0643139 test: 0.1379122 best: 0.1379081 (4625) 4628: learn: 0.0643097 test: 0.1379109 best: 0.1379081 (4625) 4629: learn: 0.0642998 test: 0.1379079 best: 0.1379079 (4629) 4630: learn: 0.0642935 test: 0.1379080 best: 0.1379079 (4629) 4631: learn: 0.0642851 test: 0.1379085 best: 0.1379079 (4629) 4632: learn: 0.0642747 test: 0.1379102 best: 0.1379079 (4629) 4633: learn: 0.0642659 test: 0.1379106 best: 0.1379079 (4629) 4634: learn: 0.0642551 test: 0.1379127 best: 0.1379079 (4629) 4635: learn: 0.0642471 test: 0.1379127 best: 0.1379079 (4629) 4636: learn: 0.0642373 test: 0.1379148 best: 0.1379079 (4629) 4637: learn: 0.0642277 test: 0.1379155 best: 0.1379079 (4629) 4638: learn: 0.0642185 test: 0.1379163 best: 0.1379079 (4629) 4639: learn: 0.0642092 test: 0.1379152 best: 0.1379079 (4629) 4640: learn: 0.0642012 test: 0.1379134 best: 0.1379079 (4629) 4641: learn: 0.0641928 test: 0.1379130 best: 0.1379079 (4629) 4642: learn: 0.0641840 test: 0.1379138 best: 0.1379079 (4629) 4643: learn: 0.0641749 test: 0.1379131 best: 0.1379079 (4629) 4644: learn: 0.0641660 test: 0.1379168 best: 0.1379079 (4629) 4645: learn: 0.0641595 test: 0.1379135 best: 0.1379079 (4629) 4646: learn: 0.0641491 test: 0.1379143 best: 0.1379079 (4629) 4647: learn: 0.0641416 test: 0.1379087 best: 0.1379079 (4629) 4648: learn: 0.0641346 test: 0.1379079 best: 0.1379079 (4648) 4649: learn: 0.0641265 test: 0.1379060 best: 0.1379060 (4649) 4650: learn: 0.0641216 test: 0.1379037 best: 0.1379037 (4650) 4651: learn: 0.0641127 test: 0.1379030 best: 0.1379030 (4651) 4652: learn: 0.0641058 test: 0.1379012 best: 0.1379012 (4652) 4653: learn: 0.0640958 test: 0.1378975 best: 0.1378975 (4653) 4654: learn: 0.0640887 test: 0.1378975 best: 0.1378975 (4653) 4655: learn: 0.0640808 test: 0.1379001 best: 0.1378975 (4653) 4656: learn: 0.0640717 test: 0.1379007 best: 0.1378975 (4653) 4657: learn: 0.0640630 test: 0.1379025 best: 0.1378975 (4653) 4658: learn: 0.0640553 test: 0.1379026 best: 0.1378975 (4653) 4659: learn: 0.0640474 test: 0.1378996 best: 0.1378975 (4653) 4660: learn: 0.0640366 test: 0.1379017 best: 0.1378975 (4653) 4661: learn: 0.0640275 test: 0.1378996 best: 0.1378975 (4653) 4662: learn: 0.0640190 test: 0.1378993 best: 0.1378975 (4653) total: 53.2s remaining: 15.2s 4663: learn: 0.0640109 test: 0.1379010 best: 0.1378975 (4653) 4664: learn: 0.0640036 test: 0.1379006 best: 0.1378975 (4653) 4665: learn: 0.0639971 test: 0.1379005 best: 0.1378975 (4653) 4666: learn: 0.0639919 test: 0.1378988 best: 0.1378975 (4653) 4667: learn: 0.0639856 test: 0.1378937 best: 0.1378937 (4667) 4668: learn: 0.0639753 test: 0.1378913 best: 0.1378913 (4668) 4669: learn: 0.0639664 test: 0.1378907 best: 0.1378907 (4669) 4670: learn: 0.0639571 test: 0.1378891 best: 0.1378891 (4670) 4671: learn: 0.0639467 test: 0.1378891 best: 0.1378891 (4671) 4672: learn: 0.0639410 test: 0.1378896 best: 0.1378891 (4671) 4673: learn: 0.0639326 test: 0.1378900 best: 0.1378891 (4671) 4674: learn: 0.0639245 test: 0.1378915 best: 0.1378891 (4671) 4675: learn: 0.0639153 test: 0.1378922 best: 0.1378891 (4671) 4676: learn: 0.0639067 test: 0.1378914 best: 0.1378891 (4671) 4677: learn: 0.0638981 test: 0.1378916 best: 0.1378891 (4671) 4678: learn: 0.0638905 test: 0.1378893 best: 0.1378891 (4671) 4679: learn: 0.0638816 test: 0.1378883 best: 0.1378883 (4679) 4680: learn: 0.0638754 test: 0.1378876 best: 0.1378876 (4680) 4681: learn: 0.0638648 test: 0.1378846 best: 0.1378846 (4681) 4682: learn: 0.0638560 test: 0.1378852 best: 0.1378846 (4681) 4683: learn: 0.0638487 test: 0.1378834 best: 0.1378834 (4683) 4684: learn: 0.0638418 test: 0.1378828 best: 0.1378828 (4684) 4685: learn: 0.0638322 test: 0.1378825 best: 0.1378825 (4685) 4686: learn: 0.0638257 test: 0.1378828 best: 0.1378825 (4685) 4687: learn: 0.0638191 test: 0.1378853 best: 0.1378825 (4685) 4688: learn: 0.0638073 test: 0.1378795 best: 0.1378795 (4688) 4689: learn: 0.0637984 test: 0.1378784 best: 0.1378784 (4689) 4690: learn: 0.0637891 test: 0.1378797 best: 0.1378784 (4689) 4691: learn: 0.0637830 test: 0.1378802 best: 0.1378784 (4689) 4692: learn: 0.0637765 test: 0.1378777 best: 0.1378777 (4692) 4693: learn: 0.0637723 test: 0.1378742 best: 0.1378742 (4693) 4694: learn: 0.0637652 test: 0.1378748 best: 0.1378742 (4693) 4695: learn: 0.0637563 test: 0.1378734 best: 0.1378734 (4695) 4696: learn: 0.0637479 test: 0.1378750 best: 0.1378734 (4695) 4697: learn: 0.0637413 test: 0.1378739 best: 0.1378734 (4695) 4698: learn: 0.0637313 test: 0.1378724 best: 0.1378724 (4698) 4699: learn: 0.0637191 test: 0.1378718 best: 0.1378718 (4699) 4700: learn: 0.0637100 test: 0.1378700 best: 0.1378700 (4700) 4701: learn: 0.0637010 test: 0.1378679 best: 0.1378679 (4701) 4702: learn: 0.0636949 test: 0.1378671 best: 0.1378671 (4702) 4703: learn: 0.0636837 test: 0.1378662 best: 0.1378662 (4703) 4704: learn: 0.0636747 test: 0.1378607 best: 0.1378607 (4704) 4705: learn: 0.0636663 test: 0.1378583 best: 0.1378583 (4705) 4706: learn: 0.0636553 test: 0.1378545 best: 0.1378545 (4706) 4707: learn: 0.0636520 test: 0.1378536 best: 0.1378536 (4707) 4708: learn: 0.0636418 test: 0.1378537 best: 0.1378536 (4707) 4709: learn: 0.0636323 test: 0.1378519 best: 0.1378519 (4709) 4710: learn: 0.0636252 test: 0.1378499 best: 0.1378499 (4710) 4711: learn: 0.0636191 test: 0.1378519 best: 0.1378499 (4710) 4712: learn: 0.0636110 test: 0.1378525 best: 0.1378499 (4710) 4713: learn: 0.0636050 test: 0.1378508 best: 0.1378499 (4710) 4714: learn: 0.0635958 test: 0.1378518 best: 0.1378499 (4710) 4715: learn: 0.0635892 test: 0.1378501 best: 0.1378499 (4710) 4716: learn: 0.0635830 test: 0.1378489 best: 0.1378489 (4716) 4717: learn: 0.0635787 test: 0.1378486 best: 0.1378486 (4717) 4718: learn: 0.0635713 test: 0.1378476 best: 0.1378476 (4718) 4719: learn: 0.0635609 test: 0.1378483 best: 0.1378476 (4718) 4720: learn: 0.0635524 test: 0.1378499 best: 0.1378476 (4718) 4721: learn: 0.0635459 test: 0.1378497 best: 0.1378476 (4718) 4722: learn: 0.0635392 test: 0.1378472 best: 0.1378472 (4722) 4723: learn: 0.0635304 test: 0.1378472 best: 0.1378472 (4722) 4724: learn: 0.0635216 test: 0.1378455 best: 0.1378455 (4724) 4725: learn: 0.0635149 test: 0.1378461 best: 0.1378455 (4724) 4726: learn: 0.0635052 test: 0.1378456 best: 0.1378455 (4724) 4727: learn: 0.0634947 test: 0.1378436 best: 0.1378436 (4727) 4728: learn: 0.0634872 test: 0.1378442 best: 0.1378436 (4727) 4729: learn: 0.0634800 test: 0.1378429 best: 0.1378429 (4729) 4730: learn: 0.0634733 test: 0.1378411 best: 0.1378411 (4730) 4731: learn: 0.0634643 test: 0.1378416 best: 0.1378411 (4730) 4732: learn: 0.0634533 test: 0.1378410 best: 0.1378410 (4732) 4733: learn: 0.0634465 test: 0.1378396 best: 0.1378396 (4733) 4734: learn: 0.0634398 test: 0.1378370 best: 0.1378370 (4734) 4735: learn: 0.0634315 test: 0.1378370 best: 0.1378370 (4735) 4736: learn: 0.0634225 test: 0.1378372 best: 0.1378370 (4735) 4737: learn: 0.0634115 test: 0.1378373 best: 0.1378370 (4735) 4738: learn: 0.0634046 test: 0.1378342 best: 0.1378342 (4738) 4739: learn: 0.0633940 test: 0.1378341 best: 0.1378341 (4739) 4740: learn: 0.0633869 test: 0.1378344 best: 0.1378341 (4739) 4741: learn: 0.0633796 test: 0.1378332 best: 0.1378332 (4741) 4742: learn: 0.0633729 test: 0.1378331 best: 0.1378331 (4742) 4743: learn: 0.0633628 test: 0.1378311 best: 0.1378311 (4743) 4744: learn: 0.0633552 test: 0.1378270 best: 0.1378270 (4744) 4745: learn: 0.0633505 test: 0.1378286 best: 0.1378270 (4744) 4746: learn: 0.0633411 test: 0.1378228 best: 0.1378228 (4746) 4747: learn: 0.0633337 test: 0.1378218 best: 0.1378218 (4747) 4748: learn: 0.0633242 test: 0.1378219 best: 0.1378218 (4747) 4749: learn: 0.0633154 test: 0.1378204 best: 0.1378204 (4749) 4750: learn: 0.0633088 test: 0.1378201 best: 0.1378201 (4750) 4751: learn: 0.0633017 test: 0.1378184 best: 0.1378184 (4751) 4752: learn: 0.0632935 test: 0.1378153 best: 0.1378153 (4752) 4753: learn: 0.0632865 test: 0.1378159 best: 0.1378153 (4752) 4754: learn: 0.0632807 test: 0.1378129 best: 0.1378129 (4754) 4755: learn: 0.0632753 test: 0.1378100 best: 0.1378100 (4755) 4756: learn: 0.0632681 test: 0.1378078 best: 0.1378078 (4756) 4757: learn: 0.0632614 test: 0.1378038 best: 0.1378038 (4757) 4758: learn: 0.0632543 test: 0.1378019 best: 0.1378019 (4758) 4759: learn: 0.0632458 test: 0.1378015 best: 0.1378015 (4759) 4760: learn: 0.0632401 test: 0.1378002 best: 0.1378002 (4760) 4761: learn: 0.0632315 test: 0.1377975 best: 0.1377975 (4761) 4762: learn: 0.0632251 test: 0.1377975 best: 0.1377975 (4761) 4763: learn: 0.0632185 test: 0.1377964 best: 0.1377964 (4763) 4764: learn: 0.0632083 test: 0.1377972 best: 0.1377964 (4763) 4765: learn: 0.0631965 test: 0.1377958 best: 0.1377958 (4765) 4766: learn: 0.0631865 test: 0.1377990 best: 0.1377958 (4765) 4767: learn: 0.0631756 test: 0.1378006 best: 0.1377958 (4765) 4768: learn: 0.0631682 test: 0.1377982 best: 0.1377958 (4765) 4769: learn: 0.0631607 test: 0.1377951 best: 0.1377951 (4769) 4770: learn: 0.0631523 test: 0.1377969 best: 0.1377951 (4769) 4771: learn: 0.0631459 test: 0.1377966 best: 0.1377951 (4769) 4772: learn: 0.0631340 test: 0.1377998 best: 0.1377951 (4769) 4773: learn: 0.0631315 test: 0.1377974 best: 0.1377951 (4769) 4774: learn: 0.0631236 test: 0.1377954 best: 0.1377951 (4769) 4775: learn: 0.0631137 test: 0.1377957 best: 0.1377951 (4769) 4776: learn: 0.0631039 test: 0.1377948 best: 0.1377948 (4776) 4777: learn: 0.0630979 test: 0.1377914 best: 0.1377914 (4777) 4778: learn: 0.0630903 test: 0.1377919 best: 0.1377914 (4777) 4779: learn: 0.0630828 test: 0.1377934 best: 0.1377914 (4777) 4780: learn: 0.0630728 test: 0.1377920 best: 0.1377914 (4777) 4781: learn: 0.0630641 test: 0.1377902 best: 0.1377902 (4781) 4782: learn: 0.0630575 test: 0.1377890 best: 0.1377890 (4782) 4783: learn: 0.0630477 test: 0.1377912 best: 0.1377890 (4782) 4784: learn: 0.0630330 test: 0.1377892 best: 0.1377890 (4782) 4785: learn: 0.0630271 test: 0.1377859 best: 0.1377859 (4785) 4786: learn: 0.0630187 test: 0.1377841 best: 0.1377841 (4786) 4787: learn: 0.0630106 test: 0.1377856 best: 0.1377841 (4786) 4788: learn: 0.0630016 test: 0.1377878 best: 0.1377841 (4786) 4789: learn: 0.0629940 test: 0.1377873 best: 0.1377841 (4786) 4790: learn: 0.0629852 test: 0.1377879 best: 0.1377841 (4786) 4791: learn: 0.0629788 test: 0.1377893 best: 0.1377841 (4786) 4792: learn: 0.0629733 test: 0.1377858 best: 0.1377841 (4786) 4793: learn: 0.0629663 test: 0.1377833 best: 0.1377833 (4793) 4794: learn: 0.0629580 test: 0.1377839 best: 0.1377833 (4793) 4795: learn: 0.0629479 test: 0.1377833 best: 0.1377833 (4793) 4796: learn: 0.0629356 test: 0.1377873 best: 0.1377833 (4793) 4797: learn: 0.0629293 test: 0.1377846 best: 0.1377833 (4793) 4798: learn: 0.0629232 test: 0.1377819 best: 0.1377819 (4798) 4799: learn: 0.0629138 test: 0.1377781 best: 0.1377781 (4799) 4800: learn: 0.0629052 test: 0.1377818 best: 0.1377781 (4799) 4801: learn: 0.0628967 test: 0.1377822 best: 0.1377781 (4799) 4802: learn: 0.0628882 test: 0.1377850 best: 0.1377781 (4799) 4803: learn: 0.0628791 test: 0.1377840 best: 0.1377781 (4799) 4804: learn: 0.0628717 test: 0.1377837 best: 0.1377781 (4799) 4805: learn: 0.0628627 test: 0.1377816 best: 0.1377781 (4799) 4806: learn: 0.0628520 test: 0.1377846 best: 0.1377781 (4799) 4807: learn: 0.0628448 test: 0.1377847 best: 0.1377781 (4799) 4808: learn: 0.0628400 test: 0.1377825 best: 0.1377781 (4799) 4809: learn: 0.0628320 test: 0.1377817 best: 0.1377781 (4799) 4810: learn: 0.0628280 test: 0.1377785 best: 0.1377781 (4799) 4811: learn: 0.0628180 test: 0.1377788 best: 0.1377781 (4799) 4812: learn: 0.0628097 test: 0.1377771 best: 0.1377771 (4812) total: 54.8s remaining: 13.5s 4813: learn: 0.0628000 test: 0.1377783 best: 0.1377771 (4812) 4814: learn: 0.0627909 test: 0.1377757 best: 0.1377757 (4814) 4815: learn: 0.0627839 test: 0.1377753 best: 0.1377753 (4815) 4816: learn: 0.0627732 test: 0.1377735 best: 0.1377735 (4816) 4817: learn: 0.0627664 test: 0.1377716 best: 0.1377716 (4817) 4818: learn: 0.0627560 test: 0.1377710 best: 0.1377710 (4818) 4819: learn: 0.0627477 test: 0.1377684 best: 0.1377684 (4819) 4820: learn: 0.0627373 test: 0.1377667 best: 0.1377667 (4820) 4821: learn: 0.0627295 test: 0.1377633 best: 0.1377633 (4821) 4822: learn: 0.0627226 test: 0.1377609 best: 0.1377609 (4822) 4823: learn: 0.0627120 test: 0.1377649 best: 0.1377609 (4822) 4824: learn: 0.0627039 test: 0.1377666 best: 0.1377609 (4822) 4825: learn: 0.0626992 test: 0.1377657 best: 0.1377609 (4822) 4826: learn: 0.0626912 test: 0.1377644 best: 0.1377609 (4822) 4827: learn: 0.0626856 test: 0.1377629 best: 0.1377609 (4822) 4828: learn: 0.0626802 test: 0.1377632 best: 0.1377609 (4822) 4829: learn: 0.0626726 test: 0.1377614 best: 0.1377609 (4822) 4830: learn: 0.0626654 test: 0.1377604 best: 0.1377604 (4830) 4831: learn: 0.0626590 test: 0.1377570 best: 0.1377570 (4831) 4832: learn: 0.0626533 test: 0.1377554 best: 0.1377554 (4832) 4833: learn: 0.0626445 test: 0.1377526 best: 0.1377526 (4833) 4834: learn: 0.0626360 test: 0.1377567 best: 0.1377526 (4833) 4835: learn: 0.0626292 test: 0.1377557 best: 0.1377526 (4833) 4836: learn: 0.0626245 test: 0.1377537 best: 0.1377526 (4833) 4837: learn: 0.0626177 test: 0.1377541 best: 0.1377526 (4833) 4838: learn: 0.0626080 test: 0.1377538 best: 0.1377526 (4833) 4839: learn: 0.0625998 test: 0.1377550 best: 0.1377526 (4833) 4840: learn: 0.0625967 test: 0.1377545 best: 0.1377526 (4833) 4841: learn: 0.0625902 test: 0.1377564 best: 0.1377526 (4833) 4842: learn: 0.0625828 test: 0.1377535 best: 0.1377526 (4833) 4843: learn: 0.0625722 test: 0.1377525 best: 0.1377525 (4843) 4844: learn: 0.0625641 test: 0.1377526 best: 0.1377525 (4843) 4845: learn: 0.0625576 test: 0.1377526 best: 0.1377525 (4843) 4846: learn: 0.0625478 test: 0.1377550 best: 0.1377525 (4843) 4847: learn: 0.0625415 test: 0.1377528 best: 0.1377525 (4843) 4848: learn: 0.0625353 test: 0.1377518 best: 0.1377518 (4848) 4849: learn: 0.0625270 test: 0.1377520 best: 0.1377518 (4848) 4850: learn: 0.0625194 test: 0.1377530 best: 0.1377518 (4848) 4851: learn: 0.0625131 test: 0.1377518 best: 0.1377518 (4848) 4852: learn: 0.0625066 test: 0.1377531 best: 0.1377518 (4848) 4853: learn: 0.0625001 test: 0.1377522 best: 0.1377518 (4848) 4854: learn: 0.0624909 test: 0.1377546 best: 0.1377518 (4848) 4855: learn: 0.0624825 test: 0.1377528 best: 0.1377518 (4848) 4856: learn: 0.0624765 test: 0.1377519 best: 0.1377518 (4848) 4857: learn: 0.0624703 test: 0.1377513 best: 0.1377513 (4857) 4858: learn: 0.0624650 test: 0.1377504 best: 0.1377504 (4858) 4859: learn: 0.0624575 test: 0.1377473 best: 0.1377473 (4859) 4860: learn: 0.0624507 test: 0.1377453 best: 0.1377453 (4860) 4861: learn: 0.0624427 test: 0.1377476 best: 0.1377453 (4860) 4862: learn: 0.0624352 test: 0.1377494 best: 0.1377453 (4860) 4863: learn: 0.0624289 test: 0.1377510 best: 0.1377453 (4860) 4864: learn: 0.0624196 test: 0.1377507 best: 0.1377453 (4860) 4865: learn: 0.0624093 test: 0.1377548 best: 0.1377453 (4860) 4866: learn: 0.0624005 test: 0.1377541 best: 0.1377453 (4860) 4867: learn: 0.0623929 test: 0.1377526 best: 0.1377453 (4860) 4868: learn: 0.0623823 test: 0.1377531 best: 0.1377453 (4860) 4869: learn: 0.0623748 test: 0.1377520 best: 0.1377453 (4860) 4870: learn: 0.0623697 test: 0.1377505 best: 0.1377453 (4860) 4871: learn: 0.0623574 test: 0.1377495 best: 0.1377453 (4860) 4872: learn: 0.0623465 test: 0.1377487 best: 0.1377453 (4860) 4873: learn: 0.0623399 test: 0.1377498 best: 0.1377453 (4860) 4874: learn: 0.0623302 test: 0.1377524 best: 0.1377453 (4860) 4875: learn: 0.0623212 test: 0.1377499 best: 0.1377453 (4860) 4876: learn: 0.0623111 test: 0.1377457 best: 0.1377453 (4860) 4877: learn: 0.0623033 test: 0.1377443 best: 0.1377443 (4877) 4878: learn: 0.0622966 test: 0.1377432 best: 0.1377432 (4878) 4879: learn: 0.0622901 test: 0.1377439 best: 0.1377432 (4878) 4880: learn: 0.0622825 test: 0.1377404 best: 0.1377404 (4880) 4881: learn: 0.0622739 test: 0.1377429 best: 0.1377404 (4880) 4882: learn: 0.0622632 test: 0.1377451 best: 0.1377404 (4880) 4883: learn: 0.0622564 test: 0.1377445 best: 0.1377404 (4880) 4884: learn: 0.0622505 test: 0.1377431 best: 0.1377404 (4880) 4885: learn: 0.0622455 test: 0.1377412 best: 0.1377404 (4880) 4886: learn: 0.0622381 test: 0.1377385 best: 0.1377385 (4886) 4887: learn: 0.0622277 test: 0.1377405 best: 0.1377385 (4886) 4888: learn: 0.0622173 test: 0.1377419 best: 0.1377385 (4886) 4889: learn: 0.0622097 test: 0.1377431 best: 0.1377385 (4886) 4890: learn: 0.0622038 test: 0.1377437 best: 0.1377385 (4886) 4891: learn: 0.0621969 test: 0.1377429 best: 0.1377385 (4886) 4892: learn: 0.0621924 test: 0.1377412 best: 0.1377385 (4886) 4893: learn: 0.0621849 test: 0.1377420 best: 0.1377385 (4886) 4894: learn: 0.0621758 test: 0.1377384 best: 0.1377384 (4894) 4895: learn: 0.0621675 test: 0.1377371 best: 0.1377371 (4895) 4896: learn: 0.0621627 test: 0.1377359 best: 0.1377359 (4896) 4897: learn: 0.0621528 test: 0.1377378 best: 0.1377359 (4896) 4898: learn: 0.0621464 test: 0.1377371 best: 0.1377359 (4896) 4899: learn: 0.0621407 test: 0.1377368 best: 0.1377359 (4896) 4900: learn: 0.0621355 test: 0.1377369 best: 0.1377359 (4896) 4901: learn: 0.0621274 test: 0.1377380 best: 0.1377359 (4896) 4902: learn: 0.0621203 test: 0.1377385 best: 0.1377359 (4896) 4903: learn: 0.0621110 test: 0.1377358 best: 0.1377358 (4903) 4904: learn: 0.0621037 test: 0.1377343 best: 0.1377343 (4904) 4905: learn: 0.0620967 test: 0.1377335 best: 0.1377335 (4905) 4906: learn: 0.0620866 test: 0.1377313 best: 0.1377313 (4906) 4907: learn: 0.0620768 test: 0.1377299 best: 0.1377299 (4907) 4908: learn: 0.0620671 test: 0.1377307 best: 0.1377299 (4907) 4909: learn: 0.0620570 test: 0.1377324 best: 0.1377299 (4907) 4910: learn: 0.0620514 test: 0.1377310 best: 0.1377299 (4907) 4911: learn: 0.0620437 test: 0.1377297 best: 0.1377297 (4911) 4912: learn: 0.0620356 test: 0.1377293 best: 0.1377293 (4912) 4913: learn: 0.0620269 test: 0.1377304 best: 0.1377293 (4912) 4914: learn: 0.0620183 test: 0.1377297 best: 0.1377293 (4912) 4915: learn: 0.0620116 test: 0.1377285 best: 0.1377285 (4915) 4916: learn: 0.0620035 test: 0.1377241 best: 0.1377241 (4916) 4917: learn: 0.0619955 test: 0.1377227 best: 0.1377227 (4917) 4918: learn: 0.0619892 test: 0.1377246 best: 0.1377227 (4917) 4919: learn: 0.0619805 test: 0.1377245 best: 0.1377227 (4917) 4920: learn: 0.0619706 test: 0.1377235 best: 0.1377227 (4917) 4921: learn: 0.0619645 test: 0.1377212 best: 0.1377212 (4921) 4922: learn: 0.0619571 test: 0.1377211 best: 0.1377211 (4922) 4923: learn: 0.0619495 test: 0.1377178 best: 0.1377178 (4923) 4924: learn: 0.0619428 test: 0.1377145 best: 0.1377145 (4924) 4925: learn: 0.0619353 test: 0.1377149 best: 0.1377145 (4924) 4926: learn: 0.0619258 test: 0.1377138 best: 0.1377138 (4926) 4927: learn: 0.0619171 test: 0.1377145 best: 0.1377138 (4926) 4928: learn: 0.0619115 test: 0.1377116 best: 0.1377116 (4928) 4929: learn: 0.0619039 test: 0.1377112 best: 0.1377112 (4929) 4930: learn: 0.0618973 test: 0.1377118 best: 0.1377112 (4929) 4931: learn: 0.0618889 test: 0.1377092 best: 0.1377092 (4931) 4932: learn: 0.0618859 test: 0.1377086 best: 0.1377086 (4932) 4933: learn: 0.0618758 test: 0.1377045 best: 0.1377045 (4933) 4934: learn: 0.0618651 test: 0.1377061 best: 0.1377045 (4933) 4935: learn: 0.0618573 test: 0.1377091 best: 0.1377045 (4933) 4936: learn: 0.0618492 test: 0.1377061 best: 0.1377045 (4933) 4937: learn: 0.0618431 test: 0.1377057 best: 0.1377045 (4933) 4938: learn: 0.0618318 test: 0.1377095 best: 0.1377045 (4933) 4939: learn: 0.0618278 test: 0.1377062 best: 0.1377045 (4933) 4940: learn: 0.0618146 test: 0.1377022 best: 0.1377022 (4940) 4941: learn: 0.0618098 test: 0.1376986 best: 0.1376986 (4941) 4942: learn: 0.0617998 test: 0.1376974 best: 0.1376974 (4942) 4943: learn: 0.0617941 test: 0.1376961 best: 0.1376961 (4943) 4944: learn: 0.0617914 test: 0.1376948 best: 0.1376948 (4944) 4945: learn: 0.0617856 test: 0.1376961 best: 0.1376948 (4944) 4946: learn: 0.0617788 test: 0.1376965 best: 0.1376948 (4944) 4947: learn: 0.0617705 test: 0.1376985 best: 0.1376948 (4944) 4948: learn: 0.0617642 test: 0.1376967 best: 0.1376948 (4944) total: 56.4s remaining: 12s 4949: learn: 0.0617562 test: 0.1376976 best: 0.1376948 (4944) 4950: learn: 0.0617491 test: 0.1376954 best: 0.1376948 (4944) 4951: learn: 0.0617429 test: 0.1376938 best: 0.1376938 (4951) 4952: learn: 0.0617349 test: 0.1376906 best: 0.1376906 (4952) 4953: learn: 0.0617287 test: 0.1376880 best: 0.1376880 (4953) 4954: learn: 0.0617203 test: 0.1376876 best: 0.1376876 (4954) 4955: learn: 0.0617067 test: 0.1376899 best: 0.1376876 (4954) 4956: learn: 0.0616985 test: 0.1376898 best: 0.1376876 (4954) 4957: learn: 0.0616896 test: 0.1376886 best: 0.1376876 (4954) 4958: learn: 0.0616847 test: 0.1376873 best: 0.1376873 (4958) 4959: learn: 0.0616807 test: 0.1376865 best: 0.1376865 (4959) 4960: learn: 0.0616739 test: 0.1376861 best: 0.1376861 (4960) 4961: learn: 0.0616660 test: 0.1376833 best: 0.1376833 (4961) 4962: learn: 0.0616577 test: 0.1376883 best: 0.1376833 (4961) 4963: learn: 0.0616531 test: 0.1376855 best: 0.1376833 (4961) 4964: learn: 0.0616464 test: 0.1376851 best: 0.1376833 (4961) 4965: learn: 0.0616378 test: 0.1376831 best: 0.1376831 (4965) 4966: learn: 0.0616306 test: 0.1376831 best: 0.1376831 (4965) 4967: learn: 0.0616272 test: 0.1376840 best: 0.1376831 (4965) 4968: learn: 0.0616189 test: 0.1376845 best: 0.1376831 (4965) 4969: learn: 0.0616112 test: 0.1376834 best: 0.1376831 (4965) 4970: learn: 0.0616034 test: 0.1376830 best: 0.1376830 (4970) 4971: learn: 0.0615966 test: 0.1376811 best: 0.1376811 (4971) 4972: learn: 0.0615865 test: 0.1376792 best: 0.1376792 (4972) 4973: learn: 0.0615761 test: 0.1376771 best: 0.1376771 (4973) 4974: learn: 0.0615646 test: 0.1376799 best: 0.1376771 (4973) 4975: learn: 0.0615563 test: 0.1376773 best: 0.1376771 (4973) 4976: learn: 0.0615491 test: 0.1376809 best: 0.1376771 (4973) 4977: learn: 0.0615404 test: 0.1376799 best: 0.1376771 (4973) 4978: learn: 0.0615348 test: 0.1376807 best: 0.1376771 (4973) 4979: learn: 0.0615302 test: 0.1376825 best: 0.1376771 (4973) 4980: learn: 0.0615245 test: 0.1376818 best: 0.1376771 (4973) 4981: learn: 0.0615172 test: 0.1376790 best: 0.1376771 (4973) 4982: learn: 0.0615094 test: 0.1376808 best: 0.1376771 (4973) 4983: learn: 0.0615030 test: 0.1376805 best: 0.1376771 (4973) 4984: learn: 0.0614932 test: 0.1376793 best: 0.1376771 (4973) 4985: learn: 0.0614880 test: 0.1376812 best: 0.1376771 (4973) 4986: learn: 0.0614803 test: 0.1376800 best: 0.1376771 (4973) 4987: learn: 0.0614706 test: 0.1376749 best: 0.1376749 (4987) 4988: learn: 0.0614665 test: 0.1376739 best: 0.1376739 (4988) 4989: learn: 0.0614604 test: 0.1376746 best: 0.1376739 (4988) 4990: learn: 0.0614499 test: 0.1376755 best: 0.1376739 (4988) 4991: learn: 0.0614422 test: 0.1376763 best: 0.1376739 (4988) total: 56.9s remaining: 11.5s 4992: learn: 0.0614329 test: 0.1376750 best: 0.1376739 (4988) 4993: learn: 0.0614272 test: 0.1376729 best: 0.1376729 (4993) 4994: learn: 0.0614182 test: 0.1376719 best: 0.1376719 (4994) 4995: learn: 0.0614114 test: 0.1376696 best: 0.1376696 (4995) 4996: learn: 0.0614054 test: 0.1376701 best: 0.1376696 (4995) 4997: learn: 0.0613988 test: 0.1376688 best: 0.1376688 (4997) 4998: learn: 0.0613939 test: 0.1376680 best: 0.1376680 (4998) 4999: learn: 0.0613821 test: 0.1376669 best: 0.1376669 (4999) 5000: learn: 0.0613744 test: 0.1376700 best: 0.1376669 (4999) 5001: learn: 0.0613667 test: 0.1376701 best: 0.1376669 (4999) 5002: learn: 0.0613597 test: 0.1376680 best: 0.1376669 (4999) 5003: learn: 0.0613545 test: 0.1376687 best: 0.1376669 (4999) 5004: learn: 0.0613490 test: 0.1376660 best: 0.1376660 (5004) 5005: learn: 0.0613443 test: 0.1376639 best: 0.1376639 (5005) 5006: learn: 0.0613390 test: 0.1376630 best: 0.1376630 (5006) 5007: learn: 0.0613293 test: 0.1376633 best: 0.1376630 (5006) 5008: learn: 0.0613181 test: 0.1376620 best: 0.1376620 (5008) 5009: learn: 0.0613127 test: 0.1376613 best: 0.1376613 (5009) 5010: learn: 0.0613068 test: 0.1376595 best: 0.1376595 (5010) 5011: learn: 0.0612987 test: 0.1376591 best: 0.1376591 (5011) 5012: learn: 0.0612894 test: 0.1376574 best: 0.1376574 (5012) 5013: learn: 0.0612844 test: 0.1376570 best: 0.1376570 (5013) 5014: learn: 0.0612751 test: 0.1376528 best: 0.1376528 (5014) 5015: learn: 0.0612656 test: 0.1376524 best: 0.1376524 (5015) 5016: learn: 0.0612562 test: 0.1376505 best: 0.1376505 (5016) 5017: learn: 0.0612520 test: 0.1376500 best: 0.1376500 (5017) 5018: learn: 0.0612455 test: 0.1376480 best: 0.1376480 (5018) 5019: learn: 0.0612384 test: 0.1376466 best: 0.1376466 (5019) 5020: learn: 0.0612348 test: 0.1376460 best: 0.1376460 (5020) 5021: learn: 0.0612273 test: 0.1376484 best: 0.1376460 (5020) 5022: learn: 0.0612162 test: 0.1376482 best: 0.1376460 (5020) 5023: learn: 0.0612043 test: 0.1376475 best: 0.1376460 (5020) 5024: learn: 0.0611954 test: 0.1376482 best: 0.1376460 (5020) 5025: learn: 0.0611867 test: 0.1376470 best: 0.1376460 (5020) 5026: learn: 0.0611759 test: 0.1376496 best: 0.1376460 (5020) 5027: learn: 0.0611643 test: 0.1376483 best: 0.1376460 (5020) 5028: learn: 0.0611602 test: 0.1376476 best: 0.1376460 (5020) 5029: learn: 0.0611570 test: 0.1376464 best: 0.1376460 (5020) 5030: learn: 0.0611494 test: 0.1376457 best: 0.1376457 (5030) 5031: learn: 0.0611403 test: 0.1376471 best: 0.1376457 (5030) 5032: learn: 0.0611310 test: 0.1376493 best: 0.1376457 (5030) 5033: learn: 0.0611220 test: 0.1376492 best: 0.1376457 (5030) 5034: learn: 0.0611169 test: 0.1376489 best: 0.1376457 (5030) 5035: learn: 0.0611093 test: 0.1376482 best: 0.1376457 (5030) 5036: learn: 0.0611002 test: 0.1376468 best: 0.1376457 (5030) 5037: learn: 0.0610924 test: 0.1376456 best: 0.1376456 (5037) 5038: learn: 0.0610839 test: 0.1376476 best: 0.1376456 (5037) 5039: learn: 0.0610743 test: 0.1376494 best: 0.1376456 (5037) 5040: learn: 0.0610668 test: 0.1376497 best: 0.1376456 (5037) 5041: learn: 0.0610605 test: 0.1376495 best: 0.1376456 (5037) 5042: learn: 0.0610522 test: 0.1376490 best: 0.1376456 (5037) 5043: learn: 0.0610480 test: 0.1376496 best: 0.1376456 (5037) 5044: learn: 0.0610399 test: 0.1376465 best: 0.1376456 (5037) 5045: learn: 0.0610311 test: 0.1376464 best: 0.1376456 (5037) 5046: learn: 0.0610231 test: 0.1376482 best: 0.1376456 (5037) 5047: learn: 0.0610149 test: 0.1376477 best: 0.1376456 (5037) 5048: learn: 0.0610039 test: 0.1376475 best: 0.1376456 (5037) 5049: learn: 0.0609962 test: 0.1376474 best: 0.1376456 (5037) 5050: learn: 0.0609896 test: 0.1376469 best: 0.1376456 (5037) 5051: learn: 0.0609832 test: 0.1376469 best: 0.1376456 (5037) 5052: learn: 0.0609730 test: 0.1376506 best: 0.1376456 (5037) 5053: learn: 0.0609652 test: 0.1376490 best: 0.1376456 (5037) 5054: learn: 0.0609579 test: 0.1376491 best: 0.1376456 (5037) 5055: learn: 0.0609507 test: 0.1376486 best: 0.1376456 (5037) 5056: learn: 0.0609455 test: 0.1376479 best: 0.1376456 (5037) 5057: learn: 0.0609370 test: 0.1376456 best: 0.1376456 (5037) 5058: learn: 0.0609282 test: 0.1376465 best: 0.1376456 (5037) 5059: learn: 0.0609176 test: 0.1376478 best: 0.1376456 (5037) 5060: learn: 0.0609121 test: 0.1376476 best: 0.1376456 (5037) 5061: learn: 0.0609082 test: 0.1376498 best: 0.1376456 (5037) 5062: learn: 0.0609002 test: 0.1376470 best: 0.1376456 (5037) 5063: learn: 0.0608931 test: 0.1376457 best: 0.1376456 (5037) 5064: learn: 0.0608811 test: 0.1376444 best: 0.1376444 (5064) 5065: learn: 0.0608718 test: 0.1376436 best: 0.1376436 (5065) 5066: learn: 0.0608634 test: 0.1376419 best: 0.1376419 (5066) 5067: learn: 0.0608547 test: 0.1376419 best: 0.1376419 (5067) 5068: learn: 0.0608457 test: 0.1376399 best: 0.1376399 (5068) 5069: learn: 0.0608376 test: 0.1376412 best: 0.1376399 (5068) 5070: learn: 0.0608295 test: 0.1376377 best: 0.1376377 (5070) 5071: learn: 0.0608195 test: 0.1376399 best: 0.1376377 (5070) 5072: learn: 0.0608092 test: 0.1376413 best: 0.1376377 (5070) 5073: learn: 0.0608027 test: 0.1376397 best: 0.1376377 (5070) 5074: learn: 0.0607943 test: 0.1376384 best: 0.1376377 (5070) 5075: learn: 0.0607850 test: 0.1376354 best: 0.1376354 (5075) 5076: learn: 0.0607762 test: 0.1376339 best: 0.1376339 (5076) 5077: learn: 0.0607705 test: 0.1376335 best: 0.1376335 (5077) 5078: learn: 0.0607603 test: 0.1376318 best: 0.1376318 (5078) 5079: learn: 0.0607517 test: 0.1376300 best: 0.1376300 (5079) 5080: learn: 0.0607471 test: 0.1376256 best: 0.1376256 (5080) 5081: learn: 0.0607386 test: 0.1376278 best: 0.1376256 (5080) 5082: learn: 0.0607339 test: 0.1376273 best: 0.1376256 (5080) 5083: learn: 0.0607272 test: 0.1376277 best: 0.1376256 (5080) 5084: learn: 0.0607220 test: 0.1376258 best: 0.1376256 (5080) 5085: learn: 0.0607156 test: 0.1376266 best: 0.1376256 (5080) 5086: learn: 0.0607042 test: 0.1376289 best: 0.1376256 (5080) 5087: learn: 0.0606940 test: 0.1376277 best: 0.1376256 (5080) 5088: learn: 0.0606844 test: 0.1376246 best: 0.1376246 (5088) 5089: learn: 0.0606786 test: 0.1376265 best: 0.1376246 (5088) 5090: learn: 0.0606722 test: 0.1376264 best: 0.1376246 (5088) 5091: learn: 0.0606649 test: 0.1376243 best: 0.1376243 (5091) 5092: learn: 0.0606579 test: 0.1376251 best: 0.1376243 (5091) 5093: learn: 0.0606503 test: 0.1376258 best: 0.1376243 (5091) 5094: learn: 0.0606401 test: 0.1376265 best: 0.1376243 (5091) 5095: learn: 0.0606330 test: 0.1376241 best: 0.1376241 (5095) 5096: learn: 0.0606258 test: 0.1376255 best: 0.1376241 (5095) 5097: learn: 0.0606173 test: 0.1376269 best: 0.1376241 (5095) 5098: learn: 0.0606107 test: 0.1376273 best: 0.1376241 (5095) 5099: learn: 0.0606017 test: 0.1376266 best: 0.1376241 (5095) 5100: learn: 0.0605937 test: 0.1376279 best: 0.1376241 (5095) 5101: learn: 0.0605859 test: 0.1376265 best: 0.1376241 (5095) 5102: learn: 0.0605784 test: 0.1376274 best: 0.1376241 (5095) 5103: learn: 0.0605706 test: 0.1376259 best: 0.1376241 (5095) 5104: learn: 0.0605630 test: 0.1376209 best: 0.1376209 (5104) 5105: learn: 0.0605539 test: 0.1376201 best: 0.1376201 (5105) 5106: learn: 0.0605454 test: 0.1376198 best: 0.1376198 (5106) 5107: learn: 0.0605391 test: 0.1376170 best: 0.1376170 (5107) 5108: learn: 0.0605322 test: 0.1376144 best: 0.1376144 (5108) 5109: learn: 0.0605239 test: 0.1376155 best: 0.1376144 (5108) 5110: learn: 0.0605142 test: 0.1376133 best: 0.1376133 (5110) 5111: learn: 0.0605044 test: 0.1376131 best: 0.1376131 (5111) 5112: learn: 0.0604997 test: 0.1376116 best: 0.1376116 (5112) 5113: learn: 0.0604893 test: 0.1376121 best: 0.1376116 (5112) 5114: learn: 0.0604820 test: 0.1376091 best: 0.1376091 (5114) 5115: learn: 0.0604756 test: 0.1376064 best: 0.1376064 (5115) 5116: learn: 0.0604654 test: 0.1376081 best: 0.1376064 (5115) 5117: learn: 0.0604552 test: 0.1376058 best: 0.1376058 (5117) 5118: learn: 0.0604473 test: 0.1376049 best: 0.1376049 (5118) 5119: learn: 0.0604384 test: 0.1376015 best: 0.1376015 (5119) 5120: learn: 0.0604294 test: 0.1375989 best: 0.1375989 (5120) 5121: learn: 0.0604237 test: 0.1375998 best: 0.1375989 (5120) 5122: learn: 0.0604167 test: 0.1375988 best: 0.1375988 (5122) 5123: learn: 0.0604102 test: 0.1375982 best: 0.1375982 (5123) 5124: learn: 0.0604052 test: 0.1376001 best: 0.1375982 (5123) 5125: learn: 0.0603976 test: 0.1375984 best: 0.1375982 (5123) 5126: learn: 0.0603944 test: 0.1375990 best: 0.1375982 (5123) 5127: learn: 0.0603850 test: 0.1375988 best: 0.1375982 (5123) 5128: learn: 0.0603735 test: 0.1375953 best: 0.1375953 (5128) 5129: learn: 0.0603648 test: 0.1375980 best: 0.1375953 (5128) 5130: learn: 0.0603537 test: 0.1375990 best: 0.1375953 (5128) 5131: learn: 0.0603468 test: 0.1376022 best: 0.1375953 (5128) 5132: learn: 0.0603383 test: 0.1376052 best: 0.1375953 (5128) total: 58.5s remaining: 9.88s 5133: learn: 0.0603300 test: 0.1376037 best: 0.1375953 (5128) 5134: learn: 0.0603222 test: 0.1376007 best: 0.1375953 (5128) 5135: learn: 0.0603155 test: 0.1376014 best: 0.1375953 (5128) 5136: learn: 0.0603092 test: 0.1376009 best: 0.1375953 (5128) 5137: learn: 0.0602985 test: 0.1376026 best: 0.1375953 (5128) 5138: learn: 0.0602936 test: 0.1376026 best: 0.1375953 (5128) 5139: learn: 0.0602864 test: 0.1376024 best: 0.1375953 (5128) 5140: learn: 0.0602776 test: 0.1376042 best: 0.1375953 (5128) 5141: learn: 0.0602694 test: 0.1376075 best: 0.1375953 (5128) 5142: learn: 0.0602643 test: 0.1376046 best: 0.1375953 (5128) 5143: learn: 0.0602566 test: 0.1376030 best: 0.1375953 (5128) 5144: learn: 0.0602504 test: 0.1376038 best: 0.1375953 (5128) 5145: learn: 0.0602415 test: 0.1376042 best: 0.1375953 (5128) 5146: learn: 0.0602318 test: 0.1376025 best: 0.1375953 (5128) 5147: learn: 0.0602248 test: 0.1376016 best: 0.1375953 (5128) 5148: learn: 0.0602162 test: 0.1376013 best: 0.1375953 (5128) 5149: learn: 0.0602084 test: 0.1376000 best: 0.1375953 (5128) 5150: learn: 0.0601992 test: 0.1375995 best: 0.1375953 (5128) 5151: learn: 0.0601921 test: 0.1375998 best: 0.1375953 (5128) 5152: learn: 0.0601838 test: 0.1375990 best: 0.1375953 (5128) 5153: learn: 0.0601779 test: 0.1376003 best: 0.1375953 (5128) 5154: learn: 0.0601712 test: 0.1375989 best: 0.1375953 (5128) 5155: learn: 0.0601601 test: 0.1375978 best: 0.1375953 (5128) 5156: learn: 0.0601512 test: 0.1375957 best: 0.1375953 (5128) 5157: learn: 0.0601432 test: 0.1375964 best: 0.1375953 (5128) 5158: learn: 0.0601328 test: 0.1375932 best: 0.1375932 (5158) 5159: learn: 0.0601231 test: 0.1375911 best: 0.1375911 (5159) 5160: learn: 0.0601166 test: 0.1375883 best: 0.1375883 (5160) 5161: learn: 0.0601073 test: 0.1375861 best: 0.1375861 (5161) 5162: learn: 0.0600978 test: 0.1375850 best: 0.1375850 (5162) 5163: learn: 0.0600852 test: 0.1375834 best: 0.1375834 (5163) 5164: learn: 0.0600793 test: 0.1375821 best: 0.1375821 (5164) 5165: learn: 0.0600710 test: 0.1375810 best: 0.1375810 (5165) 5166: learn: 0.0600650 test: 0.1375793 best: 0.1375793 (5166) 5167: learn: 0.0600613 test: 0.1375780 best: 0.1375780 (5167) 5168: learn: 0.0600510 test: 0.1375782 best: 0.1375780 (5167) 5169: learn: 0.0600428 test: 0.1375789 best: 0.1375780 (5167) 5170: learn: 0.0600344 test: 0.1375796 best: 0.1375780 (5167) 5171: learn: 0.0600266 test: 0.1375800 best: 0.1375780 (5167) 5172: learn: 0.0600167 test: 0.1375755 best: 0.1375755 (5172) total: 59s remaining: 9.43s 5173: learn: 0.0600074 test: 0.1375720 best: 0.1375720 (5173) 5174: learn: 0.0599997 test: 0.1375717 best: 0.1375717 (5174) 5175: learn: 0.0599952 test: 0.1375710 best: 0.1375710 (5175) 5176: learn: 0.0599878 test: 0.1375687 best: 0.1375687 (5176) 5177: learn: 0.0599781 test: 0.1375698 best: 0.1375687 (5176) 5178: learn: 0.0599744 test: 0.1375691 best: 0.1375687 (5176) 5179: learn: 0.0599671 test: 0.1375684 best: 0.1375684 (5179) 5180: learn: 0.0599553 test: 0.1375662 best: 0.1375662 (5180) 5181: learn: 0.0599495 test: 0.1375668 best: 0.1375662 (5180) 5182: learn: 0.0599413 test: 0.1375697 best: 0.1375662 (5180) 5183: learn: 0.0599347 test: 0.1375689 best: 0.1375662 (5180) 5184: learn: 0.0599254 test: 0.1375696 best: 0.1375662 (5180) 5185: learn: 0.0599205 test: 0.1375660 best: 0.1375660 (5185) 5186: learn: 0.0599126 test: 0.1375678 best: 0.1375660 (5185) 5187: learn: 0.0599045 test: 0.1375687 best: 0.1375660 (5185) 5188: learn: 0.0598966 test: 0.1375686 best: 0.1375660 (5185) 5189: learn: 0.0598915 test: 0.1375685 best: 0.1375660 (5185) 5190: learn: 0.0598856 test: 0.1375679 best: 0.1375660 (5185) 5191: learn: 0.0598770 test: 0.1375690 best: 0.1375660 (5185) 5192: learn: 0.0598667 test: 0.1375685 best: 0.1375660 (5185) 5193: learn: 0.0598603 test: 0.1375698 best: 0.1375660 (5185) 5194: learn: 0.0598511 test: 0.1375699 best: 0.1375660 (5185) 5195: learn: 0.0598435 test: 0.1375679 best: 0.1375660 (5185) 5196: learn: 0.0598395 test: 0.1375680 best: 0.1375660 (5185) 5197: learn: 0.0598321 test: 0.1375672 best: 0.1375660 (5185) 5198: learn: 0.0598262 test: 0.1375679 best: 0.1375660 (5185) 5199: learn: 0.0598181 test: 0.1375667 best: 0.1375660 (5185) 5200: learn: 0.0598093 test: 0.1375696 best: 0.1375660 (5185) 5201: learn: 0.0597984 test: 0.1375683 best: 0.1375660 (5185) 5202: learn: 0.0597900 test: 0.1375664 best: 0.1375660 (5185) 5203: learn: 0.0597820 test: 0.1375675 best: 0.1375660 (5185) 5204: learn: 0.0597758 test: 0.1375662 best: 0.1375660 (5185) 5205: learn: 0.0597691 test: 0.1375689 best: 0.1375660 (5185) 5206: learn: 0.0597620 test: 0.1375643 best: 0.1375643 (5206) 5207: learn: 0.0597539 test: 0.1375650 best: 0.1375643 (5206) 5208: learn: 0.0597430 test: 0.1375648 best: 0.1375643 (5206) 5209: learn: 0.0597375 test: 0.1375650 best: 0.1375643 (5206) 5210: learn: 0.0597268 test: 0.1375658 best: 0.1375643 (5206) 5211: learn: 0.0597204 test: 0.1375646 best: 0.1375643 (5206) 5212: learn: 0.0597100 test: 0.1375650 best: 0.1375643 (5206) 5213: learn: 0.0597032 test: 0.1375642 best: 0.1375642 (5213) 5214: learn: 0.0596936 test: 0.1375653 best: 0.1375642 (5213) 5215: learn: 0.0596854 test: 0.1375632 best: 0.1375632 (5215) 5216: learn: 0.0596760 test: 0.1375638 best: 0.1375632 (5215) 5217: learn: 0.0596699 test: 0.1375635 best: 0.1375632 (5215) 5218: learn: 0.0596628 test: 0.1375613 best: 0.1375613 (5218) 5219: learn: 0.0596544 test: 0.1375581 best: 0.1375581 (5219) 5220: learn: 0.0596493 test: 0.1375592 best: 0.1375581 (5219) 5221: learn: 0.0596396 test: 0.1375598 best: 0.1375581 (5219) 5222: learn: 0.0596325 test: 0.1375585 best: 0.1375581 (5219) 5223: learn: 0.0596237 test: 0.1375591 best: 0.1375581 (5219) 5224: learn: 0.0596169 test: 0.1375584 best: 0.1375581 (5219) 5225: learn: 0.0596076 test: 0.1375606 best: 0.1375581 (5219) 5226: learn: 0.0595981 test: 0.1375656 best: 0.1375581 (5219) 5227: learn: 0.0595902 test: 0.1375647 best: 0.1375581 (5219) 5228: learn: 0.0595856 test: 0.1375623 best: 0.1375581 (5219) 5229: learn: 0.0595779 test: 0.1375606 best: 0.1375581 (5219) 5230: learn: 0.0595717 test: 0.1375604 best: 0.1375581 (5219) 5231: learn: 0.0595644 test: 0.1375640 best: 0.1375581 (5219) 5232: learn: 0.0595559 test: 0.1375639 best: 0.1375581 (5219) 5233: learn: 0.0595488 test: 0.1375631 best: 0.1375581 (5219) 5234: learn: 0.0595387 test: 0.1375612 best: 0.1375581 (5219) 5235: learn: 0.0595313 test: 0.1375613 best: 0.1375581 (5219) 5236: learn: 0.0595238 test: 0.1375598 best: 0.1375581 (5219) 5237: learn: 0.0595182 test: 0.1375600 best: 0.1375581 (5219) 5238: learn: 0.0595081 test: 0.1375632 best: 0.1375581 (5219) 5239: learn: 0.0595009 test: 0.1375648 best: 0.1375581 (5219) 5240: learn: 0.0594945 test: 0.1375659 best: 0.1375581 (5219) 5241: learn: 0.0594862 test: 0.1375643 best: 0.1375581 (5219) 5242: learn: 0.0594752 test: 0.1375637 best: 0.1375581 (5219) 5243: learn: 0.0594691 test: 0.1375658 best: 0.1375581 (5219) 5244: learn: 0.0594614 test: 0.1375629 best: 0.1375581 (5219) 5245: learn: 0.0594531 test: 0.1375639 best: 0.1375581 (5219) 5246: learn: 0.0594456 test: 0.1375640 best: 0.1375581 (5219) 5247: learn: 0.0594364 test: 0.1375662 best: 0.1375581 (5219) 5248: learn: 0.0594283 test: 0.1375664 best: 0.1375581 (5219) 5249: learn: 0.0594199 test: 0.1375667 best: 0.1375581 (5219) 5250: learn: 0.0594130 test: 0.1375672 best: 0.1375581 (5219) 5251: learn: 0.0594057 test: 0.1375663 best: 0.1375581 (5219) 5252: learn: 0.0593972 test: 0.1375635 best: 0.1375581 (5219) 5253: learn: 0.0593898 test: 0.1375645 best: 0.1375581 (5219) 5254: learn: 0.0593813 test: 0.1375681 best: 0.1375581 (5219) 5255: learn: 0.0593776 test: 0.1375683 best: 0.1375581 (5219) 5256: learn: 0.0593748 test: 0.1375667 best: 0.1375581 (5219) 5257: learn: 0.0593671 test: 0.1375665 best: 0.1375581 (5219) 5258: learn: 0.0593590 test: 0.1375685 best: 0.1375581 (5219) 5259: learn: 0.0593502 test: 0.1375721 best: 0.1375581 (5219) 5260: learn: 0.0593439 test: 0.1375730 best: 0.1375581 (5219) 5261: learn: 0.0593343 test: 0.1375701 best: 0.1375581 (5219) 5262: learn: 0.0593261 test: 0.1375676 best: 0.1375581 (5219) 5263: learn: 0.0593169 test: 0.1375665 best: 0.1375581 (5219) 5264: learn: 0.0593117 test: 0.1375666 best: 0.1375581 (5219) 5265: learn: 0.0593031 test: 0.1375624 best: 0.1375581 (5219) 5266: learn: 0.0592953 test: 0.1375623 best: 0.1375581 (5219) 5267: learn: 0.0592878 test: 0.1375583 best: 0.1375581 (5219) 5268: learn: 0.0592832 test: 0.1375577 best: 0.1375577 (5268) 5269: learn: 0.0592747 test: 0.1375561 best: 0.1375561 (5269) 5270: learn: 0.0592682 test: 0.1375561 best: 0.1375561 (5270) 5271: learn: 0.0592608 test: 0.1375572 best: 0.1375561 (5270) 5272: learn: 0.0592521 test: 0.1375549 best: 0.1375549 (5272) 5273: learn: 0.0592432 test: 0.1375577 best: 0.1375549 (5272) 5274: learn: 0.0592372 test: 0.1375582 best: 0.1375549 (5272) 5275: learn: 0.0592293 test: 0.1375588 best: 0.1375549 (5272) 5276: learn: 0.0592218 test: 0.1375574 best: 0.1375549 (5272) 5277: learn: 0.0592160 test: 0.1375554 best: 0.1375549 (5272) 5278: learn: 0.0592104 test: 0.1375554 best: 0.1375549 (5272) 5279: learn: 0.0592050 test: 0.1375539 best: 0.1375539 (5279) 5280: learn: 0.0591987 test: 0.1375520 best: 0.1375520 (5280) 5281: learn: 0.0591906 test: 0.1375524 best: 0.1375520 (5280) 5282: learn: 0.0591818 test: 0.1375509 best: 0.1375509 (5282) 5283: learn: 0.0591752 test: 0.1375521 best: 0.1375509 (5282) 5284: learn: 0.0591671 test: 0.1375539 best: 0.1375509 (5282) 5285: learn: 0.0591614 test: 0.1375533 best: 0.1375509 (5282) 5286: learn: 0.0591547 test: 0.1375521 best: 0.1375509 (5282) 5287: learn: 0.0591463 test: 0.1375501 best: 0.1375501 (5287) 5288: learn: 0.0591386 test: 0.1375468 best: 0.1375468 (5288) 5289: learn: 0.0591311 test: 0.1375469 best: 0.1375468 (5288) 5290: learn: 0.0591230 test: 0.1375483 best: 0.1375468 (5288) 5291: learn: 0.0591137 test: 0.1375478 best: 0.1375468 (5288) 5292: learn: 0.0591090 test: 0.1375482 best: 0.1375468 (5288) 5293: learn: 0.0591017 test: 0.1375487 best: 0.1375468 (5288) 5294: learn: 0.0590919 test: 0.1375491 best: 0.1375468 (5288) 5295: learn: 0.0590835 test: 0.1375494 best: 0.1375468 (5288) 5296: learn: 0.0590751 test: 0.1375507 best: 0.1375468 (5288) 5297: learn: 0.0590728 test: 0.1375503 best: 0.1375468 (5288) 5298: learn: 0.0590651 test: 0.1375479 best: 0.1375468 (5288) 5299: learn: 0.0590542 test: 0.1375479 best: 0.1375468 (5288) 5300: learn: 0.0590463 test: 0.1375475 best: 0.1375468 (5288) 5301: learn: 0.0590380 test: 0.1375448 best: 0.1375448 (5301) 5302: learn: 0.0590337 test: 0.1375433 best: 0.1375433 (5302) 5303: learn: 0.0590262 test: 0.1375432 best: 0.1375432 (5303) 5304: learn: 0.0590181 test: 0.1375457 best: 0.1375432 (5303) 5305: learn: 0.0590095 test: 0.1375446 best: 0.1375432 (5303) 5306: learn: 0.0590016 test: 0.1375458 best: 0.1375432 (5303) 5307: learn: 0.0589943 test: 0.1375427 best: 0.1375427 (5307) 5308: learn: 0.0589861 test: 0.1375406 best: 0.1375406 (5308) 5309: learn: 0.0589742 test: 0.1375397 best: 0.1375397 (5309) 5310: learn: 0.0589659 test: 0.1375388 best: 0.1375388 (5310) 5311: learn: 0.0589605 test: 0.1375390 best: 0.1375388 (5310) 5312: learn: 0.0589535 test: 0.1375388 best: 0.1375388 (5312) 5313: learn: 0.0589462 test: 0.1375406 best: 0.1375388 (5312) 5314: learn: 0.0589384 test: 0.1375428 best: 0.1375388 (5312) 5315: learn: 0.0589323 test: 0.1375413 best: 0.1375388 (5312) 5316: learn: 0.0589243 test: 0.1375384 best: 0.1375384 (5316) 5317: learn: 0.0589159 test: 0.1375391 best: 0.1375384 (5316) total: 1m remaining: 7.77s 5318: learn: 0.0589101 test: 0.1375399 best: 0.1375384 (5316) 5319: learn: 0.0589054 test: 0.1375386 best: 0.1375384 (5316) 5320: learn: 0.0588966 test: 0.1375401 best: 0.1375384 (5316) 5321: learn: 0.0588922 test: 0.1375391 best: 0.1375384 (5316) 5322: learn: 0.0588867 test: 0.1375393 best: 0.1375384 (5316) 5323: learn: 0.0588832 test: 0.1375364 best: 0.1375364 (5323) 5324: learn: 0.0588765 test: 0.1375349 best: 0.1375349 (5324) 5325: learn: 0.0588712 test: 0.1375341 best: 0.1375341 (5325) 5326: learn: 0.0588622 test: 0.1375350 best: 0.1375341 (5325) 5327: learn: 0.0588554 test: 0.1375358 best: 0.1375341 (5325) 5328: learn: 0.0588470 test: 0.1375394 best: 0.1375341 (5325) 5329: learn: 0.0588395 test: 0.1375391 best: 0.1375341 (5325) 5330: learn: 0.0588316 test: 0.1375363 best: 0.1375341 (5325) 5331: learn: 0.0588238 test: 0.1375368 best: 0.1375341 (5325) 5332: learn: 0.0588167 test: 0.1375387 best: 0.1375341 (5325) 5333: learn: 0.0588123 test: 0.1375378 best: 0.1375341 (5325) 5334: learn: 0.0588059 test: 0.1375380 best: 0.1375341 (5325) 5335: learn: 0.0587975 test: 0.1375380 best: 0.1375341 (5325) 5336: learn: 0.0587874 test: 0.1375412 best: 0.1375341 (5325) 5337: learn: 0.0587815 test: 0.1375413 best: 0.1375341 (5325) 5338: learn: 0.0587722 test: 0.1375428 best: 0.1375341 (5325) 5339: learn: 0.0587663 test: 0.1375423 best: 0.1375341 (5325) 5340: learn: 0.0587577 test: 0.1375418 best: 0.1375341 (5325) 5341: learn: 0.0587496 test: 0.1375402 best: 0.1375341 (5325) 5342: learn: 0.0587411 test: 0.1375401 best: 0.1375341 (5325) 5343: learn: 0.0587332 test: 0.1375381 best: 0.1375341 (5325) 5344: learn: 0.0587241 test: 0.1375354 best: 0.1375341 (5325) 5345: learn: 0.0587148 test: 0.1375360 best: 0.1375341 (5325) 5346: learn: 0.0587087 test: 0.1375360 best: 0.1375341 (5325) 5347: learn: 0.0587002 test: 0.1375339 best: 0.1375339 (5347) 5348: learn: 0.0586968 test: 0.1375330 best: 0.1375330 (5348) 5349: learn: 0.0586894 test: 0.1375307 best: 0.1375307 (5349) 5350: learn: 0.0586805 test: 0.1375272 best: 0.1375272 (5350) 5351: learn: 0.0586740 test: 0.1375268 best: 0.1375268 (5351) 5352: learn: 0.0586659 test: 0.1375221 best: 0.1375221 (5352) 5353: learn: 0.0586568 test: 0.1375237 best: 0.1375221 (5352) 5354: learn: 0.0586487 test: 0.1375227 best: 0.1375221 (5352) 5355: learn: 0.0586405 test: 0.1375230 best: 0.1375221 (5352) 5356: learn: 0.0586326 test: 0.1375224 best: 0.1375221 (5352) 5357: learn: 0.0586246 test: 0.1375232 best: 0.1375221 (5352) 5358: learn: 0.0586146 test: 0.1375246 best: 0.1375221 (5352) 5359: learn: 0.0586056 test: 0.1375254 best: 0.1375221 (5352) 5360: learn: 0.0585983 test: 0.1375265 best: 0.1375221 (5352) 5361: learn: 0.0585892 test: 0.1375249 best: 0.1375221 (5352) 5362: learn: 0.0585802 test: 0.1375266 best: 0.1375221 (5352) 5363: learn: 0.0585727 test: 0.1375242 best: 0.1375221 (5352) total: 1m 1s remaining: 7.26s 5364: learn: 0.0585671 test: 0.1375240 best: 0.1375221 (5352) 5365: learn: 0.0585599 test: 0.1375242 best: 0.1375221 (5352) 5366: learn: 0.0585517 test: 0.1375224 best: 0.1375221 (5352) 5367: learn: 0.0585413 test: 0.1375202 best: 0.1375202 (5367) 5368: learn: 0.0585323 test: 0.1375207 best: 0.1375202 (5367) 5369: learn: 0.0585217 test: 0.1375248 best: 0.1375202 (5367) 5370: learn: 0.0585139 test: 0.1375216 best: 0.1375202 (5367) 5371: learn: 0.0585061 test: 0.1375190 best: 0.1375190 (5371) 5372: learn: 0.0585004 test: 0.1375189 best: 0.1375189 (5372) 5373: learn: 0.0584949 test: 0.1375178 best: 0.1375178 (5373) 5374: learn: 0.0584850 test: 0.1375163 best: 0.1375163 (5374) 5375: learn: 0.0584769 test: 0.1375167 best: 0.1375163 (5374) 5376: learn: 0.0584720 test: 0.1375159 best: 0.1375159 (5376) 5377: learn: 0.0584636 test: 0.1375199 best: 0.1375159 (5376) 5378: learn: 0.0584550 test: 0.1375206 best: 0.1375159 (5376) 5379: learn: 0.0584474 test: 0.1375226 best: 0.1375159 (5376) 5380: learn: 0.0584408 test: 0.1375216 best: 0.1375159 (5376) 5381: learn: 0.0584343 test: 0.1375209 best: 0.1375159 (5376) 5382: learn: 0.0584270 test: 0.1375180 best: 0.1375159 (5376) 5383: learn: 0.0584193 test: 0.1375179 best: 0.1375159 (5376) 5384: learn: 0.0584097 test: 0.1375176 best: 0.1375159 (5376) 5385: learn: 0.0584022 test: 0.1375167 best: 0.1375159 (5376) 5386: learn: 0.0583947 test: 0.1375180 best: 0.1375159 (5376) 5387: learn: 0.0583883 test: 0.1375164 best: 0.1375159 (5376) 5388: learn: 0.0583821 test: 0.1375151 best: 0.1375151 (5388) 5389: learn: 0.0583762 test: 0.1375134 best: 0.1375134 (5389) 5390: learn: 0.0583687 test: 0.1375132 best: 0.1375132 (5390) 5391: learn: 0.0583603 test: 0.1375119 best: 0.1375119 (5391) 5392: learn: 0.0583516 test: 0.1375125 best: 0.1375119 (5391) 5393: learn: 0.0583464 test: 0.1375109 best: 0.1375109 (5393) 5394: learn: 0.0583400 test: 0.1375102 best: 0.1375102 (5394) 5395: learn: 0.0583325 test: 0.1375128 best: 0.1375102 (5394) 5396: learn: 0.0583260 test: 0.1375138 best: 0.1375102 (5394) 5397: learn: 0.0583152 test: 0.1375137 best: 0.1375102 (5394) 5398: learn: 0.0583077 test: 0.1375112 best: 0.1375102 (5394) 5399: learn: 0.0583022 test: 0.1375080 best: 0.1375080 (5399) 5400: learn: 0.0582974 test: 0.1375077 best: 0.1375077 (5400) 5401: learn: 0.0582874 test: 0.1375080 best: 0.1375077 (5400) 5402: learn: 0.0582805 test: 0.1375050 best: 0.1375050 (5402) 5403: learn: 0.0582735 test: 0.1375025 best: 0.1375025 (5403) 5404: learn: 0.0582680 test: 0.1375037 best: 0.1375025 (5403) 5405: learn: 0.0582631 test: 0.1375029 best: 0.1375025 (5403) 5406: learn: 0.0582588 test: 0.1375025 best: 0.1375025 (5406) 5407: learn: 0.0582502 test: 0.1375013 best: 0.1375013 (5407) 5408: learn: 0.0582398 test: 0.1375033 best: 0.1375013 (5407) 5409: learn: 0.0582309 test: 0.1375029 best: 0.1375013 (5407) 5410: learn: 0.0582211 test: 0.1374998 best: 0.1374998 (5410) 5411: learn: 0.0582143 test: 0.1374999 best: 0.1374998 (5410) 5412: learn: 0.0582058 test: 0.1374967 best: 0.1374967 (5412) 5413: learn: 0.0581971 test: 0.1374981 best: 0.1374967 (5412) 5414: learn: 0.0581900 test: 0.1374957 best: 0.1374957 (5414) 5415: learn: 0.0581820 test: 0.1374944 best: 0.1374944 (5415) 5416: learn: 0.0581757 test: 0.1374981 best: 0.1374944 (5415) 5417: learn: 0.0581657 test: 0.1374983 best: 0.1374944 (5415) 5418: learn: 0.0581581 test: 0.1374992 best: 0.1374944 (5415) 5419: learn: 0.0581517 test: 0.1374981 best: 0.1374944 (5415) 5420: learn: 0.0581460 test: 0.1374978 best: 0.1374944 (5415) 5421: learn: 0.0581371 test: 0.1374945 best: 0.1374944 (5415) 5422: learn: 0.0581281 test: 0.1374961 best: 0.1374944 (5415) 5423: learn: 0.0581189 test: 0.1374982 best: 0.1374944 (5415) 5424: learn: 0.0581127 test: 0.1374998 best: 0.1374944 (5415) 5425: learn: 0.0581055 test: 0.1374985 best: 0.1374944 (5415) 5426: learn: 0.0580954 test: 0.1375022 best: 0.1374944 (5415) 5427: learn: 0.0580887 test: 0.1375015 best: 0.1374944 (5415) 5428: learn: 0.0580832 test: 0.1375022 best: 0.1374944 (5415) 5429: learn: 0.0580753 test: 0.1375013 best: 0.1374944 (5415) 5430: learn: 0.0580685 test: 0.1375005 best: 0.1374944 (5415) 5431: learn: 0.0580606 test: 0.1375002 best: 0.1374944 (5415) 5432: learn: 0.0580524 test: 0.1374996 best: 0.1374944 (5415) 5433: learn: 0.0580456 test: 0.1375015 best: 0.1374944 (5415) 5434: learn: 0.0580331 test: 0.1375014 best: 0.1374944 (5415) 5435: learn: 0.0580247 test: 0.1375038 best: 0.1374944 (5415) 5436: learn: 0.0580181 test: 0.1375011 best: 0.1374944 (5415) 5437: learn: 0.0580121 test: 0.1375008 best: 0.1374944 (5415) 5438: learn: 0.0580040 test: 0.1375026 best: 0.1374944 (5415) 5439: learn: 0.0579970 test: 0.1375049 best: 0.1374944 (5415) 5440: learn: 0.0579895 test: 0.1375051 best: 0.1374944 (5415) 5441: learn: 0.0579829 test: 0.1375056 best: 0.1374944 (5415) 5442: learn: 0.0579747 test: 0.1375067 best: 0.1374944 (5415) 5443: learn: 0.0579699 test: 0.1375036 best: 0.1374944 (5415) 5444: learn: 0.0579642 test: 0.1375034 best: 0.1374944 (5415) 5445: learn: 0.0579601 test: 0.1375020 best: 0.1374944 (5415) 5446: learn: 0.0579512 test: 0.1374987 best: 0.1374944 (5415) 5447: learn: 0.0579467 test: 0.1375010 best: 0.1374944 (5415) 5448: learn: 0.0579390 test: 0.1375025 best: 0.1374944 (5415) 5449: learn: 0.0579321 test: 0.1375031 best: 0.1374944 (5415) 5450: learn: 0.0579261 test: 0.1375042 best: 0.1374944 (5415) 5451: learn: 0.0579171 test: 0.1375035 best: 0.1374944 (5415) 5452: learn: 0.0579091 test: 0.1375028 best: 0.1374944 (5415) 5453: learn: 0.0579019 test: 0.1375026 best: 0.1374944 (5415) 5454: learn: 0.0578972 test: 0.1375013 best: 0.1374944 (5415) 5455: learn: 0.0578944 test: 0.1374978 best: 0.1374944 (5415) 5456: learn: 0.0578853 test: 0.1374964 best: 0.1374944 (5415) 5457: learn: 0.0578761 test: 0.1374982 best: 0.1374944 (5415) 5458: learn: 0.0578684 test: 0.1374951 best: 0.1374944 (5415) 5459: learn: 0.0578606 test: 0.1374978 best: 0.1374944 (5415) 5460: learn: 0.0578529 test: 0.1374986 best: 0.1374944 (5415) 5461: learn: 0.0578454 test: 0.1375002 best: 0.1374944 (5415) 5462: learn: 0.0578377 test: 0.1375013 best: 0.1374944 (5415) 5463: learn: 0.0578299 test: 0.1375031 best: 0.1374944 (5415) 5464: learn: 0.0578223 test: 0.1375023 best: 0.1374944 (5415) 5465: learn: 0.0578147 test: 0.1375019 best: 0.1374944 (5415) 5466: learn: 0.0578073 test: 0.1375019 best: 0.1374944 (5415) 5467: learn: 0.0577981 test: 0.1375033 best: 0.1374944 (5415) 5468: learn: 0.0577914 test: 0.1375030 best: 0.1374944 (5415) 5469: learn: 0.0577856 test: 0.1375004 best: 0.1374944 (5415) 5470: learn: 0.0577759 test: 0.1375004 best: 0.1374944 (5415) 5471: learn: 0.0577675 test: 0.1375042 best: 0.1374944 (5415) 5472: learn: 0.0577579 test: 0.1375044 best: 0.1374944 (5415) 5473: learn: 0.0577525 test: 0.1375044 best: 0.1374944 (5415) 5474: learn: 0.0577431 test: 0.1375031 best: 0.1374944 (5415) 5475: learn: 0.0577347 test: 0.1375011 best: 0.1374944 (5415) 5476: learn: 0.0577290 test: 0.1374964 best: 0.1374944 (5415) 5477: learn: 0.0577242 test: 0.1374965 best: 0.1374944 (5415) 5478: learn: 0.0577140 test: 0.1374955 best: 0.1374944 (5415) 5479: learn: 0.0577081 test: 0.1374959 best: 0.1374944 (5415) 5480: learn: 0.0576999 test: 0.1374979 best: 0.1374944 (5415) 5481: learn: 0.0576948 test: 0.1374980 best: 0.1374944 (5415) 5482: learn: 0.0576904 test: 0.1374971 best: 0.1374944 (5415) 5483: learn: 0.0576834 test: 0.1374978 best: 0.1374944 (5415) 5484: learn: 0.0576758 test: 0.1374992 best: 0.1374944 (5415) 5485: learn: 0.0576654 test: 0.1374980 best: 0.1374944 (5415) 5486: learn: 0.0576603 test: 0.1374949 best: 0.1374944 (5415) 5487: learn: 0.0576526 test: 0.1374927 best: 0.1374927 (5487) 5488: learn: 0.0576451 test: 0.1374936 best: 0.1374927 (5487) 5489: learn: 0.0576368 test: 0.1374936 best: 0.1374927 (5487) 5490: learn: 0.0576284 test: 0.1374945 best: 0.1374927 (5487) 5491: learn: 0.0576191 test: 0.1374933 best: 0.1374927 (5487) 5492: learn: 0.0576132 test: 0.1374919 best: 0.1374919 (5492) 5493: learn: 0.0576078 test: 0.1374889 best: 0.1374889 (5493) 5494: learn: 0.0576024 test: 0.1374901 best: 0.1374889 (5493) 5495: learn: 0.0575970 test: 0.1374907 best: 0.1374889 (5493) 5496: learn: 0.0575891 test: 0.1374940 best: 0.1374889 (5493) 5497: learn: 0.0575817 test: 0.1374945 best: 0.1374889 (5493) 5498: learn: 0.0575768 test: 0.1374944 best: 0.1374889 (5493) 5499: learn: 0.0575682 test: 0.1374935 best: 0.1374889 (5493) 5500: learn: 0.0575613 test: 0.1374937 best: 0.1374889 (5493) 5501: learn: 0.0575535 test: 0.1374932 best: 0.1374889 (5493) 5502: learn: 0.0575489 test: 0.1374911 best: 0.1374889 (5493) 5503: learn: 0.0575443 test: 0.1374912 best: 0.1374889 (5493) 5504: learn: 0.0575342 test: 0.1374927 best: 0.1374889 (5493) 5505: learn: 0.0575243 test: 0.1374923 best: 0.1374889 (5493) 5506: learn: 0.0575180 test: 0.1374948 best: 0.1374889 (5493) 5507: learn: 0.0575099 test: 0.1374933 best: 0.1374889 (5493) 5508: learn: 0.0575033 test: 0.1374918 best: 0.1374889 (5493) 5509: learn: 0.0574928 test: 0.1374929 best: 0.1374889 (5493) total: 1m 2s remaining: 5.59s 5510: learn: 0.0574862 test: 0.1374940 best: 0.1374889 (5493) 5511: learn: 0.0574766 test: 0.1374969 best: 0.1374889 (5493) 5512: learn: 0.0574704 test: 0.1374970 best: 0.1374889 (5493) 5513: learn: 0.0574634 test: 0.1374962 best: 0.1374889 (5493) 5514: learn: 0.0574548 test: 0.1374973 best: 0.1374889 (5493) 5515: learn: 0.0574493 test: 0.1374964 best: 0.1374889 (5493) 5516: learn: 0.0574428 test: 0.1374957 best: 0.1374889 (5493) 5517: learn: 0.0574352 test: 0.1374935 best: 0.1374889 (5493) 5518: learn: 0.0574273 test: 0.1374906 best: 0.1374889 (5493) 5519: learn: 0.0574210 test: 0.1374934 best: 0.1374889 (5493) 5520: learn: 0.0574130 test: 0.1374916 best: 0.1374889 (5493) 5521: learn: 0.0574065 test: 0.1374952 best: 0.1374889 (5493) 5522: learn: 0.0574004 test: 0.1374950 best: 0.1374889 (5493) 5523: learn: 0.0573916 test: 0.1374949 best: 0.1374889 (5493) 5524: learn: 0.0573849 test: 0.1374974 best: 0.1374889 (5493) 5525: learn: 0.0573811 test: 0.1374970 best: 0.1374889 (5493) 5526: learn: 0.0573741 test: 0.1374957 best: 0.1374889 (5493) 5527: learn: 0.0573659 test: 0.1374952 best: 0.1374889 (5493) 5528: learn: 0.0573582 test: 0.1374957 best: 0.1374889 (5493) 5529: learn: 0.0573512 test: 0.1374929 best: 0.1374889 (5493) 5530: learn: 0.0573456 test: 0.1374951 best: 0.1374889 (5493) 5531: learn: 0.0573396 test: 0.1374963 best: 0.1374889 (5493) 5532: learn: 0.0573351 test: 0.1374973 best: 0.1374889 (5493) 5533: learn: 0.0573249 test: 0.1375001 best: 0.1374889 (5493) 5534: learn: 0.0573162 test: 0.1375003 best: 0.1374889 (5493) 5535: learn: 0.0573097 test: 0.1374982 best: 0.1374889 (5493) 5536: learn: 0.0573011 test: 0.1374976 best: 0.1374889 (5493) total: 1m 3s remaining: 5.29s 5537: learn: 0.0572947 test: 0.1374971 best: 0.1374889 (5493) 5538: learn: 0.0572878 test: 0.1374972 best: 0.1374889 (5493) 5539: learn: 0.0572784 test: 0.1374974 best: 0.1374889 (5493) 5540: learn: 0.0572737 test: 0.1374939 best: 0.1374889 (5493) 5541: learn: 0.0572678 test: 0.1374920 best: 0.1374889 (5493) 5542: learn: 0.0572605 test: 0.1374896 best: 0.1374889 (5493) 5543: learn: 0.0572521 test: 0.1374902 best: 0.1374889 (5493) 5544: learn: 0.0572483 test: 0.1374890 best: 0.1374889 (5493) 5545: learn: 0.0572439 test: 0.1374854 best: 0.1374854 (5545) 5546: learn: 0.0572365 test: 0.1374879 best: 0.1374854 (5545) 5547: learn: 0.0572316 test: 0.1374863 best: 0.1374854 (5545) 5548: learn: 0.0572218 test: 0.1374886 best: 0.1374854 (5545) 5549: learn: 0.0572131 test: 0.1374865 best: 0.1374854 (5545) 5550: learn: 0.0572058 test: 0.1374837 best: 0.1374837 (5550) 5551: learn: 0.0571946 test: 0.1374814 best: 0.1374814 (5551) 5552: learn: 0.0571843 test: 0.1374783 best: 0.1374783 (5552) 5553: learn: 0.0571742 test: 0.1374795 best: 0.1374783 (5552) 5554: learn: 0.0571658 test: 0.1374788 best: 0.1374783 (5552) 5555: learn: 0.0571608 test: 0.1374786 best: 0.1374783 (5552) 5556: learn: 0.0571517 test: 0.1374776 best: 0.1374776 (5556) 5557: learn: 0.0571447 test: 0.1374752 best: 0.1374752 (5557) 5558: learn: 0.0571398 test: 0.1374727 best: 0.1374727 (5558) 5559: learn: 0.0571291 test: 0.1374717 best: 0.1374717 (5559) 5560: learn: 0.0571250 test: 0.1374722 best: 0.1374717 (5559) 5561: learn: 0.0571189 test: 0.1374730 best: 0.1374717 (5559) 5562: learn: 0.0571098 test: 0.1374715 best: 0.1374715 (5562) 5563: learn: 0.0571014 test: 0.1374728 best: 0.1374715 (5562) 5564: learn: 0.0570941 test: 0.1374758 best: 0.1374715 (5562) 5565: learn: 0.0570868 test: 0.1374754 best: 0.1374715 (5562) 5566: learn: 0.0570817 test: 0.1374736 best: 0.1374715 (5562) 5567: learn: 0.0570758 test: 0.1374724 best: 0.1374715 (5562) 5568: learn: 0.0570690 test: 0.1374734 best: 0.1374715 (5562) 5569: learn: 0.0570617 test: 0.1374731 best: 0.1374715 (5562) 5570: learn: 0.0570572 test: 0.1374715 best: 0.1374715 (5570) 5571: learn: 0.0570499 test: 0.1374714 best: 0.1374714 (5571) 5572: learn: 0.0570436 test: 0.1374716 best: 0.1374714 (5571) 5573: learn: 0.0570395 test: 0.1374694 best: 0.1374694 (5573) 5574: learn: 0.0570331 test: 0.1374695 best: 0.1374694 (5573) 5575: learn: 0.0570252 test: 0.1374681 best: 0.1374681 (5575) 5576: learn: 0.0570180 test: 0.1374687 best: 0.1374681 (5575) 5577: learn: 0.0570118 test: 0.1374689 best: 0.1374681 (5575) 5578: learn: 0.0570037 test: 0.1374676 best: 0.1374676 (5578) 5579: learn: 0.0569966 test: 0.1374668 best: 0.1374668 (5579) 5580: learn: 0.0569896 test: 0.1374640 best: 0.1374640 (5580) 5581: learn: 0.0569852 test: 0.1374606 best: 0.1374606 (5581) 5582: learn: 0.0569792 test: 0.1374618 best: 0.1374606 (5581) 5583: learn: 0.0569711 test: 0.1374602 best: 0.1374602 (5583) 5584: learn: 0.0569612 test: 0.1374584 best: 0.1374584 (5584) 5585: learn: 0.0569555 test: 0.1374563 best: 0.1374563 (5585) 5586: learn: 0.0569497 test: 0.1374554 best: 0.1374554 (5586) 5587: learn: 0.0569418 test: 0.1374562 best: 0.1374554 (5586) 5588: learn: 0.0569352 test: 0.1374569 best: 0.1374554 (5586) 5589: learn: 0.0569256 test: 0.1374578 best: 0.1374554 (5586) 5590: learn: 0.0569194 test: 0.1374543 best: 0.1374543 (5590) 5591: learn: 0.0569133 test: 0.1374525 best: 0.1374525 (5591) 5592: learn: 0.0569074 test: 0.1374513 best: 0.1374513 (5592) 5593: learn: 0.0568985 test: 0.1374525 best: 0.1374513 (5592) 5594: learn: 0.0568878 test: 0.1374494 best: 0.1374494 (5594) 5595: learn: 0.0568825 test: 0.1374496 best: 0.1374494 (5594) 5596: learn: 0.0568769 test: 0.1374482 best: 0.1374482 (5596) 5597: learn: 0.0568678 test: 0.1374481 best: 0.1374481 (5597) 5598: learn: 0.0568616 test: 0.1374522 best: 0.1374481 (5597) 5599: learn: 0.0568559 test: 0.1374516 best: 0.1374481 (5597) 5600: learn: 0.0568468 test: 0.1374544 best: 0.1374481 (5597) 5601: learn: 0.0568420 test: 0.1374545 best: 0.1374481 (5597) 5602: learn: 0.0568351 test: 0.1374540 best: 0.1374481 (5597) 5603: learn: 0.0568271 test: 0.1374542 best: 0.1374481 (5597) 5604: learn: 0.0568218 test: 0.1374537 best: 0.1374481 (5597) 5605: learn: 0.0568153 test: 0.1374526 best: 0.1374481 (5597) 5606: learn: 0.0568063 test: 0.1374503 best: 0.1374481 (5597) 5607: learn: 0.0568000 test: 0.1374518 best: 0.1374481 (5597) 5608: learn: 0.0567901 test: 0.1374523 best: 0.1374481 (5597) 5609: learn: 0.0567816 test: 0.1374514 best: 0.1374481 (5597) 5610: learn: 0.0567728 test: 0.1374516 best: 0.1374481 (5597) 5611: learn: 0.0567674 test: 0.1374481 best: 0.1374481 (5611) 5612: learn: 0.0567589 test: 0.1374462 best: 0.1374462 (5612) 5613: learn: 0.0567505 test: 0.1374449 best: 0.1374449 (5613) 5614: learn: 0.0567421 test: 0.1374397 best: 0.1374397 (5614) 5615: learn: 0.0567335 test: 0.1374372 best: 0.1374372 (5615) 5616: learn: 0.0567263 test: 0.1374397 best: 0.1374372 (5615) 5617: learn: 0.0567215 test: 0.1374386 best: 0.1374372 (5615) 5618: learn: 0.0567145 test: 0.1374361 best: 0.1374361 (5618) 5619: learn: 0.0567073 test: 0.1374378 best: 0.1374361 (5618) 5620: learn: 0.0567005 test: 0.1374388 best: 0.1374361 (5618) 5621: learn: 0.0566943 test: 0.1374427 best: 0.1374361 (5618) 5622: learn: 0.0566851 test: 0.1374440 best: 0.1374361 (5618) 5623: learn: 0.0566767 test: 0.1374435 best: 0.1374361 (5618) 5624: learn: 0.0566703 test: 0.1374417 best: 0.1374361 (5618) 5625: learn: 0.0566625 test: 0.1374440 best: 0.1374361 (5618) 5626: learn: 0.0566547 test: 0.1374428 best: 0.1374361 (5618) 5627: learn: 0.0566473 test: 0.1374399 best: 0.1374361 (5618) 5628: learn: 0.0566389 test: 0.1374381 best: 0.1374361 (5618) 5629: learn: 0.0566320 test: 0.1374366 best: 0.1374361 (5618) 5630: learn: 0.0566294 test: 0.1374370 best: 0.1374361 (5618) 5631: learn: 0.0566229 test: 0.1374387 best: 0.1374361 (5618) 5632: learn: 0.0566186 test: 0.1374392 best: 0.1374361 (5618) 5633: learn: 0.0566116 test: 0.1374406 best: 0.1374361 (5618) 5634: learn: 0.0566050 test: 0.1374406 best: 0.1374361 (5618) 5635: learn: 0.0566015 test: 0.1374399 best: 0.1374361 (5618) 5636: learn: 0.0565932 test: 0.1374398 best: 0.1374361 (5618) 5637: learn: 0.0565865 test: 0.1374397 best: 0.1374361 (5618) 5638: learn: 0.0565769 test: 0.1374350 best: 0.1374350 (5638) 5639: learn: 0.0565705 test: 0.1374370 best: 0.1374350 (5638) 5640: learn: 0.0565663 test: 0.1374369 best: 0.1374350 (5638) 5641: learn: 0.0565612 test: 0.1374379 best: 0.1374350 (5638) 5642: learn: 0.0565543 test: 0.1374354 best: 0.1374350 (5638) 5643: learn: 0.0565480 test: 0.1374338 best: 0.1374338 (5643) 5644: learn: 0.0565408 test: 0.1374329 best: 0.1374329 (5644) 5645: learn: 0.0565346 test: 0.1374342 best: 0.1374329 (5644) 5646: learn: 0.0565255 test: 0.1374345 best: 0.1374329 (5644) 5647: learn: 0.0565179 test: 0.1374342 best: 0.1374329 (5644) 5648: learn: 0.0565126 test: 0.1374333 best: 0.1374329 (5644) 5649: learn: 0.0565056 test: 0.1374297 best: 0.1374297 (5649) 5650: learn: 0.0564999 test: 0.1374284 best: 0.1374284 (5650) 5651: learn: 0.0564921 test: 0.1374265 best: 0.1374265 (5651) 5652: learn: 0.0564861 test: 0.1374290 best: 0.1374265 (5651) 5653: learn: 0.0564813 test: 0.1374298 best: 0.1374265 (5651) 5654: learn: 0.0564747 test: 0.1374263 best: 0.1374263 (5654) 5655: learn: 0.0564683 test: 0.1374281 best: 0.1374263 (5654) 5656: learn: 0.0564623 test: 0.1374281 best: 0.1374263 (5654) 5657: learn: 0.0564562 test: 0.1374265 best: 0.1374263 (5654) 5658: learn: 0.0564477 test: 0.1374239 best: 0.1374239 (5658) 5659: learn: 0.0564389 test: 0.1374244 best: 0.1374239 (5658) 5660: learn: 0.0564313 test: 0.1374230 best: 0.1374230 (5660) 5661: learn: 0.0564282 test: 0.1374213 best: 0.1374213 (5661) 5662: learn: 0.0564212 test: 0.1374194 best: 0.1374194 (5662) 5663: learn: 0.0564138 test: 0.1374185 best: 0.1374185 (5663) 5664: learn: 0.0564046 test: 0.1374195 best: 0.1374185 (5663) 5665: learn: 0.0563986 test: 0.1374190 best: 0.1374185 (5663) 5666: learn: 0.0563890 test: 0.1374204 best: 0.1374185 (5663) 5667: learn: 0.0563839 test: 0.1374174 best: 0.1374174 (5667) 5668: learn: 0.0563764 test: 0.1374168 best: 0.1374168 (5668) 5669: learn: 0.0563723 test: 0.1374166 best: 0.1374166 (5669) 5670: learn: 0.0563662 test: 0.1374143 best: 0.1374143 (5670) 5671: learn: 0.0563620 test: 0.1374138 best: 0.1374138 (5671) 5672: learn: 0.0563535 test: 0.1374125 best: 0.1374125 (5672) total: 1m 4s remaining: 3.74s 5673: learn: 0.0563459 test: 0.1374102 best: 0.1374102 (5673) 5674: learn: 0.0563410 test: 0.1374111 best: 0.1374102 (5673) 5675: learn: 0.0563364 test: 0.1374087 best: 0.1374087 (5675) 5676: learn: 0.0563311 test: 0.1374073 best: 0.1374073 (5676) 5677: learn: 0.0563249 test: 0.1374076 best: 0.1374073 (5676) 5678: learn: 0.0563181 test: 0.1374046 best: 0.1374046 (5678) 5679: learn: 0.0563136 test: 0.1374050 best: 0.1374046 (5678) 5680: learn: 0.0563051 test: 0.1374035 best: 0.1374035 (5680) 5681: learn: 0.0562998 test: 0.1374040 best: 0.1374035 (5680) 5682: learn: 0.0562938 test: 0.1374037 best: 0.1374035 (5680) 5683: learn: 0.0562847 test: 0.1374017 best: 0.1374017 (5683) 5684: learn: 0.0562764 test: 0.1374013 best: 0.1374013 (5684) 5685: learn: 0.0562678 test: 0.1374011 best: 0.1374011 (5685) 5686: learn: 0.0562598 test: 0.1373980 best: 0.1373980 (5686) 5687: learn: 0.0562513 test: 0.1373972 best: 0.1373972 (5687) 5688: learn: 0.0562456 test: 0.1373943 best: 0.1373943 (5688) 5689: learn: 0.0562408 test: 0.1373923 best: 0.1373923 (5689) 5690: learn: 0.0562317 test: 0.1373937 best: 0.1373923 (5689) 5691: learn: 0.0562270 test: 0.1373922 best: 0.1373922 (5691) 5692: learn: 0.0562200 test: 0.1373914 best: 0.1373914 (5692) 5693: learn: 0.0562139 test: 0.1373897 best: 0.1373897 (5693) 5694: learn: 0.0562106 test: 0.1373895 best: 0.1373895 (5694) 5695: learn: 0.0562026 test: 0.1373903 best: 0.1373895 (5694) 5696: learn: 0.0561939 test: 0.1373930 best: 0.1373895 (5694) 5697: learn: 0.0561859 test: 0.1373933 best: 0.1373895 (5694) 5698: learn: 0.0561777 test: 0.1373947 best: 0.1373895 (5694) 5699: learn: 0.0561696 test: 0.1373911 best: 0.1373895 (5694) 5700: learn: 0.0561581 test: 0.1373913 best: 0.1373895 (5694) 5701: learn: 0.0561493 test: 0.1373938 best: 0.1373895 (5694) 5702: learn: 0.0561428 test: 0.1373954 best: 0.1373895 (5694) 5703: learn: 0.0561379 test: 0.1373935 best: 0.1373895 (5694) 5704: learn: 0.0561308 test: 0.1373906 best: 0.1373895 (5694) 5705: learn: 0.0561240 test: 0.1373903 best: 0.1373895 (5694) 5706: learn: 0.0561167 test: 0.1373897 best: 0.1373895 (5694) 5707: learn: 0.0561092 test: 0.1373916 best: 0.1373895 (5694) 5708: learn: 0.0561046 test: 0.1373924 best: 0.1373895 (5694) 5709: learn: 0.0561002 test: 0.1373917 best: 0.1373895 (5694) 5710: learn: 0.0560941 test: 0.1373909 best: 0.1373895 (5694) 5711: learn: 0.0560860 test: 0.1373911 best: 0.1373895 (5694) 5712: learn: 0.0560829 test: 0.1373904 best: 0.1373895 (5694) 5713: learn: 0.0560759 test: 0.1373917 best: 0.1373895 (5694) 5714: learn: 0.0560664 test: 0.1373929 best: 0.1373895 (5694) 5715: learn: 0.0560604 test: 0.1373909 best: 0.1373895 (5694) 5716: learn: 0.0560550 test: 0.1373893 best: 0.1373893 (5716) 5717: learn: 0.0560468 test: 0.1373899 best: 0.1373893 (5716) 5718: learn: 0.0560394 test: 0.1373894 best: 0.1373893 (5716) 5719: learn: 0.0560320 test: 0.1373884 best: 0.1373884 (5719) 5720: learn: 0.0560285 test: 0.1373872 best: 0.1373872 (5720) 5721: learn: 0.0560234 test: 0.1373859 best: 0.1373859 (5721) 5722: learn: 0.0560162 test: 0.1373848 best: 0.1373848 (5722) 5723: learn: 0.0560098 test: 0.1373838 best: 0.1373838 (5723) 5724: learn: 0.0560023 test: 0.1373837 best: 0.1373837 (5724) 5725: learn: 0.0559945 test: 0.1373819 best: 0.1373819 (5725) 5726: learn: 0.0559872 test: 0.1373828 best: 0.1373819 (5725) 5727: learn: 0.0559804 test: 0.1373829 best: 0.1373819 (5725) 5728: learn: 0.0559713 test: 0.1373861 best: 0.1373819 (5725) 5729: learn: 0.0559649 test: 0.1373854 best: 0.1373819 (5725) 5730: learn: 0.0559585 test: 0.1373812 best: 0.1373812 (5730) 5731: learn: 0.0559519 test: 0.1373792 best: 0.1373792 (5731) 5732: learn: 0.0559448 test: 0.1373791 best: 0.1373791 (5732) 5733: learn: 0.0559431 test: 0.1373777 best: 0.1373777 (5733) 5734: learn: 0.0559356 test: 0.1373812 best: 0.1373777 (5733) 5735: learn: 0.0559311 test: 0.1373802 best: 0.1373777 (5733) 5736: learn: 0.0559269 test: 0.1373790 best: 0.1373777 (5733) 5737: learn: 0.0559206 test: 0.1373776 best: 0.1373776 (5737) 5738: learn: 0.0559120 test: 0.1373773 best: 0.1373773 (5738) 5739: learn: 0.0559043 test: 0.1373771 best: 0.1373771 (5739) 5740: learn: 0.0558960 test: 0.1373728 best: 0.1373728 (5740) 5741: learn: 0.0558891 test: 0.1373745 best: 0.1373728 (5740) 5742: learn: 0.0558828 test: 0.1373756 best: 0.1373728 (5740) 5743: learn: 0.0558752 test: 0.1373757 best: 0.1373728 (5740) 5744: learn: 0.0558671 test: 0.1373762 best: 0.1373728 (5740) 5745: learn: 0.0558579 test: 0.1373743 best: 0.1373728 (5740) 5746: learn: 0.0558518 test: 0.1373738 best: 0.1373728 (5740) 5747: learn: 0.0558444 test: 0.1373736 best: 0.1373728 (5740) 5748: learn: 0.0558397 test: 0.1373713 best: 0.1373713 (5748) 5749: learn: 0.0558328 test: 0.1373720 best: 0.1373713 (5748) 5750: learn: 0.0558287 test: 0.1373732 best: 0.1373713 (5748) 5751: learn: 0.0558214 test: 0.1373717 best: 0.1373713 (5748) 5752: learn: 0.0558119 test: 0.1373712 best: 0.1373712 (5752) 5753: learn: 0.0558047 test: 0.1373715 best: 0.1373712 (5752) 5754: learn: 0.0557995 test: 0.1373701 best: 0.1373701 (5754) 5755: learn: 0.0557933 test: 0.1373716 best: 0.1373701 (5754) 5756: learn: 0.0557871 test: 0.1373679 best: 0.1373679 (5756) 5757: learn: 0.0557805 test: 0.1373655 best: 0.1373655 (5757) 5758: learn: 0.0557730 test: 0.1373655 best: 0.1373655 (5757) 5759: learn: 0.0557673 test: 0.1373666 best: 0.1373655 (5757) 5760: learn: 0.0557616 test: 0.1373648 best: 0.1373648 (5760) 5761: learn: 0.0557551 test: 0.1373673 best: 0.1373648 (5760) 5762: learn: 0.0557463 test: 0.1373696 best: 0.1373648 (5760) 5763: learn: 0.0557402 test: 0.1373680 best: 0.1373648 (5760) 5764: learn: 0.0557332 test: 0.1373686 best: 0.1373648 (5760) 5765: learn: 0.0557266 test: 0.1373661 best: 0.1373648 (5760) 5766: learn: 0.0557194 test: 0.1373671 best: 0.1373648 (5760) 5767: learn: 0.0557135 test: 0.1373661 best: 0.1373648 (5760) 5768: learn: 0.0557043 test: 0.1373698 best: 0.1373648 (5760) 5769: learn: 0.0556950 test: 0.1373714 best: 0.1373648 (5760) 5770: learn: 0.0556885 test: 0.1373694 best: 0.1373648 (5760) 5771: learn: 0.0556818 test: 0.1373674 best: 0.1373648 (5760) 5772: learn: 0.0556767 test: 0.1373640 best: 0.1373640 (5772) 5773: learn: 0.0556683 test: 0.1373622 best: 0.1373622 (5773) 5774: learn: 0.0556620 test: 0.1373617 best: 0.1373617 (5774) 5775: learn: 0.0556548 test: 0.1373601 best: 0.1373601 (5775) 5776: learn: 0.0556478 test: 0.1373587 best: 0.1373587 (5776) 5777: learn: 0.0556424 test: 0.1373576 best: 0.1373576 (5777) 5778: learn: 0.0556375 test: 0.1373561 best: 0.1373561 (5778) 5779: learn: 0.0556325 test: 0.1373547 best: 0.1373547 (5779) 5780: learn: 0.0556251 test: 0.1373519 best: 0.1373519 (5780) 5781: learn: 0.0556175 test: 0.1373511 best: 0.1373511 (5781) 5782: learn: 0.0556115 test: 0.1373510 best: 0.1373510 (5782) 5783: learn: 0.0556068 test: 0.1373500 best: 0.1373500 (5783) 5784: learn: 0.0555989 test: 0.1373503 best: 0.1373500 (5783) 5785: learn: 0.0555907 test: 0.1373499 best: 0.1373499 (5785) 5786: learn: 0.0555843 test: 0.1373491 best: 0.1373491 (5786) 5787: learn: 0.0555771 test: 0.1373521 best: 0.1373491 (5786) 5788: learn: 0.0555707 test: 0.1373518 best: 0.1373491 (5786) 5789: learn: 0.0555634 test: 0.1373510 best: 0.1373491 (5786) 5790: learn: 0.0555595 test: 0.1373501 best: 0.1373491 (5786) 5791: learn: 0.0555524 test: 0.1373490 best: 0.1373490 (5791) 5792: learn: 0.0555481 test: 0.1373486 best: 0.1373486 (5792) 5793: learn: 0.0555395 test: 0.1373478 best: 0.1373478 (5793) 5794: learn: 0.0555304 test: 0.1373477 best: 0.1373477 (5794) 5795: learn: 0.0555254 test: 0.1373471 best: 0.1373471 (5795) 5796: learn: 0.0555185 test: 0.1373511 best: 0.1373471 (5795) 5797: learn: 0.0555106 test: 0.1373517 best: 0.1373471 (5795) 5798: learn: 0.0555025 test: 0.1373539 best: 0.1373471 (5795) 5799: learn: 0.0554954 test: 0.1373531 best: 0.1373471 (5795) 5800: learn: 0.0554872 test: 0.1373542 best: 0.1373471 (5795) 5801: learn: 0.0554802 test: 0.1373518 best: 0.1373471 (5795) 5802: learn: 0.0554732 test: 0.1373522 best: 0.1373471 (5795) 5803: learn: 0.0554694 test: 0.1373516 best: 0.1373471 (5795) 5804: learn: 0.0554616 test: 0.1373507 best: 0.1373471 (5795) 5805: learn: 0.0554554 test: 0.1373476 best: 0.1373471 (5795) 5806: learn: 0.0554474 test: 0.1373470 best: 0.1373470 (5806) 5807: learn: 0.0554440 test: 0.1373459 best: 0.1373459 (5807) 5808: learn: 0.0554366 test: 0.1373454 best: 0.1373454 (5808) 5809: learn: 0.0554292 test: 0.1373451 best: 0.1373451 (5809) 5810: learn: 0.0554212 test: 0.1373457 best: 0.1373451 (5809) 5811: learn: 0.0554114 test: 0.1373439 best: 0.1373439 (5811) 5812: learn: 0.0554040 test: 0.1373411 best: 0.1373411 (5812) 5813: learn: 0.0553949 test: 0.1373381 best: 0.1373381 (5813) 5814: learn: 0.0553854 test: 0.1373386 best: 0.1373381 (5813) 5815: learn: 0.0553792 test: 0.1373380 best: 0.1373380 (5815) 5816: learn: 0.0553732 test: 0.1373362 best: 0.1373362 (5816) 5817: learn: 0.0553662 test: 0.1373373 best: 0.1373362 (5816) 5818: learn: 0.0553590 test: 0.1373384 best: 0.1373362 (5816) 5819: learn: 0.0553551 test: 0.1373382 best: 0.1373362 (5816) total: 1m 6s remaining: 2.06s 5820: learn: 0.0553480 test: 0.1373364 best: 0.1373362 (5816) 5821: learn: 0.0553405 test: 0.1373347 best: 0.1373347 (5821) 5822: learn: 0.0553347 test: 0.1373346 best: 0.1373346 (5822) 5823: learn: 0.0553306 test: 0.1373330 best: 0.1373330 (5823) 5824: learn: 0.0553251 test: 0.1373310 best: 0.1373310 (5824) 5825: learn: 0.0553182 test: 0.1373297 best: 0.1373297 (5825) 5826: learn: 0.0553130 test: 0.1373303 best: 0.1373297 (5825) 5827: learn: 0.0553037 test: 0.1373296 best: 0.1373296 (5827) 5828: learn: 0.0553012 test: 0.1373295 best: 0.1373295 (5828) 5829: learn: 0.0552944 test: 0.1373287 best: 0.1373287 (5829) 5830: learn: 0.0552853 test: 0.1373264 best: 0.1373264 (5830) 5831: learn: 0.0552809 test: 0.1373276 best: 0.1373264 (5830) 5832: learn: 0.0552761 test: 0.1373248 best: 0.1373248 (5832) 5833: learn: 0.0552725 test: 0.1373248 best: 0.1373248 (5832) 5834: learn: 0.0552668 test: 0.1373242 best: 0.1373242 (5834) 5835: learn: 0.0552586 test: 0.1373231 best: 0.1373231 (5835) 5836: learn: 0.0552527 test: 0.1373231 best: 0.1373231 (5836) 5837: learn: 0.0552448 test: 0.1373213 best: 0.1373213 (5837) 5838: learn: 0.0552415 test: 0.1373198 best: 0.1373198 (5838) 5839: learn: 0.0552336 test: 0.1373200 best: 0.1373198 (5838) 5840: learn: 0.0552302 test: 0.1373165 best: 0.1373165 (5840) 5841: learn: 0.0552236 test: 0.1373171 best: 0.1373165 (5840) 5842: learn: 0.0552173 test: 0.1373143 best: 0.1373143 (5842) 5843: learn: 0.0552101 test: 0.1373152 best: 0.1373143 (5842) 5844: learn: 0.0552035 test: 0.1373145 best: 0.1373143 (5842) 5845: learn: 0.0551978 test: 0.1373163 best: 0.1373143 (5842) 5846: learn: 0.0551916 test: 0.1373167 best: 0.1373143 (5842) 5847: learn: 0.0551847 test: 0.1373155 best: 0.1373143 (5842) 5848: learn: 0.0551784 test: 0.1373156 best: 0.1373143 (5842) 5849: learn: 0.0551723 test: 0.1373172 best: 0.1373143 (5842) 5850: learn: 0.0551659 test: 0.1373188 best: 0.1373143 (5842) 5851: learn: 0.0551599 test: 0.1373176 best: 0.1373143 (5842) 5852: learn: 0.0551541 test: 0.1373158 best: 0.1373143 (5842) 5853: learn: 0.0551450 test: 0.1373146 best: 0.1373143 (5842) 5854: learn: 0.0551385 test: 0.1373138 best: 0.1373138 (5854) 5855: learn: 0.0551329 test: 0.1373131 best: 0.1373131 (5855) 5856: learn: 0.0551253 test: 0.1373116 best: 0.1373116 (5856) 5857: learn: 0.0551195 test: 0.1373115 best: 0.1373115 (5857) 5858: learn: 0.0551104 test: 0.1373114 best: 0.1373114 (5858) 5859: learn: 0.0551040 test: 0.1373135 best: 0.1373114 (5858) 5860: learn: 0.0550972 test: 0.1373126 best: 0.1373114 (5858) 5861: learn: 0.0550921 test: 0.1373131 best: 0.1373114 (5858) 5862: learn: 0.0550860 test: 0.1373130 best: 0.1373114 (5858) 5863: learn: 0.0550789 test: 0.1373133 best: 0.1373114 (5858) 5864: learn: 0.0550740 test: 0.1373133 best: 0.1373114 (5858) 5865: learn: 0.0550681 test: 0.1373164 best: 0.1373114 (5858) 5866: learn: 0.0550612 test: 0.1373149 best: 0.1373114 (5858) 5867: learn: 0.0550544 test: 0.1373154 best: 0.1373114 (5858) 5868: learn: 0.0550485 test: 0.1373140 best: 0.1373114 (5858) 5869: learn: 0.0550404 test: 0.1373140 best: 0.1373114 (5858) 5870: learn: 0.0550353 test: 0.1373118 best: 0.1373114 (5858) 5871: learn: 0.0550282 test: 0.1373106 best: 0.1373106 (5871) 5872: learn: 0.0550198 test: 0.1373102 best: 0.1373102 (5872) 5873: learn: 0.0550105 test: 0.1373126 best: 0.1373102 (5872) 5874: learn: 0.0550045 test: 0.1373131 best: 0.1373102 (5872) 5875: learn: 0.0549975 test: 0.1373133 best: 0.1373102 (5872) 5876: learn: 0.0549914 test: 0.1373121 best: 0.1373102 (5872) 5877: learn: 0.0549823 test: 0.1373133 best: 0.1373102 (5872) 5878: learn: 0.0549758 test: 0.1373103 best: 0.1373102 (5872) 5879: learn: 0.0549680 test: 0.1373098 best: 0.1373098 (5879) 5880: learn: 0.0549630 test: 0.1373087 best: 0.1373087 (5880) 5881: learn: 0.0549543 test: 0.1373081 best: 0.1373081 (5881) 5882: learn: 0.0549457 test: 0.1373046 best: 0.1373046 (5882) 5883: learn: 0.0549395 test: 0.1373066 best: 0.1373046 (5882) 5884: learn: 0.0549340 test: 0.1373054 best: 0.1373046 (5882) 5885: learn: 0.0549281 test: 0.1373034 best: 0.1373034 (5885) 5886: learn: 0.0549217 test: 0.1373042 best: 0.1373034 (5885) 5887: learn: 0.0549163 test: 0.1373021 best: 0.1373021 (5887) 5888: learn: 0.0549134 test: 0.1373005 best: 0.1373005 (5888) 5889: learn: 0.0549099 test: 0.1372980 best: 0.1372980 (5889) 5890: learn: 0.0549033 test: 0.1372968 best: 0.1372968 (5890) 5891: learn: 0.0548969 test: 0.1372964 best: 0.1372964 (5891) 5892: learn: 0.0548918 test: 0.1372975 best: 0.1372964 (5891) 5893: learn: 0.0548839 test: 0.1373015 best: 0.1372964 (5891) 5894: learn: 0.0548764 test: 0.1373014 best: 0.1372964 (5891) 5895: learn: 0.0548704 test: 0.1372997 best: 0.1372964 (5891) 5896: learn: 0.0548652 test: 0.1372982 best: 0.1372964 (5891) 5897: learn: 0.0548566 test: 0.1372972 best: 0.1372964 (5891) 5898: learn: 0.0548506 test: 0.1372976 best: 0.1372964 (5891) 5899: learn: 0.0548432 test: 0.1373006 best: 0.1372964 (5891) 5900: learn: 0.0548364 test: 0.1372997 best: 0.1372964 (5891) 5901: learn: 0.0548289 test: 0.1372986 best: 0.1372964 (5891) 5902: learn: 0.0548225 test: 0.1373007 best: 0.1372964 (5891) 5903: learn: 0.0548188 test: 0.1372998 best: 0.1372964 (5891) 5904: learn: 0.0548115 test: 0.1372973 best: 0.1372964 (5891) 5905: learn: 0.0548056 test: 0.1372976 best: 0.1372964 (5891) 5906: learn: 0.0548000 test: 0.1372939 best: 0.1372939 (5906) 5907: learn: 0.0547940 test: 0.1372947 best: 0.1372939 (5906) 5908: learn: 0.0547869 test: 0.1372944 best: 0.1372939 (5906) 5909: learn: 0.0547853 test: 0.1372921 best: 0.1372921 (5909) 5910: learn: 0.0547825 test: 0.1372913 best: 0.1372913 (5910) 5911: learn: 0.0547738 test: 0.1372890 best: 0.1372890 (5911) 5912: learn: 0.0547678 test: 0.1372883 best: 0.1372883 (5912) 5913: learn: 0.0547581 test: 0.1372865 best: 0.1372865 (5913) 5914: learn: 0.0547546 test: 0.1372845 best: 0.1372845 (5914) 5915: learn: 0.0547464 test: 0.1372865 best: 0.1372845 (5914) 5916: learn: 0.0547418 test: 0.1372847 best: 0.1372845 (5914) 5917: learn: 0.0547332 test: 0.1372850 best: 0.1372845 (5914) 5918: learn: 0.0547252 test: 0.1372860 best: 0.1372845 (5914) 5919: learn: 0.0547171 test: 0.1372875 best: 0.1372845 (5914) 5920: learn: 0.0547133 test: 0.1372869 best: 0.1372845 (5914) 5921: learn: 0.0547071 test: 0.1372868 best: 0.1372845 (5914) 5922: learn: 0.0546984 test: 0.1372862 best: 0.1372845 (5914) 5923: learn: 0.0546913 test: 0.1372897 best: 0.1372845 (5914) total: 1m 7s remaining: 869ms 5924: learn: 0.0546849 test: 0.1372901 best: 0.1372845 (5914) 5925: learn: 0.0546789 test: 0.1372895 best: 0.1372845 (5914) 5926: learn: 0.0546711 test: 0.1372901 best: 0.1372845 (5914) 5927: learn: 0.0546640 test: 0.1372868 best: 0.1372845 (5914) 5928: learn: 0.0546557 test: 0.1372845 best: 0.1372845 (5914) 5929: learn: 0.0546512 test: 0.1372838 best: 0.1372838 (5929) 5930: learn: 0.0546489 test: 0.1372852 best: 0.1372838 (5929) 5931: learn: 0.0546440 test: 0.1372845 best: 0.1372838 (5929) 5932: learn: 0.0546397 test: 0.1372845 best: 0.1372838 (5929) 5933: learn: 0.0546332 test: 0.1372845 best: 0.1372838 (5929) 5934: learn: 0.0546237 test: 0.1372841 best: 0.1372838 (5929) 5935: learn: 0.0546188 test: 0.1372828 best: 0.1372828 (5935) 5936: learn: 0.0546137 test: 0.1372815 best: 0.1372815 (5936) 5937: learn: 0.0546068 test: 0.1372808 best: 0.1372808 (5937) 5938: learn: 0.0545984 test: 0.1372839 best: 0.1372808 (5937) 5939: learn: 0.0545917 test: 0.1372846 best: 0.1372808 (5937) 5940: learn: 0.0545848 test: 0.1372847 best: 0.1372808 (5937) 5941: learn: 0.0545764 test: 0.1372852 best: 0.1372808 (5937) 5942: learn: 0.0545702 test: 0.1372852 best: 0.1372808 (5937) 5943: learn: 0.0545608 test: 0.1372848 best: 0.1372808 (5937) 5944: learn: 0.0545542 test: 0.1372847 best: 0.1372808 (5937) 5945: learn: 0.0545457 test: 0.1372797 best: 0.1372797 (5945) 5946: learn: 0.0545383 test: 0.1372789 best: 0.1372789 (5946) 5947: learn: 0.0545338 test: 0.1372760 best: 0.1372760 (5947) 5948: learn: 0.0545255 test: 0.1372769 best: 0.1372760 (5947) 5949: learn: 0.0545180 test: 0.1372778 best: 0.1372760 (5947) 5950: learn: 0.0545117 test: 0.1372777 best: 0.1372760 (5947) 5951: learn: 0.0545035 test: 0.1372756 best: 0.1372756 (5951) 5952: learn: 0.0544944 test: 0.1372761 best: 0.1372756 (5951) 5953: learn: 0.0544879 test: 0.1372779 best: 0.1372756 (5951) 5954: learn: 0.0544814 test: 0.1372785 best: 0.1372756 (5951) 5955: learn: 0.0544701 test: 0.1372798 best: 0.1372756 (5951) 5956: learn: 0.0544661 test: 0.1372808 best: 0.1372756 (5951) 5957: learn: 0.0544595 test: 0.1372813 best: 0.1372756 (5951) 5958: learn: 0.0544528 test: 0.1372843 best: 0.1372756 (5951) 5959: learn: 0.0544465 test: 0.1372848 best: 0.1372756 (5951) 5960: learn: 0.0544429 test: 0.1372842 best: 0.1372756 (5951) 5961: learn: 0.0544368 test: 0.1372847 best: 0.1372756 (5951) 5962: learn: 0.0544304 test: 0.1372816 best: 0.1372756 (5951) 5963: learn: 0.0544261 test: 0.1372813 best: 0.1372756 (5951) 5964: learn: 0.0544201 test: 0.1372829 best: 0.1372756 (5951) 5965: learn: 0.0544142 test: 0.1372820 best: 0.1372756 (5951) 5966: learn: 0.0544074 test: 0.1372803 best: 0.1372756 (5951) 5967: learn: 0.0543981 test: 0.1372807 best: 0.1372756 (5951) total: 1m 8s remaining: 366ms 5968: learn: 0.0543887 test: 0.1372806 best: 0.1372756 (5951) 5969: learn: 0.0543839 test: 0.1372805 best: 0.1372756 (5951) 5970: learn: 0.0543811 test: 0.1372832 best: 0.1372756 (5951) 5971: learn: 0.0543739 test: 0.1372823 best: 0.1372756 (5951) 5972: learn: 0.0543659 test: 0.1372786 best: 0.1372756 (5951) 5973: learn: 0.0543612 test: 0.1372764 best: 0.1372756 (5951) 5974: learn: 0.0543535 test: 0.1372766 best: 0.1372756 (5951) 5975: learn: 0.0543462 test: 0.1372757 best: 0.1372756 (5951) 5976: learn: 0.0543393 test: 0.1372765 best: 0.1372756 (5951) 5977: learn: 0.0543323 test: 0.1372789 best: 0.1372756 (5951) 5978: learn: 0.0543239 test: 0.1372793 best: 0.1372756 (5951) 5979: learn: 0.0543161 test: 0.1372784 best: 0.1372756 (5951) 5980: learn: 0.0543092 test: 0.1372760 best: 0.1372756 (5951) 5981: learn: 0.0543059 test: 0.1372729 best: 0.1372729 (5981) 5982: learn: 0.0542984 test: 0.1372737 best: 0.1372729 (5981) 5983: learn: 0.0542930 test: 0.1372727 best: 0.1372727 (5983) 5984: learn: 0.0542878 test: 0.1372695 best: 0.1372695 (5984) 5985: learn: 0.0542830 test: 0.1372677 best: 0.1372677 (5985) 5986: learn: 0.0542751 test: 0.1372707 best: 0.1372677 (5985) 5987: learn: 0.0542673 test: 0.1372686 best: 0.1372677 (5985) 5988: learn: 0.0542579 test: 0.1372692 best: 0.1372677 (5985) 5989: learn: 0.0542498 test: 0.1372689 best: 0.1372677 (5985) 5990: learn: 0.0542436 test: 0.1372677 best: 0.1372677 (5985) 5991: learn: 0.0542347 test: 0.1372675 best: 0.1372675 (5991) 5992: learn: 0.0542284 test: 0.1372677 best: 0.1372675 (5991) 5993: learn: 0.0542251 test: 0.1372652 best: 0.1372652 (5993) 5994: learn: 0.0542230 test: 0.1372633 best: 0.1372633 (5994) 5995: learn: 0.0542159 test: 0.1372622 best: 0.1372622 (5995) 5996: learn: 0.0542080 test: 0.1372594 best: 0.1372594 (5996) 5997: learn: 0.0541991 test: 0.1372590 best: 0.1372590 (5997) 5998: learn: 0.0541947 test: 0.1372590 best: 0.1372590 (5997) 5999: learn: 0.0541875 test: 0.1372628 best: 0.1372590 (5997) total: 1m 8s remaining: 0us . params = {&#39;iterations&#39;: 6000, &#39;learning_rate&#39;: 0.005, &#39;depth&#39;: 4, &#39;l2_leaf_reg&#39;: 1, &#39;eval_metric&#39;:&#39;RMSE&#39;, &#39;early_stopping_rounds&#39;: 200, &#39;verbose&#39;: 200, &#39;random_seed&#39;: 42} cat_f = CatBoostRegressor(**params) cat_model_f = cat_f.fit(X_train,y_train, eval_set = (X_val,y_val), plot=True, verbose = False) catf_pred = cat_model_f.predict(X_val) catf_score = rmse(y_val, catf_pred) .",
            "url": "https://raukrauk.github.io/ML-DL/ssuda/kaggle/machine%20learning/2021/09/18/House_Price_%ED%95%84%EC%82%AC.html",
            "relUrl": "/ssuda/kaggle/machine%20learning/2021/09/18/House_Price_%ED%95%84%EC%82%AC.html",
            "date": " • Sep 18, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "[BOAZ] CNN_assignment - 17기 손형락",
            "content": "Library Import . import numpy as np import tensorflow as tf from tensorflow.keras.layers import Input, Flatten, Dense, Conv2D, BatchNormalization, LeakyReLU, Dropout, Activation, ReLU, MaxPooling2D from tensorflow.keras.models import Model from tensorflow.keras.optimizers import Adam from tensorflow.keras.optimizers import SGD from tensorflow.keras.utils import to_categorical import tensorflow.keras.backend as K from tensorflow.keras.datasets import cifar10 . NUM_CLASSES = 10 . (X_train, y_train), (X_test, y_test) = cifar10.load_data() . Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz 170500096/170498071 [==============================] - 6s 0us/step 170508288/170498071 [==============================] - 6s 0us/step . Assignment 1: X_train, X_test, y_train, y_test의 shape과 형식을 출력해보세요. (각각 출력해도 됩니다!) | . print(&quot;X_train shape : &quot;, X_train.shape) print(&quot;X_test shape : &quot;, X_test.shape) print(&quot;y_train shape : &quot;, y_train.shape) print(&quot;y_test shape : &quot;, y_test.shape) . X_train shape : (50000, 32, 32, 3) X_test shape : (10000, 32, 32, 3) y_train shape : (50000, 1) y_test shape : (10000, 1) . # 신경망은 input이 -1~1 범위에서 가장 잘 동작하기 때문에 데이터를 255로 나눈다. X_train = X_train.astype(&#39;float32&#39;) / 255.0 X_test = X_test.astype(&#39;float32&#39;) / 255.0 . y_train = to_categorical(y_train, NUM_CLASSES) y_test = to_categorical(y_test, NUM_CLASSES) . Assignment 2: 변환된 y_train, y_test의 shape을 출력해보세요. | . print(&quot;y_train shape : &quot;, y_train.shape) print(&quot;y_test shape : &quot;, y_test.shape) . y_train shape : (50000, 10) y_test shape : (10000, 10) . Assignment 3: X_train에서, index 54의 이미지에서 (12,13) 위치에 해당하는 픽셀의 초록 채널(1) 값을 출력해보세요. (힌트: X_train[?,?,?,?]) | . X_train[54,12,13,1] . 0.36862746 . Modeling . Keras에서 신경망 구조를 정의하는 방법은 Sequential 모델과 함수형 API 두 가지가 있습니다. . Sequential 모델은 일렬로 층을 쌓은 네트워크를 빠르게 만들 때 사용하기 좋습니다. 즉 어떤 분기가 없이 이전 층이 그대로 다음 층으로 연결됩니다. 하지만 한 층의 출력이 여러 개의 별도의 층으로 전달되거나, 한 층이 여러 층으로부터 입력을 받는 등의 가지가 있는 네트워크를 만들기 위해서는 유연성이 많은 함수형 API를 사용하는 것이 좋습니다. Keras에서도 단순한 모델을 만들더라도 Sequential 모델보다 함수형 API를 권장합니다. 장기적으로 보면 모델이 점점 복잡한 구조가 될 수 있기 때문입니다. 함수형 API는 Deep Neural Network 구조를 설계하는 데에 있어 엄청난 자유를 제공합니다. . 따라서 본 assignment에서는 함수형 API를 사용하여 Modeling을 진행하였습니다. . input_layer = Input(shape=(32,32,3)) x = Conv2D(filters=32, kernel_size=3, strides=1, padding=&#39;same&#39;)(input_layer) x = BatchNormalization()(x) x = LeakyReLU()(x) x = Conv2D(filters=32, kernel_size=3, strides=2, padding=&#39;same&#39;)(x) x = LeakyReLU()(x) x = Conv2D(filters=64, kernel_size=3, strides=1, padding=&#39;same&#39;)(x) x = LeakyReLU()(x) x = Conv2D(filters=64, kernel_size=3, strides=2, padding=&#39;same&#39;)(x) x = LeakyReLU()(x) x = Flatten()(x) x = Dense(128)(x) x = LeakyReLU()(x) x = Dropout(rate=0.5)(x) x = Dense(NUM_CLASSES)(x) output_layer = Activation(&#39;softmax&#39;)(x) model = Model(input_layer, output_layer) . model.summary() . Model: &#34;model&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 32, 32, 3)] 0 _________________________________________________________________ conv2d (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ batch_normalization (BatchNo (None, 32, 32, 32) 128 _________________________________________________________________ leaky_re_lu (LeakyReLU) (None, 32, 32, 32) 0 _________________________________________________________________ conv2d_1 (Conv2D) (None, 16, 16, 32) 9248 _________________________________________________________________ leaky_re_lu_1 (LeakyReLU) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ leaky_re_lu_2 (LeakyReLU) (None, 16, 16, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 8, 8, 64) 36928 _________________________________________________________________ leaky_re_lu_3 (LeakyReLU) (None, 8, 8, 64) 0 _________________________________________________________________ flatten (Flatten) (None, 4096) 0 _________________________________________________________________ dense (Dense) (None, 128) 524416 _________________________________________________________________ leaky_re_lu_4 (LeakyReLU) (None, 128) 0 _________________________________________________________________ dropout (Dropout) (None, 128) 0 _________________________________________________________________ dense_1 (Dense) (None, 10) 1290 _________________________________________________________________ activation (Activation) (None, 10) 0 ================================================================= Total params: 591,402 Trainable params: 591,338 Non-trainable params: 64 _________________________________________________________________ . Train . model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=Adam(lr=0.0005), metrics=[&#39;accuracy&#39;]) . /usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead. &#34;The `lr` argument is deprecated, use `learning_rate` instead.&#34;) . Assignment 4: loss를 categorical_crossentropy로 설정한 이유는 무엇일까요? | 답: 특정 물체에 대해 맞다(1) 아니다(0)의 이진분류가 아닌 여러 가지의 것들을 분류하는 Multiclass Classification이므로 categorical_crossentropy 설정하였다. | 입력으로 들어온 값들을 출력층에서 softmax함수로 처리하여 각 class별로 확률값이 나오는데 확률이 낮을수록 loss가 커지고, 크면 loss가 낮다. | . BATCH_SIZE = 32 EPOCHS = 10 . model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, shuffle=True, validation_data=(X_test, y_test)) . Epoch 1/10 1563/1563 [==============================] - 126s 80ms/step - loss: 1.4686 - accuracy: 0.4771 - val_loss: 1.2742 - val_accuracy: 0.5435 Epoch 2/10 1563/1563 [==============================] - 123s 79ms/step - loss: 1.1058 - accuracy: 0.6147 - val_loss: 1.0209 - val_accuracy: 0.6522 Epoch 3/10 1563/1563 [==============================] - 126s 80ms/step - loss: 0.9768 - accuracy: 0.6595 - val_loss: 1.0363 - val_accuracy: 0.6468 Epoch 4/10 1563/1563 [==============================] - 122s 78ms/step - loss: 0.8893 - accuracy: 0.6888 - val_loss: 0.9643 - val_accuracy: 0.6619 Epoch 5/10 1563/1563 [==============================] - 126s 81ms/step - loss: 0.8192 - accuracy: 0.7128 - val_loss: 0.9293 - val_accuracy: 0.6801 Epoch 6/10 1563/1563 [==============================] - 128s 82ms/step - loss: 0.7722 - accuracy: 0.7285 - val_loss: 0.9058 - val_accuracy: 0.6875 Epoch 7/10 1563/1563 [==============================] - 128s 82ms/step - loss: 0.7207 - accuracy: 0.7473 - val_loss: 0.9308 - val_accuracy: 0.6850 Epoch 8/10 1563/1563 [==============================] - 128s 82ms/step - loss: 0.6793 - accuracy: 0.7610 - val_loss: 0.9644 - val_accuracy: 0.6796 Epoch 9/10 1563/1563 [==============================] - 129s 82ms/step - loss: 0.6393 - accuracy: 0.7751 - val_loss: 0.9523 - val_accuracy: 0.6814 Epoch 10/10 1563/1563 [==============================] - 125s 80ms/step - loss: 0.6074 - accuracy: 0.7852 - val_loss: 0.9293 - val_accuracy: 0.6967 . &lt;keras.callbacks.History at 0x7f18cf7fb710&gt; . model.layers[6].get_weights() . [array([[[[ 1.02993563e-01, 1.68986395e-02, -2.35061664e-02, ..., 2.45099366e-02, -1.31832939e-02, 3.58976796e-02], [ 4.76399949e-03, -3.80432419e-02, 3.73236015e-02, ..., -5.22712804e-02, 9.44375545e-02, -3.38581332e-04], [-3.50388922e-02, 1.55683747e-02, -1.13446608e-01, ..., 3.15690525e-02, 4.60819565e-02, -7.37078786e-02], ..., [ 4.31538187e-02, 4.77315299e-02, 2.04020236e-02, ..., 1.09359376e-01, -9.00737047e-02, 7.43126497e-02], [-7.39016607e-02, 5.80044575e-02, -7.46317133e-02, ..., 7.65801221e-02, -9.80091766e-02, 3.65623436e-03], [ 1.93389282e-02, -2.13231090e-02, 1.73931122e-02, ..., -1.10441931e-02, 6.27352446e-02, 6.31651953e-02]], [[-2.41224673e-02, 1.66741282e-01, -1.25196110e-02, ..., -1.15408294e-01, 1.14265524e-01, 1.51600018e-01], [ 9.24101621e-02, -7.98162445e-02, -1.17908038e-01, ..., 3.79995406e-02, 5.85781559e-02, 4.84003089e-02], [-9.21615306e-03, 1.39684796e-01, 8.42941478e-02, ..., -1.24531426e-01, -8.41253623e-02, 1.76590122e-02], ..., [ 1.26724586e-01, -8.31582863e-03, 6.23508990e-02, ..., -8.70531946e-02, 4.97864522e-02, 6.76477980e-03], [-5.82547225e-02, -4.57511283e-02, -4.46860902e-02, ..., -9.96176302e-02, -3.90417129e-02, -4.40130718e-02], [ 1.08231269e-01, -4.61809114e-02, -1.07329883e-01, ..., 2.36179054e-04, 1.50818124e-01, 9.09932479e-02]], [[-5.87937124e-02, 6.08157180e-02, -6.47349730e-02, ..., -5.95355295e-02, 3.70826088e-02, 5.88752590e-02], [-1.93341821e-02, 2.97656134e-02, -1.93958357e-02, ..., -2.51176339e-02, -1.73121502e-04, -9.05075446e-02], [-2.99716350e-02, 2.02616714e-02, -4.28465754e-02, ..., 4.61334996e-02, -9.16699991e-02, 3.66518572e-02], ..., [ 8.87458920e-02, 2.72214655e-02, -8.02991167e-02, ..., 4.51493217e-03, 7.54410475e-02, 9.66548845e-02], [ 1.51703376e-02, -1.65516630e-01, -2.28246655e-02, ..., 5.70494235e-02, 9.25088488e-03, 7.51073509e-02], [ 9.15668309e-02, -1.65644124e-01, 1.94898732e-02, ..., -9.85167250e-02, -1.88267250e-02, -3.89020890e-02]]], [[[-5.14357425e-02, -3.94212380e-02, -1.11651547e-01, ..., 1.12782009e-01, 8.12118500e-02, 4.16137092e-02], [-5.42055443e-02, -3.16030793e-02, 3.31691094e-02, ..., 4.92927730e-02, 1.94143225e-02, 1.00128904e-01], [ 6.55785874e-02, 6.69757500e-02, -1.13995967e-03, ..., -2.31385883e-02, 1.14942426e-02, -7.35568628e-02], ..., [-2.95250081e-02, 5.79540357e-02, -4.62911837e-02, ..., -4.55115214e-02, -1.41485155e-01, 7.17825368e-02], [ 5.81375360e-02, 4.99973632e-02, 6.65043145e-02, ..., -3.31985988e-02, -1.67644396e-02, 2.53436454e-02], [ 9.53757092e-02, -3.05540655e-02, 4.04361226e-02, ..., -1.69663057e-02, 3.67480256e-02, 2.80912798e-02]], [[ 3.22931707e-02, 1.48611084e-01, 1.10536911e-01, ..., -5.31109571e-02, 4.54818550e-03, 7.73752853e-03], [-3.67736369e-02, 4.94462252e-02, -5.25301788e-03, ..., 5.77935725e-02, -1.37001961e-01, -9.77522656e-02], [ 1.20281354e-01, 9.86293927e-02, -1.62862260e-02, ..., 6.07445948e-02, 8.00729319e-02, -6.32392690e-02], ..., [-7.99681433e-03, 3.44072655e-02, 5.01954788e-03, ..., 8.27329457e-02, -4.56041098e-02, 9.46947783e-02], [ 5.38688339e-02, -7.70247951e-02, 3.83565240e-02, ..., -1.40362769e-01, 1.37969732e-01, 1.08361021e-01], [ 6.37254566e-02, -6.22862279e-02, 4.06824984e-02, ..., 2.12405995e-02, 1.64155997e-02, 5.21868235e-04]], [[-3.23167220e-02, -1.54025862e-02, 1.52952680e-02, ..., 2.26543490e-02, 4.42544483e-02, -2.86810077e-03], [-8.51637721e-02, 1.37528881e-01, -6.89380011e-03, ..., -4.83051650e-02, -1.23119876e-01, 6.91695511e-02], [ 9.55659226e-02, -2.33933181e-02, -4.61889766e-02, ..., 1.11191727e-01, -1.77515540e-02, 8.31896719e-03], ..., [ 1.52893931e-01, -6.85836375e-02, -5.76596297e-02, ..., -3.84736657e-02, -6.25258535e-02, 8.77485722e-02], [ 1.28602177e-01, -1.85981989e-02, -1.62874907e-01, ..., 1.34006411e-01, -3.54333594e-02, -8.37618560e-02], [-4.62604277e-02, -2.93599311e-02, 3.59457619e-02, ..., 2.56524403e-02, -1.11942831e-03, 2.59690955e-02]]], [[[ 1.09946942e-02, -3.30493320e-03, 7.80983269e-02, ..., 2.40796320e-02, 1.88834164e-02, 2.73712128e-02], [ 8.10671449e-02, -1.07273437e-01, 1.87123835e-01, ..., 1.10040363e-02, -9.50134546e-02, -3.07901427e-02], [ 1.58130184e-01, -4.18112203e-02, -1.51140960e-02, ..., 2.85742767e-02, 2.39653662e-02, 4.56392486e-03], ..., [-6.51995242e-02, 6.82314858e-02, 1.38012040e-03, ..., 6.70181215e-02, -2.74193697e-02, -9.95451026e-03], [ 7.95096681e-02, 1.07494622e-01, -1.04864143e-01, ..., -1.60712183e-01, -2.89411116e-02, 8.08258876e-02], [-9.94835272e-02, -7.58632049e-02, -2.08420083e-02, ..., -5.19842505e-02, -9.20637995e-02, 1.07028084e-02]], [[-1.45042967e-02, 4.73570861e-02, 5.61839789e-02, ..., 9.86548048e-03, -5.45225069e-02, 1.15173571e-01], [ 3.50065678e-02, -5.54779433e-02, -2.56078131e-02, ..., -1.63263250e-02, 6.14389591e-03, 5.63474447e-02], [-4.68311347e-02, 1.35550246e-01, -6.67459890e-02, ..., 1.53890908e-01, -1.39688179e-02, -9.27559063e-02], ..., [-6.50519803e-02, -1.73980534e-01, 1.10826693e-01, ..., 1.73692718e-01, 3.28884721e-02, -2.11175494e-02], [ 7.04764500e-02, -1.03408337e-01, 6.57264665e-02, ..., 1.11076921e-01, -7.74565190e-02, -1.08204875e-02], [-1.26352072e-01, 6.28453819e-03, 9.58728492e-02, ..., 1.39453495e-02, 5.98711446e-02, 3.65390852e-02]], [[ 3.89258340e-02, 2.03303136e-02, -6.07079752e-02, ..., -6.15629880e-03, 8.21043563e-04, 3.00455820e-02], [-2.22864207e-02, -3.33360806e-02, -5.90347014e-02, ..., -8.40429142e-02, 6.93817139e-02, -1.86811406e-02], [-2.42123920e-02, 3.10379267e-02, -1.51371047e-01, ..., -1.24514513e-01, -5.49802743e-02, -1.80819687e-02], ..., [-1.23447273e-02, -1.05017349e-01, 1.03736147e-01, ..., -7.96944797e-02, 5.53098433e-02, -3.17067280e-02], [-3.97214890e-02, -1.79001078e-01, -9.20400321e-02, ..., -4.36789775e-03, -2.95934379e-02, -5.47334887e-02], [-7.93383829e-03, -9.58529115e-02, 8.97854641e-02, ..., -4.47729304e-02, -1.33101821e-01, 2.12891605e-02]]]], dtype=float32), array([-0.10433507, -0.04940714, -0.03527314, -0.15207024, -0.15927896, -0.03190419, -0.1004374 , -0.0589538 , -0.18236142, -0.11058657, -0.12578438, -0.09651061, -0.0983176 , -0.11464427, -0.16012084, -0.13739306, -0.08376803, -0.16504046, 0.02258816, -0.01237906, -0.12554996, -0.00740156, -0.04167728, -0.0118931 , -0.02345587, 0.06072985, -0.0546971 , -0.11418829, -0.0516886 , -0.03684666, -0.13153227, -0.02037417, -0.06172171, -0.03595609, -0.09555534, -0.08138073, -0.08652312, -0.0236791 , -0.097178 , -0.15321495, -0.01158412, -0.03402332, -0.09944332, 0.02315609, -0.10294089, -0.03749435, 0.03101602, -0.06628774, -0.09498091, -0.07641236, -0.08766613, -0.15231535, -0.00590586, -0.01078225, -0.09937983, 0.03555339, -0.11332794, -0.01666988, -0.04097625, -0.07423437, -0.01674258, -0.01301443, -0.08092259, -0.00999493], dtype=float32)] . Evaluate . model.evaluate(X_test, y_test, batch_size=1000) . 10/10 [==============================] - 5s 522ms/step - loss: 0.9293 - accuracy: 0.6967 . [0.9293103218078613, 0.6966999769210815] . [loss, accuracy] 가 출력된다. 결과를 시각화해 보자. . CLASSES = np.array([&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;,&#39;dog&#39;,&#39;frog&#39;,&#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;]) preds = model.predict(X_test) preds_single = CLASSES[np.argmax(preds, axis=-1)] actual_single = CLASSES[np.argmax(y_test, axis=-1)] . import matplotlib.pyplot as plt n_to_show = 10 indices = np.random.choice(range(len(X_test)), n_to_show) fig = plt.figure(figsize=(15, 3)) fig.subplots_adjust(hspace=0.5, wspace=0.5) for i, idx in enumerate(indices): img = X_test[idx] ax = fig.add_subplot(1, n_to_show, i+1) ax.axis(&#39;off&#39;) ax.text(0.5, -0.35, &#39;pred = &#39; + str(preds_single[idx]), fontsize=10, ha=&#39;center&#39;, transform=ax.transAxes) ax.text(0.5, -0.7, &#39;act = &#39; + str(actual_single[idx]), fontsize=10, ha=&#39;center&#39;, transform=ax.transAxes) ax.imshow(img) . - Assignment 5: model및 training 환경을 자유롭게 변경하여 Accuracy 75% 이상을 달성해 보세요! . # 다른 모델을 한번 참고해 보았습니다. from keras.models import Sequential from keras.layers import Conv2D, MaxPool2D, Dropout, Flatten, Dense model = Sequential() # Convolutional Block (Conv-Conv-Pool-Dropout) model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;, input_shape=(32, 32, 3))) model.add(Conv2D(32, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)) model.add(MaxPool2D(pool_size=(2, 2))) model.add(Dropout(0.25)) # Convolutional Block (Conv-Conv-Pool-Dropout) model.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)) model.add(Conv2D(64, (3, 3), activation=&#39;relu&#39;, padding=&#39;same&#39;)) model.add(MaxPool2D(pool_size=(2, 2))) model.add(Dropout(0.25)) model.add(Flatten()) model.add(Dense(512, activation=&#39;relu&#39;)) model.add(Dropout(0.5)) model.add(Dense(10, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . model.summary() . Model: &#34;sequential_9&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d_167 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ conv2d_168 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ max_pooling2d_66 (MaxPooling (None, 16, 16, 32) 0 _________________________________________________________________ dropout_120 (Dropout) (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_169 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ conv2d_170 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ max_pooling2d_67 (MaxPooling (None, 8, 8, 64) 0 _________________________________________________________________ dropout_121 (Dropout) (None, 8, 8, 64) 0 _________________________________________________________________ flatten_34 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_74 (Dense) (None, 512) 2097664 _________________________________________________________________ dropout_122 (Dropout) (None, 512) 0 _________________________________________________________________ dense_75 (Dense) (None, 10) 5130 ================================================================= Total params: 2,168,362 Trainable params: 2,168,362 Non-trainable params: 0 _________________________________________________________________ . hist = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 20 ,batch_size=32) . Epoch 1/20 1563/1563 [==============================] - 24s 15ms/step - loss: 1.4682 - accuracy: 0.4642 - val_loss: 1.1244 - val_accuracy: 0.5934 Epoch 2/20 1563/1563 [==============================] - 24s 15ms/step - loss: 1.0728 - accuracy: 0.6200 - val_loss: 0.9039 - val_accuracy: 0.6802 Epoch 3/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.9318 - accuracy: 0.6713 - val_loss: 0.8040 - val_accuracy: 0.7192 Epoch 4/20 1563/1563 [==============================] - 23s 15ms/step - loss: 0.8413 - accuracy: 0.7024 - val_loss: 0.8071 - val_accuracy: 0.7164 Epoch 5/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.7818 - accuracy: 0.7269 - val_loss: 0.7390 - val_accuracy: 0.7469 Epoch 6/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.7252 - accuracy: 0.7451 - val_loss: 0.7098 - val_accuracy: 0.7498 Epoch 7/20 1563/1563 [==============================] - 23s 15ms/step - loss: 0.6882 - accuracy: 0.7579 - val_loss: 0.7078 - val_accuracy: 0.7538 Epoch 8/20 1563/1563 [==============================] - 23s 15ms/step - loss: 0.6500 - accuracy: 0.7722 - val_loss: 0.7263 - val_accuracy: 0.7554 Epoch 9/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.6203 - accuracy: 0.7815 - val_loss: 0.6785 - val_accuracy: 0.7654 Epoch 10/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.5897 - accuracy: 0.7928 - val_loss: 0.6800 - val_accuracy: 0.7753 Epoch 11/20 1563/1563 [==============================] - 23s 15ms/step - loss: 0.5667 - accuracy: 0.7985 - val_loss: 0.7167 - val_accuracy: 0.7586 Epoch 12/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.5467 - accuracy: 0.8064 - val_loss: 0.6692 - val_accuracy: 0.7750 Epoch 13/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.5324 - accuracy: 0.8111 - val_loss: 0.6742 - val_accuracy: 0.7749 Epoch 14/20 1563/1563 [==============================] - 23s 14ms/step - loss: 0.5148 - accuracy: 0.8165 - val_loss: 0.6650 - val_accuracy: 0.7786 Epoch 15/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.4947 - accuracy: 0.8247 - val_loss: 0.6667 - val_accuracy: 0.7780 Epoch 16/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.4847 - accuracy: 0.8280 - val_loss: 0.7022 - val_accuracy: 0.7724 Epoch 17/20 1563/1563 [==============================] - 22s 14ms/step - loss: 0.4732 - accuracy: 0.8320 - val_loss: 0.7058 - val_accuracy: 0.7811 Epoch 18/20 1563/1563 [==============================] - 23s 15ms/step - loss: 0.4575 - accuracy: 0.8386 - val_loss: 0.6951 - val_accuracy: 0.7795 Epoch 19/20 1563/1563 [==============================] - 23s 14ms/step - loss: 0.4434 - accuracy: 0.8440 - val_loss: 0.7001 - val_accuracy: 0.7773 Epoch 20/20 1563/1563 [==============================] - 23s 15ms/step - loss: 0.4408 - accuracy: 0.8438 - val_loss: 0.6683 - val_accuracy: 0.7896 . &#51221;&#54869;&#46020;&#50752; Loss &#48320;&#46041; &#49884;&#44033;&#54868; . import matplotlib.pyplot as plt fig = plt.figure(figsize = (8,8)) ax = fig.add_subplot() ax.plot(hist.history[&#39;accuracy&#39;], label = &quot;Train_Accuracy&quot;) ax.plot(hist.history[&#39;val_accuracy&#39;], label = &quot;Validation_Accuracy&quot;) ax.set_ylabel(&#39;Accuracy&#39;) ax.set_xlabel(&#39;Epoch&#39;) ax.set_title(&#39;Model Accuracy and Loss&#39;) ax.legend() ax2 = ax.twinx() ax2.plot(hist.history[&#39;loss&#39;], label = &quot;Train_Loss&quot;, color = &quot;red&quot;) ax2.plot(hist.history[&#39;val_loss&#39;], label = &quot;Validation_Loss&quot;, color = &quot;green&quot;) ax2.set_ylabel(&#39;Loss&#39;) ax2.legend() . &lt;matplotlib.legend.Legend at 0x7fcaeb2dc890&gt; . model.evaluate(X_test, y_test, batch_size=1000) . 10/10 [==============================] - 2s 49ms/step - loss: 0.5317 - accuracy: 0.8273 . [0.5317152738571167, 0.8273000121116638] . 참조모델에서는 80% 이상의 정확도를 보여준다!! | . My Model . 함수형 API로 진행 | 참조모델을 참고해 keras 함수형 API에 약간 변화를 주어 모델을 만들어 보았습니다. | . input_layer_n = Input(shape=(32,32,3)) x = Conv2D(filters=32, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;)(input_layer_n) x = Conv2D(filters=32, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) x = Dropout(0.25)(x) x = MaxPool2D(pool_size=(2, 2))(x) x = Conv2D(filters=64, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) x = Conv2D(filters=64, kernel_size=3, strides=1, activation=&#39;relu&#39;, padding=&#39;same&#39;)(x) x = Dropout(0.25)(x) x = MaxPool2D(pool_size=(2, 2))(x) x = Flatten()(x) x = Dense(256)(x) x = Activation(&#39;relu&#39;)(x) x = Dropout(rate=0.5)(x) x = Dense(NUM_CLASSES)(x) output_layer_n = Activation(&#39;softmax&#39;)(x) model_n = Model(input_layer_n, output_layer_n) . model_n.summary() . Model: &#34;model_28&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_37 (InputLayer) [(None, 32, 32, 3)] 0 _________________________________________________________________ conv2d_155 (Conv2D) (None, 32, 32, 32) 896 _________________________________________________________________ conv2d_156 (Conv2D) (None, 32, 32, 32) 9248 _________________________________________________________________ dropout_111 (Dropout) (None, 32, 32, 32) 0 _________________________________________________________________ max_pooling2d_60 (MaxPooling (None, 16, 16, 32) 0 _________________________________________________________________ conv2d_157 (Conv2D) (None, 16, 16, 64) 18496 _________________________________________________________________ conv2d_158 (Conv2D) (None, 16, 16, 64) 36928 _________________________________________________________________ dropout_112 (Dropout) (None, 16, 16, 64) 0 _________________________________________________________________ max_pooling2d_61 (MaxPooling (None, 8, 8, 64) 0 _________________________________________________________________ flatten_31 (Flatten) (None, 4096) 0 _________________________________________________________________ dense_68 (Dense) (None, 256) 1048832 _________________________________________________________________ activation_96 (Activation) (None, 256) 0 _________________________________________________________________ dropout_113 (Dropout) (None, 256) 0 _________________________________________________________________ dense_69 (Dense) (None, 10) 2570 _________________________________________________________________ activation_97 (Activation) (None, 10) 0 ================================================================= Total params: 1,116,970 Trainable params: 1,116,970 Non-trainable params: 0 _________________________________________________________________ . model_n.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=Adam(learning_rate = 0.001), metrics=[&#39;accuracy&#39;]) . history = model_n.fit(X_train, y_train, batch_size = 32, epochs=10, shuffle=True, validation_data=(X_test, y_test)) . Epoch 1/10 1563/1563 [==============================] - 23s 14ms/step - loss: 1.5080 - accuracy: 0.4493 - val_loss: 1.1746 - val_accuracy: 0.5947 Epoch 2/10 1563/1563 [==============================] - 22s 14ms/step - loss: 1.0817 - accuracy: 0.6176 - val_loss: 0.9353 - val_accuracy: 0.6759 Epoch 3/10 1563/1563 [==============================] - 22s 14ms/step - loss: 0.9008 - accuracy: 0.6842 - val_loss: 0.7994 - val_accuracy: 0.7256 Epoch 4/10 1563/1563 [==============================] - 22s 14ms/step - loss: 0.8085 - accuracy: 0.7172 - val_loss: 0.7787 - val_accuracy: 0.7380 Epoch 5/10 1563/1563 [==============================] - 22s 14ms/step - loss: 0.7230 - accuracy: 0.7458 - val_loss: 0.7673 - val_accuracy: 0.7334 Epoch 6/10 1563/1563 [==============================] - 22s 14ms/step - loss: 0.6798 - accuracy: 0.7605 - val_loss: 0.7378 - val_accuracy: 0.7412 Epoch 7/10 1563/1563 [==============================] - 21s 14ms/step - loss: 0.6359 - accuracy: 0.7766 - val_loss: 0.7173 - val_accuracy: 0.7513 Epoch 8/10 1563/1563 [==============================] - 22s 14ms/step - loss: 0.5932 - accuracy: 0.7903 - val_loss: 0.7045 - val_accuracy: 0.7575 Epoch 9/10 1563/1563 [==============================] - 22s 14ms/step - loss: 0.5521 - accuracy: 0.8053 - val_loss: 0.7750 - val_accuracy: 0.7324 Epoch 10/10 1563/1563 [==============================] - 22s 14ms/step - loss: 0.5323 - accuracy: 0.8115 - val_loss: 0.6717 - val_accuracy: 0.7682 . 76.82%&#51032; &#51221;&#54869;&#46020;&#47484; &#48372;&#50668;&#51456;&#45796; ~~ Good . model_n.evaluate(X_test, y_test, batch_size=1000) . 10/10 [==============================] - 2s 46ms/step - loss: 0.6717 - accuracy: 0.7682 . [0.6717145442962646, 0.7681999802589417] . &#51221;&#54869;&#46020;&#50752; Loss &#48320;&#46041; &#49884;&#44033;&#54868; . import matplotlib.pyplot as plt fig = plt.figure(figsize = (8,8)) ax = fig.add_subplot() ax.plot(history.history[&#39;accuracy&#39;], label = &quot;Train_Accuracy&quot;) ax.plot(history.history[&#39;val_accuracy&#39;], label = &quot;Validation_Accuracy&quot;) ax.set_ylabel(&#39;Accuracy&#39;) ax.set_xlabel(&#39;Epoch&#39;) ax.set_title(&#39;Model Accuracy and Loss&#39;) ax.legend() ax2 = ax.twinx() ax2.plot(history.history[&#39;loss&#39;], label = &quot;Train_Loss&quot;, color = &quot;red&quot;) ax2.plot(history.history[&#39;val_loss&#39;], label = &quot;Validation_Loss&quot;, color = &quot;green&quot;) ax2.set_ylabel(&#39;Loss&#39;) ax2.legend() . &lt;matplotlib.legend.Legend at 0x7fcaeb5b8fd0&gt; . &#50948;&#51032; &#49324;&#51652;&#49884;&#44033;&#54868; &#53076;&#46300;&#47484; &#51060;&#50857;&#54616;&#50668; &#51228;&#45824;&#47196; &#47582;&#52628;&#45716;&#51648; &#54869;&#51064;&#54644; &#48372;&#51088; . CLASSES = np.array([&#39;airplane&#39;,&#39;automobile&#39;,&#39;bird&#39;,&#39;cat&#39;,&#39;deer&#39;,&#39;dog&#39;,&#39;frog&#39;,&#39;horse&#39;,&#39;ship&#39;,&#39;truck&#39;]) preds = model_n.predict(X_test) preds_single = CLASSES[np.argmax(preds, axis=-1)] actual_single = CLASSES[np.argmax(y_test, axis=-1)] . import matplotlib.pyplot as plt n_to_show = 10 indices = np.random.choice(range(len(X_test)), n_to_show) fig = plt.figure(figsize=(15, 3)) fig.subplots_adjust(hspace=0.5, wspace=0.5) for i, idx in enumerate(indices): img = X_test[idx] ax = fig.add_subplot(1, n_to_show, i+1) ax.axis(&#39;off&#39;) ax.text(0.5, -0.35, &#39;pred = &#39; + str(preds_single[idx]), fontsize=10, ha=&#39;center&#39;, transform=ax.transAxes) ax.text(0.5, -0.7, &#39;act = &#39; + str(actual_single[idx]), fontsize=10, ha=&#39;center&#39;, transform=ax.transAxes) ax.imshow(img) . &#51096;&#47803;&#50696;&#52769;&#54620; &#44163; &#54869;&#51064; . 개를 고양이로 예측 -&gt; 사람눈으로 봐도 개인지 고양이인지 정확하게 파악이 안된다. | 말을 사슴으로 예측 | 고양이를 비행기로 예측(?) | .",
            "url": "https://raukrauk.github.io/ML-DL/boaz/deeplearning/cnn/2021/08/27/CNN_assignment_17%EA%B8%B0_%EC%86%90%ED%98%95%EB%9D%BD.html",
            "relUrl": "/boaz/deeplearning/cnn/2021/08/27/CNN_assignment_17%EA%B8%B0_%EC%86%90%ED%98%95%EB%9D%BD.html",
            "date": " • Aug 27, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://raukrauk.github.io/ML-DL/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://raukrauk.github.io/ML-DL/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://raukrauk.github.io/ML-DL/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://raukrauk.github.io/ML-DL/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}